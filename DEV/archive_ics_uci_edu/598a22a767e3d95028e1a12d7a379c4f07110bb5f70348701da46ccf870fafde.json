{"url": "http://archive.ics.uci.edu/ml/datasets/OPPORTUNITY+Activity+Recognition", "content": "\n\n\n\n<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\n<html>\n<head>\n<title>UCI Machine Learning Repository: OPPORTUNITY Activity Recognition Data Set</title>\n\n<!-- Stylesheet link -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../assets/ml.css\" />\n\n<script language=\"JavaScript\" type=\"text/javascript\">\n<!--\nfunction checkform ( form )\n{\n  // see http://www.thesitewizard.com/archive/validation.shtml\n  // for an explanation of this script and how to use it on your\n  // own website\n\n  // ** START **\n  if (form.q.value == \"\")\n  {\n    alert( \"Please enter search terms.\" );\n    form.q.focus();\n    return false ;\n  }\n\n  if (getCheckedValue(form.sitesearch) == \"ics.uci.edu\" && form.q.value.indexOf(\"site:archive.ics.uci.edu/ml\") == -1)\n  {\n    form.q.value = form.q.value + \" site:archive.ics.uci.edu/ml\";\n  }\n\n  // ** END **\n  return true ;\n}\n\n// return the value of the radio button that is checked\n// return an empty string if none are checked, or\n// there are no radio buttons\nfunction getCheckedValue(radioObj) {\n\tif(!radioObj)\n\t\treturn \"\";\n\tvar radioLength = radioObj.length;\n\tif(radioLength == undefined)\n\t\tif(radioObj.checked)\n\t\t\treturn radioObj.value;\n\t\telse\n\t\t\treturn \"\";\n\tfor(var i = 0; i < radioLength; i++) {\n\t\tif(radioObj[i].checked) {\n\t\t\treturn radioObj[i].value;\n\t\t}\n\t}\n\treturn \"\";\n}\n//-->\n</script>\n\n</head>\n\n<body>\n\n\n<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->\n\n<table width=100% bgcolor=\"#003366\">\n<tr>\n\t<td>\n\t\t<span class=\"normal\"><a href=\"../index.html\" alt=\"Home\"><img src=\"../assets/logo.gif\"\nborder=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://cml.ics.uci.edu\"><font color=\"FFDD33\">Center for Machine Learning and Intelligent Systems</font></a></span>\n\t</td>\n\t<td width=100% valign=top align=\"right\">\n\t\t<span class=\"whitetext\">\n\t\t<a href=\"../about.html\">About</a>&nbsp;\n\t\t<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;\n\t\t<a href=\"../donation_policy.html\">Donate a Data Set</a>&nbsp;\n\t\t<a href=\"../contact.html\">Contact</a>\n\t\t</span>\n\n\t\t<br>\n\t\t<br>\n\t\t<!-- Search Google -->\n\n\t\t<FORM method=GET action=http://www.google.com/custom onsubmit=\"return checkform(this);\">\n\t\t<INPUT TYPE=text name=q size=30 maxlength=255 value=\"\">\n\t\t<INPUT type=submit name=sa VALUE=\"Search\">\n\t\t<INPUT type=hidden name=cof VALUE=\"AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;\">\n\t\t<input type=hidden name=domains value=\"ics.uci.edu\">\n\t\t<br>\n\t\t<input type=radio name=sitesearch value=\"ics.uci.edu\" checked> <span class=\"whitetext\"><font size=\"1\">Repository</font></span>\n\t\t<input type=radio name=sitesearch value=\"\"> <span class=\"whitetext\"><font size=\"1\">Web</font></span>\n\t\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n\t\t<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>\n\t\t<br>\n\t\t</FORM>\n\t\t<!-- Search Google -->\n\n\n\t\t<span class=\"whitetext\"><a href=\"../datasets.php\"><font size=\"3\" color=\"#FFDD33\"><b>View\nALL Data Sets</b></font></a></span>\n\t\t<br>\n\t</td>\n</tr>\n</table>\n\n\n<br />\n<table width=100% border=0 cellpadding=2><tr><td>\n\n   <table><tr>\n     <td valign=top>\n\t<p>\n\t<span class=\"heading\"><b>OPPORTUNITY Activity Recognition Data Set</b></span>\n\t<br><span class=\"normal\"><i><font size=4 >Download</font></i>: <a href=\"../machine-learning-databases/00226/\"><font\nstyle=\"BACKGROUND-COLOR: #FFFFAA\" size=4>Data Folder</font></a>, <a href=\"#\"><font\nstyle=\"BACKGROUND-COLOR: #FFFFAA\" size=4>Data Set Description</font></a></span></p>\n\n\t<p class=\"normal\"><b>Abstract</b>: The OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors is a dataset devised to benchmark human activity recognition algorithms (classification, automatic data segmentation, sensor fusion, feature extraction, etc).</p>\n     </td>\n     <td><img\nsrc=\"../assets/MLimages/Large226.jpg\"\nhspace=20 vspace=10  /> </td>\n   </tr></table>\n\n<table border=1 cellpadding=6>\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>\n\t\t<td><p class=\"normal\">Multivariate, Time-Series</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Instances:</b></p></td>\n\t\t<td><p class=\"normal\">2551</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Area:</b></p></td>\n\t\t<td><p class=\"normal\">Computer</p></td>\n\t</tr>\n\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Attribute Characteristics:</b></p></td>\n\t\t<td><p class=\"normal\">Real</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Attributes:</b></p></td>\n\t\t<td><p class=\"normal\">242</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Date Donated</b></p></td>\n\t\t<td><p class=\"normal\">2012-06-09</p></td>\n\t</tr>\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Associated Tasks:</b></p></td>\n\t\t<td><p class=\"normal\">Classification</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Missing Values?</b></p></td>\n\t\t<td><p class=\"normal\">Yes</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Web Hits:</b></p></td>\n\t\t<td><p class=\"normal\">88492</p></td>\n\t</tr>\n\t<!--\n\t<tr>\n\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Highest Percentage Achieved:&nbsp;&nbsp;</b></p></td>\n\t\t<td><p class=\"normal\">N/A</p></td>\n\t</tr>\n\t-->\n</table>\n\n\n<br />\n\n<p class=\"small-heading\"><b>Source:</b></p>\n<p class=\"normal\">Daniel Roggen, Wearable Computing Laboratory ETH Zurich, <u>droggen <b>'@'</b> gmail.com</u>\r<br>Alberto Calatroni, Wearable Computing Laboratory ETH Zurich, <u>calatroni.alberto <b>'@'</b> gmail.com</u>\r<br>Long-Van Nguyen-Dinh, Wearable Computing Laboratory ETH Zurich\r<br>Ricardo Chavarriaga, Chair in Non-Invasive Brain-Machine Interface, EPFL, <u>ricardo.chavarriaga <b>'@'</b> epfl.ch</u>\r<br>Hesam Sagha, Chair in Non-Invasive Brain-Machine Interface, EPFL, <u>hesam.sagha <b>'@'</b> epfl.ch</u>\r<br>Sundara Tejaswi Digumarti, Chair in Non-Invasive Brain-Machine Interface, EPFL</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Data Set Information:</b></p>\n<p class=\"normal\">The OPPORTUNITY Dataset for Human Activity Recognition from Wearable, Object, and Ambient Sensors is a dataset devised to benchmark human activity recognition algorithms (classification, automatic data segmentation, sensor fusion, feature extraction, etc).\r<br>\r<br>A subset of this dataset was used for the \"OPPORTUNITY Activity Recognition Challenge\" organized for the 2011 IEEE conf on Systems, Man and Cybernetics Workshop on \"Robust machine learning techniques for human activity recognition\". \r<br>\r<br>The dataset comprises the readings of motion sensors recorded while users executed typical daily activities:\r<br>  * Body-worn sensors: 7 inertial measurement units, 12 3D acceleration sensors, 4 3D localization information\r<br>  * Object sensors: 12 objects with 3D acceleration and 2D rate of turn\r<br>  * Ambient sensors: 13 switches and 8 3D acceleration sensors\r<br>  * Recordings: 4 users, 6 runs per users. Of these, 5 are Activity of Daily Living runs characterized by a natural execution of daily activities. The 6th run is a \"drill\" run, where users execute a scripted sequence of activities. \r<br>  * Annotations/classes: the activities of the user in the scenario are annotated on different levels: \"modes of locomotion\" classes; low-level actions relating 13 actions to 23 objects; 17 mid-level gesture classes; and 5 high-level activity classes\r<br>\r<br>** Recording scenario **\r<br>\r<br>The activity recognition environment and scenario has been designed to generate many activity primitives, yet in a realistic manner. Subjects operated in a room simulating a studio flat with a deckchair, a kitchen, doors giving access to the outside, a coffee machine, a table and a chair.\r<br>\r<br>We achieved a natural execution of activities by instructing users to follow a high-level script but leaving them free interpretation as how to achieve the high-level goals. We furthermore encouraged them to perform as naturally as possible with all the variations they were used to.\r<br>\r<br>For each subject we recorded 6 different runs. Five of them, termed activity of daily living (ADL), followed a given scenario as detailed below. The remaining one, a drill run, was designed to generate a large number of activity instances. The ADL run consists of temporally unfolding situations. In each situation (e.g. preparing sandwich), a large number of action primitives occur (e.g. reach for bread, move to bread cutter, operate bread cutter).\r<br>\r<br>* ADL run *\r<br>\r<br>The ADL run consists of temporally unfolding situations:\r<br>\r<br>    Start: lying on the deckchair, get up\r<br>    Groom: move in the room, check that all the objects are in the right places in the drawers and on shelves\r<br>    Relax: go outside and have a walk around the building\r<br>    Prepare coffee: prepare a coffee with milk and sugar using the coffee machine\r<br>    Drink coffee: take coffee sips, move around in the environment\r<br>    Prepare sandwich: include bread, cheese and salami, using the bread cutter and various knifes and plates\r<br>    Eat sandwich\r<br>    Cleanup: put objects used to original place or dish washer, cleanup the table\r<br>    Break: lie on the deckchair\r<br>\r<br>* Drill run *\r<br>\r<br>The drill run consists of 20 repetitions of the following sequence of activities:\r<br>\r<br>    Open then close the fridge\r<br>    Open then close the dishwasher\r<br>    Open then close 3 drawers (at different heights)\r<br>    Open then close door 1\r<br>    Open then close door 2\r<br>    Toggle the lights on then off\r<br>    Clean the table\r<br>    Drink while standing\r<br>    Drink while seated\r<br>\r<br>** Annotations **\r<br>\r<br>The annotations are done on five \u00c2\u2018tracks\u00c2\u2019. One track contains modes of locomotion (e.g. sitting, standing, walking). Two other tracks indicate the actions of the left and right hand (e.g. reach, grasp, release), and to which object they apply (e.g. milk, switch, door).\r<br>The fourth track indicates the high level activities (e.g. prepare sandwich). The high level activities relate to the situations indicated in the description of the ADL runs as follows (in parenthesis the number of the situations indicated above): relaxing (1, 9), early morning (2, 3), coffee time (4, 5), sandwich time (6, 7), cleanup (8).\r<br>\r<br>The mid-level gesture annotations is generated automatically from the low-level hand actions. It comprises coarser characterization of the user's activities. For instance the low-level annotations 'reach door' and 'open door' are combined into a single 'open door' mid-level annotation. Here, the mid-level annotations comprise actions of the left and right hand indiscriminately. However, in practice, the users mostly interacted with the environment with their right hand. We recommend to use the mid-level annotations in first attempts to use this dataset.\r<br>\r<br>** Applications **\r<br>\r<br>This dataset offers a rich playground to assess methods such as, e.g:\r<br>\r<br>  * Classification, (semi-) supervised machine learning\r<br>  * Automatic segmentation\r<br>  * Unsupervised structure discovery\r<br>  * Data imputation\r<br>  * Multi-modal sensor fusion\r<br>  * Sensor network research\r<br>  * Transfer learning, multitask learning\r<br>  * Sensor selection\r<br>  * Feature extraction\r<br>  * Classifier calibration and adaptation\r<br>  * ...\r<br>\r<br>** Baseline benchmarks **\r<br>\r<br>Baseline benchmarks for the OPPORTUNITY Activity Recognition Challenge subset of the dataset are available in reference [2]. Scripts to replicate the benchmarks are provided in the package.</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Attribute Information:</b></p>\n<p class=\"normal\">The dataset comprises the readings of motion sensors recorded while users executed typical daily activities. The detailed format is described in the package. The attributes correspond to raw sensor readings. There is a total of 242 attributes.\r<br>\r<br>* Body-worn sensors (145 attributes) *\r<br>\r<br>The body-worn sensors include 7 inertial measurement units and 12 3D acceleration sensors.\r<br>\r<br>The inertial measurement units provide readings of: 3D acceleration, 3D rate of turn, 3D magnetic field, and orientation of the sensor with respect to a world coordinate system in quaternions. Five sensors are on the upper body and two are mounted on the user's shoes.\r<br>\r<br>The acceleration sensors provide 3D acceleration. They are mounted on the upper body, hip and leg.\r<br>\r<br>Four tags for an ultra-wideband localization system are placed on the left/right front/back side of the shoulder.  \r<br>\r<br>* Object sensors (60 attributes) *\r<br>\r<br>12 objects are instrumented with wireless sensors measuring 3D acceleration and 2D rate of turn. This allows to detect which objects are used, and possibly also the kind of usage that is made of them.\r<br>\r<br>* Ambient sensors (37 attributes) *\r<br>\r<br>Ambient sensors include 13 switches and 8 3D acceleration sensors in drawers, kitchen appliances and doors.\r<br>\r<br>The reed switches are placed in triplets on the fridge, dishwasher and drawer 2 and drawer 3. They may be used to detect three states of the furniture element: closed, half open, and fully open. \r<br>\r<br>The acceleration sensors may allow to assess if an element of furniture is used, and whether it may be opened or closed.</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Relevant Papers:</b></p>\n<p class=\"normal\">**First party**\r<br>[1] Daniel Roggen, Alberto Calatroni, Mirco Rossi, Thomas Holleczek, Gerhard Tr\u00f6ster, Paul Lukowicz, Gerald Pirkl, David Bannach, Alois Ferscha, Jakob Doppler, Clemens Holzmann, Marc Kurz, Gerald Holl, Ricardo Chavarriaga, Hesam Sagha, Hamidreza Bayati, and Jos\u00e9 del R. Mill\u00e1n. \"Collecting complex activity data sets in highly rich networked sensor environments\" In Seventh International Conference on Networked Sensing Systems (INSS\u201910), Kassel, Germany, 6 2010.\r<br>[2] Hesam Sagha, Sundara Tejaswi Digumarti, Jos\u00e9 del R. Mill\u00e1n, Ricardo Chavarriaga, Alberto Calatroni, Daniel Roggen, Gerhard Tr\u00f6ster. Benchmarking classification techniques using the Opportunity human activity dataset. IEEE International Conference on Systems, Man, and Cybernetics, Anchorage, AK, USA, October 9-12, 2011\r<br>[3] Video presenting the dataset: <a href=\"http://vimeo.com/8704668\">[Web Link]</a>\r<br>[4] R. Chavarriaga et al. Ensemble creation and reconfiguration for activity recognition: An information theoretic approach. IEEE Conf Systems, Man, and Cybernetics (SMC), 2011\r<br>[5] H. Sagha et al. Detecting anomalies to improve classification performance in an opportunistic sensor network, 7th IEEE International Workshop on Sensor Networks and Systems for Pervasive Computing (PerSens), 2011.\r<br>[6] A. Calatroni et al., Automatic transfer of activity recognition capabilities between body-worn motion sensors: Training newcomers to recognize locomotion, 8th International Conference on Networked Sensing Systems (INSS), 2011\r<br>[7] M. Kurz et al. Dynamic Quantification of Activity Recognition Capabilities in Opportunistic Systems. Fourth Conference on Context Awareness for Proactive Systems, 2011\r<br>[8] H. Sagha et al. Detecting and rectifying anomalies in Opportunistic sensor networks. International Conference on Body Sensor Networks (BSN), 2011\r<br>[9] R. Chavarriaga et al. Robust activity recognition for assistive technologies: Benchmarking ML techniques, Workshop on Machine Learning for Assistive Technologies at the 24th Annual Conference on Neural Information Processing Systems (NIPS), 2010.\r<br>[10] P. Lukowicz et al. Recording a complex, multi modal activity data set for context recognition 1st Workshop on Context-Systems Design, Evaluation and Optimisation at ARCS, 2010, 2010\r<br>[11] R. Chavarriaga, H. Sagha, A. Calatroni, S. Digumarti, G. Tr\u00f6ster, J. del R. Mill\u00e1n, D. Roggen. The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition, Pattern Recognition Letters, 2013\r<br>[12] L.-V. Nguyen-Dinh, D. Roggen, A. Calatroni, G. Tr\u00f6ster. Improving online gesture recognition with template matching methods in accelerometer data, Proc 12th Int Conf on Intelligent Systems Design and Applications, 2012\r<br>\r<br>**Third party**\r<br>Here are a few of the papers from third parties using the OPPORTUNITY dataset:\r<br>\r<br>[100] T. Pl\u00f6tz, N. Y.  Hammerla, P. Olivier. Feature Learning for Activity Recognition in Ubiquitous Computing, IJCAI, 2011\r<br>[101] A. Manzoor et al., Identifying Important Action Primitives for High Level Activity Recognition, Proc. European Conference on Smart Sensing and Context (EuroSSC), 2010\r<br>[102] T. Ploetz, N. Hammerla, A. Rozga, A. Reavis, N. Call, G. Abowd. Automatic Assessment of Problem Behavior in Individuals with Developmental Disabilities. Proc. 14th Int Conf on Ubiquitous Computing, 2012.\r<br>[103] D. Gordon, J. Czerny, M. Beigl. Activity Recognition for Creatures of Habit: Energy-Efficient Embedded Classification using Prediction. Personal and Ubiquitous Computing, 2013.</p>\n\n<br />\n\n\n<!-- OLD CODE:\n\n<p class=\"small-heading\"><b>Papers That Cite This Data Set<sup>1</sup>:</b></p>\n<img src=\"../assets/rexa.jpg\" />\n<p class=\"normal\">N/A</p>\n\n-->\n\n\n\n<br />\n\n<p class=\"small-heading\"><b>Citation Request:</b></p>\n<p class=\"normal\">Use of this dataset in publications must be acknowledged by referencing the following publication [1] or [2]. \r<br>We recommend to refer to this dataset as the \"OPPORTUNITY Activity Recognition Dataset\" in publications.\r<br>We also appreciate if you drop us an email (<u>daniel.roggen <b>'@'</b> ieee.org</u>) to inform us of any publication using this dataset, so we can point to your publication on our webpage.\r<br>\r<br>Reference [1] details the overall dataset, the scenario, the multimodality and sensor networking aspects of the setup, quality metrics, and best practices for the recording of complex multimodal activity datasets. Reference [2] provides the performance of a baseline activity recognition system on the OPPORTUNITY dataset, which can be used as a benchmark performance.\r<br>\r<br>[1] Daniel Roggen, Alberto Calatroni, Mirco Rossi, Thomas Holleczek, Gerhard Tr\u00f6ster, Paul Lukowicz, Gerald Pirkl, David Bannach, Alois Ferscha, Jakob Doppler, Clemens Holzmann, Marc Kurz, Gerald Holl, Ricardo Chavarriaga, Hesam Sagha, Hamidreza Bayati, and Jos\u00e9 del R. Mill\u00e0n. \"Collecting complex activity data sets in highly rich networked sensor environments\" In Seventh International Conference on Networked Sensing Systems (INSS\u201910), Kassel, Germany, 2010.\r<br>\r<br>[2] Ricardo Chavarriaga, Hesam Sagha, Alberto Calatroni, Sundaratejaswi Digumarti, Gerhard Tr\u00f6ster, Jos\u00e9 del R. Mill\u00e1n, Daniel Roggen. \"The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition\", Pattern Recognition Letters, 2013</p>\n\n</td></tr></table>\n\n\n<hr>\n\n<!-- OLD CODE:\n<p class=\"normal\"><font size=1>[1] Papers were automatically harvested and associated with this data set, in collaboration with <a href=\"http://rexa.info\"><font size=1>Rexa.info</font></a></font></p>\n-->\n\n\n\n<table cellpadding=5 align=center><tr valign=center>\n\t\t<td><p class=\"normal\">Supported By:</p></td>\n        <td><img src=\"../assets/nsfe.gif\" height=60 /> </td>\n        <td><p class=\"normal\">&nbsp;In Collaboration With:</p></td>\n        <td><img src=\"../assets/rexaSmall.jpg\" /></td>\n</tr></table>\n\n<center>\n<span class=\"normal\">\n<a href=\"../about.html\">About</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../donation_policy.html\">Donation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../contact.html\">Contact</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"http://cml.ics.uci.edu\">CML</a>\n</span>\n</center>\n\n\n\n\n</body>\n</html>\n", "encoding": "utf-8"}