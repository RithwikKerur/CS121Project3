{"url": "http://archive.ics.uci.edu/ml/support/Iris#632fe4095475bee152843a02969ade56a290db39", "content": "\n\n\n\n<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\n<html>\n<head>\n<title>UCI Machine Learning Repository: Iris Data Set: Support</title>\n\n<!-- Stylesheet link -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../assets/ml.css\" />\n\n<script language=\"JavaScript\" type=\"text/javascript\">\n<!--\nfunction checkform ( form )\n{\n  // see http://www.thesitewizard.com/archive/validation.shtml\n  // for an explanation of this script and how to use it on your\n  // own website\n\n  // ** START **\n  if (form.q.value == \"\")\n  {\n    alert( \"Please enter search terms.\" );\n    form.q.focus();\n    return false ;\n  }\n\n  if (getCheckedValue(form.sitesearch) == \"ics.uci.edu\" && form.q.value.indexOf(\"site:archive.ics.uci.edu/ml\") == -1)\n  {\n    form.q.value = form.q.value + \" site:archive.ics.uci.edu/ml\";\n  }\n\n  // ** END **\n  return true ;\n}\n\n// return the value of the radio button that is checked\n// return an empty string if none are checked, or\n// there are no radio buttons\nfunction getCheckedValue(radioObj) {\n\tif(!radioObj)\n\t\treturn \"\";\n\tvar radioLength = radioObj.length;\n\tif(radioLength == undefined)\n\t\tif(radioObj.checked)\n\t\t\treturn radioObj.value;\n\t\telse\n\t\t\treturn \"\";\n\tfor(var i = 0; i < radioLength; i++) {\n\t\tif(radioObj[i].checked) {\n\t\t\treturn radioObj[i].value;\n\t\t}\n\t}\n\treturn \"\";\n}\n//-->\n</script>\n\n</head>\n\n<body>\n\n\n<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->\n\n<table width=100% bgcolor=\"#003366\">\n<tr>\n\t<td>\n\t\t<span class=\"normal\"><a href=\"../index.html\" \nalt=\"Home\"><img src=\"../assets/logo.gif\" \nborder=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://cml.ics.uci.edu\"><font color=\"FFDD33\">Center for Machine Learning and Intelligent Systems</font></a></span>\n\t</td>\n\t<td width=100% valign=top align=\"right\">\n\t\t<span class=\"whitetext\">\n\t\t<a href=\"../about.html\">About</a>&nbsp;\n\t\t<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;\n\t\t<a href=\"../donation_policy.html\">Donate a Data Set</a>&nbsp;\n\t\t<a href=\"../contact.html\">Contact</a>\n\t\t</span>\n\n\t\t<br>\n\t\t<br>\n\t\t<!-- Search Google -->\n\n\t\t<FORM method=GET action=http://www.google.com/custom onsubmit=\"return checkform(this);\">\n\t\t<INPUT TYPE=text name=q size=30 maxlength=255 value=\"\">\n\t\t<INPUT type=submit name=sa VALUE=\"Search\">\n\t\t<INPUT type=hidden name=cof VALUE=\"AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;\">\n\t\t<input type=hidden name=domains value=\"ics.uci.edu\">\n\t\t<br>\n\t\t<input type=radio name=sitesearch value=\"ics.uci.edu\" checked> <span class=\"whitetext\"><font size=\"1\">Repository</font></span>\n\t\t<input type=radio name=sitesearch value=\"\"> <span class=\"whitetext\"><font size=\"1\">Web</font></span>\n\t\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n\t\t<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>\n\t\t<br>\n\t\t</FORM>\n\t\t<!-- Search Google -->\n\n\n\t\t<span class=\"whitetext\"><a href=\"../datasets.php\"><font size=\"3\" color=\"#FFDD33\"><b>View ALL Data Sets</b></font></a></span>\n\t\t<br>\n\t</td>\n</tr>\n</table>\n\n<br />\n<table width=100% border=0 cellpadding=2><tr><td>\n\n\n   <table><tr>\n     <td valign=top>\n\t<p>\n\t<span class=\"heading\"><b>Iris Data Set</b></span>\n\n\t\t\n\t\t<img src=\"../assets/MLimages/Large53.jpg\" hspace=20 vspace=10 align=right />\t\t<p class=\"normal\">Below are papers that cite this data set, with context shown.\n\t\tPapers were automatically harvested and associated with this data set, in collaboration with <a href=\"http://rexa.info\">Rexa.info</a>.</p>\n\t\t<img src=\"../assets/rexa.jpg\" />\n\t\t<p class=\"normal\"><a href=\"/ml/datasets/Iris\">Return to Iris data set page</a>.\n\t\t<hr><p class=\"normal\"><a name=\"03e3fbdc694fa4f16c10fbc88658f4e6e2e462e8\"></a><i>Sotiris B. Kotsiantis and Panayiotis E. Pintelas. <a href=\"http://rexa.info/paper/03e3fbdc694fa4f16c10fbc88658f4e6e2e462e8\">Logitboost of Simple Bayesian Classifier</a>. Informatica. 2005. </i><br><br>were hand selected so as to come from real-world problems and to vary in characteristics. Thus, we have used data sets from the domains of: pattern recognition  <b>iris</b>  zoo), image recognition (ionosphere, sonar), medical diagnosis (breast-cancer, breast-w, colic, diabetes, heart-c, heart-h, heart-statlog, hepatitis,<br></p><hr><p class=\"normal\"><a name=\"5b07457d7b968260c0c1a712f9120a243bcfbf8c\"></a><i>Manuel Oliveira. <a href=\"http://rexa.info/paper/5b07457d7b968260c0c1a712f9120a243bcfbf8c\">Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted</a>. University of Alberta Library. 2005. </i><br><br>(d o =18,d r = 12). 119 7.13 Average of F-measure (10 trials) for the <b>Iris</b> dataset (d o =5,d r =3)......120 7.14 An example of partitioning for the Pumsb dataset. . . . . . . . . . . . . . . . 120 7.15 Average of F-measure (10 trials) for the Pumsb dataset over vertically<br></p><hr><p class=\"normal\"><a name=\"81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6\"></a><i>Ping Zhong and Masao Fukushima. <a href=\"http://rexa.info/paper/81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6\">A Regularized Nonsmooth Newton Method for Multi-class Support Vector Machines</a>. 2005. </i><br><br>the starting point of the next (k + 1)th iteration. The parameters \u00ba 1 and \u00ba 2 in (3) are both set 0.01. In Algorithm 3.1, we replaced the standard Armijo-rule in (S.3) by 10 Table 1: Six benchmark datasets from UCI name <b>iris</b> wine glass vowel vehicle segment #pts 150 178 214 528 846 2310 {fiats|flats} 4 13 9 10 18 19 #cls 3 3 6 11 4 7 #pts: the number of training data; {fiats|flats}: the number of<br></p><hr><p class=\"normal\"><a name=\"3c1b37ca3a2f0825890509a5ff17081cf012fffd\"></a><i>Anthony K H Tung and Xin Xu and Beng Chin Ooi. <a href=\"http://rexa.info/paper/3c1b37ca3a2f0825890509a5ff17081cf012fffd\">CURLER: Finding and Visualizing Nonlinear Correlated Clusters</a>. SIGMOD Conference. 2005. </i><br><br>of three helix clusters with different cluster existence spaces, the <b>iris</b> plant dataset and the image segmentation dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters<br></p><hr><p class=\"normal\"><a name=\"bcdb5e21043b224589c8ba0c670a49d10f5f5580\"></a><i>Igor Fischer and Jan Poland. <a href=\"http://rexa.info/paper/bcdb5e21043b224589c8ba0c670a49d10f5f5580\">Amplifying the Block Matrix Structure for Spectral Clustering</a>. Telecommunications Lab. 2005. </i><br><br>are common benchmark sets with real-world data (Murphy & Aha, 1994): the <b>iris</b>  the wine and the breast cancer data set. Both our methods perform very well on iris and breast cancer. However, the wine data set is too sparse for context-dependent method: only 178 points in 13 dimensions, giving the conductivity too<br></p><hr><p class=\"normal\"><a name=\"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3\"></a><i>Remco R. Bouckaert and Eibe Frank. <a href=\"http://rexa.info/paper/56eb4e64499baa9ef620d2aeb5383e1739a2e6e3\">Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms</a>. PAKDD. 2004. </i><br><br>perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the <b>iris</b> and Hungarian heart disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all<br></p><hr><p class=\"normal\"><a name=\"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a\"></a><i>Mikhail Bilenko and Sugato Basu and Raymond J. Mooney. <a href=\"http://rexa.info/paper/a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a\">Integrating constraints and metric learning in semi-supervised clustering</a>. ICML. 2004. </i><br><br>Experiments were conducted on three datasets from the UCI repository: <b>Iris</b>  Wine, and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits<br></p><hr><p class=\"normal\"><a name=\"8e674b6eff0f726ba6fff46ef6eaff968dc89f39\"></a><i>Qingping Tao Ph. D. <a href=\"http://rexa.info/paper/8e674b6eff0f726ba6fff46ef6eaff968dc89f39\">MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES</a>. Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements. 2004. </i><br><br>(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets <b>iris</b> car breast cancer voting auto annealing n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7<br></p><hr><p class=\"normal\"><a name=\"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a\"></a><i>Yuan Jiang and Zhi-Hua Zhou. <a href=\"http://rexa.info/paper/5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a\">Editing Training Data for kNN Classifiers with Neural Network Ensemble</a>. ISNN (1). 2004. </i><br><br>i.e. glass, hayes-roth and wine. It is surprising that Depuration obtains the best performance on only one data set, i.e. <b>iris</b>  as RelabelOnly does. These observations indicate that NNEE is a better editing approach than Depuration. Moreover, since the e\u00aeect of Depuration is only comparable to that of<br></p><hr><p class=\"normal\"><a name=\"104a89897dc4a05cb068858194d1d78ad98d5012\"></a><i>Sugato Basu. <a href=\"http://rexa.info/paper/104a89897dc4a05cb068858194d1d78ad98d5012\">Semi-Supervised Clustering with Limited Background Knowledge</a>. AAAI. 2004. </i><br><br>like stop-word removal, tf-idf weighting, and removal of very high-frequency and very low-frequency words (Dhillon & Modha, 2001). From the UCI collection we selected <b>Iris</b>  which is a well-known dataset having 150 points in 4 dimensions. We used the active pairwise constrained version of KMeans on Iris, and SPKMeans on Classic3-subset. Learning curves with cross validation For all algorithms on<br></p><hr><p class=\"normal\"><a name=\"5736888202fbd07e1945b8a64a3e0f3ddeb2840f\"></a><i>Judith E. Devaney and Steven G. Satterfield and John G. Hagedorn and John T. Kelso and Adele P. Peskin and William George and Terence J. Griffin and Howard K. Hung and Ronald D. Kriz. <a href=\"http://rexa.info/paper/5736888202fbd07e1945b8a64a3e0f3ddeb2840f\">Science at the Speed of Thought</a>. Ambient Intelligence for Scientific Discovery. 2004. </i><br><br>EXAMPLES Figure 1 shows part of our visualization of the <b>Iris</b> data set [2]. (The full visualization contains multiple rooms with an alternate visualization of the same data set in each room, enabling a scientist to visit each of the rooms.) On the near side of the left<br></p><hr><p class=\"normal\"><a name=\"5b3417aa2824988405f9ac934b692af30729b447\"></a><i>Jennifer G. Dy and Carla Brodley. <a href=\"http://rexa.info/paper/5b3417aa2824988405f9ac934b692af30729b447\">Feature Selection for Unsupervised Learning</a>. Journal of Machine Learning Research, 5. 2004. </i><br><br>EM-k-STD (e) Figure 9: Feature selection versus without feature selection on the four-class data. 6.5 Experiments on Real Data We examine the FSSEM variants on the <b>iris</b>  wine, and ionosphere data set from the UCI learning repository (Blake and Merz, 1998), and on a high resolution computed tomography (HRCT) lung 867 DY AND BRODLEY image data which we collected from IUPUI medical center (Dy et<br></p><hr><p class=\"normal\"><a name=\"d64d2705cabed449e8cb2ecc3c3c77c54ee71051\"></a><i>Jeroen Eggermont and Joost N. Kok and Walter A. Kosters. <a href=\"http://rexa.info/paper/d64d2705cabed449e8cb2ecc3c3c77c54ee71051\">Genetic Programming for data classification: partitioning the search space</a>. SAC. 2004. </i><br><br>is disappointing as only our clustering gp algorithm with 3 clusters per numerical valued attribute manages to really outperform our simple gp but still performs much worse than C4.5. The <b>Iris</b> Data Set If we look at the results of our gp algorithms on the Iris data set in Table 8 we see that by far the best performance is achieved by our clustering gp algorithm with 3 clusters per numerical valued<br></p><hr><p class=\"normal\"><a name=\"0e637175e298e5a11f5e7a69a442a65d7245f297\"></a><i>Sugato Basu. <a href=\"http://rexa.info/paper/0e637175e298e5a11f5e7a69a442a65d7245f297\">Also Appears as Technical Report, UT-AI</a>. PhD Proposal. 2003. </i><br><br>like stop-word removal, tf-idf weighting, and removal of very high-frequency and very low-frequency words (Dhillon & Modha, 2001). From the UCI collection we selected <b>Iris</b>  which is a well-known dataset having 150 points in 4 dimensions. We used the active pairwise constrained version of KMeans on Iris, and SPKMeans on Classic3-subset. Learning curves with cross validation For all algorithms on<br></p><hr><p class=\"normal\"><a name=\"545d588c45ddf6031f6472c1573778506fbae9cf\"></a><i>Dick de Ridder and Olga Kouropteva and Oleg Okun and Matti Pietik\u00e4inen and Robert P W Duin. <a href=\"http://rexa.info/paper/545d588c45ddf6031f6472c1573778506fbae9cf\">Supervised Locally Linear Embedding</a>. ICANN. 2003. </i><br><br>retained in the remaining M dimensions [3]. This local intrinsic dimensionality estimate is denoted by ML . The feature extraction process is illustrated in Figure 1: the C = 3 classes in the <b>iris</b> data set [1] are mapped onto single points by 1-SLLE. #-SLLE retains some of the class structure, but reduces within-class dispersion compared to LLE. Clearly, SLLE is suitable as a feature extraction step<br></p><hr><p class=\"normal\"><a name=\"d293cd6bea79dd218c3a9e76177d026bf1d581f5\"></a><i>Aristidis Likas and Nikos A. Vlassis and Jakob J. Verbeek. <a href=\"http://rexa.info/paper/d293cd6bea79dd218c3a9e76177d026bf1d581f5\">The global k-means clustering algorithm</a>. Pattern Recognition, 36. 2003. </i><br><br>it is also possible to employ the above presented k-d tree approach with the global k-means algorithm. 4 Experimental results We have tested the proposed clustering algorithms on several well-known data sets, namely the <b>iris</b> data set [8], the synthetic data set [9] and the image segmentation data set [8]. In all data sets we conducted experiments for the clustering problems obtained by considering only<br></p><hr><p class=\"normal\"><a name=\"faf03ce4427609aa6db25e3d0e6479fb2ae153a7\"></a><i>Zhi-Hua Zhou and Yuan Jiang and Shifu Chen. <a href=\"http://rexa.info/paper/faf03ce4427609aa6db25e3d0e6479fb2ae153a7\">Extracting symbolic rules from trained neural network ensembles</a>. AI Commun, 16. 2003. </i><br><br>80 2 19 13 6 <b>iris</b> plant iris 150 3 4 0 4 statlog australian credit approval credit-a 690 2 15 9 6 statlog german credit credit-g 1,000 2 20 13 7 Table 2 Fidelity of rules extracted via REFNE data set balance voting hepatitis iris credit-a credit-g average fidelity 87.88% 89.26% 84.50% 96.25% 84.13% 74.10% 86.02% Table 3 Comparison of generalization error data set REFNE ensemble single NN C4.5<br></p><hr><p class=\"normal\"><a name=\"fcba21d24e397e9639efbad04b5be06fbe9c0df2\"></a><i>Jeremy Kubica and Andrew Moore. <a href=\"http://rexa.info/paper/fcba21d24e397e9639efbad04b5be06fbe9c0df2\">Probabilistic Noise Identification and Data Cleaning</a>. ICDM. 2003. </i><br><br>We also compared the algorithms by their ability to identify artificial corruptions. Three different test sets were used: a noise free version of the rock data described above, the UCI <b>Iris</b> data set, and the UCI Wine data set [3]. Noise was generated by choosing to corrupt each record with some probability p. For each record chosen, corruption and noise vectors were sampled from their<br></p><hr><p class=\"normal\"><a name=\"09b1c64b200c3b3acff18a3e45a2d75ba0aef2b7\"></a><i>Julie Greensmith. <a href=\"http://rexa.info/paper/09b1c64b200c3b3acff18a3e45a2d75ba0aef2b7\">New Frontiers For An Artificial Immune System</a>. Digital Media Systems Laboratory HP Laboratories Bristol. 2003. </i><br><br>using the g++ compiler version 2.96 for Red Hat Linux 7.3 2.96-113, and was run on one out of 4 of Intel Pentium fiff 4 CPU 1.80GHz HP `e-PC's'. On completion of the compilation process, the <b>iris</b> dataset (provided with the source code) was used to perform preliminary testing on the system. Once it was clear on how to use the various parameter settings, and that classification could be performed,<br></p><hr><p class=\"normal\"><a name=\"865daf468f10c99d290f47cc77c2a1c463e001ce\"></a><i>Manoranjan Dash and Huan Liu and Peter Scheuermann and Kian-Lee Tan. <a href=\"http://rexa.info/paper/865daf468f10c99d290f47cc77c2a1c463e001ce\">Fast hierarchical clustering and its validation</a>. Data Knowl. Eng, 44. 2003. </i><br><br>consists of 10,992 objects in 16 dimensions. There are 10 classes corresponding to digits 0...9. The 16 dimensions are drawn by re-sampling from handwritten digits. <b>Iris</b> dataset has 150 points in 4 dimensions in 3 clusters. Dimensions are sepal length, sepal width, petal length, and petal width. Clusters are Iris Setosa, Iris Versicolour, and Iris Virginia. Each of the 3<br></p><hr><p class=\"normal\"><a name=\"20b8d5b9285a402f14753a766e96889e990a99f3\"></a><i>Bob Ricks and Dan Ventura. <a href=\"http://rexa.info/paper/20b8d5b9285a402f14753a766e96889e990a99f3\">Training a Quantum Neural Network</a>. NIPS. 2003. </i><br><br>an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, Hayes-Roth and the <b>iris</b> datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different<br></p><hr><p class=\"normal\"><a name=\"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a\"></a><i>Eibe Frank and Mark Hall. <a href=\"http://rexa.info/paper/bf8052c51e6338fa2ab5479c08d657a06e4dbd4a\">Visualizing Class Probability Estimators</a>. PKDD. 2003. </i><br><br><= 1.7 <b>iris</b> virginica (46.0/1.0) } 1.7 Iris-versicolor (48.0/1.0) <= 4.9 petalwidth } 4.9 Iris-virginica (3.0) <= 1.5 Iris-versicolor (3.0/1.0) } 1.5 Fig. 5. The decision tree for the two-class iris dataset. (a) (b) (c) Fig. 6. Visualizing the decision tree for the two-class iris data using (a) petallength and petalwidth, (b) petallength and sepallength, and (c) sepallength and sepalwidth (with the<br></p><hr><p class=\"normal\"><a name=\"395f41d678ee7dfc42742e514a39927f80c4c538\"></a><i>Ross J. Micheals and Patrick Grother and P. Jonathon Phillips. <a href=\"http://rexa.info/paper/395f41d678ee7dfc42742e514a39927f80c4c538\">The NIST HumanID Evaluation Framework</a>. AVBPA. 2003. </i><br><br>Jonathon's signature therefore contains five sigmembers: one for the <b>iris</b> scan, three for each facial image, and one for the gait video. For the first sigmember, the iris scan, there is a single dataset with a single file that contains the iris data. Three sigmembers, for the facial imagery, each have a single dataset, each with a single file that each contain a facial image. The fifth sigmember,<br></p><hr><p class=\"normal\"><a name=\"a8a26052ead9d54b217e18b4e159137da340946f\"></a><i>Jun Wang and Bin Yu and Les Gasser. <a href=\"http://rexa.info/paper/a8a26052ead9d54b217e18b4e159137da340946f\">Concept Tree Based Clustering Visualization with Shaded Similarity Matrices</a>. ICDM. 2002. </i><br><br>we will briefly show how shaded similarity matrices are constructed and how one looks through an example. The data used in the example is part of the <b>Iris</b> data from the UCI repository[9]. The Iris data set contains 150 instances, evenly distributed in 3 classes. We fetch 5 instances from each class, and thus obtain 15 instances (Table 1). The similarity matrix was computed based on Euclidean distance<br></p><hr><p class=\"normal\"><a name=\"df4390b4164d70e186a080009bf9b23fb367478c\"></a><i>Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas. <a href=\"http://rexa.info/paper/df4390b4164d70e186a080009bf9b23fb367478c\">Non-linear dimensionality reduction techniques for classification and visualization</a>. KDD. 2002. </i><br><br>used in our experiments Dataset ] data ] dims ] classes experiment <b>Iris</b> 100 4 2 leave 1 out c-v Sonar 208 60 2 leave 1 out c-v Glass 214 9 6 leave 1 out c-v Liver 345 6 2 leave 1 out c-v Lung 32 56 3 leave 1 out c-v Image 640 16<br></p><hr><p class=\"normal\"><a name=\"b8a6771d62cfaaffe0071e498102432d8d800573\"></a><i>Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall. <a href=\"http://rexa.info/paper/b8a6771d62cfaaffe0071e498102432d8d800573\">Multiclass Alternating Decision Trees</a>. ECML. 2002. </i><br><br>90.49 89.72 labor 84.67 87.5 + promoters 86.8 87.3 sick-euthyroid 97.71 97.85 + sonar 76.65 74.12 vote 96.5 96.18 +, statistically significant difference Table 3. Wrapping two-class ADTree results dataset 1vs1 1vsRest Random Exhaustive <b>iris</b> 95.13 95.33 95.33 95.33 balance-scale 83.94 85.06 + 85.06 + 85.06 + hypothyroid 99.61 99.63 99.64 99.64 anneal 99.01 98.96 99.05 99.19 + zoo 90.38 93.45 + 95.05 +<br></p><hr><p class=\"normal\"><a name=\"9acaa3dfebccac9d79abb08f7eced65482be1631\"></a><i>Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler. <a href=\"http://rexa.info/paper/9acaa3dfebccac9d79abb08f7eced65482be1631\">Class visualization of high-dimensional data with applications</a>. Department of Computer Sciences, University of Texas. 2002. </i><br><br>sketch the outline of the paper. Section 2 introduces class-preserving projections and class-eigenvector plots, and contains several illustrations of the <b>Iris</b> plant and ISOLET speech recognition data sets [27]. Class-similarity graphs and class tours are discussed in Sections 3 and 4. We illustrate the value of the above visualization tools in Section 5, where we present a detailed study of the<br></p><hr><p class=\"normal\"><a name=\"5047fbe99b73ea1e127150b6688d65effd51f4c1\"></a><i>Manoranjan Dash and Kiseok Choi and Peter Scheuermann and Huan Liu. <a href=\"http://rexa.info/paper/5047fbe99b73ea1e127150b6688d65effd51f4c1\">Feature Selection for Clustering - A Filter Solution</a>. ICDM. 2002. </i><br><br>are almost correct as well as the selected features are all important and it missed out only one important feature. 5.2 Benchmark and Real Datasets <b>Iris</b> dataset, popularly used for testing clustering and classification algorithms, is taken from UCI ML repository [5]. It contains 3 classes of 50 instances each, where each class refers to a type<br></p><hr><p class=\"normal\"><a name=\"7a13c34f809076592c23c1e2c9c6a9a72464d911\"></a><i>Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts. <a href=\"http://rexa.info/paper/7a13c34f809076592c23c1e2c9c6a9a72464d911\">A Genetic Algorithm Approach for Semi-Supervised Clustering</a>. E-Business Department, Verizon Inc.. 2002. </i><br><br>506 points), House Votes (16 variables, 435 points), Breast Cancer Diagnostic (30 variables, 569 points), Pima Diabetes ( 8 variables, 769 points), and <b>Iris</b> ( 4 variables, 150 points). The datasets have categorical dependent variables except Housing. The continuous dependent variable for this dataset was categorized at the level of 21.5. Iris is a three class problem. The other datasets are<br></p><hr><p class=\"normal\"><a name=\"ccde1df96c7ca5add43e1578b912d95bb86da659\"></a><i>Wai Lam and Kin Keung and Charles X. Ling. <a href=\"http://rexa.info/paper/ccde1df96c7ca5add43e1578b912d95bb86da659\">PR 1527</a>. Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong. 2001. </i><br><br>from di3erent real-world application in various domains, such as the city-cycle fuel consumption (Am), Wisconsin breast cancer (Bc) and the 43 famous <b>iris</b> plant database (Ir). Table 1 shows the data sets and their corresponding code used in this paper. 45 For each data set, we randomly partitioned the data into ten even portions. Ten trials derived from 10-fold 47 cross-validation were conducted<br></p><hr><p class=\"normal\"><a name=\"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79\"></a><i>Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong. <a href=\"http://rexa.info/paper/ca1c2b19089d0ca0d4069b04c35d6f6c4312db79\">DeEPs: A New Instance-based Discovery and Classification System</a>. Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases. 2001. </i><br><br>we highlight some interesting points. 1. DeEPs versus k-NN. ffl Both DeEPs and k-NN perform equally accurately on soybean-small (100%) and on <b>iris</b> (96%). ffl DeEPs wins on 26 data sets; k-NN wins on 11. It can be seen that the accuracy of DeEPs is generally better than that of k-NN. ffl The speed of DeEPs is about 1.5 times slower than that of k-NN. The main reason is that DeEPs<br></p><hr><p class=\"normal\"><a name=\"220290933c845bcae1348931e08740d3e16b5360\"></a><i>David Hershberger and Hillol Kargupta. <a href=\"http://rexa.info/paper/220290933c845bcae1348931e08740d3e16b5360\">Distributed Multivariate Regression Using Wavelet-Based Collective Data Mining</a>. J. Parallel Distrib. Comput, 61. 2001. </i><br><br>Application of this method to Linear Discriminant Analysis, which is related to parametric multivariate regression, produced classification results on the <b>Iris</b> data set that are comparable to those obtained with centralized data analysis. Key Words: data mining, distributed data mining, collective data mining, knowledge discovery, wavelets, regression 1.<br></p><hr><p class=\"normal\"><a name=\"b7408989feafd783e1ebdcdac949100e6d133b9e\"></a><i>David Horn and A. Gottlieb. <a href=\"http://rexa.info/paper/b7408989feafd783e1ebdcdac949100e6d133b9e\">The Method of Quantum Clustering</a>. NIPS. 2001. </i><br><br>minima appear, as seen in Fig. 3. Nonetheless, they lie high and contain only a few data points. The major minima are the same as in Fig. 2. 3.2 <b>iris</b> Data Our second example consists of the iris data set [10], which is a standard benchmark obtainable from the UCI repository [11]. Here we use the first two principal components to define the two dimensions in which we apply our method. Fig. 4, which<br></p><hr><p class=\"normal\"><a name=\"9a9186e8fcb4a3d9a2f01bb4033499e4a59833e9\"></a><i>Neil Davey and Rod Adams and Mary J. George. <a href=\"http://rexa.info/paper/9a9186e8fcb4a3d9a2f01bb4033499e4a59833e9\">The Architecture and Performance of a Stochastic Competitive Evolutionary Neural Tree Network</a>. Appl. Intell, 12. 2000. </i><br><br>5 and 6 are illustrated in Figures 2 and 3. The <b>IRIS</b> data set is included to provide a benchmark performance. Set 1 2-D single source Gaussian cluster, zero mean and unit variance. Simple cluster, base line test. Set 2 20-D single source Gaussian cluster, zero<br></p><hr><p class=\"normal\"><a name=\"32cb39d6ec7d1af5c8f8ad0fff8300527cf9f188\"></a><i>Edgar Acuna and Alex Rojas. <a href=\"http://rexa.info/paper/32cb39d6ec7d1af5c8f8ad0fff8300527cf9f188\">Ensembles of classifiers based on Kernel density estimators</a>. Department of Mathematics University of Puerto Rico. 2000. </i><br><br>has been developed to carry out all our tasks. The results are shown in the table 7. Table 6. Comparison of Bagging using classical and adaptive kernel classifiers Classical Kernel Adaptive Kernel Dataset Single Bagged Improv Single Bagged Improv <b>Iris</b> 4.00 3.33 16.75 4.67 4.00 14.34 Glass 44.97 40.52 9.90 35.20 33.25 5.54 Heart-C 22.09 20.05 9.23 23.60 19.80 16.10 Breast-W 4.34 4.10 5.53 4.88 4.53<br></p><hr><p class=\"normal\"><a name=\"cfcdd7693a196acd2e60944a030abaf8d477e337\"></a><i>Manoranjan Dash and Huan Liu. <a href=\"http://rexa.info/paper/cfcdd7693a196acd2e60944a030abaf8d477e337\">Feature Selection for Clustering</a>. PAKDD. 2000. </i><br><br>in Figure 3. The X-axis of the plots is for number of most important features and Y -axis is for tr(P Gamma 1 W PB ) value for the corresponding subset of most important features. For <b>Iris</b> data set trace value was the maximum for the two most important features. For D3C, D4C and D6C data trace value increases with addition of important features in a fast rate but slows down to almost a halt<br></p><hr><p class=\"normal\"><a name=\"d18b9cca12173a8ca3d5a781cadcf847442930f4\"></a><i>Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos. <a href=\"http://rexa.info/paper/d18b9cca12173a8ca3d5a781cadcf847442930f4\">An Adaptive Metric Machine for Pattern Classification</a>. NIPS. 2000. </i><br><br>used were taken from the UCI Machine Learning Database Repository [10], except for the unreleased image data set. They are: 1. <b>Iris</b> data. This data set consists of q = 4 measurements made on each of N = 100 iris plants of J = 2 species; 2. Sonar data. This data set consists of q = 60 frequency measurements<br></p><hr><p class=\"normal\"><a name=\"e7f14adabe196dbd08024790f0df92a43728d643\"></a><i>Asa Ben-Hur and David Horn and Hava T. Siegelmann and Vladimir Vapnik. <a href=\"http://rexa.info/paper/e7f14adabe196dbd08024790f0df92a43728d643\">A Support Vector Method for Clustering</a>. NIPS. 2000. </i><br><br>the core regions by an SV method with a global optimal solution. We have found examples where a local maximum is hard to identify by Roberts' method. 3.2 The <b>iris</b> data We ran SVC on the iris data set [9], which is a standard benchmark in the pattern recognition literature. It can be obtained from the UCI repository [10]. The data set contains 150 instances, each containing four measurements of<br></p><hr><p class=\"normal\"><a name=\"a6fa92d0caaa949b6ebd6531b2e6d4e7e0f03387\"></a><i>David M J Tax and Robert P W Duin. <a href=\"http://rexa.info/paper/a6fa92d0caaa949b6ebd6531b2e6d4e7e0f03387\">Support vector domain description</a>. Pattern Recognition Letters, 20. 1999. </i><br><br>almost Gaussian distributed and class 2 is scattered around it. The SVDD cannot distinguish one class 2 object from class 1. Finally, the performance of the outlier methods are applied on the <b>iris</b> dataset. Here, all methods work reasonably well, which indicates that the data distributions of the classes are well clustered. Only the Parzen density estimation slightly overtrains. From these results we<br></p><hr><p class=\"normal\"><a name=\"0dccc15e45577745f11d643e8dab5db77827831a\"></a><i>Ismail Taha and Joydeep Ghosh. <a href=\"http://rexa.info/paper/0dccc15e45577745f11d643e8dab5db77827831a\">Symbolic Interpretation of Artificial Neural Networks</a>. IEEE Trans. Knowl. Data Eng, 11. 1999. </i><br><br>and universal approach. A rule evaluation technique that orders extracted rules based on three performance measures is then proposed. The three techniques are applied to the <b>iris</b> and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and compared with those obtained by other approaches. Index Terms: rule extraction, hybrid systems, knowledge refinement, neural<br></p><hr><p class=\"normal\"><a name=\"3ce34105eca9577e44de969f9ef94ca3cdcad3d9\"></a><i>Foster J. Provost and Tom Fawcett and Ron Kohavi. <a href=\"http://rexa.info/paper/3ce34105eca9577e44de969f9ef94ca3cdcad3d9\">The Case against Accuracy Estimation for Comparing Induction Algorithms</a>. ICML. 1998. </i><br><br>we often do not know whether the existing distribution is the natural distribution, or whether it has been stratified. The <b>iris</b> data set has exactly 50 instances of each class. The splice junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very<br></p><hr><p class=\"normal\"><a name=\"d320ece010630c32341d927b2573372abbbca524\"></a><i>Stephen D. Bay. <a href=\"http://rexa.info/paper/d320ece010630c32341d927b2573372abbbca524\">Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets</a>. ICML. 1998. </i><br><br>comparison, we used the Wilcoxon signed rank test and found that MFS1 and MFS2 were significantly better than all others with a confidence level greater than 99%. MFS only performed poorly on two datasets: <b>Iris</b> and Tic-Tac-Toe. For Iris, both MFS1 and MFS2 gave the lowest accuracy out of all the classifiers. This can possibly be explained by the small number of features in the Iris dataset. With<br></p><hr><p class=\"normal\"><a name=\"9a0d2c026da6cf5175639b0c375bf7ab56106e1e\"></a><i>Wojciech Kwedlo and Marek Kretowski. <a href=\"http://rexa.info/paper/9a0d2c026da6cf5175639b0c375bf7ab56106e1e\">Discovery of Decision Rules from Databases: An Evolutionary Approach</a>. PKDD. 1998. </i><br><br>Features Examples Classes australian 15 (9 nominal) 690 2 diabetes 8 768 2 german 20 (13 nominal) 1000 2 glass 9 214 7 hepatitis 19 (13 nominal) 155 2 <b>iris</b> 4 150 3 Table 1. Description of the datasets used in the experiments. Dataset Majority C4.5 EDRL fi australian 55.5 85:3 Sigma 0:2 86:1 Sigma 0:4 0.05 diabetes 65.1 74:6 Sigma 0:3 77:9 Sigma 0:3 0.2 german 70.0 71:6 Sigma 0:3 70:1 Sigma<br></p><hr><p class=\"normal\"><a name=\"b0009a0081cc5fbfbae758def55cfd5b3256623b\"></a><i><a href=\"http://rexa.info/paper/b0009a0081cc5fbfbae758def55cfd5b3256623b\">Prototype Selection for Composite Nearest Neighbor Classifiers</a>. Department of Computer Science University of Massachusetts. 1997. </i><br><br>Fitness--Feature Selection. : : : : : : : : : : : : : : : : 116 4.10 Relationships between component accuracy and diversity for the Monks-2, Breast Cancer Ljubljana, Diabetes and <b>Iris</b> Plants data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :<br></p><hr><p class=\"normal\"><a name=\"cde93f878ecbecffbd0b885d62b8fcd32fcabdb5\"></a><i>Ke Wang and Han Chong Goh. <a href=\"http://rexa.info/paper/cde93f878ecbecffbd0b885d62b8fcd32fcabdb5\">Minimum Splits Based Discretization for Continuous Features</a>. IJCAI (2). 1997. </i><br><br>but never explored multi-way split of a continuous feature, making the simple structure disappear. Consider the following two decision trees built in one of the 10fold cross validation on <b>Iris</b> dataset. The first tree is produced by the multi-way split proposed in this paper, and the second by C4.5. Though both trees have the same size and same error rate on test data, the first tree classifies<br></p><hr><p class=\"normal\"><a name=\"6f2537b9354cfbd865cc2057f04a6216cbafd89c\"></a><i>Ethem Alpaydin. <a href=\"http://rexa.info/paper/6f2537b9354cfbd865cc2057f04a6216cbafd89c\">Voting over Multiple Condensed Nearest Neighbors</a>. Artif. Intell. Rev, 11. 1997. </i><br><br>accuracy goes higher but the variance also decreases. This indicates better generalization and is the clear advantage of voting. Complete results are given in Table 4. Results for the <b>IRIS</b> and WINE datasets are similar and are omitted. When one increases the number of voting subsets, after a certain number, new subsets do not contribute much. Whether an additional subset pays off the additional<br></p><hr><p class=\"normal\"><a name=\"632fe4095475bee152843a02969ade56a290db39\"></a><i>Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja. <a href=\"http://rexa.info/paper/632fe4095475bee152843a02969ade56a290db39\">Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF</a>. Appl. Intell, 7. 1997. </i><br><br>We compared the performance of the algorithms also on the following non-medical real world data sets (SOYB, <b>IRIS</b>  and VOTE are obtained from the Irvine database[21], SAT is obtained from the StatLog database [18]): SOYB: The famous soybean data set used by Michalski & Chilausky [17]. IRIS: The<br></p><hr><p class=\"normal\"><a name=\"5d851945e80949d177fa69dc5d4799f6a0f9d9f7\"></a><i>Tapio Elomaa and Juho Rousu. <a href=\"http://rexa.info/paper/5d851945e80949d177fa69dc5d4799f6a0f9d9f7\">Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning</a>. ESPRIT Working Group in Neural and Computational Learning. 1996. </i><br><br>used. Data set Examples Attributes Classes Num. Total <b>Iris</b> plant classification 150 4 4 3 Glass type identification 214 9 9 6 Australian credit card assessment 690 6 14 2 Wisconsin breast cancer data 699 9 9 2<br></p><hr><p class=\"normal\"><a name=\"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e\"></a><i>Ron Kohavi. <a href=\"http://rexa.info/paper/bf6cec50b7f7d48d105c8c649210cc3a42d3d71e\">Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid</a>. KDD. 1996. </i><br><br>easy to understand when the log probabilities were presented as evidence that adds up in favor of different classes. Figure 1 shows a visualization of the Naive-Bayes classifier for Fisher's <b>iris</b> data set, where the task is to determine the type of iris based on four attributes. Each bar represents evidence for a given class and attribute value. Users can immediately see that all values for<br></p><hr><p class=\"normal\"><a name=\"91c9c07720ed14d74e8cda2ec09b5b6789dda2b2\"></a><i>Daniel C. St and Ralph W. Wilkerson and Cihan H. Dagli. <a href=\"http://rexa.info/paper/91c9c07720ed14d74e8cda2ec09b5b6789dda2b2\">RULE SET QUALITY MEASURES FOR INDUCTIVE LEARNING ALGORITHMS</a>. proceedings of the Artificial Neural Networks In Engineering Conference 1996 (ANNIE. 1996. </i><br><br>distribution of the 148 instances among the four classes \"normal\" with 2 instances, \"metastases\" with 81 instances, \"malign\" with 61 instances, and \"fibrosis\" with 4 instances. The <b>Iris</b> data set, developed by R. A. Fisher (1936), lists the measurements of four characteristics of Iris flowers: petal length, petal width, sepal length, and sepal width. The set includes the measurements of 50<br></p><hr><p class=\"normal\"><a name=\"b90249d5778eb237815fe93968e894cb5adb7f8a\"></a><i>Ron Kohavi. <a href=\"http://rexa.info/paper/b90249d5778eb237815fe93968e894cb5adb7f8a\">A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection</a>. IJCAI. 1995. </i><br><br>then an over-represented class in one subset will be a underrepresented in the other. To demonstrate the issue, we simulated a 2/3, 1/3 split of Fisher's famous <b>iris</b> dataset and used a majority inducer that builds a classifier predicting the prevalent class in the training set. The iris dataset describes iris plants using four continuous features, and the task is to<br></p><hr><p class=\"normal\"><a name=\"dd3f32548422fd3db3846c2fba689a4406d9cf0c\"></a><i>Ron Kohavi. <a href=\"http://rexa.info/paper/dd3f32548422fd3db3846c2fba689a4406d9cf0c\">The Power of Decision Tables</a>. ECML. 1995. </i><br><br>other class is the more prevalent in the training set and the majority inducer predicts the wrong label for the test instance. We have observed a similar phenomenon even with ten-fold CV. The <b>iris</b> dataset has 150 instances, 50 of each class. Predicting any class would yield 33.3% accuracy, but ten-fold CV using a majority induction algorithm yields 21.5% accuracy (averaged over 100 runs of ten-fold<br></p><hr><p class=\"normal\"><a name=\"b7d95df073db5bba49052a1801fdc7deb9edd091\"></a><i>Zoubin Ghahramani and Michael I. Jordan. <a href=\"http://rexa.info/paper/b7d95df073db5bba49052a1801fdc7deb9edd091\">Learning from incomplete data</a>. MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES. 1994. </i><br><br>stochastic estimator. 4.4 Classification Classification with missing inputs 0 20 40 60 80 100 20 40 60 80 100 % missing features EM % correct classification MI Figure 3: Classification of the <b>iris</b> data set. 100 data points were used for training and 50 for testing. Each data point consisted of 4 real-valued attributes and one of three class labels. The figure shows classification performance Sigma 1<br></p><hr><p class=\"normal\"><a name=\"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea\"></a><i>George H. John and Ron Kohavi and Karl Pfleger. <a href=\"http://rexa.info/paper/d4ea16b5ea06d216f7d2be28615fa9c66b98bbea\">Irrelevant Features and the Subset Selection Problem</a>. ICML. 1994. </i><br><br>performance was on parity5+5 and CorrAL using stepwise backward elimination, which reduced the error to 0% from 50% and 18.8% respectively. Experiments were also run on the <b>Iris</b>  Thyroid, and Monk1* datasets. The results on these datasets were similar to those reported in this paper. We observed high variance in the 25-fold crossvalidation estimates of the error. Since our algorithms depend on<br></p><hr><p class=\"normal\"><a name=\"287f8092a743949a6e0151893b9e3bc4d03466ed\"></a><i>Gabor Melli. <a href=\"http://rexa.info/paper/287f8092a743949a6e0151893b9e3bc4d03466ed\">A Lazy Model-Based Approach to On-Line Classification</a>. University of British Columbia. 1989. </i><br><br>................ 88 7.2 Example of one algorithm (A 1 ) being more accurate than another (A 2 ). . . . 90 7.3 Accuracy performance on the <b>iris</b> dataset for several parameter combinations of the DI n ()basedalgorithm............................ 93 7.4 Parameter settings for the DI n () based algorithm that achieve the lowest<br></p><hr><p class=\"normal\"><a name=\"877f66dae749b55055910c2f9ad01e4208eb9042\"></a><i>Michael R. Berthold and Klaus--Peter Huber. <a href=\"http://rexa.info/paper/877f66dae749b55055910c2f9ad01e4208eb9042\">From Radial to Rectangular Basis Functions: A new Approach for Rule Learning from Large Datasets</a>. Institut fur Rechnerentwurf und Fehlertoleranz (Prof. D. Schmid) Universitat Karlsruhe. </i><br><br>extracted from a Neural Network trained on the data, rather than from the data itself. In this scenario the Neural Network already took care of the noisy patterns. B. The <b>IRIS</b> -data This very famous dataset from Fisher ([5]) contains three classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are not linearly<br></p><hr><p class=\"normal\"><a name=\"546cbe749d666efb8ff699628e535e0fd083f6bc\"></a><i>Norbert Jankowski. <a href=\"http://rexa.info/paper/546cbe749d666efb8ff699628e535e0fd083f6bc\">Survey of Neural Transfer Functions</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>sphere defined by this metric. The influence of input renormalization (using Minkovsky distance functions) on the shapes of decision borders is illustrated in Fig. 30 for the classical <b>Iris</b> flowers dataset (only the last two input features, x 3 and x 4 are shown, for description of the data cf. [89]). Dramatic changes in the shapes of decision borders for different Minkovsky metrices are observed.<br></p><hr><p class=\"normal\"><a name=\"e4ce48114dcd770134f22df787d55e8daf02ac4a\"></a><i>Karthik Ramakrishnan. <a href=\"http://rexa.info/paper/e4ce48114dcd770134f22df787d55e8daf02ac4a\">UNIVERSITY OF MINNESOTA</a>. </i><br><br>classifier is shown as a straight line across the x-axis for comparison purposes. . . . . . . . . . . . . . . . 39 vi 15 Bagging, Boosting, and Distance-Weighted test set error rates for the <b>iris</b> data set as the number of classifiers in the ensemble increases. The test set error rate for a single decision tree classifier is shown as a straight line across the x-axis for comparison purposes. . . . . .<br></p><hr><p class=\"normal\"><a name=\"118fe8f48c22375803e81b47d91c0bfd96c19999\"></a><i>Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen. <a href=\"http://rexa.info/paper/118fe8f48c22375803e81b47d91c0bfd96c19999\">Neural Networks from Similarity Based Perspective</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>them on a unit sphere defined by this metric. 6 Pedagogical illustration The influence of non-Euclidean distance functions on the decision borders is illustrated here on the classical <b>Iris</b> flowers dataset, containing 50 cases in each of the 3 classes. The flowers are described by 4 measurements (petal and sepal width and length). Two classes, Iris virginica and Iris versicolor, overlap, and therefore<br></p><hr><p class=\"normal\"><a name=\"80463673ea023e8c5162497b051bd8c54346d686\"></a><i>Fernando Fern#andez and Pedro Isasi. <a href=\"http://rexa.info/paper/80463673ea023e8c5162497b051bd8c54346d686\">Designing Nearest Neighbour Classifiers by the Evolution of a Population of Prototypes</a>. Universidad Carlos III de Madrid. </i><br><br>first version is due to the high number of centroids to eliminate. An example of the classifier found is given in #gure1(a), showing the centroids located in the mean of the distributions. 3.2 <b>Iris</b> Data Set Iris Data Set from UCI Machine Learning Repository 1 [3] is used in the second experiment. This dataset consits of 150 samples of three classes, where each class has 50 examples. The dimension of<br></p><hr><p class=\"normal\"><a name=\"fdc570273fa15597fa6f45ce2a2d2129f56e2c56\"></a><i>Asa Ben-Hur and David Horn and Hava T. Siegelmann and Vladimir Vapnik. <a href=\"http://rexa.info/paper/fdc570273fa15597fa6f45ce2a2d2129f56e2c56\">A Support Vector Method for Hierarchical Clustering</a>. Faculty of IE and Management Technion. </i><br><br>cost of a decrease in efficiency, which makes our algorithm useful even for very large data-sets. To compare the performance of our algorithm with other hierarchical algorithms we ran it on the <b>Iris</b> data set [15], which is a standard benchmark in the pattern recognition literature. It can be obtained from the UCI repository [16]. The data set contains 150 instances each containing four measurements of<br></p><hr><p class=\"normal\"><a name=\"b417429a1fe95e22673736db6f6adc8ecae2ef7d\"></a><i>Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer. <a href=\"http://rexa.info/paper/b417429a1fe95e22673736db6f6adc8ecae2ef7d\">Decision Tree Learning on Very Large Data Sets</a>. Department of Computer Science and Engineering, ENB 118 University of South Florida. </i><br><br>0.6 < Petal-Width <= 1.5 and Petal-Length } 4.9 --} <b>Iris</b> Viginica R5: If 1.5 < Petal-Width <= 1.7 and Petal-Length } 4.9 --} Iris-Versicolor <= 1.7 Figure 1. The C4.5 tree produced on the full Iris dataset and the corresponding rules. The final rules will be ordered by their accuracy taken from the original tree in all cases except for conflict resolution rules for which the accuracy is calculated on<br></p><hr><p class=\"normal\"><a name=\"e8eb0092d0fc87ff35447cec823b43a406ac8372\"></a><i>G. Ratsch and B. Scholkopf and Alex Smola and K. -R Muller and T. Onoda and Sebastian Mika. <a href=\"http://rexa.info/paper/e8eb0092d0fc87ff35447cec823b43a406ac8372\">Arc: Ensemble Learning in the Presence of Outliers</a>. GMD FIRST. </i><br><br>[17] explains the good generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse on noisy tasks [10, 11], such as the <b>iris</b> and the breast cancer benchmark data sets [1]. On the latter tasks, a large margin on all training points cannot be achieved without adverse effects on the generalization error. This experimental observation was supported by the study of<br></p><hr><p class=\"normal\"><a name=\"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d\"></a><i>Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada. <a href=\"http://rexa.info/paper/8afa6796645ce4b0642db26c822cf6bfa8cc4d0d\">Computational intelligence methods for rule-based data understanding</a>. </i><br><br>larger input uncertainties do not change in subsequent minimizations. VIII. EXTRACTION OF RULES -- ILLUSTRATIVE EXAMPLE The process of rule extraction is illustrated here using the well-known <b>Iris</b> dataset, provided by Fisher in 1936. The data PROCEEDINGS OF IEEE, VOL. XX, NO. YY, 2003 17 have been obtained from the UCI machine learning repository [118]. The Iris data have 150 vectors evenly<br></p><hr><p class=\"normal\"><a name=\"cf334aad055b27faaeece97ee1630e146388cd10\"></a><i>H. Altay G uvenir and Aynur Akkus. <a href=\"http://rexa.info/paper/cf334aad055b27faaeece97ee1630e146388cd10\">WEIGHTED K NEAREST NEIGHBOR CLASSIFICATION ON FEATURE PROJECTIONS</a>. Department of Computer Engineering and Information Science Bilkent University. </i><br><br>row of each k value presents the accuracy of the WkNNFP algorithm with equal feature weigths, while the second row shows the accuracy obtained by WkNNFP using Table 1: Comparison on some real-world datasets. Data Set: cleveland glass horse hungarian <b>iris</b> liver sonar wine No. of Instances 303 214 368 294 150 345 208 178 No. of Features 13 9 22 13 4 6 60 13 No. of Classes 2 6 2 2 3 2 2 3 No. of Missing<br></p><hr><p class=\"normal\"><a name=\"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7\"></a><i>Huan Liu. <a href=\"http://rexa.info/paper/8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7\">A Family of Efficient Rule Generators</a>. Department of Information Systems and Computer Science National University of Singapore. </i><br><br>and to compare with the results reported in [22] since they have done some comparison with other methods such as ID3 [14] and the one by Han et al [7]. Then, we show the results for another two data sets: Golf-Playing [13] and <b>Iris</b> [4]. The authors of [13, 22] did not provide testing data. Only the Iris data is divided evenly into two sets (75 patterns each) for training and testing. Datasets CAR<br></p><hr><p class=\"normal\"><a name=\"bcac678940e7598fd6e6b5d1fc9cb5a2895c05f6\"></a><i>Rudy Setiono and Huan Liu. <a href=\"http://rexa.info/paper/bcac678940e7598fd6e6b5d1fc9cb5a2895c05f6\">Fragmentation Problem and Automated Feature Construction</a>. School of Computing National University of Singapore. </i><br><br>[21] which has 9 binary features x 1 ; x 2 ; : : : ; x 9 . The 512 instances are labeled as follows: (a) Class 1: x 1 x 2 x 3 + x 1 x 2 + x 7 x 8 x 9 + x 7 x 9 , (b) Class 2: Otherwise. ffl <b>Iris</b> dataset [6] which has 150 instances described by 4 continuous attributes: sepal length (A 1 ), sepal width (A 2 ), petal length (A 3 ), and petal width (A 4 ). Each pattern belongs to one of the 3 possible<br></p><hr><p class=\"normal\"><a name=\"b569ce235d64605aedc2e0bddb7006bd30fcad9f\"></a><i>Fran ois Poulet. <a href=\"http://rexa.info/paper/b569ce235d64605aedc2e0bddb7006bd30fcad9f\">Cooperation between automatic algorithms, interactive algorithms and visualization tools for Visual Data Mining</a>. ESIEA Recherche. </i><br><br>by the user on the screen and the right part shows the transformed line (the best separating plane computed with the convex hulls). Fig. 6. An example of the automatic best separating plane on <b>iris</b> data set 2.3 Clustering The interactive algorithm described in the previous section can also be used for unsupervised classification. The computation of the convex hulls and the nearest points can be<br></p><hr><p class=\"normal\"><a name=\"177158d5e743e0f71b640c20a162ee8af5c79dcf\"></a><i>Takao Mohri and Hidehiko Tanaka. <a href=\"http://rexa.info/paper/177158d5e743e0f71b640c20a162ee8af5c79dcf\">An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes</a>. Information Engineering Course, Faculty of Engineering The University of Tokyo. </i><br><br>(vote, soybean, crx, hypo) were in the distribution floppy disk of Quinlan's C4.5 book (Quinlan 1993). The remaining four data sets  <b>iris</b>  hepatitis, led, led-noise) were obtained from the Irvine Machine Learning Database (Murphy & Aha 1994). Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a<br></p><hr><p class=\"normal\"><a name=\"490ddcab7a72db65c0752abf46f5692f53e6bedf\"></a><i>Huan Li and Wenbin Chen. <a href=\"http://rexa.info/paper/490ddcab7a72db65c0752abf46f5692f53e6bedf\">Supervised Local Tangent Space Alignment for Classification</a>.  I-Fan Shen. </i><br><br>containing multiple classes. The results obtained with the unsupervised and supervised LTSA are expected to be different as is shown in Fig.1. The <b>iris</b> data set [Blake and Merz, 1998] includes 150 4-D data belonging to 3 different classes. Here first 100 data points are selected as training samples and mapped from the 4-D input space to a 2-D feature space<br></p><hr><p class=\"normal\"><a name=\"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d\"></a><i>Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe. <a href=\"http://rexa.info/paper/095d7064837557bdfbca12fb9c12dbaaeb3a8b0d\">Approximate Distance Classification</a>. Department of Mathematical Sciences The Johns Hopkins University. </i><br><br>data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the Wisconsin breast cancer data set and the Fisher <b>iris</b> data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be<br></p><hr><p class=\"normal\"><a name=\"c6b935ac4dda0316f104278bd74029b224d8ed74\"></a><i>A. da Valls and Vicen Torra. <a href=\"http://rexa.info/paper/c6b935ac4dda0316f104278bd74029b224d8ed74\">Explaining the consensus of opinions with the vocabulary of the experts</a>. Dept. d'Enginyeria Informtica i Matemtiques Universitat Rovira i Virgili. </i><br><br>as L i+1 end if return d(P i ,P c ) calculated with the definition (1). end. 4.1 Experimental results We have made different tests on different domains. Particularly, we have considered a well-known data set: <b>Iris</b> [10], which has 150 flowers described by means of 4 numerical attributes: petal and sepal length, and petal and sepal width; and a second set of data built by 5 colleagues who have described<br></p><hr><p class=\"normal\"><a name=\"cbfc5b79f03770a32505b3342b2e330a1626be7d\"></a><i>Wl/odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski. <a href=\"http://rexa.info/paper/cbfc5b79f03770a32505b3342b2e330a1626be7d\">Extraction of crisp logical rules using constrained backpropagation networks</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>a few cases. The final solution may be presented as a set of rules or as a network of nodes performing logical functions. III. Three examples A. <b>Iris</b> data In the first example the classical Iris dataset was used (all datasets were taken from the UCI machine learning repository [9]). The data has 150 vectors evenly distributed in three classes, called iris-setosa, iris-versicolor and iris-virginica.<br></p><hr><p class=\"normal\"><a name=\"04c0b59b684ddc546667ac58aa34b51f6ecbf50f\"></a><i>Eric P. Kasten and Philip K. McKinley. <a href=\"http://rexa.info/paper/04c0b59b684ddc546667ac58aa34b51f6ecbf50f\">MESO: Perceptual Memory to Support Online Learning in Adaptive Software</a>. Proceedings of the Third International Conference on Development and Learning (ICDL. </i><br><br>sizes and feature counts. Data Set Size Features Classes <b>Iris</b> 150 4 3 ATT Faces 360 10,304 40 Mult. Feature 2,000 649 10 Mushroom 8,124 22 2 Japanese Vowel 9,859 12 9 Letter 20,000 16 26 Cover Type 581,012 54 7 set. As such, no<br></p><hr><p class=\"normal\"><a name=\"68b230977077ba67eb9e5c9a9111d3ccb3672150\"></a><i>Karol Grudzi nski and Wl/odzisl/aw Duch. <a href=\"http://rexa.info/paper/68b230977077ba67eb9e5c9a9111d3ccb3672150\">SBL-PM: A Simple Algorithm for Selection of Reference Instances in Similarity Based Methods</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>the UCI repository [9] and contains 3 classes  <b>Iris</b> Setosa, Virginica and Versicolor flowers), 4 attributes (measurements of leaf and petal widths and length), 50 cases per class. The entire Iris dataset has been shown here (Fig. 1) in two dimensions, x 3 and x 4 , which are much more informative the other two (cf. [10]). In Fig 2. the reference set obtained by taking the value of # from the<br></p><hr><p class=\"normal\"><a name=\"f624e93bd6b670bc3dc31925c1c885b538131534\"></a><i>Chih-Wei Hsu and Cheng-Ru Lin. <a href=\"http://rexa.info/paper/f624e93bd6b670bc3dc31925c1c885b538131534\">A Comparison of Methods for Multi-class Support Vector Machines</a>. Department of Computer Science and Information Engineering National Taiwan University. </i><br><br>section we present experimental results on several problems from the Statlog collection [20] and the UCI Repository of machine learning databases [1]. From UCI Repository we choose the following datasets: <b>iris</b>  wine, glass, and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: vehicle, segment, dna, satimage, letter, and shuttle. Note<br></p><hr><p class=\"normal\"><a name=\"e2b2b723df700c90e69a31a4403b740c2d2a7b2f\"></a><i>Alexander K. Seewald. <a href=\"http://rexa.info/paper/e2b2b723df700c90e69a31a4403b740c2d2a7b2f\">Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften</a>. </i><br><br>ionosphere Compressed glyph visualization for dataset <b>iris</b> Compressed glyph visualization for dataset labor Compressed glyph visualization for dataset lymph Compressed glyph visualization for dataset primary-tumor Compressed glyph visualization for<br></p><hr><p class=\"normal\"><a name=\"c3f9c3303aa080beec901b74703cef88ee2b2f24\"></a><i>Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal. <a href=\"http://rexa.info/paper/c3f9c3303aa080beec901b74703cef88ee2b2f24\">A hybrid method for extraction of logical rules from data</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>for benchmark applications were taken from the UCI machine learning repository [14]. Application of the constructive MLP2LN approach to the classical <b>Iris</b> dataset was already presented in detail [15], therefore only new aspects related to the hybrid method are discussed here. The Iris data has 150 vectors evenly distributed in three iris-setosa,<br></p><hr><p class=\"normal\"><a name=\"4967c873f90995ec2e7fd690a0ec6118f1adc807\"></a><i>Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen. <a href=\"http://rexa.info/paper/4967c873f90995ec2e7fd690a0ec6118f1adc807\">Classification, Association and Pattern Completion using Neural Similarity Based Methods</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>them on a unit sphere defined by this metric. 6PEDAGOGICAL ILLUSTRATION The influence of non-Euclidean distance functions on the decision borders is illustrated here on the classical <b>Iris</b> flowers dataset, containing 50 cases in each of the 3 classes. The flowers are described by 4 measurements (petal and sepal width and length). Two classes, Iris virginica and Iris versicolor, overlap, and therefore<br></p><hr><p class=\"normal\"><a name=\"6858832ba8e9e4ac002900af151a8ffcc1796f8f\"></a><i>Stefan Aeberhard and Danny Coomans and De Vel. <a href=\"http://rexa.info/paper/6858832ba8e9e4ac002900af151a8ffcc1796f8f\">THE PERFORMANCE OF STATISTICAL PATTERN RECOGNITION METHODS IN HIGH DIMENSIONAL SETTINGS</a>. James Cook University. </i><br><br>means coincide. FDP performed very well for the exponential data. The results of the real data support the observations made from the simulations. FDP does not perform very well on well-defined data sets (wine data, <b>Iris</b> data), especially when compared to FF. It however compares somewhat better in the other cases, most noticeably in the case of the tertiary institutions data, where it equals the<br></p><hr><p class=\"normal\"><a name=\"8a6877d257fd7de465ca5f47834294629105e92c\"></a><i>Michael P. Cummings and Daniel S. Myers and Marci Mangelson. <a href=\"http://rexa.info/paper/8a6877d257fd7de465ca5f47834294629105e92c\">Applying Permuation Tests to Tree-Based Statistical Models: Extending the R Package rpart</a>. Center for Bioinformatics and Computational Biology, Institute for Advanced Computer Studies, University of Maryland. </i><br><br>In this section we show several examples of the application of permutation tests to tree-based statistical models. We begin by permutation testing a classification tree built on the famous <b>Iris</b> dataset setosa: 50 versicolor: 50 virginica: 50 virginica: 0 versiolor: 0 setosa: 0 versicolor: 50 virginica: 50 virginica: 45 versicolor: 1 virginica: 5 versicolor: 49 setosa: 0 petal length < 2.45 cm<br></p><hr><p class=\"normal\"><a name=\"17b1afb4bd706435fa92313e1e85a5d6009a42f4\"></a><i>Ping Zhong and Masao Fukushima. <a href=\"http://rexa.info/paper/17b1afb4bd706435fa92313e1e85a5d6009a42f4\">Second Order Cone Programming Formulations for Robust Multi-class Classification</a>. </i><br><br>problem as follows: max \u00ae,\u00be,\u00bf e T \u00ae- (\u00be + \u00bf) s.t. \u00af E T \u00ae = 0, \u00ae \u00b7 (1 - \u00ba)e, (38) \u00be - \u00bf = \u00ba, \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 2 4 - 1 p 2(K+1) ~ A T \u00ae \u00bf 3 5 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b7 \u00be. Table 1: Description of <b>Iris</b>  Wine and Glass datasets. name dimension (N) #classes (K) #examples (L) Iris 4 3 150 Wine 13 3 178 Glass 9 6 214 14 Table 2: Results for Iris, Wine and Glass datasets with noise (\u00bd = 0.3, \u00b7 = 2, \u00ba = 0.05). R a Robust (I)<br></p><hr><p class=\"normal\"><a name=\"030d9becf3287c5da5f82e2c2da60937121b3ce5\"></a><i>Wl odzisl/aw Duch and Rafal Adamczak and Norbert Jankowski. <a href=\"http://rexa.info/paper/030d9becf3287c5da5f82e2c2da60937121b3ce5\">Initialization of adaptive parameters in density networks</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>network parameters, but it is interesting to note that these results are frequently already of rather high quality. Except for galaxies all other data was obtained from the UCI repository [13]. <b>Iris</b> dataset contains 150 cases in 3 classes. After initialization with Gaussian functions including rotations only 4 classification errors are made (97.3% accuracy), which is a better results than many<br></p><hr><p class=\"normal\"><a name=\"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5\"></a><i>Aynur Akku and H. Altay Guvenir. <a href=\"http://rexa.info/paper/945d8ba4c7eacfaed696aaf0bd72fd576efa78d5\">Weighting Features in k Nearest Neighbor Classification on Feature Projections</a>. Department of Computer Engineering and Information Science Bilkent University. </i><br><br>significantly. This should be because all the features are equally relevant. On the cleveland, liver, <b>iris</b> and glass (except k = 1) datasets, the weights learned by the individual accuracies always performed significantly better than the others. The weight learning method based on the homogeneity performed better than the other on the<br></p><hr><p class=\"normal\"><a name=\"28734e2b9b072c9ef055eefa88999a60f6a3dc81\"></a><i>Jun Wang. <a href=\"http://rexa.info/paper/28734e2b9b072c9ef055eefa88999a60f6a3dc81\">Classification Visualization with Shaded Similarity Matrix</a>. Bei Yu Les Gasser Graduate School of Library and Information Science University of Illinois at Urbana-Champaign. </i><br><br>similarity matrix is constructed and how it looks through an example. The data used in the example is part of the <b>Iris</b> data from the UCI repository [25]. There are 150 instances in the original Iris data set, which evenly distributed in 3 classes: setosa, virginica, and versicolor. For each class, we fetch its first 5 instances from the data file, and thus obtaining 15 instances (see Table 1). Table 2<br></p><hr><p class=\"normal\"><a name=\"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9\"></a><i>Andrew Watkins and Jon Timmis and Lois C. Boggess. <a href=\"http://rexa.info/paper/b4b7ea88c807c5c8746bf3067f13b1d07899bcc9\">Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm</a>. (abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent. </i><br><br>where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher <b>Iris</b> data set, Pima diabetes data, Ionosphere data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets<br></p><hr><p class=\"normal\"><a name=\"4487f47affc309f0d984645b44cdc8e1f42c2472\"></a><i>Gaurav Marwah and Lois C. Boggess. <a href=\"http://rexa.info/paper/4487f47affc309f0d984645b44cdc8e1f42c2472\">Artificial Immune Systems for Classification : Some Issues</a>. Department of Computer Science Mississippi State University. </i><br><br>satisfying some stimulation threshold, but the stimulation threshold for out of class ARBs was somewhat relaxed as compared to in class ARBs. Table 4 shows the accuracy rates obtained for the <b>iris</b> data set using the approaches just described. Five way cross validation was performed to achieve these results. Table 4: Accuracy Rates For Iris Data Set Using Different Approaches For ARB Pool Organization<br></p><hr><p class=\"normal\"><a name=\"dbd0f89e43cc274cf80effc519740af54cccef75\"></a><i>Igor Kononenko and Edvard Simec. <a href=\"http://rexa.info/paper/dbd0f89e43cc274cf80effc519740af54cccef75\">Induction of decision trees using RELIEFF</a>. University of Ljubljana, Faculty of electrical engineering & computer science. </i><br><br>for patients suffering from hepatitis. The data was provided by Gail Gong from Carnegie-Mellon University. We also compared the performance of the algorithms on the following non-medical real world data sets (SOYB, <b>IRIS</b>  and VOTE are obtained from the Irvine database (Murphy & Aha, 1991)): SOYB: The famous soybean data set used by Michalski & Chilausky (1980). IRIS: The well known Fisher's problem of<br></p><hr><p class=\"normal\"><a name=\"b6fb41e86105131ea2ce769929b199c003161616\"></a><i>Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita. <a href=\"http://rexa.info/paper/b6fb41e86105131ea2ce769929b199c003161616\">Learning Nonstructural Distance Metric by Minimum Cluster Distortions</a>. ATR Spoken Language Translation research laboratories. </i><br><br>0 . 7 0 . 8 0 . 9 1 1 2 3 4 D i m e n s i o n P r e c i s i o n (c)  <b>iris</b>  dataset 0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 2 0 3 5 D i m e n s i o n P r e c i s i o n (d) \"soybean\" dataset Figure 4: K-means clustering of UCI Machine Learning dataset results. The horizontal axis shows<br></p><hr><p class=\"normal\"><a name=\"674344e880b3ca3ce47332c3d53ed9554b661f8b\"></a><i>Wl odzisl/aw Duch and Karol Grudzinski. <a href=\"http://rexa.info/paper/674344e880b3ca3ce47332c3d53ed9554b661f8b\">Prototype based rules - a new way to understand the data</a>. Department of Computer Methods, Nicholas Copernicus University. </i><br><br>were extracted recently [2]. For comparison we have analyzed some of these dataset here. <b>Iris</b> flowers data, taken from the UCI repository [14], has been used in many previous studies. It contains 3 classes (Iris Setosa, Virginica and Versicolor flowers), 4 attributes (sepal and<br></p><hr><p class=\"normal\"><a name=\"8eb7bb53b63501db4eb1c49eab30d76f7febba8d\"></a><i>H. Altay Guvenir. <a href=\"http://rexa.info/paper/8eb7bb53b63501db4eb1c49eab30d76f7febba8d\">A Classification Learning Algorithm Robust to Irrelevant Features</a>. Bilkent University, Department of Computer Engineering and Information Science. </i><br><br>VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Iris</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy New-thyroid data set VFI5 1NN 3NN 5NN 0 1 2<br></p><hr><p class=\"normal\"><a name=\"9ad134b043c73e8215bea14ecf8a270904b0b535\"></a><i>Enes Makalic and Lloyd Allison and David L. Dowe. <a href=\"http://rexa.info/paper/9ad134b043c73e8215bea14ecf8a270904b0b535\">MML INFERENCE OF SINGLE-LAYER NEURAL NETWORKS</a>. School of Computer Science and Software Engineering Monash University. </i><br><br>0.20, overfitting was observed -- MDL inferred four hidden neurons as optimal rather than three (see Fig. 4). 780 790 800 810 820 830 840 850 1 2 3 4 5 6 Message length (nits) Hidden Neurons <b>Iris</b> Dataset MML Figure 5. MML inference of the Iris dataset Finally, we have tested both MML and MDL-based criteria on a real problem: the Iris dataset from the UCI machine learning repository. This is<br></p><hr><p class=\"normal\"><a name=\"d4664ad584fcc55802b2bffeb0d57f8e62eca0e2\"></a><i>Ron Kohavi and Brian Frasca. <a href=\"http://rexa.info/paper/d4664ad584fcc55802b2bffeb0d57f8e62eca0e2\">Useful Feature Subsets and Rough Set Reducts</a>. the Third International Workshop on Rough Sets and Soft Computing. </i><br><br>bears no resemblance to Holte's 1R algorithm. 1993), stopping after a predetermined number of non-improving node expansions. Figure 2 shows the search through the feature subsets in the <b>IRIS</b> dataset. The number in brackets denotes the order the nodes are visited. The bootstrap estimate is given with one standard deviation of the accuracy after the +=Gamma sign. The estimated real accuracy (on<br></p><hr><p class=\"normal\"><a name=\"38848f2f66b7de9e40f1576d9dcb59b8361fccac\"></a><i>G. Ratsch and B. Scholkopf and Alex Smola and Sebastian Mika and T. Onoda and K. -R Muller. <a href=\"http://rexa.info/paper/38848f2f66b7de9e40f1576d9dcb59b8361fccac\">Robust Ensemble Learning for Data Mining</a>. GMD FIRST, Kekul#estr. </i><br><br>generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse than other learning machines on noisy tasks [6, 7], such as the <b>iris</b> and the breast cancer benchmark data sets [5]. The present paper addresses the overfitting problem of AdaBoost in two ways. Primarily, it makes an algorithmic contribution to the problem of constructing regularized boosting algorithms.<br></p><hr><p class=\"normal\"><a name=\"c4bbcd894d98036f58a23f51ef83d1860d43021c\"></a><i>YongSeog Kim and W. Nick Street and Filippo Menczer. <a href=\"http://rexa.info/paper/c4bbcd894d98036f58a23f51ef83d1860d43021c\">Optimal Ensemble Construction via Meta-Evolutionary Ensembles</a>. Business Information Systems, Utah State University. </i><br><br>with detailed information from most of input features to learn multiple patterns. Therefore, classifiers with information from few projected variables will not perform well. Note that, among 15 data sets, there are four multi-class data sets  <b>iris</b>  hypo, segment, and soybean) while the remaining 11 data sets are bi-class data sets. Out of four multi-class data sets, MEE shows consistently worse<br></p><hr><p class=\"normal\"><a name=\"22dbe26a460522ada68b637b8a3483c717b671fa\"></a><i>Maria Salamo and Elisabet Golobardes. <a href=\"http://rexa.info/paper/22dbe26a460522ada68b637b8a3483c717b671fa\">Analysing Rough Sets weighting methods for Case-Based Reasoning Systems</a>. Enginyeria i Arquitectura La Salle. </i><br><br>are obtained from the UCI repository [MM98]. They are: breast cancer, glass, ionosphere, <b>iris</b>  led, sonar, vehicle and vowel. Private datasets are from our own repository. They deal with diagnosis of breast cancer and synthetic datasets. Datasets related to diagnosis are biopsy and mammogram. Biopsy is the result of digitally processed<br></p><hr><p class=\"normal\"><a name=\"4773165a9efff672af50f07beb5d38c9afc92f5f\"></a><i>Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer. <a href=\"http://rexa.info/paper/4773165a9efff672af50f07beb5d38c9afc92f5f\">Combining Decision Trees Learned in Parallel</a>. Department of Computer Science and Engineering, ENB 118 University of South Florida. </i><br><br>0.6 < Petal-Width <= 1.5 and Petal-Length } 4.9 --} <b>Iris</b> Viginica R5: If 1.5 < Petal-Width <= 1.7 and Petal-Length } 4.9 --} Iris-Versicolor <= 1.7 Figure 1: The C4.5 tree produced on the full Iris dataset and the corresponding rules. adjust just one condition. For example, R1 no longer conflicts its test is adjusted to be petalwidthcm :5. A more complex problem is a condition in one rule overlaps<br></p><hr><p class=\"normal\"><a name=\"f9cf2dcf39f23bae2973a65af6de81e6438989d3\"></a><i>Anthony Robins and Marcus Frean. <a href=\"http://rexa.info/paper/f9cf2dcf39f23bae2973a65af6de81e6438989d3\">Learning and generalisation in a stable network</a>. Computer Science, The University of Otago. </i><br><br>network. The effectiveness of pseudorehearsal at reducing catastrophic forgetting has been proven using a range of populations, including: randomly constructed autoassociative and hetroassociative data sets [Robins, 1995]; the <b>Iris</b> data set [Robins, 1996]; a classification task using the Mushroom data set [French, 1997]; and an alphanumeric character set using a Hopfield type network [Robins and<br></p><hr><p class=\"normal\"><a name=\"7d87efd12b445262eff46191ede868722a2be569\"></a><i>Geoffrey Holmes and Leonard E. Trigg. <a href=\"http://rexa.info/paper/7d87efd12b445262eff46191ede868722a2be569\">A Diagnostic Tool for Tree Based Supervised Classification Learning Algorithms</a>. Department of Computer Science University of Waikato Hamilton New Zealand. </i><br><br>difference by the range of the tested attribute, giving the formula: cost = | v 1 - v 2 | max a 1 -min a 1 Figure 2 illustrates the problem for case 4 with an example taken from the familiar <b>iris</b> dataset. The minimum cost edit sequence to transform the tree on the left involves deleting the non-root Petal width nodes and their rightmost leaf nodes (giving a cost of 4). We are left with two trees<br></p><hr><p class=\"normal\"><a name=\"9fe9e6630b7b953504726b9b36281c4f738b964e\"></a><i>Shlomo Dubnov and Ran El and Yaniv Technion and Yoram Gdalyahu and Elad Schneidman and Naftali Tishby and Golan Yona. <a href=\"http://rexa.info/paper/9fe9e6630b7b953504726b9b36281c4f738b964e\">Clustering By Friends : A New Nonparametric Pairwise Distance Based Clustering Algorithm</a>. Ben Gurion University. </i><br><br>procedure of the cross-validation index (see Section 3) and we only report the resulting cross-validation indices obtained during the computations. In section 5.1 we consider the classical <b>Iris</b> data sets. Then, in section 5.2 we consider the Isolet data set. An application to musical data is considered in section 5.3. 5.1. The Iris Data This data set, due to Fisher (Fisher, 1936), is a classic<br></p>\n\n\n\t</td></tr></table>\n\n\n\n<hr>\n\n<p class=\"normal\"><a href=\"/datasets/Iris\">Return to Iris data set page</a>.\n\n\n<table cellpadding=5 align=center><tr valign=center>\n\t\t<td><p class=\"normal\">Supported By:</p></td>\n        <td><img src=\"../assets/nsfe.gif\" height=60 /> </td>\n        <td><p class=\"normal\">&nbsp;In Collaboration With:</p></td>\n        <td><img src=\"../assets/rexaSmall.jpg\" /></td>\n</tr></table>\n\n<center>\n<span class=\"normal\">\n<a href=\"../about.html\">About</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../donation_policy.html\">Donation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../contact.html\">Contact</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"http://cml.ics.uci.edu\">CML</a>\n</span>\n</center>\n\n\n\n\n</body>\n</html>\n", "encoding": "ISO-8859-1"}