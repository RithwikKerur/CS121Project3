{"url": "http://archive.ics.uci.edu/ml/datasets/Wall-Following+Robot+Navigation+Data", "content": "\n\n\n\n<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\n<html>\n<head>\n<title>UCI Machine Learning Repository: Wall-Following Robot Navigation Data Data Set</title>\n\n<!-- Stylesheet link -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../assets/ml.css\" />\n\n<script language=\"JavaScript\" type=\"text/javascript\">\n<!--\nfunction checkform ( form )\n{\n  // see http://www.thesitewizard.com/archive/validation.shtml\n  // for an explanation of this script and how to use it on your\n  // own website\n\n  // ** START **\n  if (form.q.value == \"\")\n  {\n    alert( \"Please enter search terms.\" );\n    form.q.focus();\n    return false ;\n  }\n\n  if (getCheckedValue(form.sitesearch) == \"ics.uci.edu\" && form.q.value.indexOf(\"site:archive.ics.uci.edu/ml\") == -1)\n  {\n    form.q.value = form.q.value + \" site:archive.ics.uci.edu/ml\";\n  }\n\n  // ** END **\n  return true ;\n}\n\n// return the value of the radio button that is checked\n// return an empty string if none are checked, or\n// there are no radio buttons\nfunction getCheckedValue(radioObj) {\n\tif(!radioObj)\n\t\treturn \"\";\n\tvar radioLength = radioObj.length;\n\tif(radioLength == undefined)\n\t\tif(radioObj.checked)\n\t\t\treturn radioObj.value;\n\t\telse\n\t\t\treturn \"\";\n\tfor(var i = 0; i < radioLength; i++) {\n\t\tif(radioObj[i].checked) {\n\t\t\treturn radioObj[i].value;\n\t\t}\n\t}\n\treturn \"\";\n}\n//-->\n</script>\n\n</head>\n\n<body>\n\n\n<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->\n\n<table width=100% bgcolor=\"#003366\">\n<tr>\n\t<td>\n\t\t<span class=\"normal\"><a href=\"../index.html\" alt=\"Home\"><img src=\"../assets/logo.gif\"\nborder=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://cml.ics.uci.edu\"><font color=\"FFDD33\">Center for Machine Learning and Intelligent Systems</font></a></span>\n\t</td>\n\t<td width=100% valign=top align=\"right\">\n\t\t<span class=\"whitetext\">\n\t\t<a href=\"../about.html\">About</a>&nbsp;\n\t\t<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;\n\t\t<a href=\"../donation_policy.html\">Donate a Data Set</a>&nbsp;\n\t\t<a href=\"../contact.html\">Contact</a>\n\t\t</span>\n\n\t\t<br>\n\t\t<br>\n\t\t<!-- Search Google -->\n\n\t\t<FORM method=GET action=http://www.google.com/custom onsubmit=\"return checkform(this);\">\n\t\t<INPUT TYPE=text name=q size=30 maxlength=255 value=\"\">\n\t\t<INPUT type=submit name=sa VALUE=\"Search\">\n\t\t<INPUT type=hidden name=cof VALUE=\"AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;\">\n\t\t<input type=hidden name=domains value=\"ics.uci.edu\">\n\t\t<br>\n\t\t<input type=radio name=sitesearch value=\"ics.uci.edu\" checked> <span class=\"whitetext\"><font size=\"1\">Repository</font></span>\n\t\t<input type=radio name=sitesearch value=\"\"> <span class=\"whitetext\"><font size=\"1\">Web</font></span>\n\t\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n\t\t<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>\n\t\t<br>\n\t\t</FORM>\n\t\t<!-- Search Google -->\n\n\n\t\t<span class=\"whitetext\"><a href=\"../datasets.php\"><font size=\"3\" color=\"#FFDD33\"><b>View\nALL Data Sets</b></font></a></span>\n\t\t<br>\n\t</td>\n</tr>\n</table>\n\n\n<br />\n<table width=100% border=0 cellpadding=2><tr><td>\n\n   <table><tr>\n     <td valign=top>\n\t<p>\n\t<span class=\"heading\"><b>Wall-Following Robot Navigation Data Data Set</b></span>\n\t<br><span class=\"normal\"><i><font size=4 >Download</font></i>: <a href=\"../machine-learning-databases/00194/\"><font\nstyle=\"BACKGROUND-COLOR: #FFFFAA\" size=4>Data Folder</font></a>, <a href=\"../machine-learning-databases/00194/Wall-following.names\"><font\nstyle=\"BACKGROUND-COLOR: #FFFFAA\" size=4>Data Set Description</font></a></span></p>\n\n\t<p class=\"normal\"><b>Abstract</b>: The data were collected as the SCITOS G5 robot navigates through the room following the wall in a clockwise direction, for 4 rounds, using 24 ultrasound sensors arranged circularly around its 'waist'.</p>\n     </td>\n     <td> </td>\n   </tr></table>\n\n<table border=1 cellpadding=6>\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Data Set Characteristics:&nbsp;&nbsp;</b></p></td>\n\t\t<td><p class=\"normal\">Multivariate, Sequential</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Instances:</b></p></td>\n\t\t<td><p class=\"normal\">5456</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Area:</b></p></td>\n\t\t<td><p class=\"normal\">Computer</p></td>\n\t</tr>\n\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Attribute Characteristics:</b></p></td>\n\t\t<td><p class=\"normal\">Real</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Attributes:</b></p></td>\n\t\t<td><p class=\"normal\">24</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Date Donated</b></p></td>\n\t\t<td><p class=\"normal\">2010-08-04</p></td>\n\t</tr>\n\t<tr>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Associated Tasks:</b></p></td>\n\t\t<td><p class=\"normal\">Classification</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Missing Values?</b></p></td>\n\t\t<td><p class=\"normal\">N/A</p></td>\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Number of Web Hits:</b></p></td>\n\t\t<td><p class=\"normal\">57617</p></td>\n\t</tr>\n\t<!--\n\t<tr>\n\n\t\t<td bgcolor=\"#DDEEFF\"><p class=\"normal\"><b>Highest Percentage Achieved:&nbsp;&nbsp;</b></p></td>\n\t\t<td><p class=\"normal\">N/A</p></td>\n\t</tr>\n\t-->\n</table>\n\n\n<br />\n\n<p class=\"small-heading\"><b>Source:</b></p>\n<p class=\"normal\">(a) Creators: \tAnanda Freire, Marcus Veloso and Guilherme Barreto\r<br>\r<br>\t\tDepartment of Teleinformatics Engineering\r<br>\r<br>\t\tFederal University of Cear\u00c3\u00a1\r<br>\t\t\tFortaleza, Cear\u00c3\u00a1, Brazil\r<br>\r<br>\r<br>\r<br>(b) Donors of database: Ananda Freire (<u>anandalf <b>'@'</b> gmail.com</u>)\r<br>\t\t\tGuilherme Barreto (<u>guilherme <b>'@'</b> deti.ufc.br</u>)</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Data Set Information:</b></p>\n<p class=\"normal\">The provided files comprise three different data sets. The first one contains the raw values of the measurements \r<br>of all 24 ultrasound sensors and the corresponding class label (see Section 7). Sensor readings are sampled at a \r<br>rate of 9 samples per second.\r<br>\r<br>The second one contains four sensor readings named 'simplified distances' and the corresponding class label (see Section 7). These simplified distances are referred to as the 'front distance', 'left distance', 'right distance' and 'back distance'. They consist, respectively, of the minimum sensor readings among those within 60 degree arcs located at the front, left, right and back parts of the robot.\r<br>\r<br>The third one contains only the front and left simplified distances and the corresponding class label.\r<br>\r<br>It is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same time step, so each file has the same number of rows (one for each sampling time step).\r<br>\r<br>The wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task is indeed a non-linearly separable classification task. Thus, linear classifiers, such as the Perceptron network, are not able to learn the task and command the robot around the room without collisions. Nonlinear neural classifiers, such as the MLP network, are able to learn the task and command the robot successfully without collisions. \r<br>\r<br>If some kind of short-term memory mechanism is provided to the neural classifiers, their performances are improved in general. For example, if past inputs are provided together with current sensor readings, even the Perceptron becomes able to learn the task and command the robot succesfully. If a recurrent neural network, such as the Elman network, is used to learn the task, the resulting dynamical classifier is able to learn the task using less hidden neurons than the MLP network.\r<br>\r<br>Files with different number of sensor readings were built in order to evaluate the performance of the classifiers with respect to the number of inputs.</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Attribute Information:</b></p>\n<p class=\"normal\">Number of Attributes \r<br>   -- sensor_readings_24.data: 24 numeric attributes and the class.\r<br>   -- sensor_readings_4.data:   4 numeric attributes and the class.\r<br>   -- sensor_readings_2.data:   2 numeric attributes and the class.\r<br>\r<br>\r<br>\r<br>For Each Attribute: \r<br>   -- File sensor_readings_24.data:\r<br>\t1. US1: ultrasound sensor at the front of the robot (reference angle: 180\u00c2\u00b0) - (numeric: real)\r<br>\t2. US2: ultrasound reading (reference angle: -165\u00c2\u00b0) - (numeric: real)\r<br>\t3. US3: ultrasound reading (reference angle: -150\u00c2\u00b0) - (numeric: real)\r<br>\t4. US4: ultrasound reading (reference angle: -135\u00c2\u00b0) - (numeric: real)\r<br>\t5. US5: ultrasound reading (reference angle: -120\u00c2\u00b0) - (numeric: real)\r<br>\t6. US6: ultrasound reading (reference angle: -105\u00c2\u00b0) - (numeric: real)\r<br>\t7. US7: ultrasound reading (reference angle: -90\u00c2\u00b0) - (numeric: real)\r<br>\t8. US8: ultrasound reading (reference angle: -75\u00c2\u00b0) - (numeric: real)\r<br>\t9. US9: ultrasound reading (reference angle: -60\u00c2\u00b0) - (numeric: real)\r<br>\t10. US10: ultrasound reading (reference angle: -45\u00c2\u00b0) - (numeric: real)\r<br>\t11. US11: ultrasound reading (reference angle: -30\u00c2\u00b0) - (numeric: real)\r<br>\t12. US12: ultrasound reading (reference angle: -15\u00c2\u00b0) - (numeric: real)\r<br>\t13. US13: reading of ultrasound sensor situated at the back of the robot (reference angle: 0\u00c2\u00b0) - (numeric: real)\r<br>\t14. US14: ultrasound reading (reference angle: 15\u00c2\u00b0) - (numeric: real)\r<br>\t15. US15: ultrasound reading (reference angle: 30\u00c2\u00b0) - (numeric: real)\r<br>\t16. US16: ultrasound reading (reference angle: 45\u00c2\u00b0) - (numeric: real)\r<br>\t17. US17: ultrasound reading (reference angle: 60\u00c2\u00b0) - (numeric: real)\r<br>\t18. US18: ultrasound reading (reference angle: 75\u00c2\u00b0) - (numeric: real)\r<br>\t19. US19: ultrasound reading (reference angle: 90\u00c2\u00b0) - (numeric: real)\r<br>\t20. US20: ultrasound reading (reference angle: 105\u00c2\u00b0) - (numeric: real)\r<br>\t21. US21: ultrasound reading (reference angle: 120\u00c2\u00b0) - (numeric: real)\r<br>\t22. US22: ultrasound reading (reference angle: 135\u00c2\u00b0) - (numeric: real)\r<br>\t23. US23: ultrasound reading (reference angle: 150\u00c2\u00b0) - (numeric: real)\r<br>\t24. US24: ultrasound reading (reference angle: 165\u00c2\u00b0) - (numeric: real)\r<br>   \t25. Class: \r<br>      \t\t-- Move-Forward\r<br>      \t\t-- Slight-Right-Turn\r<br>      \t\t-- Sharp-Right-Turn\r<br>      \t\t-- Slight-Left-Turn\r<br>\r<br>   -- File sensor_readings_4.data:\r<br>\t1. SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\r<br>\t2. SD_left:  minimum sensor reading within a 60 degree arc located at the left of the robot  - (numeric: real)\r<br>\t3. SD_right: minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric: real)\r<br>\t4. SD_back:  minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric: real)\r<br>   \t5. Class: \r<br>      \t\t-- Move-Forward\r<br>      \t\t-- Slight-Right-Turn\r<br>      \t\t-- Sharp-Right-Turn\r<br>      \t\t-- Slight-Left-Turn\r<br>\r<br>   -- File sensor_readings_2.data:\r<br>\t1. SD_front: minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric: real)\r<br>\t2. SD_left:  minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric: real)\r<br>   \t3. Class: \r<br>      \t\t-- Move-Forward\r<br>      \t\t-- Slight-Right-Turn\r<br>      \t\t-- Sharp-Right-Turn\r<br>      \t\t-- Slight-Left-Turn</p>\n\n<br />\n\n<p class=\"small-heading\"><b>Relevant Papers:</b></p>\n<p class=\"normal\">Ananda L. Freire, Guilherme A. Barreto, Marcus Veloso and Antonio T. Varela (2009),\r<br>'Short-Term Memory Mechanisms in Neural Network Learning of Robot Navigation\r<br> Tasks: A Case Study'. Proceedings of the 6th Latin American Robotics Symposium (LARS'2009),\r<br>Valpara\u00c3\u00adso-Chile, pages 1-6, DOI: 10.1109/LARS.2009.5418323 </p>\n\n<br />\n\n\n<!-- OLD CODE:\n\n<p class=\"small-heading\"><b>Papers That Cite This Data Set<sup>1</sup>:</b></p>\n<img src=\"../assets/rexa.jpg\" />\n<p class=\"normal\">N/A</p>\n\n-->\n\n\n\n<br />\n\n<p class=\"small-heading\"><b>Citation Request:</b></p>\n<p class=\"normal\">Please refer to the Machine Learning\nRepository's <a href=\"../citation_policy.html\">citation policy</a></p>\n\n</td></tr></table>\n\n\n<hr>\n\n<!-- OLD CODE:\n<p class=\"normal\"><font size=1>[1] Papers were automatically harvested and associated with this data set, in collaboration with <a href=\"http://rexa.info\"><font size=1>Rexa.info</font></a></font></p>\n-->\n\n\n\n<table cellpadding=5 align=center><tr valign=center>\n\t\t<td><p class=\"normal\">Supported By:</p></td>\n        <td><img src=\"../assets/nsfe.gif\" height=60 /> </td>\n        <td><p class=\"normal\">&nbsp;In Collaboration With:</p></td>\n        <td><img src=\"../assets/rexaSmall.jpg\" /></td>\n</tr></table>\n\n<center>\n<span class=\"normal\">\n<a href=\"../about.html\">About</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../donation_policy.html\">Donation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../contact.html\">Contact</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"http://cml.ics.uci.edu\">CML</a>\n</span>\n</center>\n\n\n\n\n</body>\n</html>\n", "encoding": "utf-8"}