{"url": "http://archive.ics.uci.edu/ml/support/Waveform+Database+Generator+(Version+1)#19d5e6db33eb1a6c31be626538ea656f79e901b1", "content": "\n\n\n\n<!DOCTYPE HTML PUBLIC \\\"-//W3C//DTD HTML 4.01 Transitional//EN\\\">\n<html>\n<head>\n<title>UCI Machine Learning Repository: Waveform Database Generator (Version 1) Data Set: Support</title>\n\n<!-- Stylesheet link -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"../assets/ml.css\" />\n\n<script language=\"JavaScript\" type=\"text/javascript\">\n<!--\nfunction checkform ( form )\n{\n  // see http://www.thesitewizard.com/archive/validation.shtml\n  // for an explanation of this script and how to use it on your\n  // own website\n\n  // ** START **\n  if (form.q.value == \"\")\n  {\n    alert( \"Please enter search terms.\" );\n    form.q.focus();\n    return false ;\n  }\n\n  if (getCheckedValue(form.sitesearch) == \"ics.uci.edu\" && form.q.value.indexOf(\"site:archive.ics.uci.edu/ml\") == -1)\n  {\n    form.q.value = form.q.value + \" site:archive.ics.uci.edu/ml\";\n  }\n\n  // ** END **\n  return true ;\n}\n\n// return the value of the radio button that is checked\n// return an empty string if none are checked, or\n// there are no radio buttons\nfunction getCheckedValue(radioObj) {\n\tif(!radioObj)\n\t\treturn \"\";\n\tvar radioLength = radioObj.length;\n\tif(radioLength == undefined)\n\t\tif(radioObj.checked)\n\t\t\treturn radioObj.value;\n\t\telse\n\t\t\treturn \"\";\n\tfor(var i = 0; i < radioLength; i++) {\n\t\tif(radioObj[i].checked) {\n\t\t\treturn radioObj[i].value;\n\t\t}\n\t}\n\treturn \"\";\n}\n//-->\n</script>\n\n</head>\n\n<body>\n\n\n<!-- SITE HEADER (INCLUDES LOGO AND SEARCH BOX) -->\n\n<table width=100% bgcolor=\"#003366\">\n<tr>\n\t<td>\n\t\t<span class=\"normal\"><a href=\"../index.html\" \nalt=\"Home\"><img src=\"../assets/logo.gif\" \nborder=0></img></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"http://cml.ics.uci.edu\"><font color=\"FFDD33\">Center for Machine Learning and Intelligent Systems</font></a></span>\n\t</td>\n\t<td width=100% valign=top align=\"right\">\n\t\t<span class=\"whitetext\">\n\t\t<a href=\"../about.html\">About</a>&nbsp;\n\t\t<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;\n\t\t<a href=\"../donation_policy.html\">Donate a Data Set</a>&nbsp;\n\t\t<a href=\"../contact.html\">Contact</a>\n\t\t</span>\n\n\t\t<br>\n\t\t<br>\n\t\t<!-- Search Google -->\n\n\t\t<FORM method=GET action=http://www.google.com/custom onsubmit=\"return checkform(this);\">\n\t\t<INPUT TYPE=text name=q size=30 maxlength=255 value=\"\">\n\t\t<INPUT type=submit name=sa VALUE=\"Search\">\n\t\t<INPUT type=hidden name=cof VALUE=\"AH:center;LH:130;L:http://archive.ics.uci.edu/assets/logo.gif;LW:384;AWFID:869c0b2eaa8d518e;\">\n\t\t<input type=hidden name=domains value=\"ics.uci.edu\">\n\t\t<br>\n\t\t<input type=radio name=sitesearch value=\"ics.uci.edu\" checked> <span class=\"whitetext\"><font size=\"1\">Repository</font></span>\n\t\t<input type=radio name=sitesearch value=\"\"> <span class=\"whitetext\"><font size=\"1\">Web</font></span>\n\t\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n\t\t<A HREF=http://www.google.com/search><IMG SRC=http://www.google.com/logos/Logo_25blk.gif border=0 ALT=Google align=middle height=27></A>\n\t\t<br>\n\t\t</FORM>\n\t\t<!-- Search Google -->\n\n\n\t\t<span class=\"whitetext\"><a href=\"../datasets.php\"><font size=\"3\" color=\"#FFDD33\"><b>View ALL Data Sets</b></font></a></span>\n\t\t<br>\n\t</td>\n</tr>\n</table>\n\n<br />\n<table width=100% border=0 cellpadding=2><tr><td>\n\n\n   <table><tr>\n     <td valign=top>\n\t<p>\n\t<span class=\"heading\"><b>Waveform Database Generator (Version 1) Data Set</b></span>\n\n\t\t\n\t\t\t\t<p class=\"normal\">Below are papers that cite this data set, with context shown.\n\t\tPapers were automatically harvested and associated with this data set, in collaboration with <a href=\"http://rexa.info\">Rexa.info</a>.</p>\n\t\t<img src=\"../assets/rexa.jpg\" />\n\t\t<p class=\"normal\"><a href=\"/ml/datasets/Waveform+Database+Generator+(Version+1)\">Return to Waveform Database Generator (Version 1) data set page</a>.\n\t\t<hr><p class=\"normal\"><a name=\"2048a44e66eac923f00c225ee12e17c38d923376\"></a><i>Giorgio Valentini. <a href=\"http://rexa.info/paper/2048a44e66eac923f00c225ee12e17c38d923376\">Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis</a>. Multiple Classifier Systems. 2004. </i><br><br>software library [13] and the SVMlight applications [9]. 4.2 Results In particular we analyzed the relationships of the components of the error with the kernels and kernel parameters, using data sets from UCI [14]  <b>Waveform</b>  Grey-Landsat, Letter-Two, Letter-Two with added noise, Spam, Musk) and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of<br></p><hr><p class=\"normal\"><a name=\"aa89144bb32b6cbc2ad3000adba9cefe83bf54d6\"></a><i>Zhi-Hua Zhou and W-D Wei and Gang Li and Honghua Dai. <a href=\"http://rexa.info/paper/aa89144bb32b6cbc2ad3000adba9cefe83bf54d6\">On the Size of Training Set and the Benefit from Ensemble</a>. PAKDD. 2004. </i><br><br>3,772 22 7 2 kr-vs-kp 3,196 36 0 2 led7 2,000 7 0 10 led24 2,000 24 0 10 sat 6,435 0 36 6 segment 2,310 0 19 7 sick 3,772 22 7 2 sick-euthyroid 3,156 22 7 2 <b>waveform</b> 5,000 0 21 3 Each original data set is partitioned into ten subsets with similar distributions. At the first time, only one subset is used; at the second time, two subsets are used; and so on. The earlier generated data sets are<br></p><hr><p class=\"normal\"><a name=\"7025b49e49b58acbb1881ee289e9d719a528d505\"></a><i>Giorgio Valentini and Thomas G. Dietterich. <a href=\"http://rexa.info/paper/7025b49e49b58acbb1881ee289e9d719a528d505\">Low Bias Bagged Support Vector Machines</a>. ICML. 2003. </i><br><br>P2 Polyn. 0.1687 0.1863 0.1892 4-1-0 4-1-0 1-4-0 Gauss. 0.1429 0.1534 0.1605 4-1-0 5-0-0 3-2-0 Data set <b>Waveform</b> Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey-Landsat Linear 0.0508 0.0510 0.0601<br></p><hr><p class=\"normal\"><a name=\"c4ef47232a73d5e27b14e63096634eb70dcbddce\"></a><i>Joao Gama and Ricardo Rocha and Pedro Medas. <a href=\"http://rexa.info/paper/c4ef47232a73d5e27b14e63096634eb70dcbddce\">Accurate decision trees for mining high-speed data streams</a>. KDD. 2003. </i><br><br>when classifying test examples: classifying using the majority class (VFDTcMC) and classifying using naive Bayes (VFDTcNB) at leaves. The experimental work has been done using the <b>Waveform</b> and LED datasets. These are well known artificial datasets. We have used the two versions of the Waveform dataset available at the UCI repository [1]. Both versions are problems with three classes. The first<br></p><hr><p class=\"normal\"><a name=\"7ee121b26e79c0a7aa5c80979e275fc3592d0638\"></a><i>Giorgio Valentini. <a href=\"http://rexa.info/paper/7ee121b26e79c0a7aa5c80979e275fc3592d0638\">Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003</a>. Dipartimento di Informatica e Scienze dell'Informazione . 2003. </i><br><br>polynomial degrees: (a) degree = 2, (b) degree = 3, (c) degree = 5, (d) degree = 10 . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.17 Bias in polynomial SVMs with (a) <b>Waveform</b> and (b) Spam data sets, varying both C and polynomial degree. . . . . . . . . . . . . . . . . . . . . . . 68 4.18 Bias-variance decomposition of error in bias, net variance, unbiased and biased variance in polynomial<br></p><hr><p class=\"normal\"><a name=\"ce55aa21eaf8e5fc2b67e9ef0b031b5c32af6b41\"></a><i>Eibe Frank and Mark Hall and Bernhard Pfahringer. <a href=\"http://rexa.info/paper/ce55aa21eaf8e5fc2b67e9ef0b031b5c32af6b41\">Locally Weighted Naive Bayes</a>. UAI. 2003. </i><br><br>3772 6.0 23 6 2 sonar 208 0.0 60 0 2 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vote 435 5.6 0 16 2 vowel 990 0.0 10 3 11 <b>waveform</b> 5000 0.0 40 0 3 zoo 101 0.0 1 15 7 19 datasets for k = 5 and k = 10 respectively. When distance weighting is used with k-nearest neighbours, our method is significantly more accurate on 13 and 17 datasets for k = 5 and k = 10 respectively.<br></p><hr><p class=\"normal\"><a name=\"7bea464dce753e6523458d22de98a96004c1aac8\"></a><i>James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao. <a href=\"http://rexa.info/paper/7bea464dce753e6523458d22de98a96004c1aac8\">Fast Algorithms for Mining Emerging Patterns</a>. PKDD. 2002. </i><br><br>using thresholds. We see that mining with a threshold value of 4 is substantially faster than mining the complete set of JEPs using a ratio tree. Classification accuracy is degraded for three of the datasets (Vehicle, <b>Waveform</b> and Letter-recognition) though. Analysis of the vehicle and chess datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is<br></p><hr><p class=\"normal\"><a name=\"6a6b4ca13137132e00185012fd7c4667ff964769\"></a><i>S. Sathiya Keerthi and Kaibo Duan and Shirish Krishnaj Shevade and Aun Neow Poo. <a href=\"http://rexa.info/paper/6a6b4ca13137132e00185012fd7c4667ff964769\">A Fast Dual Algorithm for Kernel Logistic Regression</a>. ICML. 2002. </i><br><br>software at the site http://www.ece.nwu.edu/~nocedal/lbfgs.html was used. The Gaussian kernel K(x; # x) = exp( kx # xk 2 2# 2 ) was used. In all the experiments, # was set to 10 6 . Five benchmark datasets were used: Banana, Image, Splice, <b>Waveform</b> and Tree. The Tree dataset was originally used in (Bailey, Pettit, Borocho#, Manry, and Jiang, 1993). Detailed information about the remaining datasets<br></p><hr><p class=\"normal\"><a name=\"bcf7a3acb12b41b9b37ec937c2c9aea2d1d1c855\"></a><i>Juan J Rodr\u00edguez Diez and Carlos Alonso Gonz\u00e1lez and Henrik Bostr\u00f6m. <a href=\"http://rexa.info/paper/bcf7a3acb12b41b9b37ec937c2c9aea2d1d1c855\">Learning First Order Logic Time Series Classifiers: Rules and Boosting</a>. PKDD. 2000. </i><br><br>x k(t). Figure 2.b shows two examples of each class. The data used were obtained from the UCI KDD Archive [4]. It contains 100 examples of each class, with 60 points in each example. <b>Waveform</b>  This dataset was introduced by [9]. The purpouse is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i) x 2<br></p><hr><p class=\"normal\"><a name=\"179e206cfdc14b74819ce95f39bb68610874578b\"></a><i>Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom. <a href=\"http://rexa.info/paper/179e206cfdc14b74819ce95f39bb68610874578b\">Boosting Interval Based Literals</a>. 2000. </i><br><br>iterations, and a comparison of the results for settings 2{5 (combinations of interval literals) against the results for setting 1 (point based literals), using the McNemar's test. 4.1 <b>Waveform</b> This data set was introduced by [BFOS93]. The purpose is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i)<br></p><hr><p class=\"normal\"><a name=\"339f8af547b087e9b52de872e8731ad168816065\"></a><i>Bede Liu and Mingzeng Hu and Wynne Hsu. <a href=\"http://rexa.info/paper/339f8af547b087e9b52de872e8731ad168816065\">Multi-level organization and summarization of the discovered rules</a>. KDD. 2000. </i><br><br>646 91 0.05 14 kdd 5914 68 0.22 15 mushroom 2398 57 0.08 16 pima 44 12 0.01 17 satimage 7515 191 0.30 18 splice 4302 100 0.15 19 tic-tac 266 11 0.02 20 <b>waveform</b> 1480 106 0.07 Average 1487.7 50 0.07 Dataset Table 1: Experiment results with decision trees No. of decision tree leaves No. of GSE tree leaves 105.2 38.1 doctors said that they could not obtain an overall picture of the domain from the<br></p><hr><p class=\"normal\"><a name=\"4eff6dda32898a8bdc8268d2a790a96b93f2e262\"></a><i>Thomas G. Dietterich. <a href=\"http://rexa.info/paper/4eff6dda32898a8bdc8268d2a790a96b93f2e262\">An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization</a>. Machine Learning, 40. 2000. </i><br><br>the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, vehicle, and <b>waveform</b> . These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition<br></p><hr><p class=\"normal\"><a name=\"43960812b02c29db1368816cd996112a9af54a99\"></a><i>Juan J. Rodr##guez and Carlos J. Alonso. <a href=\"http://rexa.info/paper/43960812b02c29db1368816cd996112a9af54a99\">Applying Boosting to Similarity Literals for Time Series Classification</a>. Department of Informatics University of Valladolid, Spain. 2000. </i><br><br>are sumarised in table 2. The main criterion for selecting them has been that the number of examples available were big enough, to ensure that the results were reliable. <b>Waveform</b>  This dataset was introduced by [Breiman et al., 1993]. The purpouse is to distinguish between three classes, defined by the evaluation in 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2<br></p><hr><p class=\"normal\"><a name=\"70172e511a3bc27c7927119a3b2a3405fbad99e0\"></a><i>Kai Ming Ting and Ian H. Witten. <a href=\"http://rexa.info/paper/70172e511a3bc27c7927119a3b2a3405fbad99e0\">Issues in Stacked Generalization</a>. J. Artif. Intell. Res. (JAIR, 10. 1999. </i><br><br>from the UCI Repository of machine learning databases (Blake, Keogh & Merz, 1998). Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> --each training dataset L of size 200 and 300, respectively, is generated using a different seed. The algorithms used for the experiments are then tested on a separate dataset<br></p><hr><p class=\"normal\"><a name=\"44326d77510f1b8976c3d73c224e9a1cb80ecc3c\"></a><i>Khaled A. Alsabti and Sanjay Ranka and Vineet Singh. <a href=\"http://rexa.info/paper/44326d77510f1b8976c3d73c224e9a1cb80ecc3c\">CLOUDS: A Decision Tree Classifier for Large Datasets</a>. KDD. 1998. </i><br><br>are taken from the STATLOG project, which has been a widely used benchmark in classification. 3 The Abalone,\"  <b>Waveform</b> \" and Isolet\" datasets can be found in [13]. The Synth1\" and Synth2\" datasets have been used in [15, 17] for evaluating SLIQ and SPRINT; they have been referred to as the Function2\" dataset. The main parameter of our<br></p><hr><p class=\"normal\"><a name=\"6a9871da32f3042a5de9082a36a11d5aecda6df8\"></a><i>Kai Ming Ting and Boon Toh Low. <a href=\"http://rexa.info/paper/6a9871da32f3042a5de9082a36a11d5aecda6df8\">Model Combination in the Multiple-Data-Batches Scenario</a>. ECML. 1997. </i><br><br>settings are the same as those used in IB1 3 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains (i.e., <b>waveform</b> and LED24) and four real-world datasets (i.e., euthyroid, nettalk(stress), splice junction and protein coding) obtained from the UCI repository of machine learning databases (Merz & Murphy, 1996). The two noisy artificial domains are<br></p><hr><p class=\"normal\"><a name=\"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f\"></a><i>Nir Friedman and Mois\u00e9s Goldszmidt. <a href=\"http://rexa.info/paper/4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f\">Discretizing Continuous Attributes While Learning Bayesian Networks</a>. ICML. 1996. </i><br><br>from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the \"shuttle-small\" and  <b>waveform</b> 21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These<br></p><hr><p class=\"normal\"><a name=\"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e\"></a><i>Ron Kohavi. <a href=\"http://rexa.info/paper/bf6cec50b7f7d48d105c8c649210cc3a42d3d71e\">Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid</a>. KDD. 1996. </i><br><br>segment 19 2,310 CV-10 shuttle 9 43,500 14,500 soybean-large 35 562 CV-10 tic-tac-toe 9 958 CV-10 vehicle 18 846 CV-10 vote 16 435 CV-10 vote1 15 435 CV-10 <b>waveform</b> 40 40 300 4,700 Table 1: The datasets used, the number of attributes, and the training/test-set sizes (CV-10 denotes 10-fold cross-validation was used). NBTree - C4.5 NBTree - NB tic-tac-toe chess letter vehicle vote monk1 segment<br></p><hr><p class=\"normal\"><a name=\"5d851945e80949d177fa69dc5d4799f6a0f9d9f7\"></a><i>Tapio Elomaa and Juho Rousu. <a href=\"http://rexa.info/paper/5d851945e80949d177fa69dc5d4799f6a0f9d9f7\">Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning</a>. ESPRIT Working Group in Neural and Computational Learning. 1996. </i><br><br>preprocessing time dominates the total running time, with the exception of the <b>Waveform</b> data set, which consists of truly continuos-valued attributes: Each attribute has over 500 different values in the data and almost as many boundary points. In comparison, the Shuttle domain has on average<br></p><hr><p class=\"normal\"><a name=\"1c251864a7292b2f635e211e0027653df4b382a2\"></a><i>Dietrich Wettschereck and David W. Aha. <a href=\"http://rexa.info/paper/1c251864a7292b2f635e211e0027653df4b382a2\">Weighting Features</a>. ICCBR. 1995. </i><br><br>Feature Weights Computed by MI in the <b>Waveform</b> 19 Task Table 5. Average Accuracies on the Waveform Tasks Relative to k-NN Feature Weight Learning Algorithm Training Feedback Method Ignorant Method Dataset Size k-NN Relief-F k-NNV SM MI Waveform 100 77.0Sigma1.0 79.1 77.2 78.0 300 82.1Sigma0.9 82.4 81.6 82.6 Waveform+19 100 73.4Sigma1.0 78.4 76.7 78.6 300 81.3Sigma0.9 83.0 82.5 82.3 features to<br></p><hr><p class=\"normal\"><a name=\"49c65a0dfe258129414e27f6a3b7c42b7fbe426e\"></a><i>Kai Ming Ting and Boon Toh Low. <a href=\"http://rexa.info/paper/49c65a0dfe258129414e27f6a3b7c42b7fbe426e\">Theory Combination: an alternative to Data Combination</a>. University of Waikato. </i><br><br>why theory combination can not outperform data combination in this region. Note that the behaviour of the oracle is different from the usual learning curve in the <b>waveform</b> and protein coding datasets when NB* is employed. This indicates that some learning algorithm can specialise in different regions of the description space when the sizes of the data batches are relatively small. This can<br></p><hr><p class=\"normal\"><a name=\"19d5e6db33eb1a6c31be626538ea656f79e901b1\"></a><i>Matthias Scherf and W. Brauer. <a href=\"http://rexa.info/paper/19d5e6db33eb1a6c31be626538ea656f79e901b1\">Feature Selection by Means of a Feature Weighting Approach</a>. GSF - National Research Center for Environment and Health. </i><br><br>and compare the results of EUBAFES and RELIEF-F. <b>Waveform</b> 40 The Waveform-40 data set was introduced in [3] and applied in [25] to examine how well a feature selection algorithm works in the presence of a high number of irrelevant features. The data set enfolds 300 instances with<br></p><hr><p class=\"normal\"><a name=\"c042581c25e66281bb5ce382f70738b0233e5f5a\"></a><i>Zhi-Hua Zhou and Xu-Ying Liu. <a href=\"http://rexa.info/paper/c042581c25e66281bb5ce382f70738b0233e5f5a\">Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem</a>. </i><br><br>(b) Type (c) j j j 1 2 3 1 2 3 1 2 3 1 0 1 8 1 0 3 3 1 0 3 6 i 2 1 0 9 2 1 0 1 2 3 0 1 3 1 1 0 3 6 6 0 3 4 5 0 Under each type of cost matrix, 10 times 10-fold cross validation are performed on each data set except on <b>waveform</b> where randomly generated training data size of 300 and test data size of 5000 are used in 100 trials, which is the way this data set has been used in some other cost-sensitive<br></p><hr><p class=\"normal\"><a name=\"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7\"></a><i>Giorgio Valentini. <a href=\"http://rexa.info/paper/77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7\">An experimental bias--variance analysis of SVM ensembles based on resampling techniques</a>. </i><br><br>each region is delimited by one or more of four simple polynomial and trigonometric functions 2 . The synthetic data set <b>Waveform</b> is generated from a combination of 2 of 3 \"base\" waves; we reduced the original three classes of Waveform to two, deleting all samples pertaining to class 0. The other data sets are all<br></p><hr><p class=\"normal\"><a name=\"8f88ed1098d62276d3e69961c8c154b13865c989\"></a><i>Juan J. Rodr and guez Diez and Carlos J. Alonso. <a href=\"http://rexa.info/paper/8f88ed1098d62276d3e69961c8c154b13865c989\">Learning Classification RBF Networks by Boosting</a>. Lenguajes y Sistemas Inform#aticos. </i><br><br>are summarized in table 1. The data sets <b>waveform</b>  waveform with noise [5, 6], CBF (cylinder, bell and funnel) [19] and control charts [1, 3] were already used in our work on boosting distance literals [18]. Auslan is the Australian sign<br></p><hr><p class=\"normal\"><a name=\"a210d2418e04c74da13adc0356b79daa197b9d89\"></a><i>Zoran Obradovic and Slobodan Vucetic. <a href=\"http://rexa.info/paper/a210d2418e04c74da13adc0356b79daa197b9d89\">Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples</a>. Center for Information Science and Technology Temple University. </i><br><br>algorithm on synthetic data of various statistical properties showed that it accurately estimates the class probabilities on unlabeled data [73]. In another experiment on a 3-class benchmark dataset called <b>Waveform</b> [7] the SL was constructed with balanced classes, while we experimented with different class distributions p j on SU . It was shown that for balanced classes p = [1=3; 1=3; 1=3] the<br></p><hr><p class=\"normal\"><a name=\"6046926fc134386ba1ce22047b0146525a5440b7\"></a><i>Carlos J. Alonso Gonzalez and Juan J. Rodr and iguez Diez. <a href=\"http://rexa.info/paper/6046926fc134386ba1ce22047b0146525a5440b7\">Time Series Classification by Boosting Interval Based Literals</a>. Grupo de Sistemas Inteligentes Departamento de Informatica Universidad de Valladolid. </i><br><br>after this time. t 3 is in [n/3, 2n/3]. 6. Downward: y(t) = m+ rs - kx. Figure 3.b shows two examples of three of the classes. The data used was obtained from the UCI KDD Archive [4]. <b>Waveform</b>  This dataset was introduced by [7]. The purpouse is to distinguish between three classes, defined by the evaluation in 1, 2 . . . 21, of the following functions: x 1 (i) = uh 1 (i) + (1x 2 (i) = uh 1 (i) + (1x 3<br></p><hr><p class=\"normal\"><a name=\"9755b662d1579511b54a4b2c69eea04c30c2142c\"></a><i>Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom. <a href=\"http://rexa.info/paper/9755b662d1579511b54a4b2c69eea04c30c2142c\">Learning First Order Logic Time Series Classifiers: Rules and Boosting</a>. Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain. </i><br><br>[1]. Figure 2.b shows some examples of three of the classes. The data used were obtained from the UCI KDD Archive [4]. The number of examples is 600, with 60 points in each series. { <b>Waveform</b>  This dataset was introduced by [9]. We used the version from the UCI ML Repository [7]. The number of examples is 900 and the number of points in each series is 21. { Wave + Noise. This dataset was generated in<br></p><hr><p class=\"normal\"><a name=\"939595ca638eb3390e9bb9c4e6cc1352163cbf18\"></a><i>Kai Ming Ting and Ian H. Witten. <a href=\"http://rexa.info/paper/939595ca638eb3390e9bb9c4e6cc1352163cbf18\">Stacked Generalization: when does it work</a>. Department of Computer Science University of Waikato. </i><br><br>from the UCI Repository of machine learning databases [Merz and Murphy, 1996]. Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> -- each training dataset L is generated using a different Table 1: Details of the datasets used in the experiment. Datasets # Samples # Classes # Attr & Type Led24 200--5000 10<br></p><hr><p class=\"normal\"><a name=\"51d0dcaced0a60e995d1d228b618f89c00aab45b\"></a><i>Amund Tveit. <a href=\"http://rexa.info/paper/51d0dcaced0a60e995d1d228b618f89c00aab45b\">Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers</a>. Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology. </i><br><br>As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the <b>Waveform</b> and Image Segment datasets. For the Letter Recognition dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest<br></p><hr><p class=\"normal\"><a name=\"7286601416a1cec780621a415323a2bc6c958e11\"></a><i>Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit. <a href=\"http://rexa.info/paper/7286601416a1cec780621a415323a2bc6c958e11\">Information Theoretic Feature Crediting in Multiclass Support Vector Machines</a>. </i><br><br>include : a synthetic dataset that we constructed, LED-24, <b>Waveform</b> 40, DNA, Vehicle, and Satellite Images (SAT) drawn from the UCI repository [19]; and three datasets which are a subset of the Reuters document collection [20].<br></p><hr><p class=\"normal\"><a name=\"4ccb84298ff6f0a62f8263c57259cc114cb1b328\"></a><i>Mohammed Waleed Kadous. <a href=\"http://rexa.info/paper/4ccb84298ff6f0a62f8263c57259cc114cb1b328\">Expanding the Scope of Concept Learning Using Metafeatures</a>. School of Computer Science and Engineering, University of New South Wales. </i><br><br>are: arrythmia, audiology, bach chorales, echocardiogram, isolet, mobile robots, <b>waveform</b>  unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the search such as refinement rules (Cohen, 1995). Furthermore, their most powerful feature { the use of relations { is rarely<br></p><hr><p class=\"normal\"><a name=\"208ec47695794498051a25cd425bc385a9d19602\"></a><i>Thomas T. Osugi and M. S. <a href=\"http://rexa.info/paper/208ec47695794498051a25cd425bc385a9d19602\">EXPLORATION-BASED ACTIVE MACHINE LEARNING</a>. Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements. </i><br><br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Artificial Exploration Benchmark . . . . . . . . . . . . . . . . . . . . 30 6 Conclusions and Future Work 40 Bibliography 43 iii A Dataset Descriptions 46 A.1 <b>Waveform</b> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 A.2 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 A.3 Image<br></p><hr><p class=\"normal\"><a name=\"b43d0101709b7b7b7b3fdd9fbd2efb9850d65a99\"></a><i>Pierre Geurts. <a href=\"http://rexa.info/paper/b43d0101709b7b7b7b3fdd9fbd2efb9850d65a99\">Extremely randomized trees</a>. Technical report June 2003 University of Li#ege Department of Electrical Engineering and Computer Science Institut Monte#ore. </i><br><br>Dataset Nb. attributs Nb. classes #LS #TS <b>Waveform</b> 21 3 4000 1000 Two-norm 20 2 8000 2000 Satellite 36 6 4435 2000 Pendigits 16 10 7494 3498 Dig44 16 10 14000 4000 Letter 16 26 16000 4000 Isolet 617 26 6238<br></p><hr><p class=\"normal\"><a name=\"b9b24d88e45ac7034e22363aac1347ca65caffc6\"></a><i>I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra. <a href=\"http://rexa.info/paper/b9b24d88e45ac7034e22363aac1347ca65caffc6\">Feature Subset Selection by Bayesian networks based optimization</a>. Dept. of Computer Science and Artificial Intelligence. University of the Basque Country. </i><br><br>principal reason of `overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of <b>Waveform</b> 40 dataset [15] with Naive-Bayes classification algorithm [19]: training sizes of 100; 200; 400; 800 and 1; 600 samples and tested over a fixed test set with 3; 200 instances. Figure 7 summarizes the set of<br></p>\n\n\n\t</td></tr></table>\n\n\n\n<hr>\n\n<p class=\"normal\"><a href=\"/datasets/Waveform+Database+Generator+(Version+1)\">Return to Waveform Database Generator (Version 1) data set page</a>.\n\n\n<table cellpadding=5 align=center><tr valign=center>\n\t\t<td><p class=\"normal\">Supported By:</p></td>\n        <td><img src=\"../assets/nsfe.gif\" height=60 /> </td>\n        <td><p class=\"normal\">&nbsp;In Collaboration With:</p></td>\n        <td><img src=\"../assets/rexaSmall.jpg\" /></td>\n</tr></table>\n\n<center>\n<span class=\"normal\">\n<a href=\"../about.html\">About</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../citation_policy.html\">Citation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../donation_policy.html\">Donation Policy</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"../contact.html\">Contact</a>&nbsp;&nbsp;||&nbsp;\n<a href=\"http://cml.ics.uci.edu\">CML</a>\n</span>\n</center>\n\n\n\n\n</body>\n</html>\n", "encoding": "ISO-8859-1"}