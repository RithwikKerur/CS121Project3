{"url": "https://cbcl.ics.uci.edu/public_data/DANN/readme", "content": "If you have any questions, e-mail Daniel Quang (dxquang@uci.edu).\n\nLICENSING\n=========\nBoth DANN and pre-computer DANN scores are under the MIT license.\n\nPre-computed files\n==================\nIn the data folder, there is file called DANN_whole_genome_SNVs.tsv.bgz. Its tabix index is also available. It follows the same format as the whole_genome_SNVs.tsv.gz file on the CADD website, annotating all possible SNVs throughout the genome. If you want other sets of pre-computed scores, our friends at Cold Spring Harbor have graciously hosted our scores for us: http://genome-mirror.cshl.edu/cgi-bin/hgTrackUi?g=disc\n\nYou can also look up DANN scores on VarSome: http://www.varsome.com\n\nAt the suggestion of our Cold Spring Harbor friends, we have also stored the SNP scores in DANN_whole_genome_SNVs.bed.starch. This is a starch file that is much smaller.\n\nInstallation\n============\nRequired\n--------\nYou need the following programs and packages. Some of them can be very difficult to install. We recommend a Linux setup. Our own clusters were running CentOS 6.4 with an SGE. \n\n* [Python 2.7.8](https://www.python.org/download/releases/2.7.8/). Many of the scripts and packages require the latest version of Python.\n* [svmlight-loader](https://github.com/mblondel/svmlight-loader). For converting svmlight files to sparse CSR matrices very efficiently.\n* [numpy and scipy](http://www.numpy.org/). Common Python package.\n* [scikit-learn](http://scikit-learn.org/stable/). Contains many machine learning libraries.\n* [deepnet](https://github.com/nitishsrivastava/deepnet). CUDA deep learning package.\n* [LIBOCAS](http://cmp.felk.cvut.cz/~xfrancv/ocas/html/). For SVM training.\n\nData files\n==========\nAll training and testing files are located in the data folder. All command lines in this README will assume that you all relevant executable files are located in your bin, your current working directory contains the data files, and all .gz files have been decompressed using the gzip -d command.\n\nThe original CADD training and testing data can be downloaded from [here](http://krishna.gs.washington.edu/martin/download/), which has been graciously provided by Martin Kircher.\n\nThere are four different datasets: training, validation, testing, and ClinVar_ESP. The models are trained on variants in the training set. The validation set is used for hyperparameter tuning and for selecting the \u201cbest\u201d model which maximizes accuracy on this set. The testing set is an independent set the models are tested on in order to monitor for overfitting. The ClinVar_ESP dataset is also a testing set containing a set of \u201cgold standard\u201d pathogenic and benign variants. The ClinVar_ESP dataset is the same dataset used in the Kircher et al. paper.\n\n.svmlight files contain the features of the variants in that set in svmlight format. These files are meant to be used by the LIBOCAS program. LIBOCAS only uses 1-based indexing.\n\n.X.npz files contain the feature data from the svmlight files in sparse CSR format. The features are also normalized to have unit variance because the logistic regression and deep neural network models are sensitive to feature scaling. This file can also be derived from the respective .svmlight file, but we provide the feature scaled .npz files as well for convenience.\n\n.y.npz files contain the labels from the svmlight files. -1 labels are changed to 0. Again, these files can be derived from the respective svmlight files, but we provide these files for convenience.\n\nModel training and prediction\n=============================\nLIBOCAS\n\u2014\u2014\u2014\u2014\u2014\u2014-\nThe following command line closely replicates CADD\u2019s training. The parameter C is is set to 0.0025, verbosity is set to 1, 20 cores are used in parallel, and the model is saved to cadd_svmocas.model.\n```\nsvmocas -c 0.0025 -v 1 -p 20 training.svmlight cadd_svmocas.model\n```\n\nThe following command lines will calculate scores for variants in the validation, testing, and ClinVar_ESP sets. You can use these outputs to do AUC analyses. Keep in mind that many of the variants in ClinVar_ESP have multiple gene annotations, which will skew the AUC analysis. Shoot us an e-mail if you are really serious about doing proper AUC analysis.\n\n```\nlinclassif -t 1 -e -o svm_ocas_validation.pred validation.svmlight cadd_svmocas.model \n\nlinclassif -t 1 -e -o svm_ocas_testing.pred testing.svmlight cadd_svmocas.model \n\nlinclassif -t 1 -e -o svm_ocas_ClinVar_ESP.pred ClinVar_ESP.svmlight cadd_svmocas.model \n```\nLogistic regression\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nWe rolled up the model training and model prediction into one python package. Unfortunately, when it evaluates the ClinVar and ESP dataset, it does not filter the data. Many of the variants have multiple gene annotations. If you really to do some proper AUC analysis, shoot us an e-mail. You also need to go into the logistic_regression folder and modify the lgtc_sgd.py script so that cadd_dir and ClinVar_ESP_dir are pointing to the correct directories where the files are located.\n\n```\npython lgtc_sgd.py\n```\ndeepnet\n\u2014\u2014\u2014\u2014\u2014\u2014-\ndeepnet uses Google protocol buffer files to set up the model architecture and establish file links. You have to change the file paths in all pbtxt files in the deepnet folder. Hopefully it is obvious enough based on the file names which ones to change.\n\nIn the following command line, trainer.py is in the deepnet package. You will need to find the file path to it. Training can take about 24 hours. We included the BEST trained models in the deepnet folder in case you want to skip the training.\n\n```\npython /path/to/trainer.py model.pbtxt train.pbtxt eval.pbtxt\n```\n\nFor calculating the prediction scores which you can use for AUC analyses, run the following command lines. This will get you numpy files containing the scores for the validation, testing, and ClinVar_ESP (change the data_proto line in train.pbtxt to the cadd_rep_dq2.pbtxt file sets. You will need the file path to the extract_neural_net_representation.py script from the deepnet package.\n\n```\npython /path/to/extract_neural_net_representation.py cadd_3layer_tanh_BEST train.pbtxt outputdir hidden1 hidden2 hidden3 output_layer\n```\n\n\n\n", "encoding": "utf-8"}