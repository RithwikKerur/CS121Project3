{"url": "https://cml.ics.uci.edu/category/aiml/", "content": "<!DOCTYPE html>\n<html lang=\"en-US\">\n<head>\n<meta charset=\"UTF-8\" />\n<meta name=\"viewport\" content=\"width=device-width\" />\n<title>AIML | Center for Machine Learning and Intelligent Systems</title>\n<link rel=\"profile\" href=\"http://gmpg.org/xfn/11\" />\n<link rel=\"pingback\" href=\"https://cml.ics.uci.edu/xmlrpc.php\" />\n<!--[if lt IE 9]>\n<script src=\"https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/html5.js\" type=\"text/javascript\"></script>\n<![endif]-->\n\n<link rel='dns-prefetch' href='//s.w.org' />\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Center for Machine Learning and Intelligent Systems &raquo; Feed\" href=\"https://cml.ics.uci.edu/feed/\" />\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Center for Machine Learning and Intelligent Systems &raquo; Comments Feed\" href=\"https://cml.ics.uci.edu/comments/feed/\" />\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Center for Machine Learning and Intelligent Systems &raquo; AIML Category Feed\" href=\"https://cml.ics.uci.edu/category/aiml/feed/\" />\n\t\t<script type=\"text/javascript\">\n\t\t\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/12.0.0-1\\/72x72\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/12.0.0-1\\/svg\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"https:\\/\\/cml.ics.uci.edu\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=5.2.3\"}};\n\t\t\t!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline=\"top\",l.font=\"600 32px Arial\",a){case\"flag\":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case\"emoji\":return b=d([55357,56424,55356,57342,8205,55358,56605,8205,55357,56424,55356,57340],[55357,56424,55356,57342,8203,55358,56605,8203,55357,56424,55356,57340]),!b}return!1}function f(a){var c=b.createElement(\"script\");c.src=a,c.defer=c.type=\"text/javascript\",b.getElementsByTagName(\"head\")[0].appendChild(c)}var g,h,i,j,k=b.createElement(\"canvas\"),l=k.getContext&&k.getContext(\"2d\");for(j=Array(\"flag\",\"emoji\"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],\"flag\"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener(\"DOMContentLoaded\",h,!1),a.addEventListener(\"load\",h,!1)):(a.attachEvent(\"onload\",h),b.attachEvent(\"onreadystatechange\",function(){\"complete\"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);\n\t\t</script>\n\t\t<style type=\"text/css\">\nimg.wp-smiley,\nimg.emoji {\n\tdisplay: inline !important;\n\tborder: none !important;\n\tbox-shadow: none !important;\n\theight: 1em !important;\n\twidth: 1em !important;\n\tmargin: 0 .07em !important;\n\tvertical-align: -0.1em !important;\n\tbackground: none !important;\n\tpadding: 0 !important;\n}\n</style>\n\t<link rel='stylesheet' id='wp-block-library-css'  href='https://cml.ics.uci.edu/wp-includes/css/dist/block-library/style.min.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='bonpress-style-css'  href='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-cml/style.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='tipsy-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/tipsy.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='mts_wpshortcodes-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/wp-shortcode.css?ver=5.2.3' type='text/css' media='all' />\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery.js?ver=1.12.4-wp'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/jquery.tipsy.js?ver=5.2.3'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/wp-shortcode.js?ver=5.2.3'></script>\n<link rel='https://api.w.org/' href='https://cml.ics.uci.edu/wp-json/' />\n<link rel=\"EditURI\" type=\"application/rsd+xml\" title=\"RSD\" href=\"https://cml.ics.uci.edu/xmlrpc.php?rsd\" />\n<link rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\" href=\"https://cml.ics.uci.edu/wp-includes/wlwmanifest.xml\" /> \n<meta name=\"generator\" content=\"WordPress 5.2.3\" />\n</head>\n\n<body class=\"archive category category-aiml category-5 group-blog\">\n<div id=\"page\" class=\"hfeed site\">\n\t\t<header id=\"masthead\" class=\"all-header\" role=\"banner\">\n\t\t<hgroup class=\"hgroup-wide\">\n                        <a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>\n<!--\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2> -->\n\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">Bren School of Information and Computer Science</h2>\t\t\t\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2>\n\t\t\t<div style=\"clear:both\"></div>\n\t\t</hgroup>\n\t</header>\n\t<header id=\"masthead\" class=\"site-header\" role=\"banner\">\n\t\t<hgroup class=\"hgroup-img\">\n                        <a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>\n\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2>\n\t\t</hgroup>\n\n\t\t<nav id=\"site-navigation\" class=\"navigation-main\" role=\"navigation\">\n\t\t\t<h1 class=\"menu-toggle\">Menu</h1>\n\t\t\t<div class=\"screen-reader-text skip-link\"><a href=\"#content\" title=\"Skip to content\">Skip to content</a></div>\n\n\t\t\t<div class=\"menu-navigation-container\"><ul id=\"menu-navigation\" class=\"menu\"><li id=\"menu-item-234\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234\"><a href=\"https://cml.ics.uci.edu/\">Home</a></li>\n<li id=\"menu-item-79\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79\"><a href=\"https://cml.ics.uci.edu/home/about-us/\">About CML</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-78\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-78\"><a href=\"https://cml.ics.uci.edu/home/about-us/\">About us</a></li>\n\t<li id=\"menu-item-429\" class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429\"><a href=\"https://cml.ics.uci.edu/category/news/\">News</a></li>\n\t<li id=\"menu-item-76\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-76\"><a href=\"https://cml.ics.uci.edu/home/contact-us/\">Contact Us</a></li>\n</ul>\n</li>\n<li id=\"menu-item-539\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539\"><a>People</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-55\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-55\"><a href=\"https://cml.ics.uci.edu/faculty/\">Faculty</a></li>\n\t<li id=\"menu-item-220\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-220\"><a href=\"https://cml.ics.uci.edu/alumni/\">Alumni</a></li>\n</ul>\n</li>\n<li id=\"menu-item-75\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-75\"><a href=\"https://cml.ics.uci.edu/aiml/\">Events &#038; Seminars</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-74\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-74\"><a href=\"https://cml.ics.uci.edu/aiml/\">AI/ML Seminar Series</a></li>\n\t<li id=\"menu-item-914\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-914\"><a href=\"https://cml.ics.uci.edu/aiml/ml-distinguished-speakers/\">ML Distinguished Speakers</a></li>\n\t<li id=\"menu-item-73\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-73\"><a href=\"https://cml.ics.uci.edu/aiml/ml-reading-group/\">ML Reading Group</a></li>\n</ul>\n</li>\n<li id=\"menu-item-222\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222\"><a>Education &#038; Resources</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-227\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-227\"><a href=\"https://cml.ics.uci.edu/courses/\">Courses</a></li>\n\t<li id=\"menu-item-221\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-221\"><a href=\"https://cml.ics.uci.edu/books/\">Books</a></li>\n</ul>\n</li>\n<li id=\"menu-item-81\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-81\"><a href=\"http://www.ics.uci.edu/~mlearn/MLRepository.html\">UCI Machine Learning Archive</a></li>\n<li id=\"menu-item-87\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-87\"><a href=\"https://cml.ics.uci.edu/sponsors-funding/\">Sponsors &#038; Funding</a></li>\n<li id=\"menu-item-86\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-86\"><a href=\"https://cml.ics.uci.edu/subscribe/\">Subscribe to CML List</a></li>\n</ul></div>\t\t</nav><!-- #site-navigation -->\n\t</header><!-- #masthead -->\n\n\n\t<section id=\"primary\" class=\"content-area\">\n\t\t<div id=\"content\" class=\"site-content\" role=\"main\">\n\n\t\t\n\t\t\t<header class=\"page-header clear\">\n\t\t\t\t<h1 class=\"page-title\">\n\t\t\t\t\t<span>AIML</span>\t\t\t\t</h1>\n\t\t\t\t\t\t\t</header><!-- .page-header -->\n\n\t\t\t\t\t\t\n\t\t\t\t\r\n<article id=\"post-865\" class=\"post-865 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2019/09/fall-2019/\" title=\"Permalink to Fall 2019\" rel=\"bookmark\">Fall 2019</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t\n<table class=\"wp-block-table aligncenter\"  cellpadding=5 border=1>\n   <col width=\"100\"><col>\n<tbody>\n\n  <tr>\n  <td class='aiml-none'><div class=\"aiml-date\"><b>Sep 23</b></div></td>\n  <td class='aiml-none'><div class=\"aiml-name\"><b>No Seminar</b></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Sep 30</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"http://niadowell.com/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/IMG_8437.jpg\" width=200px /><br><b>Nia Dowell</b></a><br>Assistant Professor<br>School of Education<br>University of California, Irvine</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Group Communication Analysis: Applications for Online Learning Environments</b></a></span></div><div class=\"togglec clearfix\">Educational environments have become increasingly reliant on computer-mediated communication, relying on video conferencing, synchronous chats, and asynchronous forums, in both small (5-20 learners) and massive (1000+ learner) learning environments. These platforms, which are designed to support or even supplant traditional instruction, have become common-place across all levels of education, and as a result created big data in education. In order to move forward, the learning sciences field is in need of new automated approaches that offer deeper insights into the dynamics of learner interaction and discourse across online learning platforms. This talk will present results from recent work that uses language and discourse to capture social and cognitive dynamics during collaborative interactions. I will introduce group communication analysis (GCA), a novel approach for detecting emergent learner roles from the participants\u2019 contributions and patterns of interaction. This method makes use of automated computational linguistic analysis of the sequential interactions of participants in online group communication to create distinct interaction profiles. We have applied the GCA to several collaborative learning datasets. Cluster analysis, predictive, and hierarchical linear mixed-effects modeling\nwere used to assess the validity of the GCA approach, and practical influence of learner roles on student and overall group performance. The results indicate that learners\u2019 patterns in linguistic coordination and cohesion are representative of the roles that individuals play in collaborative discussions. More broadly, GCA provides a framework for researchers to explore the micro intra- and inter-personal patterns associated with the participants\u2019 roles and the sociocognitive processes related to successful collaboration.\n<br><br>\n<b>Bio:</b> I am an assistant professor in the School of Education at UCI. My primary interests are in cognitive psychology, discourse processing, group interaction, and learning analytics. In general, my research focuses on using language and discourse to uncover the dynamics of socially significant, cognitive, and affective processes. I am currently applying computational techniques to model discourse and social dynamics in a variety of environments including small group computer-mediated collaborative learning environments, collaborative design networks, and massive open online courses (MOOCs). My research has also extended beyond the educational and learning sciences spaces and highlighted the practical applications of computational discourse science in the clinical, political and social sciences areas.</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Oct 7</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"https://www.ssriva.com\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/E7478E74-E693-4911-B5CF-61F1A7A87075.png\" width=200px /><br><b>Shashank Srivastava</b></a><br>Assistant Professor<br>Computer Science<br>UNC Chapel Hill</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Conversational Machine Learning</b></a></span></div><div class=\"togglec clearfix\">Humans can efficiently learn and communicate new knowledge about the world through natural language (e.g, the concept of important emails may be described through explanations like \u2018late night emails from my boss are usually important\u2019). Can machines be similarly taught new tasks and behavior through natural language interactions with their users? In this talk, we&#8217;ll explore two approaches towards language-based learning for classifications tasks. First, we&#8217;ll consider how language can be leveraged for interactive feature space construction for learning tasks. I&#8217;ll present a method that jointly learns to understand language and learn classification models, by using explanations in conjunction with a small number of labeled examples of the concept. Secondly, we&#8217;ll examine an approach for using language as a substitute for labeled supervision for training machine learning models, which leverages the semantics of quantifier expressions in everyday language (`definitely&#8217;, `sometimes&#8217;, etc.) to enable learning in scenarios with limited or no labeled data. \n<br><br>\n<b>Bio:</b> Shashank Srivastava is  an assistant professor in the Computer Science department at the University of North Carolina (UNC) Chapel Hill. Shashank received his PhD from the Machine Learning department at CMU in 2018, and was an AI Resident at Microsoft Research in 2018-19. Shashank&#8217;s research interests lie in conversational AI, interactive machine learning and grounded language understanding. Shashank has an undergraduate degree in Computer Science from IIT Kanpur, and a Master\u2019s degree in Language Technologies from CMU. He received the Yahoo InMind Fellowship for 2016-17; his research has been covered by popular media outlets including GeekWire and New Scientist.</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Oct 14</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"http://www.cs.cmu.edu/~bdhingra/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/55B05642-4A63-47D3-91DB-81154AC80E53.jpeg\" width=200px /><br><b>Bhuwan Dhingra</b></a><br>PhD Student<br>Language Technologies Institute<br>Carnegie Mellon University</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Text as a Virtual Knowledge Base</b></a></span></div><div class=\"togglec clearfix\">Structured Knowledge Bases (KBs) are extremely useful for applications such as question answering and dialog, but are difficult to populate and maintain. People prefer expressing information in natural language, and hence text corpora, such as Wikipedia, contain more detailed up-to-date information. This raises the question &#8212; can we directly treat text corpora as knowledge bases for extracting information on demand?\n<p>\nIn this talk I will focus on two problems related to this question. First, I will look at augmenting incomplete KBs with textual knowledge for question answering. I will describe a graph neural network model for processing heterogeneous data from the two sources. Next, I will describe a scalable approach for compositional reasoning over the contents of the text corpus, analogous to following a path of relations in a structured KB to answer multi-hop queries. I will conclude by discussing interesting future research directions in this domain.\n<br><br>\n<b>Bio:</b> Bhuwan Dhingra is a final year PhD student at Carnegie Mellon University, advised by William Cohen and Ruslan Salakhutdinov. His research uses natural language processing and machine learning to build an interface between AI applications and world knowledge (facts about people, places and things). His work is supported by the Siemens FutureMakers PhD fellowship. Prior to joining CMU, Bhuwan completed his undergraduate studies at IIT Kanpur in 2013, and spent two years at Qualcomm Research in the beautiful city of San Diego.</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Oct 21</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"https://robamler.github.io\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/robert-bamler.jpg\" width=200px/><br><b>Robert Bamler</b></a><br>Postdoctoral Researcher<br>Dept. of Computer Science<br>University of California, Irvine</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>TBA</b></a></span></div><div class=\"togglec clearfix\">TBA</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Oct 28</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"http://zhouyu.cs.ucdavis.edu/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/D4115C5B-2FC4-4B8C-A707-93940CC236DE.jpeg\" width=200px/><br><b>Zhou Yu</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>University of California, Davis</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>TBA</b></a></span></div><div class=\"togglec clearfix\">TBA</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Nov 4</b></div></td>\n  <td valign=top ><div class=\"aiml-name\"><b>&#8211;</b></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top  class='aiml-none'><div class=\"aiml-date\"><b>Nov 11</b></div></td>\n  <td valign=top  class='aiml-none'><div class=\"aiml-name\"><b>Veterans Day</b></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Nov 18</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"https://jthalloran.bitbucket.io/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/headShot.jpg\" width=200px/><br><b>John T. Halloran</b></a><br>Postdoctoral Researcher<br>Dept. of Biomedical Engineering<br>University of California, Davis</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>TBA</b></a></span></div><div class=\"togglec clearfix\">TBA</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Nov 25</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"https://www.cs.hmc.edu/~xanda/#/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/988856C9-3D84-4AD8-AFE6-1E00C483A0CE.png\" width=200px/><br><b>Xanda Schofield</b></a><br>Assistant Professor<br>Dept. of Computer Science<br>Harvey Mudd College</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>TBA</b></a></span></div><div class=\"togglec clearfix\">TBA</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top ><div class=\"aiml-date\"><b>Dec 2</b><br>4011<br>Bren Hall<br>1 pm</div></td>\n  <td valign=top ><div class=\"aiml-name\"><a href=\"https://faculty.sites.uci.edu/doroudis/\"><img src=\"http://cml.ics.uci.edu/wp-content/cml/uploads/F03258CF-B843-4597-BE1E-4119C502282A.png\" width=200px/><br><b>Shayan Doroudi</b></a><br>Assistant Professor<br>School of Education<br>University of California, Irvine</div><br> <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>TBA</b></a></span></div><div class=\"togglec clearfix\">TBA</div></div><div class=\"clear\"></div></div>\n  </td>\n  </tr>\n\n  <tr>\n  <td valign=top  class='aiml-none'><div class=\"aiml-date\"><b>Dec 9</b></div></td>\n  <td valign=top  class='aiml-none'><div class=\"aiml-name\"><b>Finals week</b></div>\n  </td>\n  </tr>\n\n</tbody></table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2019/09/fall-2019/\" title=\"5:19 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2019-09-05T17:19:13-07:00\">September 5, 2019</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-849\" class=\"post-849 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2019/04/spring-2019/\" title=\"Permalink to Spring 2019\" rel=\"bookmark\">Spring 2019</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t\n<!-- ========================================================== -->\n<!-- =====Spring Quarter ============================================== -->\n<!-- ========================================================== -->\n\n   <table cellpadding=5 border=1>\n   <col width=\"100\"><col>\n\n  <!-- ==== Apr 8 =================================== -->\n  <tr>\n  <td valign=top class='aiml-none'><div class=\"aiml-date\"><b>Apr 8</b></div></td>\n  <td valign=top class='aiml-none'><div class=\"aiml-name\"><b>No Seminar</b><br></div><br>\n  </td>\n  </tr>\n\n\n\n  <!-- ==== Apr 15 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>Apr 15</b><br>Bren Hall 4011<br>1 pm</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"http://research.dshin.org/\"><b>Daeyun Shin</b></a><br>PhD Candidate<br>Dept of Computer Science<br>UC Irvine</div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction</b></a></span></div><div class=\"togglec clearfix\">In this presentation, I will present our approach to the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image.  This challenging task requires inferring the shape of both visible and occluded surfaces.  Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion.  To improve the accuracy of view-centered representations for complex scenes, we introduce a novel \u201cEpipolar Feature Transformer\u201d that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry.  Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations.  We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.  <p><p>Project page: <a href=https://www.ics.uci.edu/~daeyuns/layered-epipolar-cnn/>https://www.ics.uci.edu/~daeyuns/layered-epipolar-cnn/</a></div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n  <!-- ==== Apr 22 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>Apr 22</b><br>Bren Hall 4011<br>1 pm</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"http://sites.uci.edu/pritchard\"><b>Mike Pritchard</b></a><br>Assistant Professor<br>Dept. of Earth System Sciences<br>University of California, Irvine</div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Improving global climate simulations using physically constrained deep learning emulators of unresolved moist turbulence processes</b></a></span></div><div class=\"togglec clearfix\">I will discuss machine-learning emulation of O(100M) cloud-resolving simulations of moist turbulence for use in multi-scale global climate simulation. First, I will present encouraging results from pilot tests on an idealized ocean-world, in which a fully connected deep neural network (DNN) is found to be capable of emulating explicit subgrid vertical heat and vapor transports across a globally diverse population of convective regimes. Next, I will demonstrate that O(10k) instances of the DNN emulator spanning the world are able to feed back realistically with a prognostic global host atmospheric model, producing viable ML-powered climate simulations that exhibit realistic space-time variability for convectively coupled weather dynamics and even some limited out-of-sample generalizability to new climate states beyond the training data\u2019s boundaries. I will then discuss a new prototype of the neural network under development that includes the ability to enforce multiple physical constraints within the DNN optimization process, which exhibits potential for further generalizability. Finally, I will conclude with some discussion of the unsolved technical issues and interesting philosophical tensions being raised in the climate modeling community by this disruptive but promising approach for next-generation global simulation.</div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n\n\n  <!-- ==== Apr 29 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>Apr 29</b><br>Bren Hall 4011<br>1 pm</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"\"><b>Nick Gallo</b></a><br>PhD Candidate<br>Department of Computer Science<br>University of California, Irvine</div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Coarse to Fine Lifted Inference</b></a></span></div><div class=\"togglec clearfix\">Large problems with repetitive sub-structure arise in many domains such as social network analysis, collective classification, and database entity resolution.  In these instances, individual data is augmented with a small set of rules that uniformly govern the relationship among groups of objects (for example: &#8220;the friend of my friend is probably my friend&#8221; in a social network).  Uncertainty is captured by a probabilistic graphical model structure.  While theoretically sound, standard reasoning techniques cannot be applied due to the massive size of the network (often millions of random variable and trillions of factors).  Previous work on lifted inference efficiently exploits symmetric structure in graphical models, but breaks down in the presence of unique individual data (contained in all real-world problems).  Current methods to address this problem are largely heuristic.  In this presentation we describe a coarse to fine approximate inference framework that initially treats all individuals identically, gradually relaxing this restriction to finer sub-groups.  This produces a sequence of inference objective bounds of monotonically increasing cost and accuracy.  We then discuss our work on incorporating high-order inference terms (over large subsets of variables) into lifted inference and ongoing challenges in this area.</div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n\n  <!-- ==== May 13 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>May 13</b><br>Bren Hall 4011<br>1 pm</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"https://allenai.org/team/mattg/\"><b>Matt Gardner</b></a><br>Senior Research Scientist<br>Allen Institute of Artificial Intelligence<br></div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Reasoning Our Way to Reading</b></a></span></div><div class=\"togglec clearfix\">Reading machines that truly understood what they read would change the world, but our current best reading systems struggle to understand text at anything more than a superficial level.  In this talk I try to reason out what it means to &#8220;read&#8221;, and how reasoning systems might help us get there.  I will introduce three reading comprehension datasets that require systems to reason at a deeper level about the text that they read, using numerical, coreferential, and implicative reasoning abilities.  I will also describe some early work on models that can perform these kinds of reasoning.  <p><p><b>Bio:</b> Matt is a senior research scientist at the Allen Institute for Artificial Intelligence (AI2) on the AllenNLP team, and a visiting scholar at UCI. His research focuses primarily on getting computers to read and answer questions, dealing both with open domain reading comprehension and with understanding question semantics in terms of some formal grounding (semantic parsing). He is particularly interested in cases where these two problems intersect, doing some kind of reasoning over open domain text. He is the original author of the AllenNLP toolkit for NLP research, and he co-hosts the NLP Highlights podcast with Waleed Ammar.</div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n  <!-- ==== May 27 =================================== -->\n  <tr>\n  <td valign=top class='aiml-none'><div class=\"aiml-date\"><b>May 27</b></div></td>\n  <td valign=top class='aiml-none'><div class=\"aiml-name\"><b>No Seminar (Memorial Day)</b><br></div><br>\n  </td>\n  </tr>\n\n\n  <!-- ==== June 3 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>June 3</b><br>Bren Hall 4011<br>12:00</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"www2.hawaii.edu/~psadow\"><b>Peter Sadowski</b></a><br>Assistant Professor<br>Information and Computer Sciences<br>University of Hawaii Manoa</div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Deep Learning for Extreme Remote Sensing: from Ocean Waves to Exocomets</b></a></span></div><div class=\"togglec clearfix\">New technologies for remote sensing and astronomy provide an unprecedented view of Earth, our Sun, and beyond. Traditional data-analysis pipelines in oceanography, atmospheric sciences, and astronomy struggle to take full advantage of the massive amounts of high-dimensional data now available. I will describe opportunities for using deep learning to process satellite and telescope data, and discuss recent work mapping extreme sea states using Satellite Aperture Radar (SAR), inferring the physics of our sun&#8217;s atmosphere, and detecting anomalous astrophysical events in other systems, such as comets transiting distant stars.<p><p><b>Bio:</b> Peter Sadowski is an Assistant Professor of Information and Computer Sciences at the University of Hawaii Manoa and Co-Director of the AI Precision Health Institute at the University of Hawaii Cancer Center. He completed his Ph.D. and Postdoc at University of California Irvine, and his undergraduate studies at  Caltech. His research focuses on deep learning and its applications to the natural sciences, particularly those at the intersection of machine learning and physics.</div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n\n\n  <!-- ==== June 3 =================================== -->\n  <tr>\n  <td valign=top><div class=\"aiml-date\"><b>June 3</b><br>Bren Hall 4011<br>1 pm</div></td>\n  <td valign=top><div class=\"aiml-name\"><a href=\"https://staff.fnwi.uva.nl/m.welling/\"><b>Max Welling</b></a><br>Research Chair, University of Amsterdam<br>VP Technologies, Qualcomm<br></div><br>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Integrating Generative Modeling into Deep Learning</b></a></span></div><div class=\"togglec clearfix\">Deep learning has boosted the performance of many applications tremendously, such as object classification and detection in images, speech recognition and understanding, machine translation, game play such as chess and go etc. However, these all constitute reasonably narrowly and well defined tasks for which it is reasonable to collect very large datasets. For artificial general intelligence (AGI) we will need to learn from a small number of samples, generalize to entirely new domains, and reason about a problem. What do we need in order to make progress to AGI? I will argue that we need to combine the data generating process, such as the physics of the domain and the causal relationships between objects, with the tools of deep learning. In this talk I will present a first attempt to integrate the theory of graphical models, which arguably was the dominating modeling machine learning paradigm around the turn of the twenty-first century, with deep learning. Graphical models express the relations between random variables in an interpretable way, while probabilistic inference in such networks can be used to reason about these variables. We will propose a new hybrid paradigm where probabilistic message passing in such networks is enhanced with graph convolutional neural networks to improve the ability of such systems to reason and make predictions.</div></div><div class=\"clear\"></div>\n  </td>\n  </tr>\n\n  <!-- ==== June 10 =================================== -->\n  <tr>\n  <td valign=top class='aiml-none'><div class=\"aiml-date\"><b>June 10</b></div></td>\n  <td valign=top class='aiml-none'><div class=\"aiml-name\"><b>No Seminar (Finals)</b><br></div><br>\n  </td>\n  </tr>\n\n\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2019/04/spring-2019/\" title=\"5:12 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2019-04-11T17:12:02-07:00\">April 11, 2019</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-811\" class=\"post-811 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2018/09/fall-2018/\" title=\"Permalink to Fall 2018\" rel=\"bookmark\">Fall 2018</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<p><!-- ========================================================== --><br />\n<!-- =====Fall Quarter ============================================== --><br />\n<!-- ========================================================== --></p>\n<table border=\"1\" cellpadding=\"5\"><!-- ==== Oct 1 =================================== --></p>\n<tbody>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Oct 1</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar</b></div>\n<p>&nbsp;</td>\n</tr>\n<p><!-- ==== Oct 8 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Oct 8</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a href=\"https://allenai.org/team/mattg/\"><b>Matt Gardner</b></a><br />\nResearch Scientist<br />\nAllen Institute for AI</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>A Tale of Two Question Answering Systems</b></a></span></div><div class=\"togglec clearfix\">The path to natural language understanding goes through increasingly challenging question answering tasks. I will present research that significantly improves performance on two such tasks: answering complex questions over tables, and open-domain factoid question answering. For answering complex questions, I will present a type-constrained encoder-decoder neural semantic parser that learns to map natural language questions to programs. For open-domain factoid QA, I will show that training paragraph-level QA systems to give calibrated confidence scores across paragraphs is crucial when the correct answer-containing paragraph is unknown. I will conclude with some thoughts about how to combine these two disparate QA paradigms, towards the goal of answering complex questions over open-domain text.</p>\n<p><b>Bio:</b>Matt Gardner is a research scientist at the Allen Institute for Artificial Intelligence (AI2), where he has been exploring various kinds of question answering systems. He is the lead designer and maintainer of the AllenNLP toolkit, a platform for doing NLP research on top of pytorch. Matt is also the co-host of the NLP Highlights podcast, where, with Waleed Ammar, he gets to interview the authors of interesting NLP papers about their work. Prior to joining AI2, Matt earned a PhD from Carnegie Mellon University, working with Tom Mitchell on the Never Ending Language Learning project.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Oct 15 =================================== --></p>\n<p><!-- ==== Oct 22 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Oct 22</b></div>\n<div>\n<div></div>\n</div>\n<div>Bren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a style=\"font-family: inherit; font-size: inherit;\" href=\"http://www.stephanmandt.com/\"><b>Stephan Mandt</b></a></div>\n</div>\n<div class=\"aiml-name\">\n<div class=\"aiml-name\">Assistant Professor<br />\nDept. of Computer Science<br />\nUC Irvine</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Deep Probabilistic Modeling</b></a></span></div><div class=\"togglec clearfix\">I will give an overview of some exciting recent developments in deep probabilistic modeling, which combines deep neural networks with probabilistic models for unsupervised learning. Deep probabilistic models are capable of synthesizing artificial data that highly resemble the training data, and are able fool both machine learning classifiers as well as humans. These models have numerous applications in creative tasks, such as voice, image, or video synthesis and manipulation. At the same time, combining neural networks with strong priors results in flexible yet highly interpretable models for finding hidden structure in large data sets. I will summarize my group\u2019s activities in this space, including measuring semantic shifts of individual words over hundreds of years, summarizing audience reactions to movies, and predicting the future evolution of video sequences with applications to neural video coding.</div></div><div class=\"clear\"></div>\n</div>\n</tr>\n<p><!-- ==== Oct 25 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Oct 25</b><br />\nBren Hall 3011<br />\n3 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\">\n<div class=\"aiml-name\">\n<div class=\"aiml-name\"></div>\n<div></div>\n</div>\n</div>\n<div class=\"aiml-name\">\n<div><strong>(Note: different day (Thurs), time (3pm), and location (3011) relative to usual Monday seminars)</p>\n<p></strong></div>\n<div><a href=\"http://pages.cs.wisc.edu/~swright/\"><b>Steven Wright</b></a><br />\nProfessor<br />\nDepartment of Computer Sciences<br />\nUniversity of Wisconsin, Madison</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Optimization in Data Science</b></a></span></div><div class=\"togglec clearfix\">Many of the computational problems that arise in data analysis and<br />\nmachine learning can be expressed mathematically as\u00a0<span class=\"m_-567682425052202021gmail-il\">optimization\u00a0</span>problems. Indeed, much new algorithmic research in\u00a0<span class=\"m_-567682425052202021gmail-il\">optimization</span>\u00a0is being driven by the need to solve large, complex problems from these areas. In this talk, we review a number of canonical problems in data analysis and their formulations as\u00a0<span class=\"m_-567682425052202021gmail-il\">optimization</span>\u00a0problems. We will cover support vector machines / kernel learning, logistic regression (including regularized and multiclass variants), matrix completion, deep learning, and several other paradigms.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Oct 29 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Oct 29</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a href=\"http://www.cs.cmu.edu/~cpsomas/\"><b>Alex Psomas</b></a><br />\nPostdoctoral Researcher<br />\nComputer Science Department<br />\nCarnegie Mellon University</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Fair Resource Allocation: From Theory to Practice</b></a></span></div><div class=\"togglec clearfix\">We study the problem of fairly allocating a set of indivisible items among $n$ agents. Typically, the literature has focused on one-shot algorithms. In this talk we depart from this paradigm and allow items to arrive online. When an item arrives we must immediately and irrevocably allocate it to an agent. A paradigmatic example is that of food banks: food donations arrive, and must be delivered to nonprofit organizations such as food pantries and soup kitchens. Items are often perishable, which is why allocation decisions must be made quickly, and donated items are typically leftovers, leading to lack of information about items that will arrive in the future. Which recipient should a new donation go to? We approach this problem from different angles.</p>\n<p>In the first part of the talk, we study the problem of minimizing the maximum envy between any two recipients, after all the goods have been allocated. We give a polynomial-time, deterministic and asymptotically optimal algorithm with vanishing envy, i.e. the maximum envy divided by the number of items T goes to zero as T goes to infinity. In the second part of the talk, we adopt and further develop an emerging paradigm called virtual democracy. We will take these ideas all the way to practice. In the last part of the talk I will present some results from an ongoing work on automating the decisions faced by a food bank called 412 Food Rescue, an organization in Pittsburgh that matches food donations with non-profit organizations.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Nov 5 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Nov 5</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a href=\"http://www.fredpark.com/\"><b>Fred Park</b></a><br />\nAssociate Professor<br />\nDept of Math &amp; Computer Science<br />\nWhittier College</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Image Segmentation and Tracking Utilizing a Difference of Convex Regularized Mumford-Shah Functional</b></a></span></div><div class=\"togglec clearfix\">In this talk I will give a brief overview of the segmentation and tracking problems and will propose a new model that tackles both of them. This model incorporates a weighted difference of anisotropic and isotropic total variation (TV) norms into a relaxed formulation of the Mumford-Shah (MS) model. We will show results exceeding those obtained by the MS model when using the standard TV norm to regularize partition boundaries. Examples illustrating the qualitative differences between the proposed model and the standard MS one will be shown as well. I will also talk about a fast numerical method that is used to optimize the proposed model utilizing the difference-of-convex algorithm (DCA) and the primal dual hybrid gradient (PDHG) method. Finally, future directions will be given that could harness the power of convolution nets for more advanced segmentation tasks.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Nov 12 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Nov 12</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (Veterans Day)</b></div>\n<p>&nbsp;</td>\n</tr>\n<p><!-- ==== Nov 19 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Nov 19</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a href=\"https://ai.google/research/people/PhilipNelson\"><b>Philip Nelson</b></a><br />\nDirector of Engineering<br />\nGoogle Research</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Accelerating bio discovery with machine learning, the promise and the peril</b></a></span></div><div class=\"togglec clearfix\">Google Accelerated Sciences is a translational research team that brings Google&#8217;s technological expertise to the scientific community.  Recent advances in machine learning have delivered incredible results in consumer applications (e.g. photo recognition, language translation), and is now beginning to play an important role in life sciences.  Taking examples from active collaborations in the biochemical, biological, and biomedical fields, I will focus on how our team transforms science problems into data problems and applies Google&#8217;s scaled computation, data-driven engineering, and machine learning to accelerate discovery. See <a href=\"http://g.co/research/gas\">http://g.co/research/gas</a> for our publications and more details.</p>\n<p><b>Bio:</b><br />Philip Nelson is a Director of Engineering in Google Research.  He joined Google in 2008 and was previously responsible for a range of Google applications and geo services.  In 2013, he helped found and currently leads the Google Accelerated Science team that collaborates with academic and commercial scientists to apply Google&#8217;s knowledge and experience and technologies to important scientific problems.  Philip graduated from MIT in 1985 where he did award-winning research on hip prosthetics at Harvard Medical School.  Before Google, Philip helped found and lead several Silicon Valley startups in search (Verity), optimization (Impresse), and genome sequencing (Complete Genomics) and was also an Entrepreneur in Residence at Accel Partners.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Nov 26 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Nov 26</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><a href=\"http://socsci.uci.edu/~rfutrell/\"><b>Richard Futrell</b></a><br />\nAssistant Professor<br />\nDept of Language Science<br />\nUC Irvine</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Natural language as a code: Modeling human language using information theory</b></a></span></div><div class=\"togglec clearfix\"><br />\nWhy is natural language the way it is? I propose that human languages can be modeled as solutions to the problem of efficient communication among intelligent agents with certain information processing constraints, in particular constraints on short-term memory. I present an analysis of dependency treebank corpora of over 50 languages showing that word orders across languages are optimized to limit short-term memory demands in parsing. Next I develop a Bayesian, information-theoretic model of human language processing, and show that this model can intuitively explain an apparently paradoxical class of comprehension errors made by both humans and state-of-the-art recurrent neural networks (RNNs). Finally I combine these insights in a model of human languages as information-theoretic codes for latent tree structures, and show that optimization of these codes for expressivity and compressibility results in grammars that resemble human languages.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Dec 3 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Dec 3</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (NIPS)</b></div>\n<p>&nbsp;</td>\n</tr>\n</tbody>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2018/09/fall-2018/\" title=\"2:24 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2018-09-18T14:24:22-07:00\">September 18, 2018</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-778\" class=\"post-778 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2018/04/spring-2018/\" title=\"Permalink to Spring 2018\" rel=\"bookmark\">Spring 2018</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<p><!-- ========================================================== --><br />\n<!-- =====Spring Quarter ============================================== --><br />\n<!-- ========================================================== --></p>\n<table border=\"1\" cellpadding=\"5\">\n<tbody>\n<!-- ==== Apr 2 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Apr 2</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar</b></div>\n</td>\n</tr>\n<p><!-- ==== Apr 9 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Apr 9</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b>Sabino Miranda, Ph.D</b></div>\n<div class=\"aiml-name\">CONACyT Researcher</div>\n<div class=\"aiml-name\">Center for Research and Innovation in Information and Communication Technologies</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Towards a Multilingual and Error-Robust Approach for Sentiment Analysis</b></a></span></div><div class=\"togglec clearfix\"><br />\nSentiment Analysis is a research area concerned with the computational analysis of people&#8217;s feelings or beliefs expressed in texts such as emotions, opinions, attitudes, appraisals, etc.  At the same time, with the growth of social media data (review websites, microblogging sites, etc.) on the Web, Twitter has received particular attention because it is a huge source of opinionated information with potential applications to decision-making tasks from business applications to the analysis of social and political events.  In this context, I will present the multilingual and error-robust approaches developed in our group to tackle sentiment analysis as a classification problem, mainly for informal written text such as Twitter. Our approaches have been tested in several benchmark contests such as SemEval (International Workshop on Semantic Evaluation),  TASS  (Workshop for Sentiment Analysis Focused on Spanish),  and PAN (Workshop on Digital Text Forensics).</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Apr 16 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Apr 16</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"https://www.math.uci.edu/~rvershyn/index.html\">Roman Vershynin</a></b></div>\n<div class=\"aiml-name\">Professor of Mathematics</div>\n<div class=\"aiml-name\">University of California, Irvine</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Boolean functions and random tensors</b></a></span></div><div class=\"togglec clearfix\">A simple way to generate a Boolean function in n variables is to take the sign of some polynomial. Such functions are called polynomial threshold functions. How many low-degree polynomial threshold functions are there? This problem was solved for degree d=1 by Zuev in 1989 and has remained open for any higher degrees, including d=2, since then. In a joint work with Pierre Baldi (UCI), we settled the problem for all degrees d>1. The solution explores connections of Boolean functions to additive combinatorics and high-dimensional probability. This leads to a program of extending random matrix theory to random tensors, which is mostly an uncharted territory at present.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Apr 23 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Apr 23</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"http://cs.brown.edu/people/ren/\">Zhile Ren</a></b></div>\n<div class=\"aiml-name\">PhD Candidate, Computer Science</div>\n<div class=\"aiml-name\">Brown University</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Semantic Three-Dimensional Understanding of Dynamic Scenes</b></a></span></div><div class=\"togglec clearfix\">We develop new representations and algorithms for three-dimensional (3D) scene understanding from images and videos. In cluttered indoor scenes, RGB-D images are typically described by local geometric features of the 3D point cloud. We introduce descriptors that account for 3D camera viewpoint, and use structured learning to perform 3D object detection and room layout prediction. We also extend this work by using latent support surfaces to capture style variations of 3D objects and help detect small objects. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses with improved accuracy. In outdoor autonomous driving applications, given two consecutive frames from a pair of stereo cameras, 3D scene flow methods simultaneously estimate the 3D geometry and motion of the observed scene. We incorporate semantic segmentation in a cascaded prediction framework to more accurately model moving objects by iteratively refining segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Apr 30 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Apr 30</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>Cancelled</b></div>\n</td>\n</tr>\n<p><!-- ==== May 7 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>May 7</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"https://svivek.com/\">Vivek Srikumar</a></b></div>\n<div class=\"aiml-name\">Assistant Professor</div>\n<div class=\"aiml-name\">University of Utah</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Natural Language Processing in the Wild: Opportunities & Challenges</b></a></span></div><div class=\"togglec clearfix\">Natural language processing (NLP) sees potential applicability in a broad array of user-facing applications. To realize this potential, however, we need to address several challenges related to representations, data availability and scalability.</p>\n<p>In this talk, I will discuss these concerns and how we may overcome them. First, as a motivating example of NLP&#8217;s broad reach, I will present our recent work on using language technology to improve mental health treatment. Then, I will focus on some of the challenges that need to be addressed. The choice of representations can make a big difference in our ability to reason about text; I will discuss recent work on developing rich semantic representations. Finally, I will touch upon the problem of systematically speeding up the entire NLP pipeline without sacrificing accuracy. As a concrete example, I will present a new algebraic characterization of the process of feature extraction, as a direct consequence of which, we can make trained classifiers significantly faster.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== May 14 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>May 14</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"http://www.ics.uci.edu/~skong2/\">Shu Kong</a></b></div>\n<div class=\"aiml-name\">PhD Candidate, Computer Science</div>\n<div class=\"aiml-name\">University of California, Irvine</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Pay Attention to the Pixel, Understand the Scene Better</b></a></span></div><div class=\"togglec clearfix\">Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size (by fusing multi-scale pooled features) in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We further integrate this depth-aware gating into a recurrent convolutional neural network to refine semantic segmentation, and show state-of-the-art performance on several benchmarks.</p>\n<p>Moreover, rather than fusing mutli-scale pooled features based on estimated depth, we show the &#8220;correct&#8221; size of pooling field for each pixel can be decided in an attentional fashion by our  Pixel-wise Attentional Gating unit (PAG), which learns to choose the pooling size for each pixel. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily \u201cplugged in\u201d to an existing model with fine-tuning. We utilize PAG in two ways: 1) learning spatially varying pooling fields that improves model performance without the extra computation cost, and 2) learning a dynamic computation policy for each pixel to decrease total computation while maintaining accuracy. We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation. We demonstrate that PAG allows competitive or state-of-the-art performance on these tasks. We also show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe that PAG reduces computation by 10% without noticeable loss in accuracy, and performance degrades gracefully when imposing stronger computational constraints.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== May 21 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>May 21</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"https://www.microsoft.com/en-us/research/people/rcaruana/\">Rich Caruana</a></b></div>\n<div class=\"aiml-name\">Principal Researcher</div>\n<div class=\"aiml-name\">Microsoft Research</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Friends Don\u2019t Let Friends Deploy Black-Box Models: The Importance of Intelligibility in Machine Learning</b></a></span></div><div class=\"togglec clearfix\">In machine learning often a tradeoff must be made between accuracy and intelligibility: the most accurate models usually are not very intelligible (e.g., deep nets, boosted trees and random forests), and the most intelligible models usually are less accurate (e.g., logistic regression and decision lists).  This tradeoff often limits the accuracy of models that can be safely deployed in mission-critical applications such as healthcare where being able to understand, validate, edit, and ultimately trust a learned model is important.  We have been working on a learning method based on generalized additive models (GAMs) that is often as accurate as full complexity models, but even more intelligible than linear models.  This makes it easy to understand what a model has learned, and also makes it easier to edit the model when it learns inappropriate things because of unanticipated problems with the data.  Making it possible for experts to understand a model and repair it is critical because most data has unanticipated landmines.  In the talk I\u2019ll present two healthcare cases studies where these high-accuracy GAMs discover surprising patterns in the data that would have made deploying a black-box model risky.  I\u2019ll also briefly show how we\u2019re using these models to detect bias in domains where fairness and transparency are paramount.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== May 28 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>May 28</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>Memorial Day</b></div>\n</td>\n</tr>\n<p><!-- ==== Jun 4 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Jun 4</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b>Stephen McAleer</b> (<a href=\"http://www.igb.uci.edu/~pfbaldi/\">Pierre Baldi</a>&#8216;s group)</div>\n<div class=\"aiml-name\">Graduate Student, Computer Science</div>\n<div class=\"aiml-name\">University of California, Irvine</div>\n</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning how to solve the Rubik's Cube with no Human Knowledge</b></a></span></div><div class=\"togglec clearfix\">We will present a novel approach to solving the Rubik&#8217;s cube effectively without any human knowledge using several ingredients including deep learning, reinforcement learning, and Monte Carlo searches. </p>\n<p>At the end, if time permits, we will describe several extensions to the neuronal Boolean complexity results presented by Roman Vershynin a few weeks ago.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Jun 11 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Jun 11</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (finals week)</b></div>\n</td>\n</tr>\n</tbody>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2018/04/spring-2018/\" title=\"3:01 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2018-04-04T15:01:11-07:00\">April 4, 2018</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-747\" class=\"post-747 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2018/01/winter-2018/\" title=\"Permalink to Winter 2018\" rel=\"bookmark\">Winter 2018</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<p><!-- ========================================================== --><br />\n<!-- =====Winter Quarter ============================================== --><br />\n<!-- ========================================================== --></p>\n<table border=\"1\" cellpadding=\"5\"><!-- ==== Jan 15 =================================== --></p>\n<tbody>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>Jan 15</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (MLK Day)</b></div>\n<p>&nbsp;</td>\n</tr>\n<p><!-- ==== Jan 22 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Jan 22</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b>Shufeng Kong</b></div>\n<div class=\"aiml-name\">PhD Candidate</div>\n<div class=\"aiml-name\">Centre for Quantum Software and Information, FEIT</div>\n<div class=\"aiml-name\">University of Technology Sydney, Australia</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Multiagent Simple Temporal Problem: The Arc-Consistency Approach</b></a></span></div><div class=\"togglec clearfix\">The Simple Temporal Problem (STP) is a fundamental temporal<br />\nreasoning problem and has recently been extended to<br />\nthe Multiagent Simple Temporal Problem (MaSTP). In this<br />\npaper we present a novel approach that is based on enforcing<br />\narc-consistency (AC) on the input (multiagent) simple temporal<br />\nnetwork. We show that the AC-based approach is sufficient<br />\nfor solving both the STP and MaSTP and provide efficient<br />\nalgorithms for them. As our AC-based approach does<br />\nnot impose new constraints between agents, it does not violate<br />\nthe privacy of the agents and is superior to the state-ofthe-art<br />\napproach to MaSTP. Empirical evaluations on diverse<br />\nbenchmark datasets also show that our AC-based algorithms<br />\nfor STP and MaSTP are significantly more efficient than existing<br />\napproaches.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Jan 29 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>Jan 29</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"http://jiyfeng.github.io/\">Yangfeng Ji</a></b></div>\n<div class=\"aiml-name\">Postdoctoral Scholar</div>\n<div class=\"aiml-name\">Paul Allen School of Computer Science and Engineering</div>\n<div class=\"aiml-name\">University of Washington</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Bringing Structural Information into Neural Network Design</b></a></span></div><div class=\"togglec clearfix\">Deep learning is one of the most important techniques used in natural language processing (NLP). A central question in deep learning for NLP is how to design a neural network that can fully utilize the information from training data and make accurate predictions. A key to solving this problem is to design a better network architecture. </p>\n<p>In this talk, I will present two examples from my work on how structural information from natural language helps design better neural network models. The first example shows adding coreference structures of entities not only helps different aspects of text modeling, but also improves the performance of language generation; the second example demonstrates structures of organizing sentences into coherent texts can help neural networks build better representations for various text classification tasks. Along the lines of this topic, I will also propose a few ideas for future work and discuss some potential challenges.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Feb 5 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>February 5</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (AAAI)</b></div>\n<p>&nbsp;</td>\n</tr>\n<p><!-- ==== Feb 12 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>February 12</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"https://www.ics.uci.edu/~enalisni/\">Eric Nalisnick</a></b></div>\n<div class=\"aiml-name\">PhD Candidate</div>\n<div class=\"aiml-name\">Computer Science</div>\n<div class=\"aiml-name\">University of California, Irvine</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Averaging and Combining Variational Models with Stein Particle Descent</b></a></span></div><div class=\"togglec clearfix\">Bayesian inference for complex models&#8212;the kinds needed to solve complex tasks such as object recognition&#8212;is inherently intractable, requiring analytically difficult integrals be solved in high dimensions.  One solution is to turn to variational Bayesian inference: a parametrized family of distributions is proposed, and optimization is carried out to find the member of the family nearest to the true posterior.  There is an innate trade-off within VI between expressive vs tractable approximations.  We wish the variational family to be as rich as possible so as it might include the true posterior (or something very close), but adding structure to the approximation increases the computational complexity of optimization.  As a result, there has been much interest in efficient optimization strategies for mixture model approximations.  In this talk, I&#8217;ll return to the problem of using mixture models for VI.  First, to motivate our approach, I&#8217;ll discuss the distinction between averaging vs combining variational models.  We show that optimization objectives aimed at fitting mixtures (i.e. model combination), in practice, are relaxed into performing something between model combination and averaging.  Our primary contribution is to formulate a novel training algorithm for variational model averaging by adapting Stein variational gradient descent to operate on the parameters of the approximating distribution.  Then, through a particular choice of kernel, we show the algorithm can be adapted to perform something closer to model combination, providing a new algorithm for optimizing (finite) mixture approximations.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Feb 19 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>February 19</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (President&#8217;s Day)</b></div>\n<p>&nbsp;</td>\n</tr>\n<p><!-- ==== Feb 26 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>February 26</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"https://www.jaypujara.org/\">Jay Pujara</a></b></div>\n<div class=\"aiml-name\">Research Scientist</div>\n<div class=\"aiml-name\">ISI/USC</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>What do Probabilistic Models Know?</b></a></span></div><div class=\"togglec clearfix\">Knowledge is an essential ingredient in the quest for artificial intelligence, yet scalable and robust approaches to acquiring knowledge have challenged AI researchers for decades. Often, the obstacle to knowledge acquisition is massive, uncertain, and changing data that obscures the underlying knowledge. In such settings, probabilistic models have excelled at exploiting the structure in the domain to overcome ambiguity, revise beliefs and produce interpretable results. In my talk, I will describe recent work using probabilistic models for knowledge graph construction and information extraction, including linking subjects across electronic health records, fusing background knowledge from scientific articles with gene association studies, disambiguating user browsing behavior across platforms and devices, and aligning structured data sources with textual summaries. I also highlight several areas of ongoing research, fusing embedding approaches with probabilistic modeling and building models that support dynamic data or human-in-the-loop interactions.</p>\n<p><b>Bio:</b><br />\nJay Pujara is a research scientist at the University of Southern California&#8217;s Information Sciences Institute whose principal areas of research are machine learning, artificial intelligence, and data science. He completed a postdoc at UC Santa Cruz, earned his PhD at the University of Maryland, College Park and received his MS and BS at Carnegie Mellon University. Prior to his PhD, Jay spent six years at Yahoo! working on mail spam detection, user trust, and contextual mail experiences, and he has also worked at Google, LinkedIn and Oracle. Jay is the author of over thirty peer-reviewed publications and has received three best paper awards for his work. He is a recognized authority on knowledge graphs, and has organized the Automatic Knowledge Base Construction (AKBC) and Statistical Relational AI (StaRAI) workshops, has presented tutorials on knowledge graph construction at AAAI and WSDM, and has had his work featured in AI Magazine.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Mar 5 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>March 5</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"http://www.cs.ucr.edu/~epapalex/\">Vagelis Papalexakis</a></b></div>\n<div class=\"aiml-name\">Assistant Professor</div>\n<div class=\"aiml-name\">UC Riverside</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Tensor Decompositions for Big Multi-aspect Data Analytics</b></a></span></div><div class=\"togglec clearfix\">Tensors and tensor decompositions have been very popular and effective tools for analyzing multi-aspect data in a wide variety of fields, ranging from Psychology to Chemometrics, and from Signal Processing to Data Mining and Machine Learning.  Using tensors in the era of big data presents us with a rich variety of applications, but also poses great challenges such as the one of scalability and efficiency. In this talk I will first motivate the effectiveness of tensor decompositions as data analytic tools in a variety of exciting, real-world applications. Subsequently, I will discuss recent techniques on tackling the scalability and efficiency challenges by parallelizing and speeding up tensor decompositions, especially for very sparse datasets, including the scenario where the data are continuously updated over time. Finally, I will discuss open problems in unsupervised tensor mining and quality assessment of the results, and present work-in-progress addressing that problem with very encouraging results.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Mar 12 =================================== --></p>\n<tr>\n<td valign=\"top\">\n<div class=\"aiml-date\"><b>March 12</b><br />\nBren Hall 4011<br />\n1 pm</div>\n</td>\n<td valign=\"top\">\n<div class=\"aiml-name\"><b><a href=\"http://vision.ucla.edu/~alex/\">Alessandro Achille</a></b></div>\n<div class=\"aiml-name\">PhD Student</div>\n<div class=\"aiml-name\">UC Los Angeles</div>\n<p><div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>The Emergence Theory of Deep Learning: Perception, Information Theory and PAC-Bayes</b></a></span></div><div class=\"togglec clearfix\">I will describe the basic elements of the Emergence Theory of Deep Learning, that started as a general theory for representations, and is comprised of three parts: (1) We formalize the desirable properties that a representation should possess, based on classical principles of statistical decision and information theory: invariance, sufficiency, minimality, disentanglement. We then show that such an optimal representation of the data can be learned by minimizing a specific loss function which is related to the notion of Information Bottleneck and Variational Inference. (2) We analyze common empirical losses employed in Deep Learning (such as empirical cross-entropy), and implicit or explicit regularizers, including Dropout and Pooling, and show that they bias the network toward recovering such an optimal representation. Finally, (3) we show that minimizing a suitably (implicitly or explicitly) regularized loss with SGD with respect to the weights of the network implies implicit optimization of the loss described in (1), with relates instead to the activations of the network. Therefore, even when we optimize a DNN as a black-box classifier, we are always biased toward learning minimal, sufficient and invariant representation. The link between (implicit or explicit) regularization of the classification loss and learning of optimal representations is specific to the architecture of deep networks, and is not found in a general classifier. The theory is related to a new version of the Information Bottleneck that studies the weights of a network, rater than the activation, and can also be derived using PAC-Bayes or Kolmogorov complexity arguments, providing independent validation.</div></div><div class=\"clear\"></div></td>\n</tr>\n<p><!-- ==== Mar 19 =================================== --></p>\n<tr>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-date\"><b>March 19</b></div>\n</td>\n<td class=\"aiml-none\" valign=\"top\">\n<div class=\"aiml-name\"><b>No Seminar (Finals Week)</b></div>\n<p>&nbsp;</td>\n</tr>\n</tbody>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2018/01/winter-2018/\" title=\"12:36 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2018-01-10T12:36:05-07:00\">January 10, 2018</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-707\" class=\"post-707 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2017/10/fall-2017/\" title=\"Permalink to Fall 2017\" rel=\"bookmark\">Fall 2017</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<p><!-- ========================================================== --><br />\n<!-- =====Fall Quarter ============================================== --><br />\n<!-- ========================================================== --></p>\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Oct 9 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Oct 9</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Columbus Day)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 16 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct 16</b><br />Bren Hall 3011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://baileykong.com/\"><b>Bailey Kong</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Cross-Domain Forensic Shoeprint Matching</b></a></span></div><div class=\"togglec clearfix\">We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for these specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance.  For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Finally, we introduce a discriminatively trained variant and fine-tune our system end-to-end, obtaining state-of-the-art performance.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 23 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct 23</b><br />Bren Hall 3011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://www.linkedin.com/in/geng-ji/\"><b>Geng Ji</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>From Patches to Images: A Nonparametric Generative Model</b></a></span></div><div class=\"togglec clearfix\">We propose a hierarchical generative model that captures the self-similar structure of image regions as well as how this structure is shared across image collections. Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches. While previous EPLL methods modeled image patches with finite Gaussian mixtures, we use nonparametric Dirichlet process (DP) mixtures to create models whose complexity grows as additional images are observed. An extension based on the hierarchical DP then captures repetitive and self-similar structure via image-specific variations in cluster frequencies. We derive a structured variational inference algorithm that adaptively creates new patch clusters to more accurately model novel image textures. Our denoising performance on standard benchmarks is superior to EPLL and comparable to the state-of-the-art, and we provide novel statistical justifications for common image processing heuristics. We also show accurate image inpainting results.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 30 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct 30</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~qlou/\"><b>Qi Lou</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Dynamic Importance Sampling for Anytime Bounds of the Partition Function</b></a></span></div><div class=\"togglec clearfix\">Computing the partition function is a key inference task in many graphical models. In this paper, we propose a dynamic importance sampling scheme that provides  anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies, heuristic search, variational bounds, and Monte Carlo methods, blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search and probabilistic bounds of the partition function. By using an intelligently chosen weighted average over the samples, we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling with the long-term benefits  of an improved proposal from search. This gives significantly improved anytime behavior, and more flexible trade-offs between memory, time, and solution quality. We demonstrate the effectiveness of our approach empirically on real-world problem instances taken from recent UAI competitions.<br /></div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov  6 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov  6</b><br />Bren Hall 3011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.stat.washington.edu/vminin/\"><b>Vladimir Minin</b></a><br />Professor<br />Department of Statistics<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Advances of Bayesian nonparametrics in population genetics of infectious diseases</b></a></span></div><div class=\"togglec clearfix\">Estimating evolutionary trees, called phylogenies or genealogies, is a fundamental task in modern biology. Once phylogenetic reconstruction is accomplished, scientists are faced with a challenging problem of interpreting phylogenetic trees. In certain situations, a coalescent process, a stochastic model that randomly generates evolutionary trees, comes to rescue by probabilistically connecting phylogenetic reconstruction with the demographic history of the population under study. An important application of the coalescent is phylodynamics, an area that aims at reconstructing past population dynamics from genomic data. Phylodynamic methods have been especially successful in analyses of genetic sequences from viruses circulating in human populations. From a Bayesian hierarchal modeling perspective, the coalescent process can be viewed as a prior for evolutionary trees, parameterized in terms of unknown demographic parameters, such as the population size trajectory. I will review Bayesian nonparametric techniques that can accomplish phylodynamic reconstruction, with a particular attention to analysis of genetic data sampled serially through time.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 13 =================================== --><br />\n<!--\n  \n\n<tr>\n  \n\n<td valign=top>\n\n<div class=\"aiml-date\"><b>Nov 13</b><br />Bren Hall 4011<br />1 pm</div>\n\n</td>\n\n\n  \n\n<td valign=top>\n\n<div class=\"aiml-name\"><a href=\"\"><b>tbd</b></a>\n\n  </div>\n\n<br />\n  [toggle title=\"<a><b>tbd</b></a>\"]tbd[/toggle]\n  </td>\n\n\n  </tr>\n\n\n--></p>\n<p>  <!-- ==== Nov 20 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Nov 20</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Thanksgiving Week)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 27 =================================== --><br />\n<!--\n  \n\n<tr>\n  \n\n<td valign=top>\n\n<div class=\"aiml-date\"><b>Nov 27</b><br />Bren Hall 4011<br />1 pm</div>\n\n</td>\n\n\n  \n\n<td valign=top>\n\n<div class=\"aiml-name\"><a href=\"\"><b>tbd</b></a>\n\n  </div>\n\n<br />\n  [toggle title=\"<a><b>tbd</b></a>\"]tbd[/toggle]\n  </td>\n\n\n  </tr>\n\n\n--></p>\n<p>  <!-- ==== Dec  4 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Dec  4</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (NIPS Conference)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Dec  13 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Dec 13</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://http://yutianchen.com/\"><b>Yutian Chen</b></a><br />Research Scientist<br />Google DeepMind</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning to Learn without Gradient Descent by Gradient Descent</b></a></span></div><div class=\"togglec clearfix\">We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2017/10/fall-2017/\" title=\"3:07 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2017-10-09T15:07:08-07:00\">October 9, 2017</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-689\" class=\"post-689 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2017/04/spring-2017/\" title=\"Permalink to Spring 2017\" rel=\"bookmark\">Spring 2017</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<br />\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Apr 10 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 10</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://izbicki.me/research.html\"><b>Mike Izbicki</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Riverside</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Divide & Conquer Techniques for Machine Learning</b></a></span></div><div class=\"togglec clearfix\">I&#8217;ll present two algorithms that use divide and conquer techniques to speed up learning.  The first algorithm (called OWA) is a communication efficient distributed learner.  OWA uses only two rounds of communication, which is sufficient to achieve optimal learning rates.  The second algorithm is a meta-algorithm for fast cross validation.  I&#8217;ll show that for any divide and conquer learning algorithm, there exists a fast cross validation procedure whose run time is asymptotically independent of the number of cross validation folds.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 17 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 17</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~jsupanci/\"><b>James Supancic</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Long-Term Tracking by Decision Making</b></a></span></div><div class=\"togglec clearfix\">Cameras can naturally capture sequences of images, or videos. And when understanding videos, connecting the past with the present requires tracking. Sometimes tracking is easy. We focus on two challenges which make tracking harder: long-term occlusions and appearance variations. To handle total occlusion, a tracker must know when it has lost track and how to reinitialize tracking when the target reappears. Reinitialization requires good appearance models. We build appearance models for humans and hands, with a particular emphasis on robustness and occlusion. For the second challenge, appearance variation, the tracker must know when and how to re-learn (or update) an appearance model. This challenge leads to the classic problem of drift: aggressively learning appearance changes allows small errors to compound, as elements of the background environment pollute the appearance model. We propose two solutions. First, we consider self-paced learning, wherein a tracker begins by learning from frames it finds easy. As the tracker becomes better at recognizing the target, it begins to learn from harder frames. We also develop a data-driven approach: train a tracking policy to decide when and how to update an appearance model. To take this direct approach to &#8220;learning when to learn&#8221;, we exploit large-scale Internet data through reinforcement learning. We interpret the resulting policy and conclude with a generalization for tracking multiple objects.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 24 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 24</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://ml.jpl.nasa.gov/people/thompson.shtml\"><b>David R Thompson</b></a></p>\n<p>Jet Propulsion Laboratory<br />California Institute of Technology</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Visible / Shortwave Infrared Imaging Spectroscopy at JPL: Instruments and Algorithms</b></a></span></div><div class=\"togglec clearfix\">Imaging spectrometers enable quantitative maps of physical and chemical properties at high spatial resolution. They have a long history of deployments for mapping terrestrial and coastal aquatic ecosystems, geology, and atmospheric properties. They are also critical tools for exploring other planetary bodies. These high-dimensional spatio-spectral datasets pose a rich challenge for computer scientists and algorithm designers. This talk will provide an introduction to remote imaging spectroscopy in the Visible and Shortwave Infrared, describing the measurement strategy and data analysis considerations including atmospheric correction. We will describe historical and current instruments, software, and public datasets.</p>\n<p><p><b>Bio:</b> David R. Thompson is a researcher and Technical Group Lead in the Imaging Spectroscopy group at the NASA Jet Propulsion Laboratory. He is Investigation Scientist for the AVIRIS imaging spectrometer project.  Other roles include software lead for the NEAScout mission, autonomy software lead for the PIXL instrument, and algorithm development for diverse JPL airborne imaging spectrometer campaigns. He is recipient of the NASA Early Career Achievement Medal and the JPL Lew Allen Award.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May  1 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May  1</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://faculty.sites.uci.edu/weinings/\"><b>Weining Shen</b></a><br />Assistant Professor<br />Department of Statistics<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Theory behind Bayesian nonparametrics</b></a></span></div><div class=\"togglec clearfix\">Bayesian nonparametric (BNP) models have been widely used in modern applications. In this talk, I will discuss some recent theoretical results for the commonly used BNP methods from a frequentist asymptotic perspective. I will cover a set of function estimation and testing problems such as density estimation, high-dimensional partial linear regression, independence testing, and independent component analysis. Minimax optimal convergence rates, adaptation and Bernstein-von Mises theorem will be discussed.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May  8 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May  8</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://www.linkedin.com/in/p-anandan-72124b120/\"><b>P. Anandan</b></a><br />VP for Research<br />Adobe Systems</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>How Data Science, Machine Learning, and AI are Transforming the Consumer Experience</b></a></span></div><div class=\"togglec clearfix\">During the last two decades the experience of consumers has been undergoing a fundamental and dramatic transformation \u2013 giving a rich variety of informed choices, online shopping, consumption of news and entertainment on the go, and personalized shopping experiences.  All of this has been powered by the massive amounts of data that is continuously being collected and the application of machine learning, data science and AI techniques to it.</p>\n<p>Adobe is a leader the Digital Marketing and is the leading provider of solutions to enterprises that are serving customers both in the B2B and B2C space.  In this talk,  we will outline the current state of the industry and the technology that is behind it, how Data Science and Machine Learning are gradually beginning to transform the experiences of the consumer as well as the marketer. We will also speculate on how recent developments in Artificial Intelligence will lead to deep personalization  and richer experiences for the consumer as well as more powerful and tailored end-to-end capabilities for the marketer.</p>\n<p><p><b>Bio:</b> Dr. P. Anandan is Vice President in Adobe Research, responsible for developing research strategy for Adobe, especially in Digital Marketing, and Leading the Adobe India Research lab. An emphasis of this lab is on Big Data Experience and Intelligence. At Adobe, he is also leading efforts in applying A.I. to Big Data. Dr. Anandan is an expert in Computer Vision with more than 60 publications that have earned 14,500 citations in Google Scholar. His research areas include visual motion analysis, video surveillance, and 3D scene modeling from images and video. His papers have won multiple awards including the Helmholtz Prize, for long term fundamental contributions to computer vision research. Prior to joining Adobe Dr. Anandan had a long tenure with Microsoft Research in Redmond, WA, and became a Distinguished Scientist. He was the Managing Director of Microsoft Research India, which he founded. Most recently he was the Managing Director of Microsoft Research\u2019s Worldwide Outreach. He earned a PhD from the University of Massachusetts specializing in Computer Vision and Artificial Intelligence. He started as an assistant professor at Yale University before moving on to work in Video Information Processing at the David Sarnoff Research Center. His research has been used in DARPA\u2019s Video Surveillance and Monitoring program as well as in creating special effects in the movies \u201cWhat Dreams May Come\u201d, \u201cPrince of Egypt,\u201d and \u201cThe Matrix.\u201d Dr. Anandan is the recipient of Distinguished Alumnus awards from both University of Massachusetts and the Indian Institute of Technology Madras, where he earned a B. Tech. in Electrical Engineering. He was inducted into the Nebraska Hall of Computing by the University of Nebraska, from where he obtained an MS in Computer Science. He is currently a member of the Board of Governors of IIT Madras.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 15 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 15</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://nakashole.com/\"><b>Ndapa Nakashole</b></a><br />Assistant Professor<br />Computer Science and Engineering<br />University of California, San Diego</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Improving Zero-shot learning for word-level translation</b></a></span></div><div class=\"togglec clearfix\">Zero-shot learning is used in computer vision, natural language, and other domains to induce mapping functions that project vectors from one vector space to another. This is a promising approach to learning, when we do not have labeled data for every possible label we want a system to recognize. This setting is common when doing NLP for low-resource languages, where labeled data is very scare.  In this talk, I will present our work on improving zero-shot learning methods for the task of word-level translation.</p>\n<p><p><b>Bio:</b> Ndapa Nakashole is an Assistant Professor in the Department of Computer Science and Engineering at the University of California, San Diego. Prior to UCSD, she was a Postdoctoral Fellow in the Machine Learning Department at Carnegie Mellon University. She obtained her PhD from Saarland University, Germany, for work done at the Max Planck Institute for Informatics at Saarbr\u00fccken.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 22 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 22</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"\"><b>Batya Kenig</b></a><br />Postdoctoral Scholar<br />Department of Information Systems Engineering<br />Technion &#8211; Israel Institute of Technology</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Querying Probabilistic Preferences in Databases</b></a></span></div><div class=\"togglec clearfix\">We propose a novel framework wherein probabilistic preferences can be naturally represented and analyzed in a probabilistic relational database.  The framework augments the relational schema with a special type of a relation symbol, a preference symbol. A deterministic instance of this symbol holds a collection of binary relations.  Abstractly, the probabilistic variant is a probability space over databases of the augmented form (i.e., probabilistic database).  Effectively, each instance of a preference symbol can be represented as a collection of parametric preference distributions such as Mallows. We establish positive and negative complexity results for evaluating Conjunctive Queries (CQs) over databases where preferences are represented in the Repeated Insertion Model (RIM), Mallows being a special case. We show how CQ evaluation reduces to a novel inference problem (of independent interest) over RIM, and devise a solver with polynomial data complexity.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 29 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>May 29</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Memorial Day)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Jun  5 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Jun  5</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://yonatanbisk.com/\"><b>Yonatan Bisk</b></a><br />Postdoctoral Scholar<br />Information Sciences Institute<br />University of Southern California</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>The Limits of Unsupervised Syntax and the Importance of Grounding in Language Acquisition</b></a></span></div><div class=\"togglec clearfix\">The future of self-driving cars, personal robots, smart homes, and intelligent assistants hinges on our ability to communicate with computers. The failures and miscommunications of Siri-style systems are untenable and become more problematic as machines become more pervasive and are given more control over our lives. Despite the creation of massive proprietary datasets to train dialogue systems, these systems still fail at the most basic tasks. Further, their reliance on big data is problematic. First, successes in English cannot be replicated in most of the 6,000+ languages of the world. Second, while big data has been a boon for supervised training methods, many of the most interesting tasks will never have enough labeled data to actually achieve our goals. It is therefore important that we build systems which can learn from naturally occurring data and grounded situated interactions.</p>\n<p>In this talk, I will discuss work from my thesis on the unsupervised acquisition of syntax which harnesses unlabeled text in over a dozen languages. This exploration leads us to novel insights into the limits of semantics-free language learning. Having isolated these stumbling blocks, I\u2019ll then present my recent work on language grounding where we attempt to learn the meaning of several linguistic constructions via interaction with the world.</p>\n<p><p><b>Bio:</b> Yonatan Bisk\u2019s research focuses on Natural Language Processing from naturally occurring data (unsupervised and weakly supervised data). He is a postdoc researcher with Daniel Marcu at USC\u2019s Information Sciences Institute. Previously, he received his Ph.D. from the University of Illinois at Urbana-Champaign under Julia Hockenmaier and his BS from the University of Texas at Austin.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2017/04/spring-2017/\" title=\"11:43 am\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2017-04-05T11:43:49-07:00\">April 5, 2017</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-667\" class=\"post-667 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2017/01/winter-2017/\" title=\"Permalink to Winter 2017\" rel=\"bookmark\">Winter 2017</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<br />\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Jan 16 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Jan 16</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (MLK Day)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Jan 23 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Jan 23</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"\"><b>Mohammad Ghavamzadeh</b></a><br />Senior Analytics Researcher<br />Adobe Research</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning Safe Policies in Sequential Decision-Making Problems</b></a></span></div><div class=\"togglec clearfix\">In online advertisement as well as many other fields such as health informatics and computational finance, we often have to deal with the situation in which we are given a batch of data generated by the current strategy(ies) of the company (hospital, investor), and we are asked to generate a good or an optimal strategy. Although there are many techniques to find a good policy given a batch of data, there are not much results to guarantee that the obtained policy will perform well in the real system without deploying it. On the other hand, deploying a policy might be risky, and thus, requires convincing the product (hospital, investment) manager that it is not going to harm the business. This is why it is extremely important to devise algorithms that generate policies with performance guarantees. </p>\n<p> In this talk, we discuss four different approaches to this fundamental problem, we call them model-based, model-free, online, and risk-sensitive. In the model-based approach, we first use the batch of data and build a simulator that mimics the behavior of the dynamical system under studies (online advertisement, hospital\u2019s ER, financial market), and then use this simulator to generate data and learn a policy. The main challenge here is to have guarantees on the performance of the learned policy, given the error in the simulator. This line of research is closely related to the area of robust learning and control. In the model-free approach, we learn a policy directly from the batch of data (without building a simulator), and the main question is whether the learned policy is guaranteed to perform at least as well as a baseline strategy. This line of research is related to off-policy evaluation and control. In the online approach, the goal is to control the exploration of the algorithm in a way that never during its execution the loss of using it instead of the baseline strategy is more than a given margin. In the risk-sensitive approach, the goal is to learn a policy that manages risk by minimizing some measure of variability in the performance in addition to maximizing a standard criterion. We present algorithms based on these approaches and demonstrate their usefulness in real-world applications such as personalized ad recommendation, energy arbitrage, traffic signal control, and American option pricing.</p>\n<p><b>Bio:</b>Mohammad Ghavamzadeh received a Ph.D. degree in Computer Science from the University of Massachusetts Amherst in 2005. From 2005 to 2008, he was a postdoctoral fellow at the University of Alberta. He has been a permanent researcher at INRIA in France since November 2008. He was promoted to first-class researcher in 2010, was the recipient of the &#8220;INRIA award for scientific excellence&#8221; in 2011, and obtained his Habilitation in 2014. He is currently (from October 2013) on a leave of absence from INRIA working as a senior analytics researcher at Adobe Research in California, on projects related to digital marketing. He has been an area chair and a senior program committee member at NIPS, IJCAI, and AAAI. He has been on the editorial board of Machine Learning Journal (MLJ), has published over 50 refereed papers in major machine learning, AI, and control journals and conferences, and has organized several tutorials and workshops at NIPS, ICML, and AAAI. His research is mainly focused on sequential decision-making under uncertainty, reinforcement learning, and online learning.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Jan 27 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Jan 27</b><br />Bren Hall 6011<br />11:00am</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.cs.cmu.edu/~rsalakhu/\"><b>Ruslan Salakhutdinov</b></a><br />Associate Professor<br />Machine Learning Department<br />Carnegie Mellon University</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning Deep Unsupervised and Multimodal Models</b></a></span></div><div class=\"togglec clearfix\">In this talk, I will first introduce a broad class of unsupervised deep learning models and show that they can learn useful hierarchical representations from large volumes of high-dimensional data with applications in information retrieval, object recognition, and speech perception. I will next introduce deep models that are capable of extracting a unified representation that fuses together multiple data modalities and present the Reverse Annealed Importance Sampling Estimator (RAISE) for evaluating these deep generative models. Finally, I will discuss models that can generate natural language descriptions (captions) of images and generate images from captions using attention, as well as introduce multiplicative and fine-grained gating mechanisms with application to reading comprehension.</p>\n<p><p><b>Bio:</b> Ruslan Salakhutdinov received his PhD in computer science from the University of Toronto in 2009. After spending two post-doctoral years at the Massachusetts Institute of Technology Artificial Intelligence Lab, he joined the University of Toronto as an Assistant Professor in the Departments of Statistics and Computer Science. In 2016 he joined the Machine Learning Department at Carnegie Mellon University as an Associate Professor. Ruslan&#8217;s primary interests lie in deep learning, machine learning, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research and served on the senior programme committee of several learning conferences including NIPS and ICML. He is an Alfred P. Sloan Research Fellow, Microsoft Research Faculty Fellow, Canada Research Chair in Statistical Machine Learning, a recipient of the Early Researcher Award, Google Faculty Award, Nvidia&#8217;s Pioneers of AI award, and is a Senior Fellow of the Canadian Institute for Advanced Research.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Jan 30 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Jan 30</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.igb.uci.edu/~pfbaldi/\"><b>Pierre Baldi &#038; Peter Sadowski</b></a><br />Chancellor&#8217;s Professor<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning in the Machine: Random Backpropagation and the Learning Channel</b></a></span></div><div class=\"togglec clearfix\">Learning in the Machine is a style of machine learning that takes into account the physical constraints of learning machines, from brains to neuromorphic chips. Taking into account these constraints leads to new insights into the foundations of learning systems, and occasionally leads also to improvements for machine learning performed on digital computers. Learning in the Machine is particularly useful when applied to message passing algorithms such as backpropagation and belief propagation, and leads to the concepts of local learning and learning channel. These concepts in turn will be applied to random backpropagation and several new variants. In addition to simulations corroborating the remarkable robustness of these algorithms, we will present new mathematical results establishing interesting connections between machine learning and Hilbert 16th problem.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Feb  6 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Feb  6</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://itensor.org/miles/\"><b>Miles Stoudenmire</b></a><br />Research Scientist<br />Department of Physics<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning with Tensor Networks</b></a></span></div><div class=\"togglec clearfix\">Tensor networks are a technique for factorizing tensors with hundreds or thousands of indices into a contracted network of low-order tensors. Originally developed at UCI in the 1990&#8217;s, tensor networks have revolutionized major areas of physics are starting to be used in applied math and machine learning. I will show that tensor networks fit naturally into a certain class of non-linear kernel learning models, such that advanced optimization techniques from physics can be applied straightforwardly (arxiv:1605.05775). I will discuss many advantages and future directions of tensor network models, for example adaptive pruning of weights and linear scaling with training set size (compared to at least quadratic scaling when using the kernel trick).</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Feb 13 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Feb 13</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~qlou/\"><b>Qi Lou</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Anytime Anyspace AND/OR Search for Bounding the Partition Function</b></a></span></div><div class=\"togglec clearfix\">Bounding the partition function is a key inference task in many graphical models.  In this paper, we develop an anytime anyspace search algorithm taking advantage of AND/OR tree structure and optimized variational heuristics to tighten deterministic bounds on the partition function.  We study how our priority-driven best-first search scheme can improve on state-of-the-art variational bounds in an anytime way within limited memory resources, as well as the effect of the AND/OR framework to exploit conditional independence structure within the search process within the context of summation.  We compare our resulting bounds to a number of existing methods, and show that our approach offers a number of advantages on real-world problem instances taken from recent UAI competitions.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Feb 20 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Feb 20</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Presidents Day)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Feb 27 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Feb 27</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~enalisni/\"><b>Eric Nalisnick</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Deep Generative Models with Stick-Breaking Priors</b></a></span></div><div class=\"togglec clearfix\">Deep generative models (such as the Variational Autoencoder) efficiently couple the expressiveness of deep neural networks with the robustness to uncertainty of probabilistic latent variables.  This talk will first give an overview of deep generative models, their applications, and approximate inference strategies for them.  Then I\u2019ll discuss our work on placing Bayesian Nonparametric priors on their latent space, which allows the hidden representations to grow as the data necessitates.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Mar  6 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Mar  6</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://levyomer.wordpress.com/\"><b>Omer Levy</b></a><br />Postdoctoral Researcher<br />Department of Computer Science &#038; Engineering<br />University of Washington</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Understanding Word Embeddings</b></a></span></div><div class=\"togglec clearfix\">Neural word embeddings, such as word2vec (Mikolov et al., 2013), have become increasingly popular in both academic and industrial NLP. These methods attempt to capture the semantic meanings of words by processing huge unlabeled corpora with methods inspired by neural networks and the recent onset of Deep Learning. The result is a vectorial representation of every word in a low-dimensional continuous space. These word vectors exhibit interesting arithmetic properties (e.g. king &#8211; man + woman = queen) (Mikolov et al., 2013), and seemingly outperform traditional vector-space models of meaning inspired by Harris&#8217;s Distributional Hypothesis (Baroni et al., 2014). Our work attempts to demystify word embeddings, and understand what makes them so much better than traditional methods at capturing semantic properties.</p>\n<p> Our main result shows that state-of-the-art word embeddings are actually &#8220;more of the same&#8221;. In particular, we show that skip-grams with negative sampling, the latest algorithm in word2vec, is implicitly factorizing a word-context PMI matrix, which has been thoroughly used and studied in the NLP community for the past 20 years. We also identify that the root of word2vec&#8217;s perceived superiority can be attributed to a collection of hyperparameter settings. While these hyperparameters were thought to be unique to neural-network inspired embedding methods, we show that they can, in fact, be ported to traditional distributional methods, significantly improving their performance. Among our qualitative results is a method for interpreting these seemingly-opaque word-vectors, and the answer to why king &#8211; man + woman = queen.</p>\n<p> <b>Bio:</b> Omer Levy is a post-doc in the Department of Computer Science &#038; Engineering at the University of Washington, working with Prof. Luke Zettlemoyer. Previously, he completed his BSc and MSc at Technion \u2013 Israel Institute of Technology with the guidance of Prof. Shaul Markovitch, and got his PhD at Bar-Ilan University with the supervision of Prof. Ido Dagan and Dr. Yoav Goldberg. Omer is interested in realizing high-level semantic applications such as question answering and summarization to help people cope with information overload. At the heart of these applications are challenges in textual entailment, semantic similarity, and reading comprehension, which form the core of my current research. He is also interested in the current advances in deep learning and how they can facilitate semantic applications.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2017/01/winter-2017/\" title=\"4:41 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2017-01-18T16:41:36-07:00\">January 18, 2017</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-633\" class=\"post-633 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2016/10/fall-2016/\" title=\"Permalink to Fall 2016\" rel=\"bookmark\">Fall 2016</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<br />\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Sep 22 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Sep 22</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://burrsettles.com/\"><b>Burr Settles</b></a><br />Duolingo</p>\n</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Duolingo: Improving Language Learning and Assessment with Data</b></a></span></div><div class=\"togglec clearfix\">Duolingo is a language education platform that teaches 20 languages to more than 150 million students worldwide. Our free flagship learning app is the \\#1 way to learn a language online, and is the most-downloaded education app for both Android and iOS devices. In this talk, I will describe the Duolingo system and several of our empirical research projects to date, which combine machine learning with computational linguistics and psychometrics to improve learning, engagement, and even language proficiency assessment through our products.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Sep 26 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Sep 26</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"\"><b>Golnaz Ghiasi</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation</b></a></span></div><div class=\"togglec clearfix\">Convolutional Neural Net (CNN) architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. We make two contributions to solving this problem: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Oct  3 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct  3</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"\"><b>Shuang Zhao</b></a><br />Assistant Professor<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Modeling and Visualizing Complex Materials</b></a></span></div><div class=\"togglec clearfix\">Despite the rapid development of computer graphics during the recent years, complex materials such as fabrics, fur, and human hair remain largely lacking in the virtual worlds. This is due to both the lack of high-fidelity data and the inability to efficiently describe these complicated objects via mathematical/statistical models. </p>\n<p> In this talk, I will present my research that introduces new means to acquire, model, and render complex materials that are essential to our daily lives with a focus on fabrics. Leveraging detailed geometric information and sophisticated optical model, our work has led to computer generated imagery with a new level of accuracy and fidelity. In particular, we measure real-world samples using volume imaging (e.g., computed micro-tomography) to obtain detailed datasets on their micro-geometries. We then fit sophisticated statistical models to the measured data, yielding highly compact yet realistic representations. Lastly, we show how to recover a sample&#8217;s optical properties (e.g., colors) using optimization.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 10 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Oct 10</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Columbus Day)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 17 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct 17</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://cs.stanford.edu/~ermon/\"><b>Stefano Ermon</b></a><br />Assistant Professor of Computer Science<br />Fellow of the Woods Institute for the Environment<br />Stanford University</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Measuring Progress towards Sustainable Development Goals with Machine Learning</b></a></span></div><div class=\"togglec clearfix\">Recent technological developments are creating new spatio-temporal data streams that contain a wealth of information relevant to sustainable development goals. Modern AI techniques have the potential to yield accurate, inexpensive, and highly scalable models to inform research and policy. As a first example, I will present a machine learning method we developed to predict and map poverty in developing countries. Our method can reliably predict economic well-being using only high-resolution satellite imagery. Because images are passively collected in every corner of the world, our method can provide timely and accurate measurements in a very scalable end economic way, and could revolutionize efforts towards global poverty eradication. As a second example, I will present some ongoing work on monitoring agricultural and food security outcomes from space.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 24 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Oct 24</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (cancelled)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Oct 31 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Oct 31</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.socsci.uci.edu/~harding1/\"><b>Matt Harding</b></a><br />Associate Professor<br />Department of Economics<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Using machine learning to understand large consumer transaction datasets</b></a></span></div><div class=\"togglec clearfix\">This talks explores recent uses of machine learning to large proprietary consumer transaction datasets. These are datasets which record barcode level transaction information on individual items purchased grouped by shopping trip and customer. Recent innovations in data collection allow us to go beyond the supermarket scanner to collect such data and include recent efforts to digitize the universe of customers\u2019 receipts across all channels from supermarkets to online purchases. Additionally, passive wifi tracking allows us to record search behavior in stores and model how it translates into sales. It also gives us the opportunity to create real time interventions to nudge consumer shopping behavior. We will explore some of the challenges of modeling consumer behavior using these data and discuss methods such as tensor decompositions for count data, discrete choice modeling with Dirichlet Process Mixtures, and the use of deep autoencoders for producing interpretable statistical hypotheses.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov  7 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov  7</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~wping/\"><b>Wei Ping</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning algorithms for RBMs and conditional RBMs</b></a></span></div><div class=\"togglec clearfix\">This talk investigates the restricted Boltzmann machine (RBM), which is the building block for many deep probabilistic models. We propose an infinite RBM model, whose maximum likelihood estimation corresponds to a constrained convex optimization. We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration. As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. We also investigate different learning algorithms for conditional RBMs. There is a pervasive opinion that loopy belief propagation does not work well on RBM-based models, especially for learning. We demonstrate that, in the conditional setting, learning RBM-based models with belief propagation and its variants can provide much better results than the state-of-the-art contrastive divergence algorithms.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 14 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov 14</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"\"><b>Cheng Zhang</b></a><br />PhD Candidate<br />Department of Mathematics<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Variational Hamiltonian Monte Carlo via Score Matching</b></a></span></div><div class=\"togglec clearfix\">Traditionally, the field of computational Bayesian statistics has been divided into two main subfields: variational inference and Markov chain Monte Carlo (MCMC). In recent years, however, several methods have been proposed based on combining variational Bayesian inference and MCMC simulation in order to improve their overall accuracy and computational efficiency. This marriage of fast evaluation and flexible approximation provides a promising means of designing scalable Bayesian inference methods. In this work, we explore the possibility of incorporating variational approximation into a state-of-the-art MCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required expensive computation involved in the sampling procedure, which is the bottleneck for many applications of HMC in big data problems. To this end, we exploit the regularity in parameter space to construct a free-form approximation of the target distribution by a fast and flexible surrogate function using an optimized additive model of proper random basis. The surrogate provides sufficiently accurate approximation while allowing for fast computation, resulting in an efficient approximate inference algorithm. We demonstrate the advantages of our method on both synthetic and real data problems.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 16 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov 16</b><br />Bren Hall 4011<br />4pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www-users.cs.umn.edu/~banerjee/\"><b>Arindam Banerjee</b></a><br />Associate Professor<br />Department of Computer Science and Engineering<br />University of Minnesota</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning with Low Samples in High-Dimensions: Estimators, Geometry, and Applications</b></a></span></div><div class=\"togglec clearfix\">Many machine learning problems, especially scientific problems in areas such as ecology, climate science, and brain sciences, operate in the so-called `low samples, high dimensions&#8217; regime. Such problems typically have numerous possible predictors or features, but the number of training examples is small, often much smaller than the number of features. In this talk, we will discuss recent advances in general formulations and estimators for such problems. These formulations generalize prior work such as the Lasso and the Dantzig selector. We will discuss the geometry underlying such formulations, and how the geometry helps in establishing finite sample properties of the estimators. We will also discuss applications of such results in structure learning in probabilistic graphical models, along with real world applications in ecology and climate science.</p>\n<p>This is joint work with Soumyadeep Chatterjee, Sheng Chen, Farideh Fazayeli, Andre Goncalves, Jens Kattge, Igor Melnyk, Peter Reich, Franziska Schrodt, Hanhuai Shan, and Vidyashankar Sivakumar.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 21 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov 21</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.cs.dartmouth.edu/~qliu/\"><b>Qiang Liu</b></a><br />Assistant Professor<br />Department of Computer Science<br />Dartmouth College</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Stein's method and practical machine learning: bridging the gap</b></a></span></div><div class=\"togglec clearfix\">Stein\u2019s method provides a remarkable theoretical tool in probability theory but has not been widely known or used in practical machine learning. In this talk, we try to bright this gap and show that some of the key ideas of Stein&#8217;s method can be naturally combined with practical machine learning and probabilistic inference techniques such as kernel method, variational inference and variance reduction, which together form a new general framework for deriving new algorithms for handling the kind of highly complex, structured probabilistic models widely used in modern (deep) machine learning. The new algorithms derived in this way often have a simple, untraditional form and have significant advantages over the traditional methods. I will show several applications, including goodness-of-fit tests for evaluating models without knowing the normalization constants, scalable Bayesian inference that combines the advantages of variational inference, Monte Carlo and gradient-based optimization, and approximate maximum likelihood training of deep generative models that can generate realistic-looking images.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Nov 28 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Nov 28</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://www.andrew.cmu.edu/user/gatt/\"><b>Wolfgang Gatterbauer</b></a><br />Assistant Professor<br />Tepper School of Business<br />Carnegie Mellon University</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Oblivious Bounds on the Probability of Boolean Functions</b></a></span></div><div class=\"togglec clearfix\">We develop upper and lower bounds for the probability of Boolean functions by treating multiple occurrences of variables as independent and assigning them new individual probabilities. We call this approach \u201cdissociation\u201d and give an exact characterization of optimal oblivious bounds, i.e. when the new probabilities are chosen independent of the probabilities of all other variables.</p>\n<p>Our motivation comes from the weighted model counting problem (or, equivalently, the problem of computing the probability of a Boolean function), which is \\#P-hard in general. By performing several dissociations, one can transform a Boolean formula whose probability is difficult to compute, into one whose probability is easy to compute, and which is guaranteed to provide an upper or lower bound on the probability of the original formula by choosing appropriate probabilities for the dissociated variables. Our new bounds shed light on the connection between previous relaxation-based and model-based approximations and unify them as concrete choices in a larger design space. We also show how our theory allows a standard relational database management system to both upper and lower bound hard probabilistic queries in guaranteed polynomial time.  (Based on joint work with Dan Suciu from TODS 2014, VLDB 2015, and VLDBJ 2016: http://arxiv.org/pdf/1409.6052,http://arxiv.org/pdf/1412.1069, http://arxiv.org/pdf/1310.6257)</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Dec  5 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Dec  5</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar</b><br />Finals Week</div>\n<p>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2016/10/fall-2016/\" title=\"3:09 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2016-10-12T15:09:37-07:00\">October 12, 2016</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t\r\n<article id=\"post-602\" class=\"post-602 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<header class=\"entry-header\">\r\n\t\t<!-- This is the output of the POST THUMBNAIL (if exists): REMOVED (h1 is clear:both) -->\r\n\t\t<!--<div class=\"entry-thumbnail\">\r\n\t\t\t\t\t</div> -->\r\n\t\t<h1 class=\"entry-title\"><a href=\"https://cml.ics.uci.edu/2016/04/spring-2016/\" title=\"Permalink to Spring 2016\" rel=\"bookmark\">Spring 2016</a></h1>\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t</header><!-- .entry-header -->\r\n\r\n\t\t<div class=\"entry-content\">\r\n\t\t<br />\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Apr 4  =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Apr 4 </b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Cancelled)</b></div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 11 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 11</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://users.cms.caltech.edu/~venkatc/\"><b>Venkat Chandrasekaran</b></a><br />Assistant Professor<br />Computing and Mathematical Sciences &#038; Electrical Engineering<br />California Institute of Technology</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Finding Planted Subgraphs using the Schur-Horn Relaxation</b></a></span></div><div class=\"togglec clearfix\">Extracting structured planted subgraphs from large graphs is a fundamental question that arises in a range of application domains. We describe a computationally tractable approach based on convex optimization to recover certain families of structured graphs that are embedded in larger graphs containing spurious edges. Our method relies on tractable semidefinite descriptions of majorization inequalities on the spectrum of a matrix, and we give conditions on the eigenstructure of a planted graph in relation to the noise level under which our algorithm succeeds. (Joint work with Utkan Candogan.)</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 18 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 18</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://zacklipton.com/\"><b>Zach Chase Lipton</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, San Deigo</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Learning to Diagnose with LSTM Recurrent Neural Networks</b></a></span></div><div class=\"togglec clearfix\">Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient&#8217;s Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 25 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 25</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://engineering.uci.edu/users/jasper-vrugt\"><b>Jasper Vrugt</b></a><br />Associate Professor<br />Department of Environmental Engineering<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Bayesian inference, hypothesis testing, epistemic errors and diagnostic model evaluation</b></a></span></div><div class=\"togglec clearfix\">Bayesian inference has found widespread application and use in science and engineering to reconcile Earth system models with data, including prediction in space (interpolation), prediction in time (forecasting), assimilation of observations and deterministic/stochastic model output, and inference of the model parameters. In this talk I will review the basic elements of the DiffeRential Evolution Adaptive Metropolis (DREAM) algorithm developed by Vrugt et al. (2008,2009) and used for Bayesian inference in fields ranging from physics, chemistry and engineering, to ecology, hydrology, and geophysics. I will also discuss recent developments of DREAM, including the development of a diagnostic model evaluation framework using likelihood free inference, and the use of dimensionality reduction techniques for calibration of CPU-intensive system models. Practical examples are used from many different fields of study.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 2  =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 2 </b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://engineering.purdue.edu/~qobi/\"><b>Jeffrey Mark Siskind</b></a><br />Associate Professor<br />Department of Electrical &#038; Computer Engineering<br />Purdue University</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Decoding the Brain to Help Build Machines</b></a></span></div><div class=\"togglec clearfix\">Humans can describe observations and act upon requests.  This requires that language be grounded in perception and motor control.  I will present several components of my long-term research program to understand the vision-language-motor interface in the human brain and emulate such on computers.</p>\n<p> In the first half of the talk, I will present fMRI investigation of the vision-language interface in the human brain.  Subjects were presented with stimuli in different modalities&#8212;spoken sentences, textual presentation of sentences, and video clips depicting activity that can be described by sentences&#8212;while undergoing fMRI.  The scan data is analyzed to allow readout of individual constituent concepts and words&#8212;people/names, objects/nouns, actions/verbs, and spatial-relations/prepositions&#8212;as well as phrases and entire sentences.  This can be done across subjects and across modality; we use classifiers trained on scan data for one subject to read out from another subject and use classifiers trained on scan data for one modality, say text, to read out from scans of another modality, say video or speech.  Analysis of this indicates that the brain regions involved in processing the different kinds of constituents are largely disjoint but also largely shared across subjects and modality.  Further, we can determine the predication relations; when the stimuli depict multiple people, objects, and actions, we can read out which people are performing which actions with which objects.  This points to a compositional mental semantic representation common across subjects and modalities.</p>\n<p> In the second half of the talk, I will use this work to motivate the development of three computational systems.  First, I will present a system that can use sentential description of human interaction with previously unseen objects in video to automatically find and track those objects.  This is done without any annotation of the objects and without any pretrained object detectors.  Second, I will present a system that learns the meanings of nouns and prepositions from video and tracks of a mobile robot navigating through its environment paired with sentential descriptions of such activity.  Such a learned language model then supports both generation of sentential description of new paths driven in new environments as well as automatic driving of paths to satisfy navigational instructions specified with new sentences in new environments.  Third, I will present a system that can play a physically grounded game of checkers using vision to determine game state and robotic arms to change the game state by reading the game rules from natural-language instructions.</p>\n<p> Joint work with Andrei Barbu, Daniel Paul Barrett, Charles Roger Bradley, Seth Benjamin Scott Alan Bronikowski, Zachary Burchill, Wei Chen, N. Siddharth, Caiming Xiong, Haonan Yu, Jason J. Corso, Christiane D. Fellbaum, Catherine Hanson, Stephen Jose Hanson, Sebastien Helie, Evguenia Malaia, Barak A. Pearlmutter, Thomas Michael Talavage, and Ronnie B. Wilbur.</p>\n<p> <b>Bio:</b> Jeffrey M. Siskind received the B.A. degree in computer science from the Technion, Israel Institute of Technology, Haifa, in 1979, the S.M. degree in computer science from the Massachusetts Institute of Technology (M.I.T.), Cambridge, in 1989, and the Ph.D. degree in computer science from M.I.T. in 1992.  He did a postdoctoral fellowship at the University of Pennsylvania Institute for Research in Cognitive Science from 1992 to 1993.  He was an assistant professor at the University of Toronto Department of Computer Science from 1993 to 1995, a senior lecturer at the Technion Department of Electrical Engineering in 1996, a visiting assistant professor at the University of Vermont Department of Computer Science and Electrical Engineering from 1996 to 1997, and a research scientist at NEC Research Institute, Inc. from 1997 to 2001.  He joined the Purdue University School of Electrical and Computer Engineering in 2002 where he is currently an associate professor.  His research interests include computer vision, robotics, artificial intelligence, neuroscience, cognitive science, computational linguistics, child language acquisition, automatic differentiation, and programming languages and compilers.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 9  =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 9 </b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"https://scholar.google.com/citations?user=R3ru5X8AAAAJ\"><b>Forest Agostinelli</b></a><br />PhD Candidate<br />Department of Computer Science<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Deep Learning for Circadian  Biology</b></a></span></div><div class=\"togglec clearfix\">Circadian rhythms date back to the origins of life, are found in virtually every species and every cell, and play fundamental roles in functions ranging from metabolism to cognition. Modern high-throughput technologies allow the measurement of concentrations of transcripts, metabolites, and other species along the circadian cycle creating novel computational challenges and opportunities, including the problems of inferring whether a given species oscillate in circadian fashion or not, and inferring the time at which a set of measurements was taken. Due to the expensive process of taking these measurements, inferring whether a given species oscillate in circadian fashion has proven to be a challenge. The sparse data with only a few replicates makes many existing methods unreliable. In addition, many differential gene expression experiments&#8211;such as those contained in the GEO repository, have been carried at single time points without taking into account circadian oscillations which can act as confounding factors.  To solve these problems we introduce two deep learning methods: BIO_CYCLE and BIO_CLOCK. BIO_CYCLE takes advantage of synthetic data to determine whether or not a signal oscillates in a circadian fashion, and infer periods, amplitudes, and phases. BIO_CLOCK, using a specialized cost function and real-world data, imputes the time at which a sample was taken, from the corresponding gene expression measurements.  These tools are a necessary step forward to better understand circadian rhythms at the molecular level and their applications to precision medicine.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 16 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 16</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://newport.eecs.uci.edu/~amowli/\"><b>Aparna Chandramowlishwaran</b></a><br />Assistant Professor<br />Department of Electrical Engineering<br />University of California, Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Performance and productivity of N-body solvers</b></a></span></div><div class=\"togglec clearfix\"><br />\nIn this talk, I&#8217;ll present my group&#8217;s work on addressing two key challenges in developing parallel algorithms and software for the class of N-body problems on current and future platforms.  The first challenge is reducing the apparent gap in performance between code generated from high-level forms and that of hand-tuned code, which we address using extensive characterization of the optimization space for these computations and automating the process through domain specific code generators. These application-specific compilers provide the domain scientists the ability to productively harness the power of these large machines and to enable large-scale scientific simulations and big data applications.</p>\n<p>The second challenge is analyzing and designing algorithms. We are entering the era of exascale.  The number of cores are growing at a much faster rate than bandwidth per node. What implications does this trend have in designing algorithms for future systems? If we were to model computation and communication costs, what inferences can we derive from such a model for the time to execute an algorithm? Our model suggests a new kind of high level analytical co-design of the algorithm and architecture and similar analysis can be applied in designing algorithms in general.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 23 =================================== --></p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 23</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.its.caltech.edu/~dvij/\"><b>Divijotham Krishnamurthy</b></a><br />Postdoctoral Fellow<br />Center for Mathematics of Information<br />California Institute of Technology</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Graphical models for power systems analysis</b></a></span></div><div class=\"togglec clearfix\">Several problems arising in the design, analysis and efficient operation of power systems are naturally posed as graph-structured optimization problems. Due to the nonlinear nature of the physical equations describing the power grid, these problems are often nonconvex and NP-hard. However, practical instances of several graph-structured optimization problems have been solved successfully in the graphical models literature by exploiting graph structure and using message-passing or belief propagation techniques. In this work, we show that a similar approach can be successfully applied to power systems, leading to theoretically and practically efficient algorithms. I will discuss two applications in detail: a) Solving mixed-integer optimal power flow problems on distribution networks, and b) Detecting and mitigating market manipulation by aggregators of renewable generation in a distribution-level market. I will also discuss possible extensions of these approaches to other power system/infrastructure network problems. <br />Based on joint work with Misha Chertkov, Sidhant Misra, Marc Vuffray, Pascal Van Hentenryck, Niangjun Chen, Navid Azizan Ruhi and Adam Wierman.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 30 =================================== --></p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>May 30</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>No Seminar (Memorial Day)</b></div>\n<p>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\t\r\n\t<footer class=\"entry-meta\">\r\n\t\t\t\t<a href=\"https://cml.ics.uci.edu/2016/04/spring-2016/\" title=\"11:18 am\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2016-04-08T11:18:41-07:00\">April 8, 2016</time></a>\t\t\r\n\t\t\r\n\t\t\t</footer><!-- .entry-meta -->\r\n</article><!-- #post-## -->\r\n\r\n\n\t\t\t\n\t\t\t\t<nav class=\"navigation-paging\" role=\"navigation\">\n\t\t<h1 class=\"screen-reader-text\">Posts navigation</h1>\n\t\t<div class=\"nav-links\">\n\n\t\t\t\t\t\t<div class=\"nav-previous\"><a href=\"https://cml.ics.uci.edu/category/aiml/page/2/\" ><span class=\"meta-nav\">&lsaquo;</span><span class=\"screen-reader-text\">Older posts</span></a></div>\n\t\t\t\n\t\t\t\n\t\t</div><!-- .nav-links -->\n\t</nav><!-- .navigation -->\n\t\n\t\t\n\t\t</div><!-- #content -->\n\t</section><!-- #primary -->\n\n\t<div id=\"secondary\" class=\"widget-area\" role=\"complementary\">\n\t\t\t\t<aside id=\"search-2\" class=\"widget widget_search\">\t<form method=\"get\" id=\"searchform\" class=\"searchform\" action=\"https://cml.ics.uci.edu/\" role=\"search\">\n\t\t<label for=\"s\" class=\"screen-reader-text\">Search</label>\n\t\t<input type=\"search\" class=\"field\" name=\"s\" value=\"\" id=\"s\" placeholder=\"Search &hellip;\" />\n\t\t<input type=\"submit\" class=\"submit\" id=\"searchsubmit\" value=\"Search\" />\n\t</form>\n</aside>\t</div><!-- #secondary -->\n\r\n</div><!-- #page -->\r\n\r\n<footer id=\"colophon\" class=\"site-footer\" role=\"contentinfo\">\r\n<p style=\"text-align:center;margin:0;\">(c) 2015 <a href=\"http://cml.ics.uci.edu\">Center for Machine Learning and Intelligent Systems</a>\r\n\t<div class=\"site-info\">\r\n\t\t\t\t<a href=\"http://wordpress.org/\" rel=\"generator\">WordPress</a>/<a href=\"http://www.wpzoom.com/\">BonPress</a>\r\n\t</div><!-- .site-info -->\r\n</footer><!-- #colophon -->\r\n\r\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/navigation.js?ver=20120206'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/wp-embed.min.js?ver=5.2.3'></script>\n\r\n</body>\r\n</html>\r\n", "encoding": "utf-8"}