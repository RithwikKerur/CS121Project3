{"url": "https://cml.ics.uci.edu/aiml/page/13/", "content": "<!DOCTYPE html>\n<html lang=\"en-US\">\n<head>\n<meta charset=\"UTF-8\" />\n<meta name=\"viewport\" content=\"width=device-width\" />\n<title>AI/ML Seminar Series | Center for Machine Learning and Intelligent Systems | Page 13</title>\n<link rel=\"profile\" href=\"http://gmpg.org/xfn/11\" />\n<link rel=\"pingback\" href=\"https://cml.ics.uci.edu/xmlrpc.php\" />\n<!--[if lt IE 9]>\n<script src=\"https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/html5.js\" type=\"text/javascript\"></script>\n<![endif]-->\n\n<link rel='dns-prefetch' href='//s.w.org' />\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Center for Machine Learning and Intelligent Systems &raquo; Feed\" href=\"https://cml.ics.uci.edu/feed/\" />\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"Center for Machine Learning and Intelligent Systems &raquo; Comments Feed\" href=\"https://cml.ics.uci.edu/comments/feed/\" />\n\t\t<script type=\"text/javascript\">\n\t\t\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/12.0.0-1\\/72x72\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/12.0.0-1\\/svg\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"https:\\/\\/cml.ics.uci.edu\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=5.2.3\"}};\n\t\t\t!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline=\"top\",l.font=\"600 32px Arial\",a){case\"flag\":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case\"emoji\":return b=d([55357,56424,55356,57342,8205,55358,56605,8205,55357,56424,55356,57340],[55357,56424,55356,57342,8203,55358,56605,8203,55357,56424,55356,57340]),!b}return!1}function f(a){var c=b.createElement(\"script\");c.src=a,c.defer=c.type=\"text/javascript\",b.getElementsByTagName(\"head\")[0].appendChild(c)}var g,h,i,j,k=b.createElement(\"canvas\"),l=k.getContext&&k.getContext(\"2d\");for(j=Array(\"flag\",\"emoji\"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],\"flag\"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener(\"DOMContentLoaded\",h,!1),a.addEventListener(\"load\",h,!1)):(a.attachEvent(\"onload\",h),b.attachEvent(\"onreadystatechange\",function(){\"complete\"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);\n\t\t</script>\n\t\t<style type=\"text/css\">\nimg.wp-smiley,\nimg.emoji {\n\tdisplay: inline !important;\n\tborder: none !important;\n\tbox-shadow: none !important;\n\theight: 1em !important;\n\twidth: 1em !important;\n\tmargin: 0 .07em !important;\n\tvertical-align: -0.1em !important;\n\tbackground: none !important;\n\tpadding: 0 !important;\n}\n</style>\n\t<link rel='stylesheet' id='wp-block-library-css'  href='https://cml.ics.uci.edu/wp-includes/css/dist/block-library/style.min.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='bonpress-style-css'  href='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-cml/style.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='tipsy-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/tipsy.css?ver=5.2.3' type='text/css' media='all' />\n<link rel='stylesheet' id='mts_wpshortcodes-css'  href='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/css/wp-shortcode.css?ver=5.2.3' type='text/css' media='all' />\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery.js?ver=1.12.4-wp'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/jquery.tipsy.js?ver=5.2.3'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/plugins/wp-shortcode/js/wp-shortcode.js?ver=5.2.3'></script>\n<link rel='https://api.w.org/' href='https://cml.ics.uci.edu/wp-json/' />\n<link rel=\"EditURI\" type=\"application/rsd+xml\" title=\"RSD\" href=\"https://cml.ics.uci.edu/xmlrpc.php?rsd\" />\n<link rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\" href=\"https://cml.ics.uci.edu/wp-includes/wlwmanifest.xml\" /> \n<meta name=\"generator\" content=\"WordPress 5.2.3\" />\n<link rel=\"canonical\" href=\"https://cml.ics.uci.edu/aiml/\" />\n<link rel='shortlink' href='https://cml.ics.uci.edu/?p=60' />\n<link rel=\"alternate\" type=\"application/json+oembed\" href=\"https://cml.ics.uci.edu/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fcml.ics.uci.edu%2Faiml%2F\" />\n<link rel=\"alternate\" type=\"text/xml+oembed\" href=\"https://cml.ics.uci.edu/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fcml.ics.uci.edu%2Faiml%2F&#038;format=xml\" />\n</head>\n\n<body class=\"paged page-template-default page page-id-60 page-parent paged-13 page-paged-13 group-blog\">\n<div id=\"page\" class=\"hfeed site\">\n\t\t<header id=\"masthead\" class=\"all-header\" role=\"banner\">\n\t\t<hgroup class=\"hgroup-wide\">\n                        <a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>\n<!--\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2> -->\n\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">Bren School of Information and Computer Science</h2>\t\t\t\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2>\n\t\t\t<div style=\"clear:both\"></div>\n\t\t</hgroup>\n\t</header>\n\t<header id=\"masthead\" class=\"site-header\" role=\"banner\">\n\t\t<hgroup class=\"hgroup-img\">\n                        <a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\"><img src='/wp-content/cml/uploads/cml-curve.jpg'></a>\n\t\t\t<h1 class=\"site-title\"><a href=\"https://cml.ics.uci.edu/\" title=\"Center for Machine Learning and Intelligent Systems\" rel=\"home\">Center for Machine Learning and Intelligent Systems</a></h1>\n\t\t\t<h2 class=\"site-description\">University of California, Irvine</h2>\n\t\t</hgroup>\n\n\t\t<nav id=\"site-navigation\" class=\"navigation-main\" role=\"navigation\">\n\t\t\t<h1 class=\"menu-toggle\">Menu</h1>\n\t\t\t<div class=\"screen-reader-text skip-link\"><a href=\"#content\" title=\"Skip to content\">Skip to content</a></div>\n\n\t\t\t<div class=\"menu-navigation-container\"><ul id=\"menu-navigation\" class=\"menu\"><li id=\"menu-item-234\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-234\"><a href=\"https://cml.ics.uci.edu/\">Home</a></li>\n<li id=\"menu-item-79\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-79\"><a href=\"https://cml.ics.uci.edu/home/about-us/\">About CML</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-78\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-78\"><a href=\"https://cml.ics.uci.edu/home/about-us/\">About us</a></li>\n\t<li id=\"menu-item-429\" class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-429\"><a href=\"https://cml.ics.uci.edu/category/news/\">News</a></li>\n\t<li id=\"menu-item-76\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-76\"><a href=\"https://cml.ics.uci.edu/home/contact-us/\">Contact Us</a></li>\n</ul>\n</li>\n<li id=\"menu-item-539\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-539\"><a>People</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-55\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-55\"><a href=\"https://cml.ics.uci.edu/faculty/\">Faculty</a></li>\n\t<li id=\"menu-item-220\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-220\"><a href=\"https://cml.ics.uci.edu/alumni/\">Alumni</a></li>\n</ul>\n</li>\n<li id=\"menu-item-75\" class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-60 current_page_item current-menu-ancestor current-menu-parent current_page_parent current_page_ancestor menu-item-has-children menu-item-75\"><a href=\"https://cml.ics.uci.edu/aiml/\" aria-current=\"page\">Events &#038; Seminars</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-74\" class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-60 current_page_item menu-item-74\"><a href=\"https://cml.ics.uci.edu/aiml/\" aria-current=\"page\">AI/ML Seminar Series</a></li>\n\t<li id=\"menu-item-914\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-914\"><a href=\"https://cml.ics.uci.edu/aiml/ml-distinguished-speakers/\">ML Distinguished Speakers</a></li>\n\t<li id=\"menu-item-73\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-73\"><a href=\"https://cml.ics.uci.edu/aiml/ml-reading-group/\">ML Reading Group</a></li>\n</ul>\n</li>\n<li id=\"menu-item-222\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-222\"><a>Education &#038; Resources</a>\n<ul class=\"sub-menu\">\n\t<li id=\"menu-item-227\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-227\"><a href=\"https://cml.ics.uci.edu/courses/\">Courses</a></li>\n\t<li id=\"menu-item-221\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-221\"><a href=\"https://cml.ics.uci.edu/books/\">Books</a></li>\n</ul>\n</li>\n<li id=\"menu-item-81\" class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-81\"><a href=\"http://www.ics.uci.edu/~mlearn/MLRepository.html\">UCI Machine Learning Archive</a></li>\n<li id=\"menu-item-87\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-87\"><a href=\"https://cml.ics.uci.edu/sponsors-funding/\">Sponsors &#038; Funding</a></li>\n<li id=\"menu-item-86\" class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-86\"><a href=\"https://cml.ics.uci.edu/subscribe/\">Subscribe to CML List</a></li>\n</ul></div>\t\t</nav><!-- #site-navigation -->\n\t</header><!-- #masthead -->\n\n\r\n\t<div id=\"primary\" class=\"content-area\">\r\n\t\t<div id=\"content\" class=\"site-content\" role=\"main\">\r\n\t\t\t<!-- <p style=\"text-align: center;\">\r\n\t\t\tAI/ML Weekly Seminar<br>Sponsored by Yahoo! Research\r\n\t\t\t</p> -->\r\n\t\t\t\t\t\t\t<article style=\"margin-bottom: 0;\" id=\"post-60\" class=\"post-60 page type-page status-publish hentry\">\r\n\t\t\t\t<header class=\"entry-header\">\r\n\t\t\t\t\t<h1 class=\"entry-title\">AI/ML Seminar Series</h1>\t\t\t\t\t<span class=\"entry-format genericon\">Standard</span>\t\t\t\t</header><!-- .entry-header -->\r\n\t\t\t\t<div class=\"entry-content\">\r\n\t\t\t\t\t<p style=\"text-align: center;\">Weekly Seminar in AI &#038; Machine Learning<br />Sponsored by Cylance</p>\n<p><!-- \n\n<p style=\"text-align: center;\">Weekly Seminar in AI & Machine Learning<br />Sponsored by Yahoo! Research</p>\n\n --></p>\n\t\t\t\t\t\t\t\t\t</div><!-- .entry-content -->\r\n\t\t\t\t<php get_template_part( 'content', 'aiml' ); >\r\n\t\t\t\t</article>\r\n\t\t\t\r\n\t\t\t<!-- Ditch old query and run new one getting schedule posts -->\r\n\t\t\t\r\n\t\t\t\t\t\t\t\r\n<article id=\"post-532\" class=\"post-532 post type-post status-publish format-standard hentry category-aiml\">\r\n\t<nav class=\"navigation-paging\" role=\"navigation\">\r\n\t\t<h1 class=\"screen-reader-text\">Navigation</h1>\r\n\t\t<div class=\"nav-links\">\r\n\t\t\t<div class=\"nav-previous\" class=\"inline\"><a href=\"https://cml.ics.uci.edu/aiml/page/14/\" ><span class=\"meta-nav\">&lsaquo;</span><span class=\"screen-reader-text\">Earlier</span></a></div>\r\n\t\t\t\t\t\t\t\t\t<h1 class=\"entry-title\">Spring 2015</h1>\t\t\t<div class=\"nav-next\" class=\"inline\"><a href=\"https://cml.ics.uci.edu/aiml/page/12/\" ><span class=\"screen-reader-text\">Later </span><span class=\"meta-nav\">&rsaquo;</span></a></div>\r\n\t\t\t\t\t\t\t\t</div><!-- .nav-links -->\r\n\t</nav><!-- .navigation -->\r\n\r\n\t<div class=\"entry-content\">\r\n\t\t<br />\n<table cellpadding=5 border=1>\n<col width=\"100\">\n<col>\n<p>  <!-- ==== Mar 30 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Mar 30</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.igb.uci.edu/~pfbaldi/\"><b>Pierre Baldi</b></a><br />Chancellor&#8217;s Professor<br />Department of Computer Science<br />UC Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>The Ebb and Flow of Deep Learning: a Theory of Local Learning</b></a></span></div><div class=\"togglec clearfix\">In a physical neural system, where storage and processing are intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons. We propose a systematic framework to define and study the space of local learning rules where one must first define the nature of the local variables, and then the functional form that ties them together into a learning rule. We consider polynomial learning rules and analyze their behavior and capabilities in both linear and non-linear networks. As a byproduct, we also show how this framework enables the discovery of new learning rules and important relationships between learning rules and group symmetries.</p>\n<p> Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, we show however that it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is propagated to the deep layers. The complexity of the propagated information about the targets and the channel through which this information is propagated partition the space of learning algorithms and highlight the remarkable power of the backpropagation algorithm. The theory clarifies the concept of Hebbian learning, what is learnable by Hebbian learning, and explains the sparsity of the space of learning rules discovered so far.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr  6 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr  6</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://ee.usc.edu/nseip/\"><b>Maryam M. Shanechi</b></a><br />Assistant Professor<br />Department of Electrical Engineering and Computer Science<br />University of Southern California</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Closed-Loop Brain-Machine Interface Architectures</b></a></span></div><div class=\"togglec clearfix\">A brain-machine-interface (BMI) is a system that interacts with the brain either to allow the brain to control an external device or to control the brain&#8217;s state. While these two BMI types are for different applications, they can both be viewed as closed-loop control systems. In this talk, I present our work on developing both these types of BMIs, specifically motor BMIs for restoring movement in paralyzed patients and a new BMI for control of the brain state under anesthesia. Motor BMIs have largely used standard signal processing techniques. However, devising novel algorithmic solutions that are tailored to the neural system can significantly improve the performance of these BMIs. Here, I develop a novel BMI paradigm for restoration of motor function that incorporates an optimal feedback-control model of the brain and directly processes the spiking activity using point process modeling. I show that this paradigm significantly outperforms the state-of-the-art in closed-loop primate experiments. In addition to motor BMIs, I construct a new BMI that controls the state of the brain under anesthesia. This is done by designing stochastic controllers that infer the brain&#8217;s anesthetic state from non-invasive observations of neural activity and control the real-time rate of drug administration to achieve a target brain state. I show the reliable performance of this BMI in rodent experiments.  </p>\n<p><b>Bio:</b></p>\n<p> Maryam Shanechi is an assistant professor in the Ming Hsieh Department of Electrical Engineering at the University of Southern California (USC). Prior to joining USC, she was an assistant professor in the School of Electrical and Computer Engineering at Cornell University. She received the B.A.Sc. degree in Engineering Science from the University of Toronto in 2004 and the S.M. and Ph.D. degrees in Electrical Engineering and Computer Science from MIT in 2006 and 2011, respectively. She is the recipient of the NSF CAREER Award and has been named by the MIT Technology Review as one of the world\u2019s top 35 innovators under the age of 35 (TR35) for her work on brain-machine interfaces.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 13 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 13</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.ics.uci.edu/~mjcarey/\"><b>Michael Carey</b></a><br />Professor<br />Department of Computer Science<br />UC Irvine</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>AsterixDB: A Scalable, Open Source BDMS</b></a></span></div><div class=\"togglec clearfix\">AsterixDB is a new BDMS (Big Data Management System) with a feature set that sets it apart from other Big Data platforms in today&#8217;s open source ecosystem. Its features make it well-suited to applications including web data warehousing, social data storage and analysis, and other use cases related to Big Data. AsterixDB has a flexible NoSQL style data model; a query language that supports a wide range of queries, a scalable runtime; partitioned, LSM-based data storage and indexing (including B+ tree, R tree, and text indexes); support for external as well as native data; a rich set of built-in types, including spatial, temporal, and textual types; support for fuzzy, spatial, and temporal queries; a built-in notion of data feeds for ingestion of data; and transaction support akin to that of a NoSQL store.  </p>\n<p> Development of AsterixDB began in 2009 and led to a mid-2013 initial open source release. This talk will provide an overview of the resulting system. Time permitting, the talk will cover the system&#8217;s data model, its query language, and its basic architecture. Also included will be a summary of the current status of the project and a discussion of some of the &#8220;plug-in points&#8221; where AsterixDB can be made to interoperate with ML technologies. The talk will conclude with some thoughts on opportunities for future ML-related collaborations related to AsterixDB.  </p>\n<p> <b>Bio:</b> </p>\n<p> Michael J. Carey is a Bren Professor of Information and Computer Sciences at UC Irvine. Before joining UCI in 2008, he worked at BEA Systems for seven years and led the development of BEA&#8217;s AquaLogic Data Services Platform product for virtual data integration. He also spent a dozen years teaching at the University of Wisconsin-Madison, five years at the IBM Almaden Research Center working on object-relational databases, and a year and a half at e-commerce platform startup Propel Software during the infamous 2000-2001 Internet bubble. Carey is an ACM Fellow, a member of the National Academy of Engineering, and a recipient of the ACM SIGMOD E.F. Codd Innovations Award. His current interests center around data-intensive computing and scalable data management (a.k.a. Big Data).</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 20 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>Apr 20</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://crisco.seas.harvard.edu/\"><b>Cris Cecka</b></a><br />Research Scientist<br />NVIDIA Research</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Hierarchical Algorithms for Fast Linear Algebra</b></a></span></div><div class=\"togglec clearfix\">N-body problems are ubiquitous with applications ranging from linear algebra to scientific computing and machine learning. N-body methods were identified as one of the original 7 dwarves or motifs of computation and are believed to be important in the next decade. These methods include FMMs, Treecodes, H-matrices, Butterfly algorithms, and geometric shattering. The relationship between these approaches is understood, but many of the demonstrated tools for developing and applying these algorithms remain ad-hoc, inaccessible, or inefficient.  </p>\n<p> We present recent developments towards a codebase that is abstracted over the primary domains of research in this field and is optimized for modern multicore systems. Core components including tree construction, tree traversal, and low-rank operators are developed independently and parallelized for multicore CPUs and GPUs. Applications include dense problems in machine learning and computational geometry (k-nearest neighbors, range search, kernel density estimation, Gaussian processes, and RBF kernels), treecode and fast multipole methods in computational physics (gravitational potentials, screened Coulomb interactions, Stokes flow, and Helmholtz equations), and matrix compression, computation, and inversion (PLR, HODLR, H2, and Butterfly).  </p>\n<p> In this presentation, we will review a high-level perspective of the research domain, the abstraction and parallelization strategies, and how these methods can be made more practical.  </p>\n<p> <b>Bio:</b> </p>\n<p> Cris received his PhD from Stanford University in Computational and Mathematical Engineering in 2011. As a lecturer and research scientist with the new Institute for Applied Computational Science at Harvard University, he developed core courses on parallel computing and robust software development for scientific computing. In 2014, Cris joined the Mathematics Department at the Massachusetts Institute of Technology as a research associate where he focused on developing and applying generalized N-body methods to dense linear algebra using hierarchical methods. Currently, he works in NVIDIA Research to continue to make these techniques accessible with modern parallel programming models. You can read more about his research on his Harvard web page.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== Apr 27 =================================== --> </p>\n<tr>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-date\"><b>Apr 27</b></div>\n</td>\n<td valign=top class='aiml-none'>\n<div class=\"aiml-name\"><b>Cancelled</b><br />(no seminar)</div>\n<p>\n  </td>\n</tr>\n<p>  <!-- ==== May  4 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May  4</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://www.cs.bgu.ac.il/~roiwei/\"><b>Roi Weiss</b></a><br />PhD student<br />Department of Computer Science<br />Ben Gurion University of the Negev</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>On learning parametric-output HMMs</b></a></span></div><div class=\"togglec clearfix\">Hidden Markov models (HMMs) are a standard tool in the modeling and analysis of time series with a wide variety of applications. Yet, learning their parameters remain a challenging problem. In the first part of the talk I will present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters.  </p>\n<p> The above approach (and other recently proposed spectral/tensor methods) strongly depends on the assumption that all states have different output distributions. In various applications, however, some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In the second part of the talk, as a first step, I will focus on parametric-output HMMs that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition parameters. We illustrate our theoretical analysis by several simulations.  </p>\n<p> A joint work with Boaz Nadler and Aryeh Kontorovich.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 11 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 11</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://acsweb.ucsd.edu/~asuresh/\"><b>Ananda Theertha Suresh</b></a><br />PhD student<br />Department of Electrical Engineering<br />UC San Diego</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Approximating Spherical Gaussian Mixtures</b></a></span></div><div class=\"togglec clearfix\">Many statistical and machine-learning applications call for estimating Gaussian mixtures using a limited number of samples and computational time. PAC (proper) learning estimates a distribution in a class by some distribution in the same class to a desired accuracy. Using spectral projections we show that spherical Gaussian mixtures in d-dimensions can be PAC learned with O*(d) samples, and that the same applies for learning the distribution&#8217;s parameters. Our algorithm is information theoretically near-optimal and significantly improves previously known time and sample complexities.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== May 18 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>May 18</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://cnl.salk.edu/People/Person/?Person=1302\"><b>Saeed Saremi</b></a><br />Postdotoral Fellow<br />The Computational Neurobiology Laboratory<br />Salk Institute</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Percolation in natural images</b></a></span></div><div class=\"togglec clearfix\">Natural images are scale invariant with structures at all length scales. After a tutorial on critical phenomena and percolation theory, I will talk about formulating a geometric view of scale invariance. In this model, the scale invariance of natural images is understood as a second-order percolation phase transition. It is further quantified by fractal dimensions, and by the scale-free distribution of clusters in natural images. This formulation leads to a method for identifying clusters in images and a starting point for image segmentation.  </p>\n<p> <b>Bio:</b> </p>\n<p> Saeed Saremi received the Ph.D. degree in theoretical physics from MIT. He then joined the lab of Terry Sejnowski at the Salk Institute as a postdoctoral fellow. His research blends machine learning, statistical mechanics, and computational neuroscience, with the long-term goal of understanding the principles for achieving artificial intelligence.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== June 1 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>June 1</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://teamcore.usc.edu/people/sorianom/\"><b>Leandro Soriano Marcolino</b></a><br />PhD student<br />Viterbi School of Engineering<br />University of Southern California</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Unleashing the Power of Multi-agent Voting Teams</b></a></span></div><div class=\"togglec clearfix\">Teams of voting agents have great potential in finding optimal solutions, and they have been used in many important domains, such as: machine learning, crowdsourcing, forecasting systems, and even board games. Voting is popular since it is highly parallelizable, easy to implement and provide theoretical guarantees. However, there are three fundamental challenges: (i) Selecting a limited number of agents to compose a team; (ii) Combining the opinions of the team members; (iii) Assessing the performance of a given team. In this talk, I address all these challenges, showing both theoretical and experimental results. I explore three different domains: Computer Go, HIV prevention via influencing social networks and architectural design.  </p>\n<p> <b>Bio:</b> </p>\n<p> Leandro Soriano Marcolino is a PhD student at University of Southern California (USC), advised by Milind Tambe. He has published in several prestigious conferences in AI, robotics and machine learning, such as AAAI, AAMAS, IJCAI, NIPS, ICRA and IROS. He received the best research assistant award from the Computer Science Department at USC, had a paper nominated for best paper from the leading multi-agent conference AAMAS, and had his undergraduate work selected as the best one by the Brazilian Computer Science Society. He has been researching continuously about teamwork and cooperation, and obtained his masters degree in Japan, with the highly-competitive Monbukagakusho scholarship. Over his career, Leandro has published about a variety of domains, such as swarm robotics, computer Go, social networks, bioinformatics and architectural design.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n<p>  <!-- ==== June 8 =================================== --> </p>\n<tr>\n<td valign=top>\n<div class=\"aiml-date\"><b>June 8</b><br />Bren Hall 4011<br />1 pm</div>\n</td>\n<td valign=top>\n<div class=\"aiml-name\"><a href=\"http://users.cms.caltech.edu/~qberthet/\"><b>Quentin Berthet</b></a><br />CMI Postdoctoral Fellow<br />Computing + Mathematical Sciences, Annenberg Center<br />California Institute of Technology</div>\n<p>\n  <div class=\"toggle clearfix wp_shortcodes_toggle\"><div class=\"wps_togglet\"><span><a><b>Resource Allocation for Statistical Estimation</b></a></span></div><div class=\"togglec clearfix\">Statistical estimation in many contemporary settings involves the acquisition, analysis, and aggregation of datasets from multiple sources, which can have significant differences in character and in value. Due to these variations, the effectiveness of employing a given resource \u2013 e.g., a sensing device or computing power \u2013 for gathering or processing data from a particular source depends on the nature of that source. As a result, the appropriate division and assignment of a collection of resources to a set of data sources can substantially impact the overall performance of an inferential strategy. In this expository article, we adopt a general view of the notion of a resource and its effect on the quality of a data source, and we describe a framework for the allocation of a given set of resources to a collection of sources in order to optimize a specified metric of statistical efficiency. We discuss several stylized examples involving inferential tasks such as parameter estimation and hypothesis testing based on heterogeneous data sources, in which optimal allocations can be computed either in closed form or via efficient numerical procedures based on convex optimization. Joint work with V. Chandrasekaran.</div></div><div class=\"clear\"></div>\n  </td>\n</tr>\n</table>\n\t\t\t</div><!-- .entry-content -->\r\n\r\n\t\t<div class=\"entry-meta\">\r\n\t\t\t<a href=\"https://cml.ics.uci.edu/2015/03/spring-2015/\" title=\"4:27 pm\" rel=\"bookmark\"><time class=\"entry-date genericon\" datetime=\"2015-03-01T16:27:01-07:00\">March 1, 2015</time></a> \r\n\r\n\t\t\t\r\n\t\t\t<span class=\"cat-links genericon\"><a href=\"https://cml.ics.uci.edu/category/aiml/\" rel=\"category tag\">AIML</a></span>\r\n\t\t\t\t\t</div>\r\n\r\n\t</article><!-- #post-## -->\r\n\r\n\t\t\t\t\t\t\t<nav class=\"navigation-paging\" role=\"navigation\">\n\t\t<h1 class=\"screen-reader-text\">Posts navigation</h1>\n\t\t<div class=\"nav-links\">\n\n\t\t\t\t\t\t<div class=\"nav-previous\"><a href=\"https://cml.ics.uci.edu/aiml/page/14/\" ><span class=\"meta-nav\">&lsaquo;</span><span class=\"screen-reader-text\">Older posts</span></a></div>\n\t\t\t\n\t\t\t\t\t\t<div class=\"nav-next\"><a href=\"https://cml.ics.uci.edu/aiml/page/12/\" ><span class=\"screen-reader-text\">Newer posts </span><span class=\"meta-nav\">&rsaquo;</span></a></div>\n\t\t\t\n\t\t</div><!-- .nav-links -->\n\t</nav><!-- .navigation -->\n\t\t\t</div><!-- #content -->\r\n\t</div><!-- #primary -->\r\n\r\n\t<div id=\"secondary\" class=\"widget-area\" role=\"complementary\">\n\t\t\t\t<aside id=\"search-2\" class=\"widget widget_search\">\t<form method=\"get\" id=\"searchform\" class=\"searchform\" action=\"https://cml.ics.uci.edu/\" role=\"search\">\n\t\t<label for=\"s\" class=\"screen-reader-text\">Search</label>\n\t\t<input type=\"search\" class=\"field\" name=\"s\" value=\"\" id=\"s\" placeholder=\"Search &hellip;\" />\n\t\t<input type=\"submit\" class=\"submit\" id=\"searchsubmit\" value=\"Search\" />\n\t</form>\n</aside>\t</div><!-- #secondary -->\n\r\n</div><!-- #page -->\r\n\r\n<footer id=\"colophon\" class=\"site-footer\" role=\"contentinfo\">\r\n<p style=\"text-align:center;margin:0;\">(c) 2015 <a href=\"http://cml.ics.uci.edu\">Center for Machine Learning and Intelligent Systems</a>\r\n\t<div class=\"site-info\">\r\n\t\t\t\t<a href=\"http://wordpress.org/\" rel=\"generator\">WordPress</a>/<a href=\"http://www.wpzoom.com/\">BonPress</a>\r\n\t</div><!-- .site-info -->\r\n</footer><!-- #colophon -->\r\n\r\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/navigation.js?ver=20120206'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-content/cml/themes/bonpress-wpcom/js/skip-link-focus-fix.js?ver=20130115'></script>\n<script type='text/javascript' src='https://cml.ics.uci.edu/wp-includes/js/wp-embed.min.js?ver=5.2.3'></script>\n\r\n</body>\r\n</html>\r\n", "encoding": "utf-8"}