{"url": "http://flamingo.ics.uci.edu/releases/4.0/src/lbaktree/src/lbaktree.cc", "content": "/*\n $Id: lbaktree.cc 5795 2010-10-23 01:18:03Z abehm $\n \n Copyright (C) 2010 by The Regents of the University of California\n \n Redistribution of this file is permitted under\n the terms of the BSD license.\n \n Date: 08/19/2010\n Author: Sattam Alsubaiee <salsubai (at) ics.uci.edu>\n*/\n\n#include <limits>\n#include \"lbaktree.h\"\n\nLBAKTree::LBAKTree(Storage *storage, AlgorithmType type, unsigned il, float simT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n\tif(type != fl)\n\t{\n\t\tcout << \"error: wrong algorithm name\" << endl;\n\t\texit(1);\n\t}\n\tinit(type, simT);\n\tindexesLevel = il;\n}\n\nLBAKTree::LBAKTree(Storage *storage, string &file, AlgorithmType type, double sb, float simT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n\tif(type != vl)\n\t{\n\t\tcout << \"error: wrong algorithm name\" << endl;\n\t\texit(1);\n\t}\n    init(type, simT);\n\tqueryWorkloadFile = file;\n\tspaceBudget = sb;\n}\n\nLBAKTree::LBAKTree(Storage *storage, string &file, AlgorithmType type, double sb, float simT, float kfT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n\tif(type != vlf)\n\t{\n\t\tcout << \"error: wrong algorithm name\" << endl;\n\t\texit(1);\n\t}\n    init(type, simT);\n\tqueryWorkloadFile = file;\n\tspaceBudget = sb;\n\tkfThreshold = kfT;\n}\n\nLBAKTree::~LBAKTree()\n{\n\tdelete gramGen;\n\tunordered_map <unsigned, vector<unsigned> *>::iterator mit1;\n\tfor (mit1 = recordsMap.begin(); mit1 != recordsMap.end(); ++mit1) \n\t{\n\t\tdelete recordsMap[mit1->first];\n\t}\n\trecordsMap.clear();\n\tunordered_map <unsigned, Array<unsigned> *>::iterator mit2;\n\tfor (mit2 = keywordsHashesMap.begin(); mit2 != keywordsHashesMap.end(); ++mit2) \n\t{\n\t\tdelete keywordsHashesMap[mit2->first];\n\t}\n\tkeywordsHashesMap.clear();\n\tunordered_map <unsigned, WrapperSimpleEdNorm *>::iterator mit3;\n\tfor (mit3 = wrappersMap.begin(); mit3 != wrappersMap.end(); ++mit3) \n\t{\n\t\tdelete wrappersMap[mit3->first];\n\t}\n\twrappersMap.clear();\n\tunordered_map <unsigned, StringContainerVector *>::iterator mit4;\n\tfor (mit4 = strContainersMap.begin(); mit4 != strContainersMap.end(); ++mit4) \n\t{\n\t\tdelete strContainersMap[mit4->first];\n\t}\n\tstrContainersMap.clear();\n}\n\nvoid LBAKTree::init(AlgorithmType type, float simT)\n{\n\tcreate();\n\talgoType = type;\n\tsimThreshold = simT;\n\tavgKwdsLength = 0;\n\tnumKwds = 0;\n\tq = 2;\n\tgramGen = new GramGenFixedLen(q);\n\tsx = 0;\n    sy = 0;\n    sx2 = 0;\n    sy2 = 0;\n    sxy = 0;\n    n = 0;\n}\n\nvoid LBAKTree::insert(const Object &obj, vector <string> &kwds)\n{\n    recordsMap[obj.id] = new vector <unsigned> ();\n    for(unsigned i = 0; i < kwds.size(); ++i)\n    {\n        unordered_map<string, unsigned>::iterator it;\n        it = keywordsMap.find(kwds.at(i));\n\t\tavgKwdsLength += (double)kwds.at(i).length();\n\t\t++numKwds;\n        if(it == keywordsMap.end())\n        {\n            dictionary.push_back(kwds.at(i));\n            keywordsMap[kwds.at(i)] = dictionary.size() - 1;\n            recordsMap[obj.id]->push_back(dictionary.size() - 1);\n        }\n        else\n        {\n            recordsMap[obj.id]->push_back(it->second);\n        }\n    }\n    RTree::insert(obj);\n}\n\nvoid LBAKTree::buildIndex()\n{\n\tif(!kf1.open(true)) \n\t{\n\t\tcout << \"fatal error: failed to open temp1 file\" << endl;\n\t\texit(1);\n\t}\n\tif(!kf2.open(true)) \n\t{\n\t\tcout << \"fatal error: failed to open temp2 file\" << endl;\n\t\texit(1);\n\t}\n\tpropagateKeywords(storage->getRoot());   \n\tselectSANodes();\n\tfillKeywordsHashesMap();  \n\tfillWrappersMap();\n    kf1.close();\n\tkf2.close();\n}\n\nvoid LBAKTree::readQueryWorkload(const Rectangle &range)\n{\n    unsigned id = storage->getRoot();\n    ++queryWorkloadMap[id];\n    readQueryWorkload(range, id);\n}\n\nvoid LBAKTree::readQueryWorkload(const Rectangle &range, unsigned objectId)\n{\n    Node *node = (Node *)storage->read(objectId);\n    for(unsigned i = 0; i < node->numChildren; ++i)\n    {\n        if(node->objects[i].mbr.intersects(range))\n        {\n            ++queryWorkloadMap[node->objects[i].id];\n            if(!node->isLeaf())\n            {\n                readQueryWorkload(range, node->objects[i].id);\n            }\n        }\n    }\n    storage->free(node);\n}\n\nvoid LBAKTree::propagateKeywords(unsigned objectId)\n{\n    Node *node = (Node *)storage->read(objectId);\n\tunordered_set<string> kwds;\n    if(node->isLeaf())\n    {\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n\t\t\tinsertKeywords(node->objects[i].id, kwds, true);\n        }\n    }\n    else\n    {\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            propagateKeywords(node->objects[i].id);\n\t\t\tinsertKeywords(node->objects[i].id, kwds, false);\n        }\n    }\n\tstring text;\n\tunordered_set<string>::iterator it;\n\tfor (it = kwds.begin(); it != kwds.end(); ++it)\n\t{\n\t\ttext += *it;\n\t\ttext += \" \";\n\t}\n\tkf1.write(text, node->id, kwds.size());\n    storage->free(node);\n}\n\nvoid LBAKTree::insertKeywords(unsigned objectId, unordered_set<string> &kwds, bool leaf)\n{\t\t\n\tif(leaf)\n\t{\n\t\tfor (unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n\t\t{\n\t\t\tkwds.insert(dictionary[recordsMap[objectId]->at(i)]);\n\t\t}\n\t}\n\telse\n\t{\n\t\tstring text = kf1.read(objectId);\n\t\tparseKeywords(text, kwds);\n\t}\n}\n\nvoid LBAKTree::parseKeywords(string &text, unordered_set<string> &kwds)\n{\n\tstring::size_type lastPos = text.find_first_not_of(\" \", 0);\n\tstring::size_type pos = text.find_first_of(\" \", lastPos);\n\twhile (string::npos != pos || string::npos != lastPos)\n\t{\n\t\tkwds.insert(text.substr(lastPos, pos - lastPos));\n\t\tlastPos = text.find_first_not_of(\" \", pos);\n\t\tpos = text.find_first_of(\" \", lastPos);\n\t}\n}\n\nvoid LBAKTree::computeGradientIntercept(unsigned objectId, vector <string> &keywords)\n{\n    Node *node = (Node *)storage->read(objectId);\n\tstring text = kf1.read(node->id);\n\tunordered_set<string> kwds;\n\tparseKeywords(text, kwds);\n\tif(objectId == storage->getRoot())\n\t{\n\t\tsrand((unsigned)time(0));\n\t\tfor(unsigned i = 0; i < 100; ++i)\n\t\t{\n\t\t\tkeywords.push_back(dictionary[rand() % dictionary.size()]);\n\t\t}\n\t}\n\tStringContainerVector strContainer;\n\tstrContainer.initStatsCollector(gramGen);\n\tunordered_set<string>::iterator it;\n\tfor (it = kwds.begin(); it != kwds.end(); ++it)\n\t{\n\t\tstrContainer.insertString(*it);\n\t}\n\tWrapperSimpleEdNorm wrapper(&strContainer, gramGen, false);\n\twrapper.buildIndex();\n\tfor(unsigned i = 0; i < keywords.size(); ++i)\n\t{\n\t\tvector<unsigned> resultStringIDs;\n\t\tstruct timeval t1, t2;\n\t\tstruct timezone tz;\n\t\tstartTimeMeasurement(t1, tz);\n\t\twrapper.search(keywords.at(i), simThreshold, resultStringIDs);\n\t\tstopTimeMeasurement(t2, tz);\n\t\tdouble timeMeasurement = getTimeMeasurement(t1, t2);\n\t\tsx += (double) strContainer.size();\n\t\tsy += timeMeasurement;\n\t\tsx2 += ((double)strContainer.size() * (double)strContainer.size());\n\t\tsy2 += (timeMeasurement * timeMeasurement);\n\t\tsxy += ((double)strContainer.size() * timeMeasurement);\n\t\t++n;\n\t}\n\tif(!node->isLeaf())\n    {\n\t\tcomputeGradientIntercept(node->objects[0].id, keywords);\n    }\n    storage->free(node);\n}\n\nvoid LBAKTree::selectSANodes()\n{\n\tavgKwdsLength /= (double)numKwds;\n\tif(algoType == fl)\n\t{\n\t\tuseFL();\n\t}\n\telse\n\t{\n\t\tvector<string> keywords;\n\t\tcomputeGradientIntercept(storage->getRoot(), keywords);\n\t\tgradient = ((sx * sy) - (n * sxy)) / ((sx * sx) - (n * sx2));\n\t\tintercept = ((sx * sxy) - (sy * sx2)) / ((sx * sx) - (n * sx2));\n\t\t\n\t\tifstream queries(queryWorkloadFile.c_str());\n\t\tif (!queries)\n\t\t{\n\t\t\tcerr << \"fatal error: failed to open query workload file\" << endl;\n\t\t\texit(1);\n\t\t}\n\t\tstring line;\n\t\twhile (getline(queries, line))\n\t\t{\n\t\t\tRectangle range;\n\t\t\tstring coordinates = line.substr(0, line.find(\",\"));\n\t\t\tistringstream coordinatesStream(coordinates);\n\t\t\tcoordinatesStream >> range.min.x >> range.min.y >> range.max.x >> range.max.y;\n\t\t\treadQueryWorkload(range);\n\t\t}\n\t\tif(algoType == vl)\n\t\t{\n\t\t\tuseVL();\n\t\t}\n\t\telse\n\t\t{\n\t\t\tfillKeywordsIntersectionsFile();\n\t\t\tuseVLF();\n\t\t}\n\t}\n}\n\nvoid LBAKTree::useFL()\n{\n\tNode *root = (Node *)storage->read(storage->getRoot());\n\tif(indexesLevel > root->level)\n\t{\n\t\tindexesLevel = root->level;\n\t}\n\tstorage->free(root);\n\t\n\tunordered_map <unsigned, IndexNode>::iterator mit; \n    for (mit = kf1.begin(); mit != kf1.end(); ++mit) \n    {\n\t\tNode *node = (Node *)storage->read(mit->first);\n        if(node->level >= indexesLevel)\n        {\n\t\t\tStringContainerVector *strContainer;\n\t\t\tstrContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n\t\t\tstrContainersMap[node->id] = strContainer;\n\t\t}\n\t\tstorage->free(node);\n\t}\n}\n\nvoid LBAKTree::useVL()\n{\n    vector<NodePriority> heap;\n    Node *root = (Node *)storage->read(storage->getRoot());\n    double pFuzzySpaceCost = (double)kf1.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    double pFuzzyTimeCost = (double)queryWorkloadMap[root->id] * (gradient * (double)kf1.getIndexNode(root->id).numKeywords + intercept);\n    double cFuzzySpaceCost = 0;\n    double cFuzzyTimeCost = 0;\n    for(unsigned i = 0; i < root->numChildren; ++i)\n    {\n        cFuzzySpaceCost += (double)kf1.getIndexNode(root->objects[i].id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n        cFuzzyTimeCost += (double)queryWorkloadMap[root->objects[i].id] * (gradient * (double)kf1.getIndexNode(root->objects[i].id).numKeywords + intercept);\n    }\n    NodePriority rootImportance;\n    rootImportance.id = root->id;\n    rootImportance.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n    rootImportance.pFuzzySpaceCost = pFuzzySpaceCost;\n    rootImportance.cFuzzySpaceCost = cFuzzySpaceCost;\n    rootImportance.pFuzzyTimeCost = pFuzzyTimeCost;\n    rootImportance.cFuzzyTimeCost = cFuzzyTimeCost;\n    spaceBudget -= pFuzzySpaceCost;\n    heap.push_back(rootImportance);\n    storage->free(root);\n    while(!heap.empty())\n    {\n        NodePriority nodeImportance;\n        nodeImportance.id = heap[0].id;\n        nodeImportance.priority = heap[0].priority;\n        nodeImportance.pFuzzySpaceCost = heap[0].pFuzzySpaceCost;\n        nodeImportance.cFuzzySpaceCost = heap[0].cFuzzySpaceCost;\n        nodeImportance.pFuzzyTimeCost = heap[0].pFuzzyTimeCost;\n        nodeImportance.cFuzzyTimeCost = heap[0].cFuzzyTimeCost;\n        pop_heap(heap.begin(), heap.end());\n        heap.pop_back();\n        Node *node = (Node *)storage->read(nodeImportance.id);\n        if(nodeImportance.priority > 0 && nodeImportance.cFuzzySpaceCost <= (spaceBudget + nodeImportance.pFuzzySpaceCost) && !node->isLeaf() && node->level != 1)\n        {\n            spaceBudget += nodeImportance.pFuzzySpaceCost;\n            spaceBudget -= nodeImportance.cFuzzySpaceCost;\n\t\t\tStringContainerVector *strContainer;\n\t\t\tstrContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n\t\t\tstrContainersMap[node->id] = strContainer;\n        }\n        else\n        {\n\t\t\tStringContainerVector *strContainer;\n\t\t\tstrContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n\t\t\tstrContainersMap[node->id] = strContainer;\n            storage->free(node);\n            continue;\n        }\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            Node *node2 = (Node *)storage->read(node->objects[i].id);\n            pFuzzySpaceCost = (double)kf1.getIndexNode(node2->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n            pFuzzyTimeCost = (double)queryWorkloadMap[node2->id] * (gradient * (double)kf1.getIndexNode(node2->id).numKeywords + intercept);\n            cFuzzySpaceCost = 0;\n            cFuzzyTimeCost = 0;\n            if(node2->isLeaf())\n            {\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = std::numeric_limits<double>::max(); \n            }\n            else\n            {\n                for(unsigned j = 0; j < node2->numChildren; ++j)\n                {\n                    cFuzzySpaceCost += (double)kf1.getIndexNode(node2->objects[j].id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                    cFuzzyTimeCost += (double)queryWorkloadMap[node2->objects[j].id] *  (gradient * (double)kf1.getIndexNode(node2->objects[j].id).numKeywords + intercept);\n                }\n            }\n            NodePriority nodeImportance2;\n            nodeImportance2.id = node2->id;\n            nodeImportance2.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n            nodeImportance2.pFuzzySpaceCost = pFuzzySpaceCost;\n            nodeImportance2.cFuzzySpaceCost = cFuzzySpaceCost;\n            nodeImportance2.pFuzzyTimeCost = pFuzzyTimeCost;\n            nodeImportance2.cFuzzyTimeCost = cFuzzyTimeCost;\n            heap.push_back(nodeImportance2);\n            push_heap(heap.begin(), heap.end());\n            storage->free(node2);\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::useVLF()\n{\n    vector<NodePriority> heap;\n    Node *root = (Node *)storage->read(storage->getRoot());\n    double pFuzzySpaceCost = (double)kf1.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    double pFuzzyTimeCost = (double)queryWorkloadMap[root->id] * (gradient * (double)kf1.getIndexNode(root->id).numKeywords + intercept);\n    double cFuzzySpaceCost = 0;\n    double cFuzzyTimeCost = 0;\n    for(unsigned j = 0; j < root->numChildren; ++j)\n    {\n        unordered_set <string> intersectedKeywords;\n        unordered_set<string>::iterator it;\n\t\tstring text = kf1.read(root->objects[j].id);\n\t\tunordered_set<string> kwds;\n\t\tparseKeywords(text, kwds);\n\t\tstring intsctText = kf2.read(root->objects[j].id);\n\t\tunordered_set<string> intsctKwds;\n\t\tparseKeywords(intsctText, intsctKwds);\n        for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n        {\n            if(kwds.find(*it) != kwds.end())\n            {\n                intersectedKeywords.insert(*it);\n            }\n        }\n        cFuzzySpaceCost += (double)(kf1.getIndexNode(root->objects[j].id).numKeywords - intersectedKeywords.size()) * (avgKwdsLength + (double)q - 1.0) * 4.0;\n        cFuzzyTimeCost += (double)queryWorkloadMap[root->objects[j].id] *  (gradient * (double)(kf1.getIndexNode(root->objects[j].id).numKeywords - intersectedKeywords.size()) + intercept);\n    }\n    cFuzzySpaceCost += (double)kf2.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    cFuzzyTimeCost += (double)queryWorkloadMap[root->id] *  (gradient * (double)kf2.getIndexNode(root->id).numKeywords + intercept);\n    NodePriority rootImportance;\n    rootImportance.id = root->id;\n    if(pFuzzySpaceCost == cFuzzySpaceCost)\n    {\n        rootImportance.priority = pFuzzyTimeCost - cFuzzyTimeCost;\n    }\n    else\n    {\n        rootImportance.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n    }\n    rootImportance.pFuzzySpaceCost = pFuzzySpaceCost;\n    rootImportance.cFuzzySpaceCost = cFuzzySpaceCost;\n    rootImportance.pFuzzyTimeCost = pFuzzyTimeCost;\n    rootImportance.cFuzzyTimeCost = cFuzzyTimeCost;\n    spaceBudget -= pFuzzySpaceCost;\n    heap.push_back(rootImportance);\n    storage->free(root);\n    while(!heap.empty())\n    {\n        NodePriority nodeImportance;\n        nodeImportance.id = heap[0].id;\n        nodeImportance.priority = heap[0].priority;\n        nodeImportance.pFuzzySpaceCost = heap[0].pFuzzySpaceCost;\n        nodeImportance.cFuzzySpaceCost = heap[0].cFuzzySpaceCost;\n        vector <unsigned> ancestorsIds = heap[0].ancestorsIds;\n        nodeImportance.pFuzzyTimeCost = heap[0].pFuzzyTimeCost;\n        nodeImportance.cFuzzyTimeCost = heap[0].cFuzzyTimeCost;\n        pop_heap(heap.begin(), heap.end());\n        heap.pop_back();\n        Node *node = (Node *)storage->read(nodeImportance.id);\n        if(nodeImportance.priority > 0 && nodeImportance.cFuzzySpaceCost <= (spaceBudget + nodeImportance.pFuzzySpaceCost) && !node->isLeaf())\n        {\n            spaceBudget += nodeImportance.pFuzzySpaceCost;\n            spaceBudget -= nodeImportance.cFuzzySpaceCost;\n\t\t\tStringContainerVector *strContainer;\n\t\t\tstrContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n\t\t\tstrContainersMap[node->id] = strContainer;\n        }\n        else\n        {\n\t\t\tStringContainerVector *strContainer;\n\t\t\tstrContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n\t\t\tstrContainersMap[node->id] = strContainer;\n            storage->free(node);\n            continue;\n        }\n        ancestorsIds.push_back(nodeImportance.id);\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            Node *node2 = (Node *)storage->read(node->objects[i].id);\n            string text = kf1.read(node2->id);\n\t\t\tunordered_set<string> kwds;\n\t\t\tparseKeywords(text, kwds);\n\t\t\tstring intsctText = kf2.read(node2->id);\n\t\t\tunordered_set<string> intsctKwds;\n\t\t\tparseKeywords(intsctText, intsctKwds);\t\n\t\t\tunordered_map<unsigned, unordered_set<string> > tempMap;\n\t\t\tfor(unsigned j = 0; j < ancestorsIds.size(); ++j)\n\t\t\t{\n\t\t\t\tstring intsctText2 = kf2.read(ancestorsIds[j]);\n\t\t\t\tparseKeywords(intsctText2, tempMap[ancestorsIds[j]]);\n\t\t\t}\n            if(node2->isLeaf())\n            {\n                for(unsigned j = 0; j < ancestorsIds.size(); ++j)\n                {\n                    unordered_set<string>::iterator it;\n                    for (it = tempMap[ancestorsIds[j]].begin(); it != tempMap[ancestorsIds[j]].end(); ++it)\n                    {\n                        kwds.erase(*it);\n                    }\n                }\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = std::numeric_limits<double>::max(); \n            }\n            else\n            {\n                for(unsigned j = 0; j < ancestorsIds.size(); ++j)\n                {\n\t\t\t\t\tunordered_set<string>::iterator it;\n                    for (it = tempMap[ancestorsIds[j]].begin(); it != tempMap[ancestorsIds[j]].end(); ++it)\n                    {\n\t\t\t\t\t\tintsctKwds.erase(*it);\n                        kwds.erase(*it);\n                    }\n                }\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = 0;\n                ancestorsIds.push_back(node2->id);\n                for(unsigned j = 0; j < node2->numChildren; ++j)\n                {\n\t\t\t\t\tstring text2 = kf1.read(node2->objects[j].id);\n\t\t\t\t\tunordered_set<string> kwds2;\n\t\t\t\t\tparseKeywords(text2, kwds2);\n                    unordered_set <string> intersectedKeywords;\n                    for(unsigned k = 0; k < ancestorsIds.size(); ++k)\n                    {\n\t\t\t\t\t\tunordered_set<string>::iterator it;\n\t\t\t\t\t\tfor (it = tempMap[ancestorsIds[k]].begin(); it != tempMap[ancestorsIds[k]].end(); ++it)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif(kwds2.find(*it) != kwds2.end())\n                            {\n                                intersectedKeywords.insert(*it);\n                            }\n\t\t\t\t\t\t}\n                    }\n                    cFuzzySpaceCost += (double)(kf1.getIndexNode(node2->objects[j].id).numKeywords - intersectedKeywords.size()) * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                    cFuzzyTimeCost += (double)queryWorkloadMap[node2->objects[j].id] *  (gradient * (double)(kf1.getIndexNode(node2->objects[j].id).numKeywords - intersectedKeywords.size()) + intercept);\n                }\n                cFuzzySpaceCost += (double)intsctKwds.size() * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                cFuzzyTimeCost += (double)queryWorkloadMap[node2->id] *  (gradient * (double)intsctKwds.size() + intercept);\n                ancestorsIds.pop_back();\n            }\n            pFuzzySpaceCost = (double)kwds.size() * (avgKwdsLength + (double)q - 1.0) * 4.0;\n            pFuzzyTimeCost = (double)queryWorkloadMap[node2->id] * (gradient * (double)kwds.size() + intercept);\n            NodePriority nodeImportance2;\n            nodeImportance2.id = node2->id;\n            if(pFuzzySpaceCost == cFuzzySpaceCost)\n            {\n                nodeImportance2.priority = pFuzzyTimeCost - cFuzzyTimeCost;\n            }\n            else\n            {\n                nodeImportance2.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n            }\n            nodeImportance2.pFuzzySpaceCost = pFuzzySpaceCost;\n            nodeImportance2.cFuzzySpaceCost = cFuzzySpaceCost;\n            nodeImportance2.ancestorsIds = ancestorsIds;\n            nodeImportance2.pFuzzyTimeCost = pFuzzyTimeCost;\n            nodeImportance2.cFuzzyTimeCost = cFuzzyTimeCost;\n            heap.push_back(nodeImportance2);\n            push_heap(heap.begin(), heap.end());\n\t\t\ttext = \"\";\n\t\t\tunordered_set<string>::iterator it;\n\t\t\tfor (it = kwds.begin(); it != kwds.end(); ++it)\n\t\t\t{\n\t\t\t\ttext += *it;\n\t\t\t\ttext += \" \";\n\t\t\t}\n\t\t\tkf1.write(text, node2->id, kwds.size());\n\t\t\tintsctText = \"\";\n\t\t\tfor (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n\t\t\t{\n\t\t\t\tintsctText += *it;\n\t\t\t\tintsctText += \" \";\n\t\t\t}\n\t\t\tkf2.write(intsctText, node2->id, intsctKwds.size());\n            storage->free(node2);\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::fillKeywordsIntersectionsFile()\n{\n\tunordered_map <unsigned, IndexNode>::iterator mit; \n    for (mit = kf1.begin(); mit != kf1.end(); ++mit) \n    {\n\t\tNode *node = (Node *)storage->read(mit->first);\n        if(!node->isLeaf())\n        {\n\t\t\tunordered_set<string> intsctKwds;\n\t\t\tstring text = kf1.read(node->id);\n\t\t\tunordered_set<string> kwds;\n\t\t\tparseKeywords(text, kwds);\t\n\t\t\tunordered_map<unsigned, unordered_set<string> > tempMap;\n\t\t\tfor(unsigned i = 0; i < node->numChildren; ++i)\n\t\t\t{\n\t\t\t\tstring text2 = kf1.read(node->objects[i].id);\n\t\t\t\tparseKeywords(text2, tempMap[node->objects[i].id]);\n\t\t\t}\n\t\t\tunordered_set<string>::iterator it;\n\t\t\tfor (it = kwds.begin(); it != kwds.end(); ++it)\n\t\t\t{\n\t\t\t\tdouble counter = 0;\n\t\t\t\tfor(unsigned i = 0; i < node->numChildren; ++i)\n\t\t\t\t{\n\t\t\t\t\tif(tempMap[node->objects[i].id].find(*it) != tempMap[node->objects[i].id].end())\n\t\t\t\t\t{\n\t\t\t\t\t\t++counter;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif(counter >= (double)node->numChildren * kfThreshold)\n\t\t\t\t{\n\t\t\t\t\tintsctKwds.insert(*it);\n\t\t\t\t}\n\t\t\t}\n\t\t\tstring intsctText;\n\t\t\tfor (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n\t\t\t{\n\t\t\t\tintsctText += *it;\n\t\t\t\tintsctText += \" \";\n\t\t\t}\n\t\t\tkf2.write(intsctText, node->id, intsctKwds.size());\n        }\n        storage->free(node);\n\t}\n}\n\nvoid LBAKTree::fillKeywordsHashesMap()\n{\t\n\tunordered_map <unsigned, IndexNode>::iterator mit; \n    for (mit = kf1.begin(); mit != kf1.end(); ++mit) \n    {\n\t\tNode *node = (Node *)storage->read(mit->first);\n\t\tif(strContainersMap.find(node->id) == strContainersMap.end())\n        {\n\t\t\tstring text = kf1.read(node->id);\n\t\t\tunordered_set<string> kwds;\n\t\t\tparseKeywords(text, kwds);\n            Array<unsigned> *array = new Array<unsigned>(kwds.size(), 1);\n            unordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n\t\t\t\tarray->append(keywordsMap[*it]);\n            }\n            sort (array->begin(), array->end());\n            keywordsHashesMap[node->id] = array;\n        }\n        storage->free(node);\n\t}\n}\n\nvoid LBAKTree::fillWrappersMap()\n{\n    WrapperSimpleEdNorm *wrapper;\n\tunordered_map <unsigned, IndexNode>::iterator mit; \n    for (mit = kf1.begin(); mit != kf1.end(); ++mit) \n    {\n        Node *node = (Node *)storage->read(mit->first);\n\t\tif(strContainersMap.find(node->id) != strContainersMap.end() && strContainersMap.find(node->objects[0].id) == strContainersMap.end())\n        {\n\t\t\tstring text = kf1.read(node->id);\n\t\t\tunordered_set<string> kwds;\n\t\t\tparseKeywords(text, kwds);\n\t\t\tunordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n                strContainersMap[node->id]->insertString(*it);\n            }\n            wrapper = new WrapperSimpleEdNorm(strContainersMap[node->id], gramGen, false);\n            wrapper->buildIndex();\n            wrappersMap[node->id] = wrapper;\n        }\n\t\telse if(strContainersMap.find(node->objects[0].id) != strContainersMap.end())\n        {\n\t\t\tstring intsctText = kf2.read(node->id);\n\t\t\tunordered_set<string> intsctKwds;\n\t\t\tparseKeywords(intsctText, intsctKwds);\n\t\t\tunordered_set<string>::iterator it;\n            for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n            {\n                strContainersMap[node->id]->insertString(*it);\n            }\n            wrapper = new WrapperSimpleEdNorm(strContainersMap[node->id], gramGen, false);\n            wrapper->buildIndex();\n            \n            wrappersMap[node->id] = wrapper;\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::rangeQuery(vector<Object> &objects, const Rectangle &range,\n\t\t\t\t\t\t  const vector <string> &kwds)\n{\n    unsigned id = storage->getRoot();\n    vector<string> strings[kwds.size()];\n    vector<unsigned> hashes[kwds.size()];\n\t\n    if(searchWrapper(id, kwds, strings, hashes, strings, hashes))\n    {\n        rangeQuery(objects, range, id, kwds, strings, hashes);\n    }\n}           \n\nvoid LBAKTree::rangeQuery(vector<Object> &objects, const Rectangle &range, \n\t\t\t\t\t\t  unsigned id, const vector <string> &kwds, vector<string> strings[], vector<unsigned> hashes[])\n{\n    Node *node = (Node *)storage->read(id);\n    \n\tif(strContainersMap.find(node->id) != strContainersMap.end() && strContainersMap.find(node->objects[0].id) == strContainersMap.end())\n    {\n        for(unsigned i = 0; i < kwds.size(); ++i)\n        {\n            if(hashes[i].empty())\n            {\n                return;\n            }\n        }\n    }\n    for(unsigned i = 0; i < node->numChildren; ++i)\n    {\n        if(node->objects[i].mbr.intersects(range))\n        {\n            if(node->isLeaf())\n            {\n                bool keywordFounded = true;\n                for (unsigned j = 0; j < kwds.size(); ++j)\n                {\n                    bool candidateFounded = false;\n                    for (unsigned k = 0; k < strings[j].size(); ++k)\n                    {\n                        if(searchVector(node->objects[i].id, strings[j].at(k)))\n                        {\n                            candidateFounded = true;\n                            break;\n                        }\n                    }\n                    if (!candidateFounded)\n                    {\n                        keywordFounded = false;\n                        break;\n                    }\n                }\n                if(keywordFounded)\n                {\n                    objects.push_back(node->objects[i]);\n                }\n            }\n            else\n            {\n                vector<string> resultStrings[kwds.size()];\n                vector<unsigned> resultHashes[kwds.size()];\n\t\t\t\tif(strContainersMap.find(node->objects[i].id) != strContainersMap.end())\n                {\n                    if(searchWrapper(node->objects[i].id, kwds, strings, hashes, resultStrings, resultHashes))\n                    {\n                        rangeQuery(objects, range, node->objects[i].id, kwds, resultStrings, resultHashes);\n                    }\n                }\n                else\n                {\n                    if(searchArray(node->objects[i].id, kwds.size(), strings, hashes, resultStrings, resultHashes))\n                    {\n                        rangeQuery(objects, range, node->objects[i].id, kwds, resultStrings, resultHashes);\n                    }\n                }\n            }\n        }\n    }\n    storage->free(node);\n}\n\nbool LBAKTree::searchWrapper(unsigned objectId, const vector <string> &kwds, vector<string> strings[], vector<unsigned> hashes[], vector<string>resultStrings[], vector<unsigned>resultHashes[])\n{\n    for(unsigned i = 0; i < kwds.size(); ++i)\n    {   \n        vector<unsigned> resultStringIDs;\n        wrappersMap[objectId]->search(kwds.at(i), simThreshold, resultStringIDs);\n        for (unsigned j = 0; j < strings[i].size(); ++j)\n        {\n            resultStrings[i].push_back(strings[i].at(j));\n            resultHashes[i].push_back(hashes[i].at(j));\n        }\n        for (unsigned j = 0; j < resultStringIDs.size(); ++j)\n        {\n            string temp;\n            strContainersMap[objectId]->retrieveString(temp, resultStringIDs.at(j));\n            resultStrings[i].push_back(temp);\n\t\t\tresultHashes[i].push_back(keywordsMap[temp]);\n        }      \n    }\n    return true;\n}\n\nbool LBAKTree::searchArray(unsigned objectId, unsigned numKeywords, vector<string> strings[], vector<unsigned> hashes[], vector<string>resultStrings[], vector<unsigned>resultHashes[])\n{\n    for(unsigned i = 0; i < numKeywords; ++i)\n    {\n        for(unsigned j = 0; j < hashes[i].size(); ++j)\n        {\n            if(keywordsHashesMap[objectId]->has(hashes[i].at(j)))\n            {\n                resultStrings[i].push_back(strings[i].at(j));\n                resultHashes[i].push_back(hashes[i].at(j));\n            }\n        }\n        if (resultStrings[i].empty())\n            return false;\n    }\n    return true;\n}\n\nbool LBAKTree::searchVector(unsigned objectId, const string &keyword)\n{\n    for(unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n    {\n        if(dictionary[recordsMap[objectId]->at(i)].length() == keyword.length())\n        {\n            if(dictionary[recordsMap[objectId]->at(i)].compare(keyword) == 0)\n            {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n\nvoid LBAKTree::getObjectKeywords(unsigned objectId, vector<string> &objectKeywords)\n{\n\tfor(unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n\t{\n\t\tobjectKeywords.push_back(dictionary[recordsMap[objectId]->at(i)]);\n\t}\n}\n\nvoid LBAKTree::startTimeMeasurement(struct timeval &t1, struct timezone &tz)\n{\n\tgettimeofday(&t1, &tz);\n}\n\nvoid LBAKTree::stopTimeMeasurement(struct timeval &t2, struct timezone &tz)\n{\n\tgettimeofday(&t2, &tz);\n}\n\ndouble LBAKTree::getTimeMeasurement(struct timeval &t1, struct timeval &t2)\n{\n\tunsigned totalTime = (t2.tv_sec - t1.tv_sec) * 1000000 +\n\t(t2.tv_usec - t1.tv_usec);\n\tdouble tval = 0;\n\ttval = static_cast<double>(totalTime) / 1000;\n\treturn tval;\n}\n", "encoding": "ascii"}