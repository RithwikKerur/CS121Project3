{"url": "http://flamingo.ics.uci.edu/releases/4.1/src/lbaktree/src/lbaktree.cc", "content": "/*\n $Id: lbaktree.cc 6148 2012-02-22 23:13:40Z salsubaiee $\n\n Copyright (C) 2010 by The Regents of the University of California\n\n Redistribution of this file is permitted under\n the terms of the BSD license.\n\n Date: 08/19/2010\n Author: Sattam Alsubaiee <salsubai (at) ics.uci.edu>\n*/\n\n#include <limits>\n#include \"lbaktree.h\"\n\nLBAKTree::LBAKTree(Storage *storage, AlgorithmType type, unsigned il, float simT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n    if(type != fl)\n    {\n        cout << \"error: wrong algorithm name\" << endl;\n        exit(1);\n    }\n    init(type, simT);\n    indexesLevel = il;\n}\n\nLBAKTree::LBAKTree(Storage *storage, string &file, AlgorithmType type, double sb, float simT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n    if(type != vl)\n    {\n        cout << \"error: wrong algorithm name\" << endl;\n        exit(1);\n    }\n    init(type, simT);\n    queryWorkloadFile = file;\n    spaceBudget = sb;\n}\n\nLBAKTree::LBAKTree(Storage *storage, string &file, AlgorithmType type, double sb, float simT, float kfT) : RTree(storage), kf1(\"temp1\"), kf2(\"temp2\")\n{\n    if(type != vlf)\n    {\n        cout << \"error: wrong algorithm name\" << endl;\n        exit(1);\n    }\n    init(type, simT);\n    queryWorkloadFile = file;\n    spaceBudget = sb;\n    kfThreshold = kfT;\n}\n\nLBAKTree::~LBAKTree()\n{\n    delete gramGen;\n    unordered_map <uintptr_t, vector<unsigned> *>::iterator mit1;\n    for (mit1 = recordsMap.begin(); mit1 != recordsMap.end(); ++mit1)\n    {\n        delete recordsMap[mit1->first];\n    }\n    recordsMap.clear();\n    unordered_map <uintptr_t, Array<unsigned> *>::iterator mit2;\n    for (mit2 = keywordsHashesMap.begin(); mit2 != keywordsHashesMap.end(); ++mit2)\n    {\n        delete keywordsHashesMap[mit2->first];\n    }\n    keywordsHashesMap.clear();\n    unordered_map <uintptr_t, WrapperSimpleEdNorm *>::iterator mit3;\n    for (mit3 = wrappersMap.begin(); mit3 != wrappersMap.end(); ++mit3)\n    {\n        delete wrappersMap[mit3->first];\n    }\n    wrappersMap.clear();\n    unordered_map <uintptr_t, StringContainerVector *>::iterator mit4;\n    for (mit4 = strContainersMap.begin(); mit4 != strContainersMap.end(); ++mit4)\n    {\n        delete strContainersMap[mit4->first];\n    }\n    strContainersMap.clear();\n}\n\nvoid LBAKTree::init(AlgorithmType type, float simT)\n{\n    create();\n    algoType = type;\n    simThreshold = simT;\n    avgKwdsLength = 0;\n    numKwds = 0;\n    q = 2;\n    gramGen = new GramGenFixedLen(q);\n    sx = 0;\n    sy = 0;\n    sx2 = 0;\n    sy2 = 0;\n    sxy = 0;\n    n = 0;\n}\n\nvoid LBAKTree::insert(const Object &obj, vector <string> &kwds)\n{\n    recordsMap[obj.id] = new vector <unsigned> ();\n    for(unsigned i = 0; i < kwds.size(); ++i)\n    {\n        unordered_map<string, unsigned>::iterator it;\n        it = keywordsMap.find(kwds.at(i));\n        avgKwdsLength += (double)kwds.at(i).length();\n        ++numKwds;\n        if(it == keywordsMap.end())\n        {\n            dictionary.push_back(kwds.at(i));\n            keywordsMap[kwds.at(i)] = dictionary.size() - 1;\n            recordsMap[obj.id]->push_back(dictionary.size() - 1);\n        }\n        else\n        {\n            recordsMap[obj.id]->push_back(it->second);\n        }\n    }\n    RTree::insert(obj);\n}\n\nvoid LBAKTree::buildIndex()\n{\n    if(!kf1.open(true))\n    {\n        cout << \"fatal error: failed to open temp1 file\" << endl;\n        exit(1);\n    }\n    if(!kf2.open(true))\n    {\n        cout << \"fatal error: failed to open temp2 file\" << endl;\n        exit(1);\n    }\n    propagateKeywords(storage->getRoot());\n    selectSANodes();\n    fillKeywordsHashesMap();\n    fillWrappersMap();\n    kf1.close();\n    kf2.close();\n}\n\nvoid LBAKTree::readQueryWorkload(const Rectangle &range)\n{\n    uintptr_t id = storage->getRoot();\n    ++queryWorkloadMap[id];\n    readQueryWorkload(range, id);\n}\n\nvoid LBAKTree::readQueryWorkload(const Rectangle &range, uintptr_t objectId)\n{\n    Node *node = (Node *)storage->read(objectId);\n    for(unsigned i = 0; i < node->numChildren; ++i)\n    {\n        if(node->objects[i].mbr.intersects(range))\n        {\n            ++queryWorkloadMap[node->objects[i].id];\n            if(!node->isLeaf())\n            {\n                readQueryWorkload(range, node->objects[i].id);\n            }\n        }\n    }\n    storage->free(node);\n}\n\nvoid LBAKTree::propagateKeywords(uintptr_t objectId)\n{\n    Node *node = (Node *)storage->read(objectId);\n    unordered_set<string> kwds;\n    if(node->isLeaf())\n    {\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            insertKeywords(node->objects[i].id, kwds, true);\n        }\n    }\n    else\n    {\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            propagateKeywords(node->objects[i].id);\n            insertKeywords(node->objects[i].id, kwds, false);\n        }\n    }\n    string text;\n    unordered_set<string>::iterator it;\n    for (it = kwds.begin(); it != kwds.end(); ++it)\n    {\n        text += *it;\n        text += \" \";\n    }\n    kf1.write(text, node->id, kwds.size());\n    storage->free(node);\n}\n\nvoid LBAKTree::insertKeywords(uintptr_t objectId, unordered_set<string> &kwds, bool leaf)\n{\n    if(leaf)\n    {\n        for (unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n        {\n            kwds.insert(dictionary[recordsMap[objectId]->at(i)]);\n        }\n    }\n    else\n    {\n        string text = kf1.read(objectId);\n        parseKeywords(text, kwds);\n    }\n}\n\nvoid LBAKTree::parseKeywords(string &text, unordered_set<string> &kwds)\n{\n    string::size_type lastPos = text.find_first_not_of(\" \", 0);\n    string::size_type pos = text.find_first_of(\" \", lastPos);\n    while (string::npos != pos || string::npos != lastPos)\n    {\n        kwds.insert(text.substr(lastPos, pos - lastPos));\n        lastPos = text.find_first_not_of(\" \", pos);\n        pos = text.find_first_of(\" \", lastPos);\n    }\n}\n\nvoid LBAKTree::computeGradientIntercept(uintptr_t objectId, vector <string> &keywords)\n{\n    Node *node = (Node *)storage->read(objectId);\n    string text = kf1.read(node->id);\n    unordered_set<string> kwds;\n    parseKeywords(text, kwds);\n    if(objectId == storage->getRoot())\n    {\n        srand((unsigned)time(0));\n        for(unsigned i = 0; i < 100; ++i)\n        {\n            keywords.push_back(dictionary[rand() % dictionary.size()]);\n        }\n    }\n    StringContainerVector strContainer;\n    strContainer.initStatsCollector(gramGen);\n    unordered_set<string>::iterator it;\n    for (it = kwds.begin(); it != kwds.end(); ++it)\n    {\n        strContainer.insertString(*it);\n    }\n    WrapperSimpleEdNorm wrapper(&strContainer, gramGen, false);\n    wrapper.buildIndex();\n    for(unsigned i = 0; i < keywords.size(); ++i)\n    {\n        vector<unsigned> resultStringIDs;\n        struct timeval t1, t2;\n        struct timezone tz;\n        startTimeMeasurement(t1, tz);\n        wrapper.search(keywords.at(i), simThreshold, resultStringIDs);\n        stopTimeMeasurement(t2, tz);\n        double timeMeasurement = getTimeMeasurement(t1, t2);\n        sx += (double) strContainer.size();\n        sy += timeMeasurement;\n        sx2 += ((double)strContainer.size() * (double)strContainer.size());\n        sy2 += (timeMeasurement * timeMeasurement);\n        sxy += ((double)strContainer.size() * timeMeasurement);\n        ++n;\n    }\n    if(!node->isLeaf())\n    {\n        computeGradientIntercept(node->objects[0].id, keywords);\n    }\n    storage->free(node);\n}\n\nvoid LBAKTree::selectSANodes()\n{\n    avgKwdsLength /= (double)numKwds;\n    if(algoType == fl)\n    {\n        useFL();\n    }\n    else\n    {\n        vector<string> keywords;\n        computeGradientIntercept(storage->getRoot(), keywords);\n        gradient = ((sx * sy) - (n * sxy)) / ((sx * sx) - (n * sx2));\n        intercept = ((sx * sxy) - (sy * sx2)) / ((sx * sx) - (n * sx2));\n\n        ifstream queries(queryWorkloadFile.c_str());\n        if (!queries)\n        {\n            cerr << \"fatal error: failed to open query workload file\" << endl;\n            exit(1);\n        }\n        string line;\n        while (getline(queries, line))\n        {\n            Rectangle range;\n            string coordinates = line.substr(0, line.find(\",\"));\n            istringstream coordinatesStream(coordinates);\n            coordinatesStream >> range.min.x >> range.min.y >> range.max.x >> range.max.y;\n            readQueryWorkload(range);\n        }\n        if(algoType == vl)\n        {\n            useVL();\n        }\n        else\n        {\n            fillKeywordsIntersectionsFile();\n            useVLF();\n        }\n    }\n}\n\nvoid LBAKTree::useFL()\n{\n    Node *root = (Node *)storage->read(storage->getRoot());\n    if(indexesLevel > root->level)\n    {\n        indexesLevel = root->level;\n    }\n    storage->free(root);\n\n    unordered_map <uintptr_t, IndexNode>::iterator mit;\n    for (mit = kf1.begin(); mit != kf1.end(); ++mit)\n    {\n        Node *node = (Node *)storage->read(mit->first);\n        if(node->level >= indexesLevel)\n        {\n            StringContainerVector *strContainer;\n            strContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n            strContainersMap[node->id] = strContainer;\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::useVL()\n{\n    vector<NodePriority> heap;\n    Node *root = (Node *)storage->read(storage->getRoot());\n    double pFuzzySpaceCost = (double)kf1.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    double pFuzzyTimeCost = (double)queryWorkloadMap[root->id] * (gradient * (double)kf1.getIndexNode(root->id).numKeywords + intercept);\n    double cFuzzySpaceCost = 0;\n    double cFuzzyTimeCost = 0;\n    for(unsigned i = 0; i < root->numChildren; ++i)\n    {\n        cFuzzySpaceCost += (double)kf1.getIndexNode(root->objects[i].id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n        cFuzzyTimeCost += (double)queryWorkloadMap[root->objects[i].id] * (gradient * (double)kf1.getIndexNode(root->objects[i].id).numKeywords + intercept);\n    }\n    NodePriority rootImportance;\n    rootImportance.id = root->id;\n    rootImportance.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n    rootImportance.pFuzzySpaceCost = pFuzzySpaceCost;\n    rootImportance.cFuzzySpaceCost = cFuzzySpaceCost;\n    rootImportance.pFuzzyTimeCost = pFuzzyTimeCost;\n    rootImportance.cFuzzyTimeCost = cFuzzyTimeCost;\n    spaceBudget -= pFuzzySpaceCost;\n    heap.push_back(rootImportance);\n    storage->free(root);\n    while(!heap.empty())\n    {\n        NodePriority nodeImportance;\n        nodeImportance.id = heap[0].id;\n        nodeImportance.priority = heap[0].priority;\n        nodeImportance.pFuzzySpaceCost = heap[0].pFuzzySpaceCost;\n        nodeImportance.cFuzzySpaceCost = heap[0].cFuzzySpaceCost;\n        nodeImportance.pFuzzyTimeCost = heap[0].pFuzzyTimeCost;\n        nodeImportance.cFuzzyTimeCost = heap[0].cFuzzyTimeCost;\n        pop_heap(heap.begin(), heap.end());\n        heap.pop_back();\n        Node *node = (Node *)storage->read(nodeImportance.id);\n        if(nodeImportance.priority > 0 && nodeImportance.cFuzzySpaceCost <= (spaceBudget + nodeImportance.pFuzzySpaceCost) && !node->isLeaf() && node->level != 1)\n        {\n            spaceBudget += nodeImportance.pFuzzySpaceCost;\n            spaceBudget -= nodeImportance.cFuzzySpaceCost;\n            StringContainerVector *strContainer;\n            strContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n            strContainersMap[node->id] = strContainer;\n        }\n        else\n        {\n            StringContainerVector *strContainer;\n            strContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n            strContainersMap[node->id] = strContainer;\n            storage->free(node);\n            continue;\n        }\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            Node *node2 = (Node *)storage->read(node->objects[i].id);\n            pFuzzySpaceCost = (double)kf1.getIndexNode(node2->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n            pFuzzyTimeCost = (double)queryWorkloadMap[node2->id] * (gradient * (double)kf1.getIndexNode(node2->id).numKeywords + intercept);\n            cFuzzySpaceCost = 0;\n            cFuzzyTimeCost = 0;\n            if(node2->isLeaf())\n            {\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = std::numeric_limits<double>::max();\n            }\n            else\n            {\n                for(unsigned j = 0; j < node2->numChildren; ++j)\n                {\n                    cFuzzySpaceCost += (double)kf1.getIndexNode(node2->objects[j].id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                    cFuzzyTimeCost += (double)queryWorkloadMap[node2->objects[j].id] *  (gradient * (double)kf1.getIndexNode(node2->objects[j].id).numKeywords + intercept);\n                }\n            }\n            NodePriority nodeImportance2;\n            nodeImportance2.id = node2->id;\n            nodeImportance2.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n            nodeImportance2.pFuzzySpaceCost = pFuzzySpaceCost;\n            nodeImportance2.cFuzzySpaceCost = cFuzzySpaceCost;\n            nodeImportance2.pFuzzyTimeCost = pFuzzyTimeCost;\n            nodeImportance2.cFuzzyTimeCost = cFuzzyTimeCost;\n            heap.push_back(nodeImportance2);\n            push_heap(heap.begin(), heap.end());\n            storage->free(node2);\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::useVLF()\n{\n    vector<NodePriority> heap;\n    Node *root = (Node *)storage->read(storage->getRoot());\n    double pFuzzySpaceCost = (double)kf1.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    double pFuzzyTimeCost = (double)queryWorkloadMap[root->id] * (gradient * (double)kf1.getIndexNode(root->id).numKeywords + intercept);\n    double cFuzzySpaceCost = 0;\n    double cFuzzyTimeCost = 0;\n    for(unsigned j = 0; j < root->numChildren; ++j)\n    {\n        unordered_set <string> intersectedKeywords;\n        unordered_set<string>::iterator it;\n        string text = kf1.read(root->objects[j].id);\n        unordered_set<string> kwds;\n        parseKeywords(text, kwds);\n        string intsctText = kf2.read(root->objects[j].id);\n        unordered_set<string> intsctKwds;\n        parseKeywords(intsctText, intsctKwds);\n        for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n        {\n            if(kwds.find(*it) != kwds.end())\n            {\n                intersectedKeywords.insert(*it);\n            }\n        }\n        cFuzzySpaceCost += (double)(kf1.getIndexNode(root->objects[j].id).numKeywords - intersectedKeywords.size()) * (avgKwdsLength + (double)q - 1.0) * 4.0;\n        cFuzzyTimeCost += (double)queryWorkloadMap[root->objects[j].id] *  (gradient * (double)(kf1.getIndexNode(root->objects[j].id).numKeywords - intersectedKeywords.size()) + intercept);\n    }\n    cFuzzySpaceCost += (double)kf2.getIndexNode(root->id).numKeywords * (avgKwdsLength + (double)q - 1.0) * 4.0;\n    cFuzzyTimeCost += (double)queryWorkloadMap[root->id] *  (gradient * (double)kf2.getIndexNode(root->id).numKeywords + intercept);\n    NodePriority rootImportance;\n    rootImportance.id = root->id;\n    if(pFuzzySpaceCost == cFuzzySpaceCost)\n    {\n        rootImportance.priority = pFuzzyTimeCost - cFuzzyTimeCost;\n    }\n    else\n    {\n        rootImportance.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n    }\n    rootImportance.pFuzzySpaceCost = pFuzzySpaceCost;\n    rootImportance.cFuzzySpaceCost = cFuzzySpaceCost;\n    rootImportance.pFuzzyTimeCost = pFuzzyTimeCost;\n    rootImportance.cFuzzyTimeCost = cFuzzyTimeCost;\n    spaceBudget -= pFuzzySpaceCost;\n    heap.push_back(rootImportance);\n    storage->free(root);\n    while(!heap.empty())\n    {\n        NodePriority nodeImportance;\n        nodeImportance.id = heap[0].id;\n        nodeImportance.priority = heap[0].priority;\n        nodeImportance.pFuzzySpaceCost = heap[0].pFuzzySpaceCost;\n        nodeImportance.cFuzzySpaceCost = heap[0].cFuzzySpaceCost;\n        vector <uintptr_t> ancestorsIds = heap[0].ancestorsIds;\n        nodeImportance.pFuzzyTimeCost = heap[0].pFuzzyTimeCost;\n        nodeImportance.cFuzzyTimeCost = heap[0].cFuzzyTimeCost;\n        pop_heap(heap.begin(), heap.end());\n        heap.pop_back();\n        Node *node = (Node *)storage->read(nodeImportance.id);\n        if(nodeImportance.priority > 0 && nodeImportance.cFuzzySpaceCost <= (spaceBudget + nodeImportance.pFuzzySpaceCost) && !node->isLeaf())\n        {\n            spaceBudget += nodeImportance.pFuzzySpaceCost;\n            spaceBudget -= nodeImportance.cFuzzySpaceCost;\n            StringContainerVector *strContainer;\n            strContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n            strContainersMap[node->id] = strContainer;\n        }\n        else\n        {\n            StringContainerVector *strContainer;\n            strContainer = new StringContainerVector(true);\n            strContainer->initStatsCollector(gramGen);\n            strContainersMap[node->id] = strContainer;\n            storage->free(node);\n            continue;\n        }\n        ancestorsIds.push_back(nodeImportance.id);\n        for(unsigned i = 0; i < node->numChildren; ++i)\n        {\n            Node *node2 = (Node *)storage->read(node->objects[i].id);\n            string text = kf1.read(node2->id);\n            unordered_set<string> kwds;\n            parseKeywords(text, kwds);\n            string intsctText = kf2.read(node2->id);\n            unordered_set<string> intsctKwds;\n            parseKeywords(intsctText, intsctKwds);\n            unordered_map<uintptr_t, unordered_set<string> > tempMap;\n            for(unsigned j = 0; j < ancestorsIds.size(); ++j)\n            {\n                string intsctText2 = kf2.read(ancestorsIds[j]);\n                parseKeywords(intsctText2, tempMap[ancestorsIds[j]]);\n            }\n            if(node2->isLeaf())\n            {\n                for(unsigned j = 0; j < ancestorsIds.size(); ++j)\n                {\n                    unordered_set<string>::iterator it;\n                    for (it = tempMap[ancestorsIds[j]].begin(); it != tempMap[ancestorsIds[j]].end(); ++it)\n                    {\n                        kwds.erase(*it);\n                    }\n                }\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = std::numeric_limits<double>::max();\n            }\n            else\n            {\n                for(unsigned j = 0; j < ancestorsIds.size(); ++j)\n                {\n                    unordered_set<string>::iterator it;\n                    for (it = tempMap[ancestorsIds[j]].begin(); it != tempMap[ancestorsIds[j]].end(); ++it)\n                    {\n                        intsctKwds.erase(*it);\n                        kwds.erase(*it);\n                    }\n                }\n                cFuzzySpaceCost = 0;\n                cFuzzyTimeCost = 0;\n                ancestorsIds.push_back(node2->id);\n                for(unsigned j = 0; j < node2->numChildren; ++j)\n                {\n                    string text2 = kf1.read(node2->objects[j].id);\n                    unordered_set<string> kwds2;\n                    parseKeywords(text2, kwds2);\n                    unordered_set <string> intersectedKeywords;\n                    for(unsigned k = 0; k < ancestorsIds.size(); ++k)\n                    {\n                        unordered_set<string>::iterator it;\n                        for (it = tempMap[ancestorsIds[k]].begin(); it != tempMap[ancestorsIds[k]].end(); ++it)\n                        {\n                            if(kwds2.find(*it) != kwds2.end())\n                            {\n                                intersectedKeywords.insert(*it);\n                            }\n                        }\n                    }\n                    cFuzzySpaceCost += (double)(kf1.getIndexNode(node2->objects[j].id).numKeywords - intersectedKeywords.size()) * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                    cFuzzyTimeCost += (double)queryWorkloadMap[node2->objects[j].id] *  (gradient * (double)(kf1.getIndexNode(node2->objects[j].id).numKeywords - intersectedKeywords.size()) + intercept);\n                }\n                cFuzzySpaceCost += (double)intsctKwds.size() * (avgKwdsLength + (double)q - 1.0) * 4.0;\n                cFuzzyTimeCost += (double)queryWorkloadMap[node2->id] *  (gradient * (double)intsctKwds.size() + intercept);\n                ancestorsIds.pop_back();\n            }\n            pFuzzySpaceCost = (double)kwds.size() * (avgKwdsLength + (double)q - 1.0) * 4.0;\n            pFuzzyTimeCost = (double)queryWorkloadMap[node2->id] * (gradient * (double)kwds.size() + intercept);\n            NodePriority nodeImportance2;\n            nodeImportance2.id = node2->id;\n            if(pFuzzySpaceCost == cFuzzySpaceCost)\n            {\n                nodeImportance2.priority = pFuzzyTimeCost - cFuzzyTimeCost;\n            }\n            else\n            {\n                nodeImportance2.priority = (cFuzzyTimeCost - pFuzzyTimeCost) / (pFuzzySpaceCost - cFuzzySpaceCost);\n            }\n            nodeImportance2.pFuzzySpaceCost = pFuzzySpaceCost;\n            nodeImportance2.cFuzzySpaceCost = cFuzzySpaceCost;\n            nodeImportance2.ancestorsIds = ancestorsIds;\n            nodeImportance2.pFuzzyTimeCost = pFuzzyTimeCost;\n            nodeImportance2.cFuzzyTimeCost = cFuzzyTimeCost;\n            heap.push_back(nodeImportance2);\n            push_heap(heap.begin(), heap.end());\n            text = \"\";\n            unordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n                text += *it;\n                text += \" \";\n            }\n            kf1.write(text, node2->id, kwds.size());\n            intsctText = \"\";\n            for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n            {\n                intsctText += *it;\n                intsctText += \" \";\n            }\n            kf2.write(intsctText, node2->id, intsctKwds.size());\n            storage->free(node2);\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::fillKeywordsIntersectionsFile()\n{\n    unordered_map <uintptr_t, IndexNode>::iterator mit;\n    for (mit = kf1.begin(); mit != kf1.end(); ++mit)\n    {\n        Node *node = (Node *)storage->read(mit->first);\n        if(!node->isLeaf())\n        {\n            unordered_set<string> intsctKwds;\n            string text = kf1.read(node->id);\n            unordered_set<string> kwds;\n            parseKeywords(text, kwds);\n            unordered_map<uintptr_t, unordered_set<string> > tempMap;\n            for(unsigned i = 0; i < node->numChildren; ++i)\n            {\n                string text2 = kf1.read(node->objects[i].id);\n                parseKeywords(text2, tempMap[node->objects[i].id]);\n            }\n            unordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n                double counter = 0;\n                for(unsigned i = 0; i < node->numChildren; ++i)\n                {\n                    if(tempMap[node->objects[i].id].find(*it) != tempMap[node->objects[i].id].end())\n                    {\n                        ++counter;\n                    }\n                }\n                if(counter >= (double)node->numChildren * kfThreshold)\n                {\n                    intsctKwds.insert(*it);\n                }\n            }\n            string intsctText;\n            for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n            {\n                intsctText += *it;\n                intsctText += \" \";\n            }\n            kf2.write(intsctText, node->id, intsctKwds.size());\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::fillKeywordsHashesMap()\n{\n    unordered_map <uintptr_t, IndexNode>::iterator mit;\n    for (mit = kf1.begin(); mit != kf1.end(); ++mit)\n    {\n        Node *node = (Node *)storage->read(mit->first);\n        if(strContainersMap.find(node->id) == strContainersMap.end())\n        {\n            string text = kf1.read(node->id);\n            unordered_set<string> kwds;\n            parseKeywords(text, kwds);\n            Array<unsigned> *array = new Array<unsigned>(kwds.size(), 1);\n            unordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n                array->append(keywordsMap[*it]);\n            }\n            sort (array->begin(), array->end());\n            keywordsHashesMap[node->id] = array;\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::fillWrappersMap()\n{\n    WrapperSimpleEdNorm *wrapper;\n    unordered_map <uintptr_t, IndexNode>::iterator mit;\n    for (mit = kf1.begin(); mit != kf1.end(); ++mit)\n    {\n        Node *node = (Node *)storage->read(mit->first);\n        if(strContainersMap.find(node->id) != strContainersMap.end() && strContainersMap.find(node->objects[0].id) == strContainersMap.end())\n        {\n            string text = kf1.read(node->id);\n            unordered_set<string> kwds;\n            parseKeywords(text, kwds);\n            unordered_set<string>::iterator it;\n            for (it = kwds.begin(); it != kwds.end(); ++it)\n            {\n                strContainersMap[node->id]->insertString(*it);\n            }\n            wrapper = new WrapperSimpleEdNorm(strContainersMap[node->id], gramGen, false);\n            wrapper->buildIndex();\n            wrappersMap[node->id] = wrapper;\n        }\n        else if(strContainersMap.find(node->objects[0].id) != strContainersMap.end())\n        {\n            string intsctText = kf2.read(node->id);\n            unordered_set<string> intsctKwds;\n            parseKeywords(intsctText, intsctKwds);\n            unordered_set<string>::iterator it;\n            for (it = intsctKwds.begin(); it != intsctKwds.end(); ++it)\n            {\n                strContainersMap[node->id]->insertString(*it);\n            }\n            wrapper = new WrapperSimpleEdNorm(strContainersMap[node->id], gramGen, false);\n            wrapper->buildIndex();\n\n            wrappersMap[node->id] = wrapper;\n        }\n        storage->free(node);\n    }\n}\n\nvoid LBAKTree::rangeQuery(vector<Object> &objects, const Rectangle &range,\n                          const vector <string> &kwds)\n{\n    uintptr_t id = storage->getRoot();\n    vector<string> strings[kwds.size()];\n    vector<unsigned> hashes[kwds.size()];\n\n    if(searchWrapper(id, kwds, strings, hashes, strings, hashes))\n    {\n        rangeQuery(objects, range, id, kwds, strings, hashes);\n    }\n}\n\nvoid LBAKTree::rangeQuery(vector<Object> &objects, const Rectangle &range,\n                          uintptr_t id, const vector <string> &kwds, vector<string> strings[], vector<unsigned> hashes[])\n{\n    Node *node = (Node *)storage->read(id);\n\n    if(strContainersMap.find(node->id) != strContainersMap.end() && strContainersMap.find(node->objects[0].id) == strContainersMap.end())\n    {\n        for(unsigned i = 0; i < kwds.size(); ++i)\n        {\n            if(hashes[i].empty())\n            {\n                return;\n            }\n        }\n    }\n    for(unsigned i = 0; i < node->numChildren; ++i)\n    {\n        if(node->objects[i].mbr.intersects(range))\n        {\n            if(node->isLeaf())\n            {\n                bool keywordFounded = true;\n                for (unsigned j = 0; j < kwds.size(); ++j)\n                {\n                    bool candidateFounded = false;\n                    for (unsigned k = 0; k < strings[j].size(); ++k)\n                    {\n                        if(searchVector(node->objects[i].id, strings[j].at(k)))\n                        {\n                            candidateFounded = true;\n                            break;\n                        }\n                    }\n                    if (!candidateFounded)\n                    {\n                        keywordFounded = false;\n                        break;\n                    }\n                }\n                if(keywordFounded)\n                {\n                    objects.push_back(node->objects[i]);\n                }\n            }\n            else\n            {\n                vector<string> resultStrings[kwds.size()];\n                vector<unsigned> resultHashes[kwds.size()];\n                if(strContainersMap.find(node->objects[i].id) != strContainersMap.end())\n                {\n                    if(searchWrapper(node->objects[i].id, kwds, strings, hashes, resultStrings, resultHashes))\n                    {\n                        rangeQuery(objects, range, node->objects[i].id, kwds, resultStrings, resultHashes);\n                    }\n                }\n                else\n                {\n                    if(searchArray(node->objects[i].id, kwds.size(), strings, hashes, resultStrings, resultHashes))\n                    {\n                        rangeQuery(objects, range, node->objects[i].id, kwds, resultStrings, resultHashes);\n                    }\n                }\n            }\n        }\n    }\n    storage->free(node);\n}\n\nbool LBAKTree::searchWrapper(uintptr_t objectId, const vector <string> &kwds, vector<string> strings[], vector<unsigned> hashes[], vector<string>resultStrings[], vector<unsigned>resultHashes[])\n{\n    for(unsigned i = 0; i < kwds.size(); ++i)\n    {\n        vector<unsigned> resultStringIDs;\n        wrappersMap[objectId]->search(kwds.at(i), simThreshold, resultStringIDs);\n        for (unsigned j = 0; j < strings[i].size(); ++j)\n        {\n            resultStrings[i].push_back(strings[i].at(j));\n            resultHashes[i].push_back(hashes[i].at(j));\n        }\n        for (unsigned j = 0; j < resultStringIDs.size(); ++j)\n        {\n            string temp;\n            strContainersMap[objectId]->retrieveString(temp, resultStringIDs.at(j));\n            resultStrings[i].push_back(temp);\n            resultHashes[i].push_back(keywordsMap[temp]);\n        }\n    }\n    return true;\n}\n\nbool LBAKTree::searchArray(uintptr_t objectId, unsigned numKeywords, vector<string> strings[], vector<unsigned> hashes[], vector<string>resultStrings[], vector<unsigned>resultHashes[])\n{\n    for(unsigned i = 0; i < numKeywords; ++i)\n    {\n        for(unsigned j = 0; j < hashes[i].size(); ++j)\n        {\n            if(keywordsHashesMap[objectId]->has(hashes[i].at(j)))\n            {\n                resultStrings[i].push_back(strings[i].at(j));\n                resultHashes[i].push_back(hashes[i].at(j));\n            }\n        }\n        if (resultStrings[i].empty())\n            return false;\n    }\n    return true;\n}\n\nbool LBAKTree::searchVector(uintptr_t objectId, const string &keyword)\n{\n    for(unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n    {\n        if(dictionary[recordsMap[objectId]->at(i)].length() == keyword.length())\n        {\n            if(dictionary[recordsMap[objectId]->at(i)].compare(keyword) == 0)\n            {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n\nvoid LBAKTree::getObjectKeywords(uintptr_t objectId, vector<string> &objectKeywords)\n{\n    for(unsigned i = 0; i < recordsMap[objectId]->size(); ++i)\n    {\n        objectKeywords.push_back(dictionary[recordsMap[objectId]->at(i)]);\n    }\n}\n\nvoid LBAKTree::startTimeMeasurement(struct timeval &t1, struct timezone &tz)\n{\n    gettimeofday(&t1, &tz);\n}\n\nvoid LBAKTree::stopTimeMeasurement(struct timeval &t2, struct timezone &tz)\n{\n    gettimeofday(&t2, &tz);\n}\n\ndouble LBAKTree::getTimeMeasurement(struct timeval &t1, struct timeval &t2)\n{\n    unsigned totalTime = (t2.tv_sec - t1.tv_sec) * 1000000 +\n                         (t2.tv_usec - t1.tv_usec);\n    double tval = 0;\n    tval = static_cast<double>(totalTime) / 1000;\n    return tval;\n}\n", "encoding": "ascii"}