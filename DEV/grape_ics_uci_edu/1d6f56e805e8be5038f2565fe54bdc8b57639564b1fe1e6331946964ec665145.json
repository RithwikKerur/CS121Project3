{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs221-2019-spring-project4?version=15&format=txt", "content": "\r\n= CS221: Project 4 - RA: Ranking =\r\n'''Test Cases Due:     ~~Week 9 Tue. (May 28)~~ Week 9 Th. (May 30), Due on Github as Pull Requests'''[[BR]]\r\n'''Review Due:           ~~Week 9 Fri. (May 31)~~ Week 9 Sun (June 2),  Due on Github as Pull Requests Comments'''[[BR]]\r\n'''Final Code Due:     Week 10, Sun. (June 9),  Due on Github.'''[[BR]]\r\n\r\n\r\n== Coding Tasks ==\r\n1. Implement TF-IDF ranking (Cosine) (10 points)\r\n1. Implement !PageRank and run with ICS websites (5 points)\r\n\r\n== Testing Tasks ==\r\n1. Write at least '''1''' test case for a task (2 points)\r\n1. Review the test cases of two teams (2 points)\r\n\r\nTotal: 19 points \r\n\r\n== Overview ==\r\n\r\nIn this project, you'll implement two ranking methods: TF-IDF (Cosine) and !PageRank. \r\n\r\n\r\n== Task 1: Implement TF-IDF ranking. ==\r\n\r\nModify '''both''' your !InvertedIndex from Project 2 and !PositionalIndex from Project 3 to include all necessary information to calculate TF-IDF scores. Implement the following `searchTfIdf` function that returns the top-K documents ranked by TF-IDF scores.\r\n\r\nFirst, we'll go over an example to calculate TF-IDF Cosine scores using the vector space model.\r\n\r\n{{{\r\nSegment 1 \r\n\td1: \u201cnew york city\u201d \r\n\td2: \u201clos angeles city\u201d \r\nSegment 2\r\n \td1: \u201cnew york post\u201d\r\nQuery q: \"new new city\". (We use two copies of \"new\" on purpose.)\r\n\r\nNumber of documents in the system: 3.\r\n\r\nDocument frequency for query keywords:\r\n\tnew  = 2 {s1d1, s2d1}\r\n\tcity = 2 {s1d1, s1d2}\r\n\r\nIDF:\r\n\tnew  = log(3/2) \u2248 0.18\r\n\tcity = log(3/2) \u2248 0.18\r\n\r\nTF (Term frequency):\r\n\tnew  = {s1d1: 1, s2d1: 1, query: 2}\r\n\tcity = {s1d1: 1, s1d2: 1, query: 1}\r\n\r\nTF-IDF vectors of documents and query:\r\n\ts1d1  = [new: 1 * IDF(new), city: 1 * IDF(city)] = [0.18, 0.18]\r\n\ts1d2  = [new: 0,            city: 1 * IDF(city)] = [0   , 0.18]\r\n\ts2d1  = [new: 1 * IDF(new), city: 0]             = [0.18, 0   ]\r\n\tquery = [new: 2 * IDF(new), city: 1 * IDF(city)] = [0.36, 0.18]\r\n\r\nCosine similarity between the query and each document:\r\n\tsim(s1d1, q) = s1d1 \u00b7 query / \u2016s1d1\u2016 = (0.18 * 0.36 + 0.18 * 0.18) / sqrt(0.18^2 + 0.18^2) \u2248 0.38\r\n\tsim(s1d2, q) = (0 * 0.36 + 0.18 * 0.18) / sqrt(0 + 0.18^2) = 0.18\r\n\tsim(s2d1, q) = (0.18 * 0.36 + 0 * 0.18) / sqrt(0.18^2 + 0) = 0.36\r\n\r\nSo the ranked order of the documents is: [s1d1, s2d1, s1d2].\r\n}}}\r\n\r\nNext we'll discuss a two-pass method to calculate these scores.  For each query keyword, since it has inverted lists in multiple LSM segments, in order to compute its global IDF, we need to know the frequency of this word in each segment.  In the first pass, we access each segment to retrieve the frequency of each query keyword, as well as the number of documents within the segment, and use these values to calculate the IDF's of the query keywords.\r\n\r\nIn the second pass, within each segment, we traverse the inverted list of a query keyword, and use the (local) TF for each document and the (global) IDF of the keyword to compute the TF-IDF value for this keyword and this document.  We accumulate the product of this value and the corresponding TF-IDF value of the query keyword in order to compute the dot product of the document and the query.  Furthermore, for each document, since we need to normalize its TF-IDF vector by its length, we also need to accumulate the squares of the TF-IDF's of the query keywords in this document.  The following is the pseudo code:\r\n\r\n{{{\r\nMap<DocID, Double> dotProductAccumulator; //  DocID is <SegmentID, LocalDocID>\r\nMap<DocID, Double> vectorLengthAccumulator;\r\n\r\nfor each segment:\r\n  for each query token w:\r\n    for each docID on the postingList of w:\r\n      tfidf = TF(w, docID) * IDF(w);\r\n      dotProductAccumulator[docID] += tfidf * queryTfIdf[w];\r\n      vectorLengthAccumulator[docID] += tfidf ^ 2;\r\n\r\n  for each docID in this segment\r\n    score(docID) =  dotProductAccumulator[docID] / sqrt(vectorLengthAccumulator[docID]);\r\n}}}\r\n\r\nIn this method, we need to store a double (IDF) for each query keyword.  In addition, for each segment, for each of its documents, we need to store 2 doubles.\r\n\r\nNotice that since we want to return topK documents, we need to maintain a global priority queue of K docIDs.  After visiting each segment, we only need to add the top-K docIDs of this segment into this priority query and free the space for the docIDs of this segment.\r\n\r\nTo implement the method, you need the following API functions for each segment:\r\n\r\n{{{\r\n\r\npublic int getNumDocuments(int segmentNum);\r\n\r\npublic int getDocumentFrequency(int segmentNum, String token);\r\n\r\n/**\r\n * Performs top-K ranked search using TF-IDF.\r\n * Returns an iterator that returns the top K documents with highest TF-IDF scores.\r\n *\r\n * Each element is a pair of <Document, Double (TF-IDF Score)>.\r\n *\r\n * If parameter `topK` is null, then returns all the matching documents.\r\n *\r\n * Unlike Boolean Query and Phrase Query where order of the documents doesn't matter,\r\n * for ranked search, order of the document returned by the iterator matters.\r\n *\r\n * @param keywords, a list of keywords in the query\r\n * @param topK, number of top documents weighted by TF-IDF, all documents if topK is null\r\n * @return a iterator of top-k ordered documents matching the query\r\n */\r\npublic Iterator<Pair<Document, Double>> searchTfIdf(List<String> keywords, Integer topK)\r\n\r\n}}}\r\n\r\n\r\n\r\n== Task 2: Implement !PageRank. ==\r\n\r\nNote: the description of this task could have minor changes.\r\n\r\nIn this task, you'll run the !PageRank algorithm on the collection of ICS websites we provided, index all the ICS website documents, and combine !PageRank with the TF-IDF weighted search results.\r\n\r\nDownload the provided [https://drive.google.com/open?id=1tuToOHHDNIAR30yGSxnslxksWpPlZRNa webpages zip file]. Here are the description of what's in the zip file:\r\n\r\n - The \"url.tsv\" file contains the mapping of a webpage ID to the original URL of the webpage. \r\n - The \"id-graph.tsv\" file contains the graph of the webpages. Each line is an edge in the graph represented as a pair of two document IDs. The first is the \"from\" documentID and the second is the \"to\" document ID. \r\n - The \"cleaned\" folder contains the web page documents with its documentID as the file name. All web page files are already cleaned - all HTML tags are removed and only plain text are kept. In each file, the first line is the documentID, the second line is the original URL, and the third line is the text of the HTML document.\r\n\r\nImplement the following `IcsSearchEngine` class.\r\n\r\n{{{\r\npublic class IcsSearchEngine {\r\n\r\n    /**\r\n     * Initializes an IcsSearchEngine from the directory containing the documents and the\r\n     *\r\n     */\r\n    public static IcsSearchEngine createSearchEngine(Path documentDirectory, InvertedIndexManager indexManager) {\r\n        return new IcsSearchEngine(documentDirectory, indexManager);\r\n    }\r\n\r\n    private IcsSearchEngine(Path documentDirectory, InvertedIndexManager indexManager) {\r\n    }\r\n\r\n    /**\r\n     * Writes all ICS web page documents in the document directory to the inverted index.\r\n     */\r\n    public void writeIndex() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Computes the page rank score from the id-graph file in the document directory.\r\n     * The results of the computation can be saved in a class variable and will be later retrieved by `getPageRankScores`.\r\n     */\r\n    public void computePageRank() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Gets the page rank score of all documents previously computed. Must be called after `computePageRank`.\r\n     * Returns a list of <DocumentID - Score> Pairs that is sorted by score in descending order (high scores first).\r\n     */\r\n    public List<Pair<Integer, Double>> getPageRankScores() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Searches the ICS document corpus and returns the top K documents ranked by combining TF-IDF and PageRank.\r\n     *\r\n     * The search process should first retrieve ALL the top documents from the InvertedIndex by TF-IDF rank,\r\n     * by calling `searchTfIdf(query, null)`.\r\n     *\r\n     * Then the corresponding PageRank score of each document should be retrieved. (`computePageRank` will be called beforehand)\r\n     * For each document, the combined score is  tfIdfScore + pageRankWeight * pageRankScore.\r\n     *\r\n     * Finally, the top K documents of the combined score are returned. Each element is a pair of <Document, combinedScore>\r\n     *\r\n     *\r\n     * Note: We could get the Document ID by reading the first line of the document.\r\n     * This is a workaround because our project doesn't support multiple fields. We cannot keep the documentID in a separate column.\r\n     */\r\n    public Iterator<Document> searchQuery(List<String> query, int topK, double pageRankWeight) {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n}\r\n\r\n}}}\r\n\r\n\r\n== Test cases ==\r\nPlease follow the similar general guideline and procedure as previous projects. Here are the [https://docs.google.com/spreadsheets/d/1_iwJOT-bnYDk9tWNNy61GCyi1kRN7s3VHdn5h_2T1DA/edit#gid=915349265 test case assignments]. Note for this project, you only need to write at least 1 test case. You should put your test case in package `edu.uci.ics.cs221.index.ranking`.\r\n", "encoding": "utf-8"}