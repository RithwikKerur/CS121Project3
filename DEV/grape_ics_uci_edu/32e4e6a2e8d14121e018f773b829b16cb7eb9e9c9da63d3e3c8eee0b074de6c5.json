{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs221-2019-spring-project4?version=4&format=txt", "content": "\r\n= CS221: Project 4 - RA: Ranking =\r\n'''Test Cases Due:     Week 9 Tue. (May 28), Due on Github as Pull Requests'''[[BR]]\r\n'''Review Due:           Week 9 Fri. (May 31),  Due on Github as Pull Requests Comments'''[[BR]]\r\n'''Final Code Due:     Week 10, Sun. (June 9),  Due on Github.'''[[BR]]\r\n\r\n\r\n== Coding Tasks ==\r\n1. Implement TF-IDF ranking (Cosine) (10 points)\r\n1. Implement !PageRank and run with ICS websites (5 points)\r\n\r\n== Testing Tasks ==\r\n1. Write at least 2 test cases for a task (2 points)\r\n1. Review the test cases of two teams (2 points)\r\n\r\nTotal: 19 points \r\n\r\n== Overview ==\r\n\r\nIn this project, you'll implement two ranking methods: TF-IDF (Cosine) and !PageRank. \r\n\r\n\r\n== Task 1: Implement TF-IDF ranking. ==\r\n\r\nModify '''both''' your !InvertedIndex from Project 2 and !PositionalIndex from Project 3 to include all necessary information to calculate TF-IDF scores. Implement the following `searchTfIdf` function that returns the top-K documents ranked by TF-IDF scores.\r\n\r\nFirst, we'll go over an example to calculate TF-IDF Cosine scores using the vector space model.\r\n\r\n{{{\r\nSegment 1 \r\n\td1: \u201cnew york city\u201d \r\n\td2: \u201clos angeles city\u201d \r\nSegment 2\r\n \td1: \u201cnew york post\u201d\r\nQuery q: \"new new city\". (We use two copies of \"new\" on purpose.)\r\n\r\nNumber of documents in the system: 3.\r\n\r\nDocument frequency for query keywords:\r\n\tnew  = 2 {s1d1, s2d1}\r\n\tcity = 2 {s1d1, s1d2}\r\n\r\nIDF:\r\n\tnew  = log(3/2) \u2248 0.18\r\n\tcity = log(3/2) \u2248 0.18\r\n\r\nTF (Term frequency):\r\n\tnew  = {s1d1: 1, s2d1: 1, query: 2}\r\n\tcity = {s1d1: 1, s1d2: 1, query: 1}\r\n\r\nTF-IDF vectors of documents and query:\r\n\ts1d1  = [new: 1 * IDF(new), city: 1 * IDF(city)] = [0.18, 0.18]\r\n\ts1d2  = [new: 0,            city: 1 * IDF(city)] = [0   , 0.18]\r\n\ts2d1  = [new: 1 * IDF(new), city: 0]             = [0.18, 0   ]\r\n\tquery = [new: 2 * IDF(new), city: 1 * IDF(city)] = [0.36, 0.18]\r\n\r\nCosine similarity between the query and each document:\r\n\tsim(s1d1, q) = s1d1 \u00b7 query / \u2016s1d1\u2016 = (0.18 * 0.36 + 0.18 * 0.18) / sqrt(0.18^2 + 0.18^2) \u2248 0.38\r\n\tsim(s1d2, q) = (0 * 0.36 + 0.18 * 0.18) / sqrt(0 + 0.18^2) = 0.18\r\n\tsim(s2d1, q) = (0.18 * 0.36 + 0 * 0.18) / sqrt(0.18^2 + 0) = 0.36\r\n\r\nSo the ranked order of the documents is: [s1d1, s2d1, s1d2].\r\n}}}\r\n\r\nNext we'll discuss a two-pass method to calculate these scores.  For each query keyword, since it has inverted lists in multiple LSM segments, in order to compute its global IDF, we need to know the frequency of this word in each segment.  In the first pass, we access each segment to retrieve the frequency of each query keyword, as well as the number of documents within the segment, and use these values to calculate the IDF's of the query keywords.\r\n\r\nIn the second pass, within each segment, we traverse the inverted list of a query keyword, and use the (local) TF for each document and the (global) IDF of the keyword to compute the TF-IDF value for this keyword and this document.  We accumulate the product of this value and the corresponding TF-IDF value of the query keyword in order to compute the dot product of the document and the query.  Furthermore, for each document, since we need to normalize its TF-IDF vector by its length, we also need to accumulate the squares of the TF-IDF's of the query keywords in this document.  The following is the pseudo code:\r\n\r\n{{{\r\nMap<DocID, Double> dotProductAccumulator; //  DocID is <SegmentID, LocalDocID>\r\nMap<DocID, Double> vectorLengthAccumulator;\r\n\r\nfor each segment:\r\n  for each query token w:\r\n    for each docID on the postingList of w:\r\n      tfidf = TF(w, docID) * IDF(w);\r\n      dotProductAccumulator[docID] += tfidf * queryTfIdf[w];\r\n      vectorLengthAccumulator[docID] += tfidf ^ 2;\r\n\r\n  for each docID in this segment\r\n    score(docID) =  dotProductAccumulator[docID] / sqrt(vectorLengthAccumulator[docID]);\r\n}}}\r\n\r\nIn this method, we need to store a double (IDF) for each query keyword.  In addition, for each segment, for each of its documents, we need to store 2 doubles.\r\n\r\nNotice that since we want to return topK documents, we need to maintain a global priority queue of K docIDs.  After visiting each segment, we only need to add the top-K docIDs of this segment into this priority query and free the space for the docIDs of this segment.\r\n\r\nTo implement the method, you need the following API functions for each segment:\r\n\r\n{{{\r\n\r\npublic int getNumDocuments(int segmentNum);\r\n\r\npublic int getDocumentFrequency(int segmentNum, String token);\r\n\r\npublic Iterator<Document> searchTfIdf(List<String> keywords, int topK);\r\n\r\n}}}\r\n\r\n\r\n\r\n== Task 2: Implement !PageRank. ==\r\n\r\nIn this task, you'll run the !PageRank algorithm on the collection of ICS websites we provided, index all the ICS website documents, and combine !PageRank with the TF-IDF weighted search results. More details will be provided later.\r\n\r\n\r\n== Test cases ==\r\nPlease follow the similar general guideline and procedure as previous projects. Test case assignments will be published later.\r\n", "encoding": "utf-8"}