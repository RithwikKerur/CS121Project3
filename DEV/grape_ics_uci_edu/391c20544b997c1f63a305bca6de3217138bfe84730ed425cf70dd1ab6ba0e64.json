{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs221-2019-spring-project4?version=5&format=txt", "content": "\r\n= CS221: Project 4 - RA: Ranking =\r\n'''Test Cases Due:     Week 9 Tue. (May 28), Due on Github as Pull Requests'''[[BR]]\r\n'''Review Due:           Week 9 Fri. (May 31),  Due on Github as Pull Requests Comments'''[[BR]]\r\n'''Final Code Due:     Week 10, Sun. (June 9),  Due on Github.'''[[BR]]\r\n\r\n\r\n== Coding Tasks ==\r\n1. Implement TF-IDF ranking (Cosine) (10 points)\r\n1. Implement !PageRank and run with ICS websites (5 points)\r\n\r\n== Testing Tasks ==\r\n1. Write at least '''1''' test cases for a task (2 points)\r\n1. Review the test cases of two teams (2 points)\r\n\r\nTotal: 19 points \r\n\r\n== Overview ==\r\n\r\nIn this project, you'll implement two ranking methods: TF-IDF (Cosine) and !PageRank. \r\n\r\n\r\n== Task 1: Implement TF-IDF ranking. ==\r\n\r\nModify '''both''' your !InvertedIndex from Project 2 and !PositionalIndex from Project 3 to include all necessary information to calculate TF-IDF scores. Implement the following `searchTfIdf` function that returns the top-K documents ranked by TF-IDF scores.\r\n\r\nFirst, we'll go over an example to calculate TF-IDF Cosine scores using the vector space model.\r\n\r\n{{{\r\nSegment 1 \r\n\td1: \u201cnew york city\u201d \r\n\td2: \u201clos angeles city\u201d \r\nSegment 2\r\n \td1: \u201cnew york post\u201d\r\nQuery q: \"new new city\". (We use two copies of \"new\" on purpose.)\r\n\r\nNumber of documents in the system: 3.\r\n\r\nDocument frequency for query keywords:\r\n\tnew  = 2 {s1d1, s2d1}\r\n\tcity = 2 {s1d1, s1d2}\r\n\r\nIDF:\r\n\tnew  = log(3/2) \u2248 0.18\r\n\tcity = log(3/2) \u2248 0.18\r\n\r\nTF (Term frequency):\r\n\tnew  = {s1d1: 1, s2d1: 1, query: 2}\r\n\tcity = {s1d1: 1, s1d2: 1, query: 1}\r\n\r\nTF-IDF vectors of documents and query:\r\n\ts1d1  = [new: 1 * IDF(new), city: 1 * IDF(city)] = [0.18, 0.18]\r\n\ts1d2  = [new: 0,            city: 1 * IDF(city)] = [0   , 0.18]\r\n\ts2d1  = [new: 1 * IDF(new), city: 0]             = [0.18, 0   ]\r\n\tquery = [new: 2 * IDF(new), city: 1 * IDF(city)] = [0.36, 0.18]\r\n\r\nCosine similarity between the query and each document:\r\n\tsim(s1d1, q) = s1d1 \u00b7 query / \u2016s1d1\u2016 = (0.18 * 0.36 + 0.18 * 0.18) / sqrt(0.18^2 + 0.18^2) \u2248 0.38\r\n\tsim(s1d2, q) = (0 * 0.36 + 0.18 * 0.18) / sqrt(0 + 0.18^2) = 0.18\r\n\tsim(s2d1, q) = (0.18 * 0.36 + 0 * 0.18) / sqrt(0.18^2 + 0) = 0.36\r\n\r\nSo the ranked order of the documents is: [s1d1, s2d1, s1d2].\r\n}}}\r\n\r\nNext we'll discuss a two-pass method to calculate these scores.  For each query keyword, since it has inverted lists in multiple LSM segments, in order to compute its global IDF, we need to know the frequency of this word in each segment.  In the first pass, we access each segment to retrieve the frequency of each query keyword, as well as the number of documents within the segment, and use these values to calculate the IDF's of the query keywords.\r\n\r\nIn the second pass, within each segment, we traverse the inverted list of a query keyword, and use the (local) TF for each document and the (global) IDF of the keyword to compute the TF-IDF value for this keyword and this document.  We accumulate the product of this value and the corresponding TF-IDF value of the query keyword in order to compute the dot product of the document and the query.  Furthermore, for each document, since we need to normalize its TF-IDF vector by its length, we also need to accumulate the squares of the TF-IDF's of the query keywords in this document.  The following is the pseudo code:\r\n\r\n{{{\r\nMap<DocID, Double> dotProductAccumulator; //  DocID is <SegmentID, LocalDocID>\r\nMap<DocID, Double> vectorLengthAccumulator;\r\n\r\nfor each segment:\r\n  for each query token w:\r\n    for each docID on the postingList of w:\r\n      tfidf = TF(w, docID) * IDF(w);\r\n      dotProductAccumulator[docID] += tfidf * queryTfIdf[w];\r\n      vectorLengthAccumulator[docID] += tfidf ^ 2;\r\n\r\n  for each docID in this segment\r\n    score(docID) =  dotProductAccumulator[docID] / sqrt(vectorLengthAccumulator[docID]);\r\n}}}\r\n\r\nIn this method, we need to store a double (IDF) for each query keyword.  In addition, for each segment, for each of its documents, we need to store 2 doubles.\r\n\r\nNotice that since we want to return topK documents, we need to maintain a global priority queue of K docIDs.  After visiting each segment, we only need to add the top-K docIDs of this segment into this priority query and free the space for the docIDs of this segment.\r\n\r\nTo implement the method, you need the following API functions for each segment:\r\n\r\n{{{\r\n\r\npublic int getNumDocuments(int segmentNum);\r\n\r\npublic int getDocumentFrequency(int segmentNum, String token);\r\n\r\npublic Iterator<Document> searchTfIdf(List<String> keywords, int topK);\r\n\r\n}}}\r\n\r\n\r\n\r\n== Task 2: Implement !PageRank. ==\r\n\r\nIn this task, you'll run the !PageRank algorithm on the collection of ICS websites we provided, index all the ICS website documents, and combine !PageRank with the TF-IDF weighted search results.\r\n\r\nDownload the webpages zip file from the attachment. Here are the description of what's in the zip file:\r\n\r\n - The \"url.tsv\" file contains the mapping of a webpage ID to the original URL of the webpage. \r\n - The \"id-graph.tsv\" file contains the graph of the webpages. Each line is an edge in the graph represented as a pair of two document IDs. The first is the \"from\" documentID and the second is the \"to\" document ID. \r\n - The \"cleaned\" folder contains the web page documents with its documentID as the file name. All web page files are already cleaned - all HTML tags are removed and only plain text are kept. In each file, the first line is the documentID, the second line is the original URL, and the third line is the text of the HTML document.\r\n\r\nImplement the following `IcsSearchEngine` class.\r\n\r\n{{{\r\npublic class IcsSearchEngine {\r\n\r\n    /**\r\n     * Initializes an IcsSearchEngine from the directory containing the documents and the\r\n     *\r\n     */\r\n    public static IcsSearchEngine createSearchEngine(Path documentDirectory, InvertedIndexManager indexManager) {\r\n        return new IcsSearchEngine(documentDirectory, indexManager);\r\n    }\r\n\r\n    private IcsSearchEngine(Path documentDirectory, InvertedIndexManager indexManager) {\r\n    }\r\n\r\n    /**\r\n     * Writes all ICS web page documents in the document directory to the inverted index.\r\n     */\r\n    public void writeIndex() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Computes the page rank score from the id-graph file in the document directory.\r\n     * The results of the computation can be saved in a class variable and will be later retrieved by `getPageRankScores`.\r\n     */\r\n    public void computePageRank() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Gets the page rank score of all documents previously computed. Must be called after `computePageRank`.\r\n     * Returns an list of <DocumentID - Score> Pairs that is sorted by score in descending order (high scores first).\r\n     */\r\n    public List<Pair<Integer, Double>> getPageRankScores() {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n    /**\r\n     * Searches the ICS document corpus with the query using TF-IDF search provided by inverted index manager.\r\n     * The search process should first retrieve the top-K documents by TF-IDF rank, \r\n     * then re-order the resulting documents by the page rank score.\r\n     *\r\n     * Note: you could get the ID of each document from its first line.\r\n     */\r\n    public Iterator<Document> searchQuery(List<String> query, int topK) {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n\r\n}\r\n\r\n}}}\r\n\r\n\r\n== Test cases ==\r\nPlease follow the similar general guideline and procedure as previous projects. Test case assignments will be published later.\r\n", "encoding": "utf-8"}