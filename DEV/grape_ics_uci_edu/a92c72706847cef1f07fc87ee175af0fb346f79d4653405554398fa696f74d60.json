{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs222-2018-fall-project4?version=2&format=txt", "content": "{{{\r\n#!div style=\"background-color:rgb(237,222,222)\"\r\n\r\n\r\n[[PageOutline]]\r\n\r\n= Project 4: Implementing a Query Engine with the Extension of the Relation Manager =\r\n   * ''' Deadline: Friday, Dec 7, 2018 at 11:45 pm, on Github.'''\r\n   * ''' Full Credit: 100 points '''\r\n   * ''' Maximum Extra Credit: 15 points. '''[[br]]\r\n   * ''' As in Projects 2 and 3, you should work with your original team member for this Project. '''\r\n\r\n== Introduction ==\r\nIn this project, you will first extend the !RelationManager (RM) component that you implemented for project 2 so that the RM layer can orchestrate both the !RecordBasedFileManager (RBF) and !IndexManager (IX) layers when tuple-level operations happen, and the RM layer will also be managing the catalog information related to indices at this level. After the RM layer extension, you will implement a !QueryEngine (QE) component. The QE component provides classes and methods for answering SQL queries. For simplicity, you only need to implement several basic relational operators. All operators are iterator-based. To give you a jumpstart, we've implemented two wrapper operators on top of the RM layer that provide file and index scanning. See the [wiki:cs222-2017-fall-project4#Appendix Appendix] for more details.[[BR]]\r\n\r\nYou can pull again from our [https://github.com/UCI-Chenli-teaching/cs222-fall18 github repo] to get the project template. \r\nSee also the test cases included in the codebase for in-depth examples of how the operators are used.\r\n\r\n\r\n[[BR]]\r\n= Part 4.1: !RelationManager Extensions =\r\n== !RelationManager ==\r\nAll of the methods that you implemented for Project 2 should now be extended to coordinate data files plus any associated indices of the data files. For example, if you insert a tuple into a table using RelationManager::insertTuple(), the tuple should be inserted into the table (via the RBF layer) and each corresponding entry should be inserted into each associated index of the table (via the IX layer). Also, if you delete a table using RelationManager::deleteTable(), all associated indices should be deleted, too. This applies both to catalog entries that record what's what and to the file artifacts themselves.  The !RelationManager class, in addition to enforcing the coordination semantics between data files and the indices in your existing methods, will also include the following newly-added index-related methods. These methods can all be implemented by simply delegating their work to the underlying IX layer that you built in Project 3. (This part of the project is largely a big wrapper. :-))\r\n\r\n{{{\r\nclass RelationManager\r\n{\r\npublic:\r\n  ...\r\n  RC createIndex(const string &tableName, const string &attributeName);\r\n\r\n  RC destroyIndex(const string &tableName, const string &attributeName);\r\n\r\n  // indexScan returns an iterator to allow the caller to go through qualified entries in index\r\n  RC indexScan(const string &tableName,\r\n                        const string &attributeName,\r\n                        const void *lowKey,\r\n                        const void *highKey,\r\n                        bool lowKeyInclusive,\r\n                        bool highKeyInclusive,\r\n                        RM_IndexScanIterator &rm_IndexScanIterator\r\n       );\r\n  ...\r\n}\r\n}}}\r\n\r\n=== RC createIndex(const string &tableName, const string &attributeName) ===\r\nThis method creates an index on a given attribute of a given table.  (It should also reflect its existence in the catalogs.)\r\n\r\n=== RC destroyIndex(const string &tableName, const string &attributeName) ===\r\nThis method destroys an index on a given attribute of a given table.  (It should also reflect its non-existence in the catalogs.)\r\n\r\n===   RC indexScan(const string &tableName, const string &attributeName, const void *lowKey, const void *highKey, bool lowKeyInclusive, bool highKeyInclusive, RM_IndexScanIterator &rm_IndexScanIterator) ===\r\nThis method should initialize a condition-based scan over the entries in the open index on the given attribute of the given table. If the scan initiation method is successful, a RM_IndexScanIterator object called rm_IndexScanIterator is returned. (Please see the RM_IndexScanIterator class below.) Once underway, by calling RM_IndexScanIterator::getNextEntry(), the iterator should produce the entries of all records whose indexed attribute key falls into the range specified by the lowKey, highKey, and inclusive flags. If lowKey is NULL, it can be interpreted as -infinity. If highKey is NULL, it can be interpreted as +infinity. The format of the parameter lowKey and highKey is the same as the format of the key in IndexManager::insertEntry(). \r\n\r\n== RM_IndexScanIterator ==\r\n{{{\r\nclass RM_IndexScanIterator {\r\n public:\r\n  RM_IndexScanIterator();  \t// Constructor\r\n  ~RM_IndexScanIterator(); \t// Destructor\r\n\r\n  // \"key\" follows the same format as in IndexManager::insertEntry()\r\n  RC getNextEntry(RID &rid, void *key);  \t// Get next matching entry\r\n  RC close();             \t\t\t// Terminate index scan\r\n};\r\n}}}\r\n\r\n=== RC getNextEntry(RID &rid, void *key) ===\r\nThis method should set its output parameters rid and key to be the RID and key, respectively, of the next record in the index scan. This method should return RM_EOF if there are no index entries left satisfying the scan condition. You may assume that RM component clients will not close the corresponding open index while a scan is underway.\r\n\r\n=== RC close() ===\r\nThis method should terminate the index scan.\r\n\r\n\r\n[[BR]]\r\n[[BR]]\r\n= Part 4.2: Query Engine =\r\n== Iterator Interface ==\r\nAll of the operators that you will implement in this part inherit from the following '''Iterator''' interface.\r\n\r\n{{{\r\nclass Iterator {\r\n    // All the relational operators and access methods are iterators\r\n    // This class is the super class of all the following operator classes\r\n    public:\r\n        virtual RC getNextTuple(void *data) = 0;\r\n        // For each attribute in vector<Attribute>, name it rel.attr\r\n        virtual void getAttributes(vector<Attribute> &attrs) const = 0;\r\n        virtual ~Iterator() {};\r\n};\r\n}}}\r\n=== virtual RC getNextTuple(void *data) ===\r\nThis method should set the output parameter '''data''' of the next record. The format of the '''data''' parameter, which refers to the next tuple of the operator's output, is the same as that used in previous projects. Also, null-indicators for the given attributes are always placed at the beginning of '''data'''. That is, the tuple value is a sequence of binary attribute values in which null-indicators are placed first and then each value is represented as follows: (1) For INT and REAL: use 4 bytes; (2) For VARCHAR: use 4 bytes for the length followed by the characters.\r\n\r\n=== virtual void getAttributes(vector<Attribute> &attrs) ===\r\nThis method returns a vector of attributes in the intermediate relation resulted from this iterator.  That is, while the previous method returns the tuples from the operator, this method makes the associated schema information for the returned tuple stream available in the query plan. The names of the attributes in vector<Attribute> should be of the form relation.attribute to clearly specify the relation from which each attribute comes.\r\n\r\n== Filter Interface ==\r\n{{{\r\nclass Filter : public Iterator {\r\n    // Filter operator\r\n    public:\r\n        Filter(Iterator *input,                         // Iterator of input R\r\n               const Condition &condition               // Selection condition\r\n        );\r\n        ~Filter();\r\n\r\n        RC getNextTuple(void *data);\r\n        // For attribute in vector<Attribute>, name it as rel.attr\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\nUsing this iterator, you can do a selection query such as \"SELECT * FROM EMP WHERE sal > 100000\". \r\n\r\nThis filter class is initialized by an input iterator and a selection condition. It filters the tuples from the input iterator by applying the filter predicate '''condition''' on them. For simplicity, we assume this filter only has a single selection condition. The schema of the returned tuples should be the same as the input tuples from the iterator.\r\n\r\n== Project Interface ==\r\n{{{\r\nclass Project : public Iterator {\r\n    // Projection operator\r\n    public:\r\n        Project(Iterator *input,                            // Iterator of input R\r\n                const vector<string> &attrNames);           // vector containing attribute names\r\n        ~Project();\r\n\r\n        RC getNextTuple(void *data);\r\n        // For attribute in vector<Attribute>, name it as rel.attr\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\nThis project class takes an iterator and a vector of attribute names as input. It projects out the values of the attributes in the '''attrNames'''. The schema of the returned tuples should be the attributes in attrNames, in the order of attributes in the vector.\r\n\r\n== Block Nested-Loop Join Interface ==\r\n   * ''' This is mandatory.  '''\r\n\r\n{{{\r\nclass BNLJoin : public Iterator {\r\n    // Block nested-loop join operator\r\n    public:\r\n        BNLJoin(Iterator *leftIn,                              // Iterator of input R\r\n               TableScan *rightIn,                             // TableScan Iterator of input S\r\n               const Condition &condition,                     // Join condition\r\n               const unsigned numPages                         // Number of pages can be loaded into memory, i.e., memory block size (decided by the optimizer)\r\n        );\r\n        ~BNLJoin();\r\n\r\n        RC getNextTuple(void *data);\r\n        // For attribute in vector<Attribute>, name it as rel.attr\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\nThe BNLJoin takes two iterators as input. The '''leftIn''' iterator works as the outer relation and the '''rightIn''' iterator is the inner relation. The '''rightIn''' is an object of the !TableScan Iterator. We have already implemented the !TableScan class for you, which is a wrapper on  RM_ScanIterator. The returned schema should be the attributes of tuples from leftIn concatenated with the attributes of tuples from rightIn. You don't need to remove any duplicate attributes. Note that '''numPages''' is the number of outer (left) pages that the algorithm can load into memory at once. In other words, '''numPages''' is equal to the memory block size (measured in pages) that your algorithm should utilize to make the number of loops through the inner (right) table smaller than a simple tuple-oriented join's would be (by a factor of '''numPages'''). That is, numPages is the number of buffers that can be used to as a read buffer and hash buckets. However, to make it simple, you can use these buffer pages to read records from R. You can construct an another separate in-memory hash table (e.g., std::map) to keep the records in the numPages buffer. This means std::map will take care of two things: loading tuples and hashing them. '''Make sure that the total number of bytes of the loaded tuples in one round does not exceed \"numPages * pageSize\".'''  You can also assume that you have one page buffer to read a page from the inner relation '''rightIn''' and one page output buffer to keep the results.\r\n\r\n== Index Nested-Loop Join Interface ==\r\n\r\n{{{\r\nclass INLJoin : public Iterator {\r\n    // Index Nested-Loop join operator\r\n    public:\r\n        INLJoin(Iterator *leftIn,                               // Iterator of input R\r\n                IndexScan *rightIn,                             // IndexScan Iterator of input S\r\n                const Condition &condition                      // Join condition\r\n        );\r\n\r\n        ~INLJoin();\r\n\r\n        RC getNextTuple(void *data);\r\n        // For attribute in vector<Attribute>, name it as rel.attr\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\nThe INLJoin iterator takes two iterators as input. The '''leftIn''' iterator works as the outer relation, and the '''rightIn''' iterator is the inner relation. The '''rightIn''' is an object of !IndexScan Iterator. Again, we have already implemented the !IndexScan class for you, which is a wrapper on RM_IndexScanIterator. The returned schema should be the attributes of tuples from leftIn concatenated with the attributes of tuples from rightIn. You don't need to remove any duplicate attributes.\r\n\r\n\r\n== Grace Hash Join Interface ==\r\n   * ''' Optional: 10 extra-credit points for ''everyone''. '''\r\n{{{\r\nclass GHJoin : public Iterator {\r\n    // Grace hash join operator\r\n    public:\r\n        GHJoin(Iterator *leftIn,                            // Iterator of input R\r\n               Iterator *rightIn,                           // Iterator of input S\r\n               const Condition &condition,                  // Join condition (CompOp is always EQ)\r\n               const unsigned numPartitions                 // Number of partitions for each relation (decided by the optimizer)\r\n        );\r\n        ~GHJoin();\r\n\r\n        RC getNextTuple(void *data);\r\n        // For attribute in vector<Attribute>, name it as rel.attr\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\nUsing this iterator you can do a join query such as \"SELECT * FROM EMP, DEPT WHERE EMP.DID = DEPT.DID\".\r\n\r\nThe GHJoin takes two iterators as input. It uses '''leftIn''' to iterate over the outer relation and the '''rightIn''' to iterate over the inner relation. Following is a sketch of how to implement this operator:\r\n   1- In the partitioning phase, create '''numPartitions''' partitions for each relation where each partition is an rbfm file. The name of the outer relation partitions must start with the word \"left\" while the name of the inner relation partitions must start with the word \"right\". In order to avoid conflicts in the file names (in the case of multiple GHJoins in the query tree) you will have to add a suffix that uniquely identify your partitions. For example, you can have something like left_join1_XX and right_join1_XX for the first join and left_join2_XX and right_join2_XX for the second join. '''Note''': It is NOT acceptable to load the entire relation into memory while building the partitions -- you should assume that a query optimizer has chosen the number of partitions based on the amount of memory it has decided to allow this operator to use.\r\n   \r\n   2- In the probing phase, load a partition of either R or S (in fact you might want to load the smaller partition^*^) into memory, then build an in-memory hash table for such partition. Next, probe the corresponding partition from the other relation for matching tuples.\r\n   \r\n   3- The output will be the join-tuples that must be passed to the next operator. The schema of these join-tuples should be the attributes of tuples from leftIn concatenated with the attributes of tuples from rightIn. You don't need to remove any duplicate attributes.\r\n\r\n^*^ Note that if you load the smaller partition (to compare the sizes of the partitions you can use '''fileHandle.getNumberOfPages()''') you may need to rearrange the output attributes if S becomes the left relation. This is NOT a requirement but a closer implementation to what happens in practice  :-).  \r\n\r\n\r\n== Aggregate Interface ==\r\n   * '''Basic aggregation is Mandatory. '''\r\n   * '''Group-based hash aggregation is Optional for everyone. You will get 5 extra-credit points for doing so.'''\r\n\r\n{{{\r\nclass Aggregate : public Iterator {\r\n    // Aggregation operator\r\n    public:\r\n        // Mandatory for everyone\r\n        // Basic aggregation\r\n        Aggregate(Iterator *input,                              // Iterator of input R\r\n                  Attribute aggAttr,                            // The attribute over which we are computing an aggregate\r\n                  AggregateOp op                                // Aggregate operation\r\n        );\r\n\r\n        // Optional for everyone. 5 extra-credit points\r\n        // Group-based hash aggregation\r\n        Aggregate(Iterator *input,                              // Iterator of input R\r\n                  Attribute aggAttr,                            // The attribute over which we are computing an aggregate\r\n                  Attribute groupAttr,                          // The attribute over which we are grouping the tuples\r\n                  AggregateOp op                                // Aggregate operation\r\n        );\r\n\r\n        ~Aggregate();\r\n\r\n        RC getNextTuple(void *data);\r\n        // Please name the output attribute as aggregateOp(aggAttr)\r\n        // E.g. Relation=rel, attribute=attr, aggregateOp=MAX\r\n        // output attrname = \"MAX(rel.attr)\"\r\n        void getAttributes(vector<Attribute> &attrs) const;\r\n};\r\n}}}\r\n'''Basic aggregation:''' Using the basic aggregation operator, you can execute a query such as: \"SELECT MAX(sal) FROM EMP\". [[br]]\r\nThe basic aggregate method takes an input iterator, an aggregated attribute, and an aggregate function (MIN, MAX, SUM, AVG, COUNT) as the arguments. You can assume we do the aggregation on a numeric attribute (INT or REAL). The returned value is just a single real value (4 bytes), even for the COUNT function. The schema of the (single) returned tuple should be \"!AggregateOp(relation.attribute)\", such as \"MAX(emp.sal)\". Also,  null-indicators always needs to be placed at the beginning of a tuple.\r\n\r\n\r\n'''Group-based hash aggregation:''' Using the group-based hash aggregation operator, you can execute a query such as: \"SELECT city, MAX(sal) FROM EMP GROUP BY city\". [[br]]\r\nTo implement the ''group-by'' feature, you need to implement the group-based hash aggregation where we add one more argument to the argument list: '''groupAttr''', which is the group-by attribute. Unlike Grace Hash Join, you are not required to implement hash-partitioned aggregation using partitions on disk. You can assume that all of the groups' aggregation values will fit in a hash table in memory while the operation is executing. (E.g., think group by age or group by state -- where the number of groups is reasonable.) Each returned tuple should include the group-by attribute value followed by the aggregation value. The group-by attribute can be INT, REAL, or VARCHAR. The aggregated attribute can be INT or REAL. The schema of the returned tuples should be the group-by attribute and the aggregation attribute, such as \"emp.city MAX(emp.sal)\". Null-indicators always needs to be placed at the beginning of each tuple. '''5 extra-credit points'''\r\n\r\n== Important Note ==\r\nYou must make sure that all operators which create temporary rbfm files to clean up after themselves. That is, such files must be deleted when the operator is closed. \r\n\r\n== An Example ==\r\nHere is an example showing how to assemble the operators to form query plans.  Example: \"SELECT Employee.name, Employee.age, Employee.DeptID, Department.Name FROM  Employee JOIN Department ON Employee.DeptID = Department.ID WHERE Employee.salary > 50000\". We are assuming for this example that the optimizer has picked the Grace Hash Join algorithm to execute the join. \r\n\r\n{{{\r\n/****** ****** ****** ****** ******\r\n *    TABLE SCANS\r\n ****** ****** ****** ****** ******/\r\n\r\nTableScan *emp_ts = new TableScan(rm, \"Employee\");\r\nTableScan *dept_ts = new TableScan(rm, \"Department\");\r\n\r\n/****** ****** ****** ****** ******\r\n *    FILTER Employee Table\r\n ****** ****** ****** ****** ******/\r\n\r\nCondition cond_f;\r\ncond_f.lhsAttr = \"Employee.Salary\";\r\ncond_f.op = GT_OP;\r\ncond_f.bRhsIsAttr = false;\r\nValue value;\r\nvalue.type = TypeInt;\r\nvalue.data = malloc(bufsize);\r\n*(int *)value.data = 50000;\r\ncond_f.rhsValue = value;\r\n\r\nFilter *filter = new Filter(emp_ts, cond_f);\r\n\r\n/****** ****** ****** ****** ******\r\n *    PROJECT Employee Table\r\n ****** ****** ****** ****** ******/\r\n\r\nvector<string> attrNames;\r\nattrNames.push_back(\"Employee.name\");\r\nattrNames.push_back(\"Employee.age\");\r\nattrNames.push_back(\"Employee.DeptID\");\r\n\r\nProject project(filter, attrNames);\r\n\r\n/****** ****** ****** ****** ******\r\n *   GRACE HASH JOIN Employee with Dept\r\n ****** ****** ****** ****** ******/\r\n\r\nCondition cond_j;\r\ncond_j.lhsAttr = \"Employee.DeptID\";\r\ncond_j.op = EQ_OP;\r\ncond_j.bRhsIsAttr = true;\r\ncond_j.rhsAttr = \"Department.ID\";\r\n\r\nGHJoin *ghJoin = new GHJoin(project, dept_ts, cond_j, 100);\r\n\r\n\r\n\r\nvoid *data = malloc(bufsize);\r\nwhile(ghJoin.getNextTuple(data) != QE_EOF)\r\n{\r\n  printAttributes(data);\r\n}\r\n}}}\r\n\r\n== Command Line Interface Interpreter ==\r\nInstead of having to manually assemble the operators to form query plans, as shown in the above example, we are also providing you with a Command Line Interface (CLI) that takes a SQL-like command and executes that command. This will hopefully provide a better, more flexible test environment than the manual approach presented above (e.g., assembling query plans manually, running them, and even debugging them). The CLI runs in interactive mode so that you can type commands and see their results interactively. To get more information about the CLI, please visit  [wiki:cs222-2017-fall-command-line-interface this page]. Note that the CLI is provided for your convenience. We will not be using the CLI to test your code.\r\n\r\n'''Important Notes:''' \r\n1. In order to run the CLI on Ubuntu you might need to install this library '''libreadline-dev'''. You can run the following command to install it:\r\n{{{\r\nsudo apt-get install libreadline-dev \r\n}}}\r\n\r\n2. CLI uses C++11 features. You might need to use g++-4.8 to compile CLI. Please refer to [http://ubuntuhandbook.org/index.php/2013/08/install-gcc-4-8-via-ppa-in-ubuntu-12-04-13-04/ this page] to install GCC 4.8 on Ubuntu.\r\n\r\n== Appendix ==\r\n\r\nBelow we list the APIs for the three classes used in the operators. For more detailed implementation information, please refer to the '''qe.h''' header file in the code base. Note that in the !TableScan and !IndexScan classes, the argument '''alias''' is used to rename the input relation. In the case of self-joins, at least one of the uses of the relations must be renamed to differentiate the two from each other in terms of attribute naming.\r\n\r\n{{{\r\nstruct Condition {\r\n    string lhsAttr;         // left-hand side attribute                     \r\n    CompOp  op;             // comparison operator                          \r\n    bool    bRhsIsAttr;     // TRUE if right-hand side is an attribute and not a value; FALSE, otherwise\r\n    string  rhsAttr;        // right-hand side attribute if bRhsIsAttr = TRUE\r\n    Value   rhsValue;       // right-hand side value if bRhsIsAttr = FALSE\r\n};\r\n}}}\r\n{{{\r\nclass TableScan : public Iterator\r\n{\r\nTableScan(RelationManager &rm, const string &tableName, const char *alias = NULL);  // constructor\r\nvoid setIterator();                                                         // Start a new iterator\r\nRC getNextTuple(void *data);                                                // Return the next tuple from the iterator\r\nvoid getAttributes(vector<Attribute> &attrs) const;                         // Return the attributes from this iterator\r\n~TableScan();                                                               // destructor\r\n};\r\n}}}\r\n{{{\r\nclass IndexScan : public Iterator\r\n{\r\nIndexScan(RelationManager &rm, const string &tableName, const string &attrName, const char *alias = NULL); // constructor\r\nvoid setIterator(void* lowKey, void* highKey, bool lowKeyInclusive, bool highKeyInclusive); // Start a new iterator given the new compOp and value\r\nRC getNextTuple(void *data);                                                                                  // Return the next tuple from the iterator\r\nvoid getAttributes(vector<Attribute> &attrs) const;                                                           // Return the attributes from this iterator\r\n~IndexScan();                                                                                                 // destructor\r\n};\r\n}}}\r\n\r\n== Testing ==\r\nPlease use the provided test files included in the codebase to test your code. Note that this file will be used to grade your project partially since we also have our own private test cases. This is by no means an exhaustive test suite. Please feel free to add more cases to this, and test your code thoroughly. The test code includes provisional points for each test case. The points are subject to change if needed.\r\n\r\n== Submission Instructions ==\r\nThe following are requirements on your submission. Points may be deducted if they are not followed.\r\n\r\n * Write a report to briefly describe the design and implementation of your query engine module.\r\n * You need to submit the source code under the \"rbf\" ,\"rm\" ,\"ix\", \"qe\", and data folder. Make sure you do a \"make clean\" first, and do NOT include any useless files (such as binary files and data files). Your makefile should make sure the files ''qetest_XX.cc''' compile and run properly. We will use our own ''qetest_XX.cc''' files to test your module.\r\n \r\n * Please organize your project in the following directory hierarchy: master / {rbf, rm, ix, qe, data, makefile.inc, readme.txt, project4-report} where rbf, rm, ix, and qe folders include your source code and the makefile.\r\n * To test whether your code is structured properly, you can make a zip file from the master (by compressing your local directory or downloading from Github). Put  [attachment:test.sh this script]\u00e2\u20ac\u2039 and the zip file under the same directory. Run it to verify that your project can be properly unzipped and tested (use your own makefile.inc and the ixtest*.cc when you are testing the script). If the script doesn't work correctly, it's likely that your folder organization doesn't meet the above requirements. Our grading will be done by automatically done by running the script. The usage of the script is:\r\n{{{\r\n    # Suppose you download your project from Github to your local laptop: cs222-fall-team-xx-master.zip, \r\n    # run the following command will unzip and test your code automatically: (if you see directory structure is not correct message, make sure to fix it.) \r\n\r\n    ./test.sh ''cs222-fall18-team-xx-master''\r\n}}}\r\n\r\n== Grading Rubrics ==\r\nThe grading rubrics is at [wiki:cs222-2018-fall-project4-grading this page]\r\n\r\n== Q & A ==\r\n * '''Q1''': For the grace hash-join and block nested loop join, can I use a std::map() as an in-memory table? [[BR]] \r\n '''A1''': Yes. You can.\r\n\r\n '''Q2''': For the block-nested loop join, we are supposed to use numPages buffer to read tuples from the leftIn relation. How to use the memory based on this parameter? [[BR]]\r\n '''A2''': Suppose we do a BNLP for two inputs R and S, where R is the left child.   To simplify the implementation, it is acceptable to read \"numPages\" pages from R, then build an in-memory hash table (with additional memory) for these records.  In addition, it is also acceptable to use the getNext() API of R to read enough records to fill in *one* page, then immediately add them to the in-memory hash table.  Then we continue the process until we have read enough records for numPages pages (one page at a time).  Since the records from S are pipelined, you can join a record from S.getNext() immediately using the hash table of R.  You are required to have an output buffer.  You pause the join process whenever the output buffer is full.\r\n\r\n}}}", "encoding": "Windows-1252"}