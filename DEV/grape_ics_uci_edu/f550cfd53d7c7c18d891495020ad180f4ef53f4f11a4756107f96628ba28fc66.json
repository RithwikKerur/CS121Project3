{"url": "https://grape.ics.uci.edu/wiki/public/wiki/cs221-2019-spring-project2?version=16&format=txt", "content": "\r\n= CS221: Project 2 - II: Inverted index, boolean search =\r\n'''Test Cases Due:     Week 4 Tu. (Apr 23), Due on Github as Pull Requests'''[[BR]]\r\n'''Review Due:           Week 4, Fri. (Apr 26),  Due on Github as Pull Requests Comments'''[[BR]]\r\n'''Final Code Due:     Week 5, Sun. (May. 5),  Due on Github.'''[[BR]]\r\n\r\n\r\n== Coding Tasks ==\r\n1. Implement LSM-like disk-based inverted index that supports insertions. (6 points)\r\n1. Implement merge of inverted index segments. (4 points)\r\n1. Implement keyword search, boolean AND search, and boolean OR search. (5 points)\r\n1. (Optional Extra Credit): Implement deletions. (2 points)\r\n\r\n== Testing Tasks ==\r\n1. Write at least 2 test cases for a task (2 points)\r\n1. Review the test cases of two teams (2 points)\r\n\r\nTotal: 19 points  (+ 2 extra credits)\r\n\r\n== Overview ==\r\n\r\nIn this project, you'll be implementing a disk-based inverted index and the search operations.\r\n\r\nAt a high level, inverted index stores a mapping from keywords to the ids of documents they appear in.\r\nA simple in-memory structure could be `Map<String, List<Integer>>`, where each key is a keyword token (also called a \"term\"),\r\nand each value is a list of (often sorted) document IDs (also called \"postings\").\r\n\r\nIn this project, the disk-based index structure is based on the idea of LSM (Log-Structured Merge tree).\r\nIts main idea is the following:\r\n\r\nThe inverted index consists of multiple index segments, where each segment is initially created in memory.\r\nOnce a segment is written to disk, it becomes immutable and is never changed.\r\n\r\nEach index segment is a fully searchable inverted index.\r\nIt contains a posting list structure as well as a document store, which is a mapping from a docID to the corresponding document.\r\nThe document IDs within each segment are local in the segment and are invisible to the user.\r\nThese in-disk segments are periodically merged to bigger segments.\r\n\r\nWhen users search a keyword, all segments are searched, and the result documents from each segment are combined.\r\n\r\nExample:\r\n\r\n\r\n{{{\r\nAdd documents Doc{\"cat dog\"} and Doc{\"cat elephant\"}, then flush to Segment0.\r\nSegment0:\r\n----------\r\nPostingList: {\"cat\": [0, 1], \"dog\": [0], \"elephant\": [1]}\r\nDocStore: {0: \"cat dog\", 1: \"cat elephant\"}\r\n----------\r\n\r\nThen add documents Doc{\"cat dog\"} and Doc{\"wolf dog\"}, and flush to Segment1.\r\nSegment1:\r\n----------\r\nPostingList: {\"cat\": [0], \"dog\": [0, 1], \"wolf\": [1]}\r\nDocStore: {0: \"cat dog\", 1: \"wolf dog\"}\r\n----------\r\n\r\nWhen searching the word \"cat\", we first search Segment0 and get [Doc{\"cat dog\"}, Doc{\"cat elephant\"}].\r\nThen we search Segment1 and get [Doc{\"cat dog\"}]. We combine these results and get [Doc{\"cat dog\"}, Doc{\"cat elephant\"}, Doc{\"cat dog\"}]\r\n}}}\r\n\r\n\r\n== Task 1: Implement LSM-like disk-based inverted index that supports insertions only. ==\r\n\r\nIn this task, you'll implement the disk file structure of a single segment.\r\nWhen a document is added via `addDocument()`, it should be first stored in an in-memory buffer.\r\nYou need to design the data structure for the in-memory segment.\r\n\r\nWhenever the number of documents reaches a parameter default_flush_threshold, or function `flush()` is called,\r\nyou should flush the segment to disk.\r\n\r\nThe specific format of the disk posting lists should follow what we cover in lectures.\r\nYou also have freedom to improve the format to make it more efficient.\r\n\r\nThe following are specific functions to implement:\r\n\r\n{{{\r\n/**\r\n * Adds a document to the inverted index.\r\n * Document should live in an in-memory buffer until flush() is called to write the segment to disk.\r\n * @param document\r\n */\r\npublic void addDocument(Document document)\r\n\r\n/**\r\n * Flushes all the documents in the in-memory buffer to disk. If the buffer is empty, it should not do anything.\r\n * flush() writes the segment to disk containing the posting list and the corresponding document store.\r\n */\r\npublic void flush()\r\n\r\n/**\r\n * Iterates through all the documents in all the disk segments.\r\n */\r\npublic Iterator<Document> documentIterator() {\r\n    throw new UnsupportedOperationException();\r\n}\r\n\r\n/**\r\n * Gets the total number of segments in the inverted index.\r\n * This function is used for checking correctness in test cases.\r\n *\r\n * @return number of index segments.\r\n */\r\npublic int getNumSegments()\r\n\r\n/**\r\n * Reads a disk segment into memory based on segmentNum.\r\n * This function is mainly used for checking correctness in test cases.\r\n *\r\n * @param segmentNum n-th segment in the inverted index (starting from 0).\r\n * @return in-memory data structure with all the contents in the index segment, null if \r\n * the segment of segmentNum doesn't exist.\r\n */\r\npublic InvertedIndexSegmentForTest getIndexSegment(int segmentNum)\r\n\r\n}}}\r\n\r\n\r\n== Task 2: Implement merge of disk segments. ==\r\n\r\nIn this task, you'll implement the merging of disk segments. We cannot let the number of segments grow indefinitely\r\nbecause otherwise searching a keyword needs to go through a lot of segments.\r\n\r\nIn general, there are many merging policies. In this task, we want to implement a particular policy.\r\nWhenever the number of segments has reached a parameter default_merge_threshold, or `mergeAllSegments()` is called,\r\nyou need to merge *all* the disk segments pair-wise.\r\nFor example, suppose there are 10 disk segments. After the merge, we should have 5 disk segments.\r\nYou could assume merging only happens when you have an even number of segments.\r\n\r\nWhen merging two segments into one, since each segment has its own local document IDs, you need to generate new document IDs for the merged segment.\r\nWhen merging two segments, you need to merge both the inverted index, as well as the documents store of the two segments.\r\nFor simplicity, you can assume we have enough memory to hold the keywords of both segments.\r\nBUT, you cannot assume we have enough memory to store the posting lists and documents of both segments.\r\n\r\nThe following is specific function to implement:\r\n\r\n{{{\r\n/**\r\n * Merges all the disk segments of the inverted index pair-wise.\r\n */\r\npublic void mergeAllSegments()\r\n}}}\r\n\r\n\r\n== Task 3: Implement keyword search, boolean AND search, and boolean OR search. ==\r\n\r\nIn this task, you'll implement searching using the inverted index.\r\nYou could assume all documents are flushed to disk segments when doing a search.\r\n\r\nHere we make the same assumption as in the merge case regarding what can be stored in memory.\r\n\r\nFor every query keyword, you need to first analyze it using the provided analyzer before using it to access the inverted index.\r\nYou can assume the analyzer will not convert one keyword to multiple keywords.\r\nIf the keyword is empty, searching should not return any results.\r\n\r\nSpecific functions to implement:\r\n\r\n\r\n{{{\r\n/**\r\n * Performs a single keyword search on the inverted index.\r\n * You could assume the analyzer won't convert the keyword into multiple tokens.\r\n * If the keyword is empty, it should not return anything.\r\n *\r\n * @param keyword keyword, cannot be null.\r\n * @return a iterator of documents matching the query\r\n */\r\npublic Iterator<Document> searchQuery(String keyword)\r\n\r\n/**\r\n * Performs an AND boolean search on the inverted index.\r\n *\r\n * @param keywords a list of keywords in the AND query\r\n * @return a iterator of documents matching the query\r\n */\r\npublic Iterator<Document> searchAndQuery(List<String> keywords)\r\n}\r\n\r\n/**\r\n * Performs an OR boolean search on the inverted index.\r\n *\r\n * @param keywords a list of keywords in the OR query\r\n * @return a iterator of documents matching the query\r\n */\r\npublic Iterator<Document> searchOrQuery(List<String> keywords) \r\n}}}\r\n\r\n\r\n== Task 4 (Optional Extra Credit): Implement deletions. ==\r\n\r\nIn our LSM-like index structure, deletion could be implemented by maintaining a list deleted document IDs per segment.\r\nThe document is not actually deleted in the inverted index nor document store.\r\nWhen reading or searching, each docID is checked to see if it has been deleted.\r\n\r\nThose deleted documents within a segment should be physically deleted when we merge it with another segment.\r\n\r\nSpecific functions to implement:\r\n\r\n{{{\r\n/**\r\n * Deletes all documents in all disk segments of the inverted index that match the keyword.\r\n * @param keyword \r\n */\r\npublic void deleteDocuments(String keyword)\r\n}}}\r\n\r\n\r\n== Test cases ==\r\nPlease follow the similar general guideline and procedure as in project 1. Here is [https://docs.google.com/spreadsheets/d/1_iwJOT-bnYDk9tWNNy61GCyi1kRN7s3VHdn5h_2T1DA/edit#gid=996032174 test task assignment]\r\nThere are some guidelines and tips for project 2 test cases:\r\n1. Put the index and document files under your own folder. Specifically, you should use folder `index/YourTestName/`, for example `index/Team0StressTest`.\r\n2. Clean up and delete all files after each test. You should use Junit `@After` to delete all written files.\r\n3. For testing task 1 and 2, you could change `default_flush_threshold` or `default_merge_threshold`, or directly call `flush()` and `mergeAllSegments()` to control when to flush or when to merge. If you changed the variables `default_flush_threshold` or `default_merge_threshold`, be sure to change them back to the original value after your tests.\r\n4. For stress test, you should collect or generate a large amount of text data to test the performance and stability. If you rely on external data sets, please don't commit the large data directly in git, instead, use a link for where to download the data.\r\n5. For all test tasks, you should also check the read/write counter values in PageFileChannel to make sure the IO number are within a reasonable range.\r\n\r\n", "encoding": "ascii"}