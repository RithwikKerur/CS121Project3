{"url": "http://sli.ics.uci.edu/Classes-CS178-Notes/KMeans", "content": "<!DOCTYPE html \n    PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n  <title>SLI | Classes-CS178-Notes / KMeans </title>\n  <meta http-equiv='Content-Style-Type' content='text/css' />\n  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />\n  <!--HTMLHeader--><style type='text/css'><!--\n  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n  code.escaped { white-space: nowrap; }\n  .vspace { margin-top:1.33em; }\n  .indent { margin-left:40px; }\n  .outdent { margin-left:40px; text-indent:-40px; }\n  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }\n  a.createlink { text-decoration:none; position:relative; top:-0.5em;\n    font-weight:bold; font-size:smaller; border-bottom:none; }\n  img { border:0px; }\n  .editconflict { color:green; \n  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }\n\n  table.markup { border:2px dotted #ccf; width:90%; }\n  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }\n  table.vert td.markup1 { border-bottom:1px solid #ccf; }\n  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }\n  table.markup caption { text-align:left; }\n  div.faq p, div.faq pre { margin-left:2em; }\n  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }\n  div.faqtoc div.faq * { display:none; }\n  div.faqtoc div.faq p.question \n    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }\n  div.faqtoc div.faq p.question * { display:inline; }\n   \n    .frame \n      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }\n    .lfloat { float:left; margin-right:0.5em; }\n    .rfloat { float:right; margin-left:0.5em; }\na.varlink { text-decoration:none;}\n\n--></style>  <meta name='robots' content='index,follow' />\n\n</head>\n<body>\n<!--PageHeaderFmt-->\n  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'\n    alt='SLI' border='0' /></a></div>\n  <div id='wikihead'>\n  <form action='http://sli.ics.uci.edu'>\n    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'\n      accesskey='c'>Recent Changes</a> -</span> --> \n    <input type='hidden' name='n' value='Classes-CS178-Notes.KMeans' />\n    <input type='hidden' name='action' value='search' />\n    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->\n    <input type='text' name='q' value='' class='inputbox searchbox' />\n    <input type='submit' class='inputbutton searchbutton'\n      value='Search' />\n    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>\n  </form></div>\n<!--/PageHeaderFmt-->\n  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>\n<!--PageLeftFmt-->\n      <td id='wikileft' valign='top'>\n        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>\n</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>\n</li></ul><div class='vspace'></div><hr />\n<div class='vspace'></div>\n</td>\n<!--/PageLeftFmt-->\n      <td id='wikibody' valign='top'>\n<!--PageActionFmt-->\n        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/KMeans?action=login'>login</a>\n</li></ul>\n</div>\n<!--PageTitleFmt-->\n        <div id='wikititle'>\n          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>\n          <h1 class='pagetitle'>KMeans</h1></div>\n<!--PageText-->\n<div id='wikitext'>\n<p>The k-means algorithm is a simple and well-known technique for finding k clusters in a set of data.  \n</p>\n<p class='vspace'>Suppose that we have N data points <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/feb9550eef79cb34ef24e0c2764c3bb0.png\" />, each made up of d features.\nIn k-means, we describe each cluster c by a \"center\" <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" /> in the d-dimensional feature space, and define the similarity of a data point x to center <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" /> by their Euclidean distance.\n</p>\n<p class='vspace'>Notationally, suppose that we identify each data point <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/feb9550eef79cb34ef24e0c2764c3bb0.png\" /> with a single cluster center c, and indicate this association with a variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/28863e85c83cc103907268047ed58d68.png\" />.  Obviously, we do not know this association; the \"true\" values of z are hidden from us.\n</p>\n<p class='vspace'>This defines a two-part problem -- if we knew the cluster centers <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png\" />, it would be relatively easy to figure out which data points went with each cluster; alternatively, if we knew the association variables, it would be straightforward to determine cluster centers that most accurately described them.  K-means will alternate between these two steps, assuming <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png\" /> known and estimating z, then assuming z known and estimating <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png\" />.\n</p>\n<p class='vspace'>Formally, we can think of K-means as minimizing the following cost function:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/845db0db7f621922fb78a58546c15a56.png\" />\nthe total Euclidean distance of each data point from its assigned cluster center.\n</p>\n<p class='vspace'>K-means updates in two steps:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a3ee354ab2e74337da949ec2e190e0a0.png\" />\n</p>\n<p class='vspace'>Consider minimizing over <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a0c0d4deb0683a7aa5e9b8761b6d767f.png\" />.  The only term in C(.) that depends on <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a0c0d4deb0683a7aa5e9b8761b6d767f.png\" /> is the distance of data point i itself: <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/14ee3732d1e6852651bab009505b4b3d.png\" />.  Clearly, this will be minimized by selecting the closest cluster center <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" />.\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/5f738e79f8c9050ea66ecbce3f77b457.png\" />\n</p>\n<p class='vspace'>Now consider minimizing over some cluster center <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" /> given z.  The only terms in C(.) that depend on <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" /> are those for which <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/dc6698fe425dd92b5fb0615a4abbe817.png\" />, i.e., the distances of the data points that are currently assigned to cluster c.  It is straightforward to show that the minimum sum of squared distances is achieved by setting <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" /> equal to the mean of the data assigned to that cluster:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/68befbaddccce574120f5820886580b1.png\" />\n</p>\n<p class='vspace'>This process is then repeated until convergence.  To see that it converges, note that each step is decreasing the cost function (no update will ever increase the cost function) and that the cost function is bounded below (it is greater than zero).  In practice, at some point the nearest cluster center to each data point will be the one already assigned to it, meaning that the z variables do not change; then, the cluster centers will not change either, since they are already equal to the mean of those data.  Then, obviously, the z's will not change at the next iteration either, and the system is converged.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/kmeans1.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/th00---kmeans1.png.jpg\" title=\"kmeans1\" alt=\"kmeans1\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/kmeans2.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/th00---kmeans2.png.jpg\" title=\"kmeans2\" alt=\"kmeans2\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/kmeans3.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KMeans/th00---kmeans3.png.jpg\" title=\"kmeans3\" alt=\"kmeans3\" border=\"0\" /></a></td><td  align='center'><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/c0d812669dc0a4c388f6ff01b2f21104.png\" /></td></tr>\n<tr ><td  align='center'>Data (black) and initial centers (blue)</td><td  align='center'>z: identify closest cluster</td><td  align='center'><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a2f925591f202b15476ff295fd4328c6.png\" />: means given z</td><td  align='center'><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/c0d812669dc0a4c388f6ff01b2f21104.png\" /></td></tr>\n</table>\n<div class='vspace'></div><h2>Initialization</h2>\n<p>K-means greedily optimizes the cost function C(.), but is easily stuck in local minima.  This makes initialization important; often K-means will be run many times from different initializations, and the best run selected.\n</p>\n<p class='vspace'>A few common means of initializing the cluster centers include:\n</p><ol><li>Random: choose K points uniformly at random from the data to be the initial cluster centers <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/bccca90bf9da55b6eaa548cbca621c80.png\" />\n</li><li>Farthest: choose the first cluster center at random.  Then, for each subsequent cluster center, find the distance from each data point to its nearest cluster center so far.  Find the data point that is farthest from a cluster center, and select it as the next center in the list.\n</li></ol><p class='vspace'>Often it is helpful to inject a bit of randomness into procedures like \"Farthest\", in part so that they can be re-run several times and the best selected.  For example, having found the distance <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/43d977268a5f5151e07498b167b7f761.png\" /> of each point <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/feb9550eef79cb34ef24e0c2764c3bb0.png\" /> to  its nearest cluster center so far, one can select data point i to be the next cluster center with probability \n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/68fa53a3d0299039a20f6c913f9f7482.png\" />\n</p>\n<div class='vspace'></div><h2>Example code</h2>\n<div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> <span  style='color: green;'>% INPUTS</span>\n <span  style='color: green;'>% Data X : n-by-d</span>\n <span  style='color: green;'>% # of clusters K</span>\n pi = randperm(n);  <span  style='color: green;'>% initialize cluster centers, in this case</span>\n C = X(pi(1:K),:);  <span  style='color: green;'>%  to a random set of K data points</span>\n z = zeros(n,1);    <span  style='color: green;'>% allocate memory for cluster assignments</span>\n while (~converged) \n   for i=1:n,\n     dists = sum( (C - repmat(X(i,:),[K,1])).^2 , 2);  <span  style='color: green;'>% compute distances from each cluster center</span>\n     [tmp,z(i)] = min(dists);                          <span  style='color: green;'>% and assign datum i to the nearest cluster</span>\n   end\n   for j=1:K,                      <span  style='color: green;'>% now update each cluster center j</span>\n     C(j,:) = mean(X(z==j,:),1);   <span  style='color: green;'>% to be the mean of the assigned data</span>\n   end\n   converged = ... <span  style='color: green;'>% compute convergence condition based on change in C, or change in sum of squared error</span>\n end;\n</pre></div>\n</div>\n\n      </td>\n    </tr></table>\n<!--PageFooterFmt-->\n  <div id='wikifoot'>\n    <div class='footnav' style='float:left'> Last modified June 27, 2012, at 10:07 PM</div>\n    <div class='footnav' style='float:right; text-align:right'>\n    <a href=\"http://www.ics.uci.edu\">Bren School of Information and Computer Science</a><br>\n    <a href=\"http://www.uci.edu\">University of California, Irvine</a>\n    </div>\n  </div>\n<!--HTMLFooter--><script type=\"text/javascript\">\n  var _gaq = _gaq || [];\n  _gaq.push([\"_setAccount\", \"UA-24148957-2\"]);\n\t_gaq.push([\"_trackPageview\"]);\n\t(function() {\n\t  var ga = document.createElement(\"script\"); ga.type = \"text/javascript\"; ga.async = true;\n\t  ga.src = (\"https:\" == document.location.protocol ? \"https://ssl\" : \"http://www\") + \".google-analytics.com/ga.js\";\n\t  var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(ga, s);\n\t  })();\n</script>\n</body>\n</html>\n", "encoding": "ascii"}