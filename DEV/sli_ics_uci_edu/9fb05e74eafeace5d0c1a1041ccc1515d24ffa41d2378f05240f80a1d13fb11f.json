{"url": "http://sli.ics.uci.edu/Classes-CS178-Notes/KNearestNeighbors", "content": "<!DOCTYPE html \n    PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n  <title>SLI | Classes-CS178-Notes / KNearestNeighbors </title>\n  <meta http-equiv='Content-Style-Type' content='text/css' />\n  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />\n  <!--HTMLHeader--><style type='text/css'><!--\n  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n  code.escaped { white-space: nowrap; }\n  .vspace { margin-top:1.33em; }\n  .indent { margin-left:40px; }\n  .outdent { margin-left:40px; text-indent:-40px; }\n  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }\n  a.createlink { text-decoration:none; position:relative; top:-0.5em;\n    font-weight:bold; font-size:smaller; border-bottom:none; }\n  img { border:0px; }\n  .editconflict { color:green; \n  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }\n\n  table.markup { border:2px dotted #ccf; width:90%; }\n  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }\n  table.vert td.markup1 { border-bottom:1px solid #ccf; }\n  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }\n  table.markup caption { text-align:left; }\n  div.faq p, div.faq pre { margin-left:2em; }\n  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }\n  div.faqtoc div.faq * { display:none; }\n  div.faqtoc div.faq p.question \n    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }\n  div.faqtoc div.faq p.question * { display:inline; }\n   \n    .frame \n      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }\n    .lfloat { float:left; margin-right:0.5em; }\n    .rfloat { float:right; margin-left:0.5em; }\na.varlink { text-decoration:none;}\n\n--></style>  <meta name='robots' content='index,follow' />\n\n</head>\n<body>\n<!--PageHeaderFmt-->\n  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'\n    alt='SLI' border='0' /></a></div>\n  <div id='wikihead'>\n  <form action='http://sli.ics.uci.edu'>\n    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'\n      accesskey='c'>Recent Changes</a> -</span> --> \n    <input type='hidden' name='n' value='Classes-CS178-Notes.KNearestNeighbors' />\n    <input type='hidden' name='action' value='search' />\n    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->\n    <input type='text' name='q' value='' class='inputbox searchbox' />\n    <input type='submit' class='inputbutton searchbutton'\n      value='Search' />\n    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>\n  </form></div>\n<!--/PageHeaderFmt-->\n  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>\n<!--PageLeftFmt-->\n      <td id='wikileft' valign='top'>\n        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>\n</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>\n</li></ul><div class='vspace'></div><hr />\n<div class='vspace'></div>\n</td>\n<!--/PageLeftFmt-->\n      <td id='wikibody' valign='top'>\n<!--PageActionFmt-->\n        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/KNearestNeighbors?action=login'>login</a>\n</li></ul>\n</div>\n<!--PageTitleFmt-->\n        <div id='wikititle'>\n          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>\n          <h1 class='pagetitle'>KNearestNeighbors</h1></div>\n<!--PageText-->\n<div id='wikitext'>\n<h2>Nearest Neighbor classifiers</h2>\n<p>Nearest-neighbor rules for classification are some of the most intuitive types of classifiers to consider.\nA nearest neighbor rule simply memorizes (stores) all of the training data, and when a new test point is given, compares the new point to all of the training examples, finds the nearest one, and predicts that the new point\nhas the same class as that one.\n</p>\n<p class='vspace'>A k-nearest neighbor rule is a simple extension -- instead of finding the single nearest training example, find the nearest k of them, and predict the majority class value, i.e., the class to which the most of those k examples belong.\n</p>\n<p class='vspace'>The following figures show several classifiers as a function of k, the number of neighbors used.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnS01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnS01.png.jpg\" title=\"knnS01\" alt=\"knnS01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnS03.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnS03.png.jpg\" title=\"knnS03\" alt=\"knnS03\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnS05.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnS05.png.jpg\" title=\"knnS05\" alt=\"knnS05\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>k=1</td><td  align='center'>k=3</td><td  align='center'>k=5</td></tr>\n</table>\n<div class='vspace'></div><h3>Decision boundary</h3>\n<p>An alternative representation to any classifier is its decision boundary, the places at which it changes from one decision to another.\n</p>\n<p class='vspace'>Since the decision of a kNN classifier is defined by the nearest training examples, the decision <em>boundary</em> of a kNN classifier consists of those locations at which the set of the nearest training data change by (at least) one example.  A location on the boundary is one balanced between those two sets -- in other words, it must be equidistant between two training data, and since it must change the balance of the set, they must be two points in different classes.  For any two points, the set of equidistant locations is a line, perpendicular to the line joining the two points.\n</p>\n<p class='vspace'>This means the decision boundary is <em>locally</em> linear (piecewise linear); this appearance can be easily seen in a small enough set of training data.  By a similar argument, the transitions between linear segments must be equidistant from at least three points.\n</p>\n<div class='vspace'></div><h3>Complexity and K</h3>\n<p>The complexity of a k-nearest neighbor rule is a bit difficult to describe precisely.  Nearest neighbor methods store all of the training examples, and don't have a simple notion of complexity like we saw in linear regression.  However, consider how the training error of a k-nearest neighbor changes with k.  At k=1, evaluating the prediction at any training data point finds a data point of distance zero (itself), and is guaranteed to predict the correct class.  As k increases, this may no longer be true -- points of class 0 that are completely surrounded by points of class 1 are unlikely to predict their class correctly.  As the following images suggest, as k is increased further the classifier is less able to memorize the data, reducing the complexity of the learner.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnL01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnL01.png.jpg\" title=\"knnL01\" alt=\"knnL01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnL03.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnL03.png.jpg\" title=\"knnL03\" alt=\"knnL03\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnL05.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnL05.png.jpg\" title=\"knnL05\" alt=\"knnL05\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnL11.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnL11.png.jpg\" title=\"knnL11\" alt=\"knnL11\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/knnL21.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/KNearestNeighbors/th00---knnL21.png.jpg\" title=\"knnL21\" alt=\"knnL21\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>k=1</td><td  align='center'>k=3</td><td  align='center'>k=5</td><td  align='center'>k=11</td><td  align='center'>k=21</td></tr>\n</table>\n<div class='vspace'></div><h3>Example code</h3>\n<p>Here is some example k nearest neighbor code in Matlab.\n</p>\n<div class='vspace'></div><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> classdef knnClassify &lt; supLearn\n <span  style='color: green;'>% Class implementing K-nearest-neighbors classifier </span><br />   properties (SetAccess=private, GetAccess=private)\n     K=1;\n     Xtrain=[];  <span  style='color: green;'>% Training features, Ndata x Nfeatures </span><br />     Ytrain=[];  <span  style='color: green;'>% Training classes, Ndata x 1 </span><br />   end;\n   methods\n     <span  style='color: green;'>% Constructor (takes zero arguments or 3) </span><br />     function obj = knnClassify(K,Xtr,Ytr)\n       if (nargin &gt; 0)\n         obj.K = K;\n         obj.Xtrain = Xtr;\n         obj.Ytrain = Ytr;\n       end;\n     end\n\n     <span  style='color: green;'>% set parameter K if desired </span><br />     function obj=setK(obj, K)\n       obj.K = K;\n     end  \n\n     <span  style='color: green;'>% Batch training: just memorize data </span><br />     function obj=train(obj, Xtr,Ytr)\n       obj.Xtrain = Xtr;\n       obj.Ytrain = Ytr;\n     end\n\n     <span  style='color: green;'>% Test function: predict on Xtest </span><br />     function Yte = predict(obj,Xte)\n       [Ntr,Mtr] = size(obj.Xtrain);         <span  style='color: green;'>% get size of training, test data </span><br />       [Nte,Mte] = size(Xte);\n       classes = unique(obj.Ytrain);         <span  style='color: green;'>% figure out how many classes &amp; their labels </span><br />       Yte = repmat(obj.Ytrain(1), [Nte,1]); <span  style='color: green;'>% make Ytest the same data type as Ytrain </span><br />       K = min(obj.K, Ntr);                  <span  style='color: green;'>% can't have more than Ntrain neighbors </span><br />       for i=1:Nte,                          <span  style='color: green;'>% For each test example: </span><br />         dist = sum( bsxfun( @minus, obj.Xtrain, Xte(i,:) ).^2 , 2); <span  style='color: green;'>% compute sum of squared differences </span><br />         [tmp,idx] = sort(dist);             <span  style='color: green;'>% find nearest neighbors over Xtrain (dimension 2) </span><br />         cMax=1; NcMax=0;                    <span  style='color: green;'>% then find the majority class within that set of neighbors </span><br />         for c=1:length(classes),\n           Nc = sum(obj.Ytrain(idx(1:K))==classes(c)); <span  style='color: green;'>% count up how many instances of that class we have </span><br />           if (Nc&gt;NcMax), cMax=c; NcMax=Nc; end;       <span  style='color: green;'>% save the largest count and its class id </span><br />         end;\n         Yte(i)=classes(cMax);               <span  style='color: green;'>% save results </span><br />       end;\n     end\n\n end <span  style='color: green;'>% end methods </span><br /> end <span  style='color: green;'>% end class </span>\n</pre></div>\n</div>\n\n      </td>\n    </tr></table>\n<!--PageFooterFmt-->\n  <div id='wikifoot'>\n    <div class='footnav' style='float:left'> Last modified January 25, 2011, at 06:28 PM</div>\n    <div class='footnav' style='float:right; text-align:right'>\n    <a href=\"http://www.ics.uci.edu\">Bren School of Information and Computer Science</a><br>\n    <a href=\"http://www.uci.edu\">University of California, Irvine</a>\n    </div>\n  </div>\n<!--HTMLFooter--><script type=\"text/javascript\">\n  var _gaq = _gaq || [];\n  _gaq.push([\"_setAccount\", \"UA-24148957-2\"]);\n\t_gaq.push([\"_trackPageview\"]);\n\t(function() {\n\t  var ga = document.createElement(\"script\"); ga.type = \"text/javascript\"; ga.async = true;\n\t  ga.src = (\"https:\" == document.location.protocol ? \"https://ssl\" : \"http://www\") + \".google-analytics.com/ga.js\";\n\t  var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(ga, s);\n\t  })();\n</script>\n</body>\n</html>\n", "encoding": "ascii"}