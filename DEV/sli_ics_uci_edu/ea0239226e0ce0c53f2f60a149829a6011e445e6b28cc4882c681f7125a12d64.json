{"url": "http://sli.ics.uci.edu/Classes-CS178-Notes/Regression", "content": "<!DOCTYPE html \n    PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n  <title>SLI | Classes-CS178-Notes / Regression </title>\n  <meta http-equiv='Content-Style-Type' content='text/css' />\n  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />\n  <!--HTMLHeader--><style type='text/css'><!--\n  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n  code.escaped { white-space: nowrap; }\n  .vspace { margin-top:1.33em; }\n  .indent { margin-left:40px; }\n  .outdent { margin-left:40px; text-indent:-40px; }\n  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }\n  a.createlink { text-decoration:none; position:relative; top:-0.5em;\n    font-weight:bold; font-size:smaller; border-bottom:none; }\n  img { border:0px; }\n  .editconflict { color:green; \n  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }\n\n  table.markup { border:2px dotted #ccf; width:90%; }\n  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }\n  table.vert td.markup1 { border-bottom:1px solid #ccf; }\n  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }\n  table.markup caption { text-align:left; }\n  div.faq p, div.faq pre { margin-left:2em; }\n  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }\n  div.faqtoc div.faq * { display:none; }\n  div.faqtoc div.faq p.question \n    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }\n  div.faqtoc div.faq p.question * { display:inline; }\n   \n    .frame \n      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }\n    .lfloat { float:left; margin-right:0.5em; }\n    .rfloat { float:right; margin-left:0.5em; }\na.varlink { text-decoration:none;}\n\n--></style>  <meta name='robots' content='index,follow' />\n\n</head>\n<body>\n<!--PageHeaderFmt-->\n  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'\n    alt='SLI' border='0' /></a></div>\n  <div id='wikihead'>\n  <form action='http://sli.ics.uci.edu'>\n    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'\n      accesskey='c'>Recent Changes</a> -</span> --> \n    <input type='hidden' name='n' value='Classes-CS178-Notes.Regression' />\n    <input type='hidden' name='action' value='search' />\n    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->\n    <input type='text' name='q' value='' class='inputbox searchbox' />\n    <input type='submit' class='inputbutton searchbutton'\n      value='Search' />\n    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>\n  </form></div>\n<!--/PageHeaderFmt-->\n  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>\n<!--PageLeftFmt-->\n      <td id='wikileft' valign='top'>\n        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>\n</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>\n</li></ul><div class='vspace'></div><hr />\n<div class='vspace'></div>\n</td>\n<!--/PageLeftFmt-->\n      <td id='wikibody' valign='top'>\n<!--PageActionFmt-->\n        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/Regression?action=login'>login</a>\n</li></ul>\n</div>\n<!--PageTitleFmt-->\n        <div id='wikititle'>\n          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>\n          <h1 class='pagetitle'>Regression</h1></div>\n<!--PageText-->\n<div id='wikitext'>\n<h2>Regression problems</h2>\n<p>Regression is a type of supervised learning task in which the target variable is real-valued.\nFor example, we may wish to predict the sale price of a house, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png\" />, given some of its observable characteristics, e.g.,\n</p><ul><li><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/3b4438902a027c8e372fa298d27203ee.png\" />: house size, in square footage\n</li><li><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/f6922537fce4f33654bc5907e61e316f.png\" />: location, distance from the coast\n</li><li><img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/c0d812669dc0a4c388f6ff01b2f21104.png\" />\n</li></ul><p class='vspace'>We are likely to base our prediction of the sale price <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png\" />, and its relationship to the observable features <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/aac87ec1e389c58fe4de9cb2554e8d89.png\" />, on some set of training data, for example historical sales data.  For simplicity of our plots, we will initially assume a single feature (e.g., size), allowing us to plot our historical data as points <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/53fe96ba7d4a8df97850d557356cf7b9.png\" /> in a scatter plot, and to plot a predictor, or function <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/e4a1008e634a177e07d07e4b8ec00cd8.png\" /> from the observed features to the predicted value of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png\" />.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegScatter.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegScatter.png.jpg\" title=\"linRegScatter\" alt=\"linRegScatter\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit01.png.jpg\" title=\"linRegFit01\" alt=\"linRegFit01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit03.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit03.png.jpg\" title=\"linRegFit03\" alt=\"linRegFit03\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>Scatter plot of training data</td><td  align='center'>Linear function f(x)</td><td  align='center'>Nonlinear function f(x)</td></tr>\n</table>\n<p class='vspace'>We will initially focus on <em>linear regression</em>, in which our prediction about y is in the form of a linear function of the observed features <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/aac87ec1e389c58fe4de9cb2554e8d89.png\" />, for example:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/1cce831afb254ba47f993613d5e82863.png\" />\nIt is helpful to define all of our variables in a matrix form; this will allow us to write very compact equations, and will also translate well into Matlab syntax. For example, defining the \"zeroth feature\" <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/81ea9089829549c9b5172deea0440980.png\" />, we can write\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b9e4c37f6c2186590853ace6abf84983.png\" />\nwhere <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/2a1c161c44fb693803e196337dbc4723.png\" />, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/174cf26ec131d4ec5548d621d30bc62d.png\" /> are vectors formed by concatenating the features (respectively parameters) together.\n</p>\n<p class='vspace'>The data that we will use for training consist of a number of examples (for example, houses that have been recently sold, or recent stock behavior); we will index each example by a parenthesized super-script: <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/91f170912c40ba310b535e189e47c0f3.png\" />.  (The parentheses differentiate the indexing from, for example, taking a number to a power, such as <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/5e9126c5afafd56deb18b2681c3047b2.png\" /> for the square of x.\nWe again can vectorize an entire collection of data to be used for training into a matrix shorthand:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/10a46bac05ccd598b89128ecaf39ffd1.png\" />\nWe have chosen to represent each <em>row</em> of X as a training example (the values of each feature observed for a particular datum, such as a single house in the historical set), while each column indicates a particular feature.  The first column (of all ones) indicates \"feature zero\", a constant value that we prepend to our features to manage the offset term; each subsequent column represents the values of a particular feature (size, distance, etc.) that are observed across examples in the training data.\n</p>\n<p class='vspace'>Note that the most potentially confusing difference in syntax for most presentations is a result of transposition of one or more of these quantities.  It is helpful to keep in mind what the correct dimensionality of each vector is: <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b46b1fcb3da0d044b4bfb4f530561c0b.png\" /> has <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/9e11aaf75f190c83065bf28df856059d.png\" /> elements, the number of training data; <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> has <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/ac1a9cca1140cf7f10f6194720236a76.png\" /> elements, one more than there are observed features; and <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/045efe45ede704c942a572ca4341436a.png\" /> is <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/50c38b607359de48ae68b225dfca0a5a.png\" />.\n</p>\n<div class='vspace'></div><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<p><span  style='color: green;'> %Matlab syntax for predicting y <br /></span><span  style='color: black;'> yhat = X * theta';     </span><span  style='color: green;'> %dot product of each example datum, in a column vector</span>\n</p></div>\n<div class='vspace'></div><hr />\n<h2>Error measures and optimization</h2>\n<p>For a given observation (x,y), we can compute the error in our prediction (also called the residual) as <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/917e3616ce9ff8d643ae2ff6e62d7ea5.png\" />:\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegResid0.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegResid0.png.jpg\" title=\"linRegResid0\" alt=\"linRegResid0\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegResid1.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegResid1.png.jpg\" title=\"linRegResid1\" alt=\"linRegResid1\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>Residuals, constant <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/9be0dd33db2b119cd9ac25f48a769663.png\" /></td><td  align='center'>Residuals, linear <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/9be0dd33db2b119cd9ac25f48a769663.png\" /></td></tr>\n</table>\n<p class='vspace'>In general, we would like to minimize the overall size of these errors.  A common, and as we shall see computationally convenient choice is the Euclidean norm of the vector of residuals, or the sum of squared errors (SSE) cost:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/0f87fb13e5b558993eaee6197d175135.png\" />\nThe cost function J(.) tells us the accuracy of a given parameter vector <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> at predicting our training data.  This is a function defined over the space of parameters <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" />; for a two-dimensional parameter vector, we can visualize it as a surface, or use colors or contours to suggest the three-dimensional height (cost) on a 2D plot.\n</p>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegCostA.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegCostA.png.jpg\" title=\"linRegCostA\" alt=\"linRegCostA\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegCostB.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegCostB.png.jpg\" title=\"linRegCostB\" alt=\"linRegCostB\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegCostC.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegCostC.png.jpg\" title=\"linRegCostC\" alt=\"linRegCostC\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegCostD.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegCostD.png.jpg\" title=\"linRegCostD\" alt=\"linRegCostD\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>Color Map</td><td  align='center'>3D surface</td><td  align='center'>Surface + contours</td><td  align='center'>Contours only</td></tr>\n</table>\n<div class='vspace'></div><h3>Gradient descent</h3>\n<p>One option is to follow the local slope of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/6df6674a593bedf0b17f24ade658e1ea.png\" /> downward towards a local minimum.  We can evaluate the gradient direction, i.e., the direction in which our cost function <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/6df6674a593bedf0b17f24ade658e1ea.png\" /> has the greatest increase; going in the opposite direction gives us the direction of greatest decrease of our cost J.  This gradient will be a vector of the same dimension as <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" />:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/f80af2fed5cf89bdf385a179b7b56768.png\" />\n</p>\n<p class='vspace'>In gradient descent, we simply initialize to some starting value of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> and repeatedly update by choosing a new <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> by moving in the direction of steepest descent, e.g.,\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/87c17a1beeaf6cc512abc04e781d23e6.png\" />\nHere, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/04fa16ba53024520f5eeda4063a1c203.png\" /> indicates that we are updating the value of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> using the quantity on the right.  The value <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a158de49bddbf8eb0feeb5683de34258.png\" /> is a step-size parameter that tells us how far to move in the direction of the gradient.  The choice of step size can be very important in gradient descent methods, as it often controls how fast we converge to a local minimum.  Values that are too small move very slowly; values that are too large can skip over or even oscillate around local minima.  \n</p>\n<p class='vspace'>A common heuristic approach to setting the step size is to let <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a158de49bddbf8eb0feeb5683de34258.png\" /> decrease with each iteration (step), for example choosing <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a158de49bddbf8eb0feeb5683de34258.png\" /> to be inversely proportional to the iteration number.\n</p>\n<p class='vspace'>We can see the behavior of gradient descent on the cost function J defined over parameter space (top; each point corresponds to a vector <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/6e4e4d33223bf37b97243c523622b77f.png\" />), and the induced predictors <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/3e3cd5c8e1a1d4e913a01950d6e19353.png\" /> (bottom; each value of <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> corresponds to a different linear predictor for y):\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradB00.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradB00.png.jpg\" title=\"linRegGradB00\" alt=\"linRegGradB00\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradB01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradB01.png.jpg\" title=\"linRegGradB01\" alt=\"linRegGradB01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradB10.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradB10.png.jpg\" title=\"linRegGradB10\" alt=\"linRegGradB10\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradB30.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradB30.png.jpg\" title=\"linRegGradB30\" alt=\"linRegGradB30\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradB90.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradB90.png.jpg\" title=\"linRegGradB90\" alt=\"linRegGradB90\" border=\"0\" /></a></td></tr>\n<tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradA00.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradA00.png.jpg\" title=\"linRegGradA00\" alt=\"linRegGradA00\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradA01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradA01.png.jpg\" title=\"linRegGradA01\" alt=\"linRegGradA01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradA10.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradA10.png.jpg\" title=\"linRegGradA10\" alt=\"linRegGradA10\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradA30.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradA30.png.jpg\" title=\"linRegGradA30\" alt=\"linRegGradA30\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegGradA90.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegGradA90.png.jpg\" title=\"linRegGradA90\" alt=\"linRegGradA90\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>Initialization</td><td  align='center'>Iteration 1</td><td  align='center'>Iteration 10</td><td  align='center'>Iteration 30</td><td  align='center'>Iteration 90</td></tr>\n</table>\n<div class='vspace'></div><h3>Closed form optimum for squared error</h3>\n<p>We can solve the quadratic equation defined by the stationary point\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/45a7599f420b82c05a7a5fc83be1486d.png\" />\nRearranging gives a quadratic equation, the solution of which is the minimum squared error (MSE) estimator\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b775c496fd917d2206faf63bc451a666.png\" />\nThe term <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/22b587f0e6d9cbe71cd638d60b1544fb.png\" /> is called the (Moore-Penrose) pseudo-inverse, and compensates for the fact that X\nis typically over-constrained: there is no value of theta that will exactly predict every value of y from x.\n</p>\n<div class='vspace'></div><hr />\n<h2>Increasing the number of features</h2>\n<p>So far we have considered linear functions of several observed features <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/3b4438902a027c8e372fa298d27203ee.png\" />, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/f6922537fce4f33654bc5907e61e316f.png\" />.  Suppose that we have only one feature, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/32bbcdd3142dd96075ffa59ab8bf7663.png\" />, but would like our predictor to be a <em>nonlinear</em> function of x, for example \n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/68cf5508be331d795469057566538568.png\" />\nWe can simply define <em>new</em> features <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/814794caa193dc72504be2e69f8d7652.png\" />, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/1896fd8ff19bae7023483d6f3ba574d2.png\" />, and so on (just as we defined <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7fe916053fbb294dce1a35e2010ea93b.png\" />), and this predictor becomes simply\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/fbf3b0119b6268149170f26d2d2fa950.png\" />\nIn other words, it still fits the linear regression model, but in a new <em>feature space</em> with additional features that are deterministic functions of our observations.  Applying our least-squares estimator gives polynomial fits, <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/c765981d3f0a9291134f1859f68186e3.png\" />:\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit00.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit00.png.jpg\" title=\"linRegFit00\" alt=\"linRegFit00\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit01.png.jpg\" title=\"linRegFit01\" alt=\"linRegFit01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit03.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit03.png.jpg\" title=\"linRegFit03\" alt=\"linRegFit03\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit07.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit07.png.jpg\" title=\"linRegFit07\" alt=\"linRegFit07\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegFit10.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegFit10.png.jpg\" title=\"linRegFit10\" alt=\"linRegFit10\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>P=0</td><td  align='center'>P=1</td><td  align='center'>P=3</td><td  align='center'>P=7</td><td  align='center'>P=10</td></tr>\n</table>\n<p class='vspace'>If we think that our target variable y is likely to be linearly related to some complex function of several observed variables, that combination can also be added as a new, observed feature. \n</p>\n<p class='vspace'>There are two ways to view this process.  First, we are creating a \"more complex\" functional predictor <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/9be0dd33db2b119cd9ac25f48a769663.png\" />; the set of functions we can represent is now larger (e.g., from the set of all lines to the set of all cubic polynomials).  An alternative is to forget that the features are now deterministically related to one another, and to imagine that we are adding extra measurements (features) with which to predict y.  In this view, we are learning a linear predictor from data, but those data lie in a higher dimensional space.  (We will return to these views later, in classification.)\n</p>\n<div class='vspace'></div><h2>Overfitting</h2>\n<p>As we saw in our polynomial fits, for 11 data points and a degree-10 polynomial (with 11 coefficients), we can predict all of our training data exactly!  And yet, something about that predictor is not satisfying -- it does not \"look\" like a good predictor.  In fact, we have <em>overfit</em> to the data -- we have chosen such a complex predictor that, although it is able to reconstruct the training data, we have no faith that it will accurately predict new data. \n</p>\n<p class='vspace'>We can see the \"generalization\" or test error of a predictor simply by gathering more data, and evaluating our cost function (e.g. mean squared error) on those new points (green).  What we find is that, for very simple predictors (P=0,1), the performance on new test data is much like the performance on the training data.  However, as the function gets more complex, the training error continues to decrease -- but the test error does not.  By P=10, training error is zero, but test error is extremely high.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegVal00.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegVal00.png.jpg\" title=\"linRegVal00\" alt=\"linRegVal00\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegVal01.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegVal01.png.jpg\" title=\"linRegVal01\" alt=\"linRegVal01\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegVal03.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegVal03.png.jpg\" title=\"linRegVal03\" alt=\"linRegVal03\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegVal07.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegVal07.png.jpg\" title=\"linRegVal07\" alt=\"linRegVal07\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegVal10.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/th00---linRegVal10.png.jpg\" title=\"linRegVal10\" alt=\"linRegVal10\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>P=0</td><td  align='center'>P=1</td><td  align='center'>P=3</td><td  align='center'>P=7</td><td  align='center'>P=10</td></tr>\n</table>\n<p class='vspace'>We can see this directly by plotting training and test error as a function of polynomial degree P.  For very simple predictors, we are unable to capture the complexity of the dependence between x and y; but for overly complex predictors, we memorize the values of y at the expense of generalization.  \n</p>\n<div class='vspace'></div><div class='img imgonly'><img src='http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/Regression/linRegValidate.png' alt='' title='' /></div>\n<p class='vspace'>Notionally, the \"P\" axis can be thought of as \"complexity\", and we find a similar curve whenever the complexity of our learner increases.  We will see that much of the practical aspects of machine learning come down to choosing and controlling our position on this curve, increasing the complexity when under-fitting and decreasing it when overfitting.\n</p>\n<div class='vspace'></div>\n</div>\n\n      </td>\n    </tr></table>\n<!--PageFooterFmt-->\n  <div id='wikifoot'>\n    <div class='footnav' style='float:left'> Last modified January 11, 2011, at 12:59 PM</div>\n    <div class='footnav' style='float:right; text-align:right'>\n    <a href=\"http://www.ics.uci.edu\">Bren School of Information and Computer Science</a><br>\n    <a href=\"http://www.uci.edu\">University of California, Irvine</a>\n    </div>\n  </div>\n<!--HTMLFooter--><script type=\"text/javascript\">\n  var _gaq = _gaq || [];\n  _gaq.push([\"_setAccount\", \"UA-24148957-2\"]);\n\t_gaq.push([\"_trackPageview\"]);\n\t(function() {\n\t  var ga = document.createElement(\"script\"); ga.type = \"text/javascript\"; ga.async = true;\n\t  ga.src = (\"https:\" == document.location.protocol ? \"https://ssl\" : \"http://www\") + \".google-analytics.com/ga.js\";\n\t  var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(ga, s);\n\t  })();\n</script>\n</body>\n</html>\n", "encoding": "ascii"}