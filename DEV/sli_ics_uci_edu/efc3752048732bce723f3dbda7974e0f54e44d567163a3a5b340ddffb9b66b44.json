{"url": "http://sli.ics.uci.edu/Classes-CS178-Notes/LinearClassify", "content": "<!DOCTYPE html \n    PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n  <title>SLI | Classes-CS178-Notes / LinearClassify </title>\n  <meta http-equiv='Content-Style-Type' content='text/css' />\n  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />\n  <!--HTMLHeader--><style type='text/css'><!--\n  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n  code.escaped { white-space: nowrap; }\n  .vspace { margin-top:1.33em; }\n  .indent { margin-left:40px; }\n  .outdent { margin-left:40px; text-indent:-40px; }\n  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }\n  a.createlink { text-decoration:none; position:relative; top:-0.5em;\n    font-weight:bold; font-size:smaller; border-bottom:none; }\n  img { border:0px; }\n  .editconflict { color:green; \n  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }\n\n  table.markup { border:2px dotted #ccf; width:90%; }\n  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }\n  table.vert td.markup1 { border-bottom:1px solid #ccf; }\n  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }\n  table.markup caption { text-align:left; }\n  div.faq p, div.faq pre { margin-left:2em; }\n  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }\n  div.faqtoc div.faq * { display:none; }\n  div.faqtoc div.faq p.question \n    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }\n  div.faqtoc div.faq p.question * { display:inline; }\n   \n    .frame \n      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }\n    .lfloat { float:left; margin-right:0.5em; }\n    .rfloat { float:right; margin-left:0.5em; }\na.varlink { text-decoration:none;}\n\n--></style>  <meta name='robots' content='index,follow' />\n\n</head>\n<body>\n<!--PageHeaderFmt-->\n  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'\n    alt='SLI' border='0' /></a></div>\n  <div id='wikihead'>\n  <form action='http://sli.ics.uci.edu'>\n    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'\n      accesskey='c'>Recent Changes</a> -</span> --> \n    <input type='hidden' name='n' value='Classes-CS178-Notes.LinearClassify' />\n    <input type='hidden' name='action' value='search' />\n    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->\n    <input type='text' name='q' value='' class='inputbox searchbox' />\n    <input type='submit' class='inputbutton searchbutton'\n      value='Search' />\n    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>\n  </form></div>\n<!--/PageHeaderFmt-->\n  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>\n<!--PageLeftFmt-->\n      <td id='wikileft' valign='top'>\n        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>\n</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>\n</li></ul><div class='vspace'></div><hr />\n<div class='vspace'></div>\n</td>\n<!--/PageLeftFmt-->\n      <td id='wikibody' valign='top'>\n<!--PageActionFmt-->\n        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/LinearClassify?action=login'>login</a>\n</li></ul>\n</div>\n<!--PageTitleFmt-->\n        <div id='wikititle'>\n          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>\n          <h1 class='pagetitle'>LinearClassify</h1></div>\n<!--PageText-->\n<div id='wikitext'>\n<h2>Linear classifiers</h2>\n<p>A linear classifier is one whose decision function is a linear function of the input features, e.g., for a binary classifier between classes <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/dfc6f0cdbcd1c8f128f874dd19221f51.png\" />, a decision of the form\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/0ba37eae0a804dbb2b460ddb64be977b.png\" />\nwhere <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/d998108dc01ef64c60e713196394ddf2.png\" /> is our usual vector of feature values, and <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/d3967e57687fd44f80cd7c1c359bf068.png\" /> is a vector of weights.\nNote here that we have used classes <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/dfc6f0cdbcd1c8f128f874dd19221f51.png\" />, rather than our usual <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/a869a04a3c7e8992ecbf10145d00204b.png\" /> binary classes, to highlight that our decision rule is choosing the \"sign\" of the linear response <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/239f09f4c7753b155bf47758f91b69f2.png\" />.\n</p>\n<p class='vspace'>Linear classifiers are sometimes called \"perceptrons\", their historical name when proposed by Rosenblatt and studied in the early days of artificial intelligence; they are a very simple type of neural network.\n</p>\n<p class='vspace'>The decision boundary for a linear classifier is also linear.  The decision boundary are the points at which we transition from decision +1 to decision -1; in our example, this occurs at precisely the solution to the linear equation <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/1287e17c1e8578b228d8fc5c4e5f9d54.png\" />.  For example, for two features <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/03583a58845d11c02ccbc2bfaf62f15c.png\" /> we can solve to find the decision boundary, which is a line in the two-dimensional feature space:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/f0ea50c38c8809d8d5e2037f8a06a526.png\" />\n</p>\n<div class='vspace'></div><h3>Linearly separable data</h3>\n<p>It is useful to differentiate between data sets that can be <em>linearly separated</em>, i.e., there exists a linear classifier that achieves zero training error, and those that cannot.  A few examples are shown here, for both real-valued features and binary-valued features:\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/linSep1.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---linSep1.png.jpg\" title=\"linSep1\" alt=\"linSep1\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/linSep2.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---linSep2.png.jpg\" title=\"linSep2\" alt=\"linSep2\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/linSepAnd.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---linSepAnd.png.jpg\" title=\"linSepAnd\" alt=\"linSepAnd\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/linSepXor.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---linSepXor.png.jpg\" title=\"linSepXor\" alt=\"linSepXor\" border=\"0\" /></a></td></tr>\n<tr ><td  align='center'>Linearly separable</td><td  align='center'>Not linearly separable</td><td  align='center'>\"AND\" (Linearly separable)</td><td  align='center'>\"XOR\" (Not lin. sep.)</td></tr>\n</table>\n<p class='vspace'>As can be seen, a perceptron / linear classifier cannot correctly learn an exclusive or (XOR) function, a result famously discussed by <a class='urllink' href='http://en.wikipedia.org/wiki/Perceptrons_%28book%29' rel='nofollow'>Minsky and Papert in 1969</a>.\n</p>\n<div class='vspace'></div><h2>Training linear classifiers</h2>\n<p>What makes a good classifier?  The usual measure of error for classification is the classification error, or misclassification rate -- the number of mistakes (misclassifications) made on the data.  Typically, we would like to minimize the misclassification rate over the training data, by finding a set of parameters (weights) that make few errors.  As with linear regression, we can notionally think about exploring the space of parameters, assigning each point a cost, and searching for the point with minimum cost.\n</p>\n<p class='vspace'>However, the misclassification rate is often difficult to optimize directly.  First, it is not smooth -- it changes value only when the decision boundary passes a data point, so it is constant until it changes abruptly one way or the other.  There is thus very little \"signal\" about which direction we should modify the parameters, in order to reduce the error.  Another consequence of such \"flat\" values can be seen when the data are linearly separable -- there may be a large set of linear classifiers that achieve zero error, but intuitively we can guess that some are likely to be better than others.  Classification accuracy alone, however, judges these classifiers exactly the same way.\n</p>\n<div class='vspace'></div>\n<table border='0' cellspacing='3' ><tr ><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/AA1.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---AA1.png.jpg\" title=\"AA1\" alt=\"AA1\" border=\"0\" /></a></td><td  align='left'><a href=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/AA2.png\" class=\"minilink\" ><img class=\"mini\" src=\"http://sli.ics.uci.edu/pmwiki/uploads/Classes-CS178-Notes/LinearClassify/th00---AA2.png.jpg\" title=\"AA2\" alt=\"AA2\" border=\"0\" /></a></td></tr>\n<tr ><td  align='left' colspan='2'>Two linear classifiers with zero training error</td></tr>\n</table>\n<p class='vspace'>Such difficulties motivate the use of \"surrogate\" loss functions -- error functions that we can use to replace the classification error that will be easier for us to optimize.  The typical training approach is to learn parameters to optimize the surrogate loss, and hope (sometimes with good reason) that this also produces good classification accuracy.\n</p>\n<div class='vspace'></div><h3>Linear classification as a regression problem</h3>\n<p>The linear classifier has the form of thresholding a linear function of the features.  How can we learn a good linear function?  One simple way we could consider is just to learn a linear predictor of the class in the <em>regression</em> sense, and then threshold that value.  So, for example, we define a variable <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/ec7edaf64bf0a0827d429d6a865a15ca.png\" />, equal to the class value c, to predict using regression.  We then regress the data <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/98da0e898d3d4e50e51274a01fdfc36d.png\" /> to find parameters <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" />, and finally, we define our (real-valued) regression prediction as <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/82b88e912a0f8c97304935df3305e2b1.png\" /> and our <em>discrete</em> class prediction as <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/0f4dbc889a6920c85bacf01ef2b94049.png\" />.\n</p>\n<p class='vspace'>This is an example of a surrogate loss.  We are normally interested in our misclassification rate, i.e., the fraction of data on which <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/cb66f5d09e1e4352ea20d5b18a32b6ba.png\" />, or:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/13700043aecaded78c661de2cc66f94d.png\" />\nBut, our training of the model parameters <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/7b2d77fc1e2a518e6981f4b3eaa38869.png\" /> using linear regression actually minimizes a different cost, the MSE of a linear predictor:\n<img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/b525f8c21df0bfc2126c24dd1092d868.png\" />\nSo the model we find will have a good <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/5a8e3a6aa9763ccb34b197804691e079.png\" />, but will it also have a good <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/79abd71c1422ce88012e24e0432bcba9.png\" />?\n</p>\n<p class='vspace'>In some cases it will, and in some it will not.\n</p>\n<div class='vspace'></div>\n</div>\n\n      </td>\n    </tr></table>\n<!--PageFooterFmt-->\n  <div id='wikifoot'>\n    <div class='footnav' style='float:left'> Last modified January 06, 2015, at 01:05 PM</div>\n    <div class='footnav' style='float:right; text-align:right'>\n    <a href=\"http://www.ics.uci.edu\">Bren School of Information and Computer Science</a><br>\n    <a href=\"http://www.uci.edu\">University of California, Irvine</a>\n    </div>\n  </div>\n<!--HTMLFooter--><script type=\"text/javascript\">\n  var _gaq = _gaq || [];\n  _gaq.push([\"_setAccount\", \"UA-24148957-2\"]);\n\t_gaq.push([\"_trackPageview\"]);\n\t(function() {\n\t  var ga = document.createElement(\"script\"); ga.type = \"text/javascript\"; ga.async = true;\n\t  ga.src = (\"https:\" == document.location.protocol ? \"https://ssl\" : \"http://www\") + \".google-analytics.com/ga.js\";\n\t  var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(ga, s);\n\t  })();\n</script>\n</body>\n</html>\n", "encoding": "ascii"}