{"url": "http://vision.ics.uci.edu/sccv/2010/schedule.html", "content": "<html><head><meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/><meta name='lsq' content='3'/><meta name='trixrows' content='20'/><meta name='trixr1' content='0'/><meta name='trixr2' content='20'/><meta name='trixdiv' content='20'/><meta name='trixcnt' content='1'/><meta name='trixlast' content='20'/><link rel=stylesheet href=\"//spreadsheets.google.com/client/css/796585177-trix_main-ssl.css\" type=\"text/css\"><style>.tblGenFixed td {padding:0 3px;overflow:hidden;white-space:normal;letter-spacing:0;word-spacing:0;background-color:#fff;z-index:1;border-top:0px none;border-left:0px none;border-bottom:1px solid #CCC;border-right:1px solid #CCC;} .dn {display:none} .tblGenFixed td.s0 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid black;border-left:1px solid #CCC;} .tblGenFixed td.s2 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s1 {background-color:#DDDDDD;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:bold;font-style:normal;color:#000000;text-decoration:none;text-align:center;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid black;} .tblGenFixed td.s16 {background-color:#C0C0C0;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s17 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:right;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;border-left:1px solid black;} .tblGenFixed td.s18 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid black;} .tblGenFixed td.s19 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;} .tblGenFixed td.s12 {background-color:#C0C0C0;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:right;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;border-left:1px solid black;} .tblGenFixed td.s9 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid black;} .tblGenFixed td.s13 {background-color:#C0C0C0;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;} .tblGenFixed td.s14 {background-color:#C0C0C0;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:right;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;border-left:1px solid black;} .tblGenFixed td.s23 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s7 {background-color:#FADCB3;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s8 {background-color:white;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:right;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;border-left:1px solid black;} .tblGenFixed td.s15 {background-color:#C0C0C0;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s5 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s21 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s6 {background-color:#FADCB3;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s22 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;border-left:1px solid #CCC;} .tblGenFixed td.s10 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid black;} .tblGenFixed td.s3 {background-color:#FADCB3;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:right;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;border-left:1px solid black;} .tblGenFixed td.s20 {background-color:#EEEEEE;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s4 {background-color:#FADCB3;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;} .tblGenFixed td.s11 {background-color:white;font-family:arial,sans,sans-serif;font-size:120.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:middle;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid black;border-bottom:1px solid black;} </style></head><body style='border:0px;margin:0px'><table border=0 cellpadding=0 cellspacing=0 id='tblMain'><tr><td><table border=0 cellpadding=0 cellspacing=0 class='tblGenFixed' id='tblMain_0'><tr class='rShim'><td class='rShim' style='width:0;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:928px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><td class='rShim' style='width:120px;'><tr><td class=hd><p style='height:16px;'>.</td><td  class='s0'><td  class='s1'>Name<td  class='s1'>Affiliation<td  class='s1'>Title<td  class='s1'>Authors<td  class='s1'>Abstract.<td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'><td  class='s2'></tr><tr><td class=hd><p style='height:293px;'>.</td><td  class='s3'>10:30<td  class='s4'>Carolina<td  class='s4'>UCSD<td  class='s4'>Multi-Class Object Localization by Combining Local Contextual Interactions<td  class='s4'>Carolina Galleguillos, Brian McFee , Serge Belongie  and Gert Lanckriet <td  class='s4'>Recent work in object localization has shown that the use of contextual cues can greatly improve accuracy over models that use appearance features alone. Although many of these models have successfully explored different types of<br/>contextual sources, they only consider one type of contextual interaction (e.g., pixel, region or object level interactions), leaving open questions about the true potential contribution of context. Furthermore, contributions across object classes and over appearance features still remain unknown.<br/>In this work, we introduce a novel model for multiclass object localization that incorporates different levels<br/>of contextual interactions. We study contextual interactions at pixel, region and object level by using three different<br/>sources of context: semantic, boundary support and contextual neighborhoods. Our framework learns a single similarity metric from multiple kernels, combining pixel and region interactions with appearance features, and then uses a conditional random field to incorporate object level interactions.<br/><br/>We perform experiments on two challenging image databases: MSRC and PASCAL VOC 2007. Experimental<br/>results show that our model outperforms current state-ofthe-art contextual frameworks and reveals individual contributions for each contextual interaction level, as well as the importance of each type of feature in object localization.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:313px;'>.</td><td  class='s3'>10:50<td  class='s4'>Dennis Park<td  class='s4'>UC Irvine<td  class='s4'>Multiresolution models for object detection<td  class='s4'>Dennis Park, Deva Ramanan, and Charless Fowlkes<td  class='s4'>Most current approaches to recognition aim to be scale-<br/>invariant. However, the cues available for recognizing a 300 pixel tall<br/>object are qualitatively dierent from those for recognizing a 3 pixel tall<br/>object. We argue that for sensors with nite resolution, one should in-<br/>stead use scale-variant, or multiresolution representations that adapt in<br/>complexity to the size of a putative detection window. We describe a<br/>multiresolution model that acts as a deformable part-based model when<br/>scoring large instances and a rigid template with scoring small instances.<br/>We also examine the interplay of resolution and context, and demon-<br/>strate that context is most helpful for detecting low-resolution instances<br/>when local models are limited in discriminative power. We demonstrate<br/>impressive results on the Caltech Pedestrian benchmark, which contains<br/>object instances at a wide range of scales. Whereas recent state-of-the-<br/>art methods demonstrate missed detection rates of 86%-37% at 1 false-<br/>positive-per-image, our multiresolution model reduces the rate to 29%.<td  class='s6'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'><td  class='s7'></tr><tr><td class=hd><p style='height:148px;'>.</td><td  class='s3'>11:10<td  class='s4'>Ricky Sethi<td  class='s4'>UCLA and UCR<td  class='s4'>The Human Action Image and its Application to Motion Recognition<td  class='s4'>Ricky J. Sethi<br/>Amit K. Roy-Chowdhury<td  class='s4'>Recognizing a person&#39;s motion is intuitive for humans but represents a challenging problem in machine vision. In this paper, we present a multi-disciplinary framework for recognizing human actions. We develop a novel descriptor, the Human Action Image (HAI), a physically-significant, compact representation for the motion of a person, which we derive from Hamilton&#39;s Action. We prove the additivity of Hamilton&#39;s Action in order to formulate the HAI and then embed the HAI as the Motion Energy Pathway of the Neurobiological model of motion recognition. The Form Pathway is modelled using existing low-level feature descriptors based on shape and appearance. Finally, we propose a Weighted Integration (WI) methodology to combine the two pathways via statistical Hypothesis Testing using the bootstrap to do the final recognition. Experimental validation of the theory is provided on the well-known Weizmann and USF Gait datasets. <td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:218px;'>.</td><td  class='s3'>11:30<td  class='s4'>Piotr Dollar<td  class='s4'>Caltech<td  class='s4'>The Fastest Pedestrian Detector in the West<td  class='s4'>Piotr Dollar, Serge Belongie, Pietro Perona<td  class='s4'>We demonstrate a multiscale pedestrian detector operating in near real time (~6 fps on 640x480 images) with state-of-the-art detection performance. The computational bottleneck of many modern detectors is the construction of an image pyramid, typically sampled at 8-16 scales per octave, and associated feature computations at each scale. We propose a technique to avoid constructing such a finely sampled image pyramid without sacrificing performance: our key insight is that for a broad family of features, including gradient histograms, the feature responses computed at a single scale can be used to approximate feature responses at nearby scales. The approximation is accurate within an entire scale octave. This allows us to decouple the sampling of the image pyramid from the sampling of detection scales. Overall, our approximation yields a speedup of 10-100 times over competing methods with only a minor loss in detection accuracy of about 1-2% on the Caltech Pedestrian dataset across a wide range of evaluation settings. The results are confirmed on three additional datasets (INRIA, ETH, and TUD-Brussels) where our method always scores within a few percent of the state-of-the-art while being 1-2 orders of magnitude faster. The approach is general and should be widely applicable.<br/><td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:73px;'>.</td><td  class='s3'>11:50<td  class='s4'>Kris Kitani<td  class='s4'>UCSD<td  class='s4'>Learning Action Categories for First Person Vision<td  class='s4'>Kris Kitani<td  class='s4'>This work explores the use of ego-motion features obtained from a head-mounted camera to learn ego-centric action categories without supervision. We show that a non-parametric Bayesian mixture model can be used to efficiently learn action categories from an unlabeled continuous video database.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:58px;'>.</td><td  class='s8'>12:10<td  class='s9'>Lunch Break<td  class='s10'><td  class='s10'><td  class='s10'><td  class='s11'><td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:168px;'>.</td><td  class='s12'>1:30<td  class='s13'>Yoav Schechner<td  class='s13'>Caltech<td  class='s13'>Audio-Visual Association: Look at Sparse Events <td  class='s13'>Zohar Barzilay and Yoav Y. Schechner<td  class='s13'>In complex scenes, a camcorder having a single microphone captures several audio-associated visual objects (AVOs), i.e., moving visual objects that emit sounds. At the same time, the camera may also view silent objects, and the microphone may sense sounds unrelated to the scene in view. We seek to: spatially localize independent AVOs; isolate the audio component corresponding to each; and ignore the other sources. Achieving this using just a single microphone is challenging. We describe computational approaches to these problems. They are based on correlating events that are sparse in both auditory and visual domains. In particular, audio onsets and instances of visual high spatial acceleration are temporally sparse. However, their coincidence helps establish audio-visual correspondence continuously throughout videos. In addition, visual spatial sparsity of AVOs is a strong cue for localization.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:105px;'>.</td><td  class='s14'>1:50<td  class='s13'>Luis Goncalves<td  class='s13'>EvoRetail, MetaModal<td  class='s13'>Human Vision and Real Intelligence - Seeing with Sound<td  class='s13'>Luis Goncalves<td  class='s13'>I will describe the &quot;Seeing-with-Sound&quot; project, where we have built a prototype sensory substitution device that transforms images into sound.   With training, blind participants are able to locate, track, walk to, and grab objects.  The challenges, obstacles, promise, and opportunities of this neuroplasticity-inspired field will be discussed.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:142px;'>.</td><td  class='s14'>2:10<td  class='s13'>Alper Ayvaci<td  class='s13'>UC, Los Angeles<td  class='s13'>Occlusion Detection and Motion Estimation with Occlusions<td  class='s13'>Alper Ayvaci, Michalis Raptis, Stefano Soatto<td  class='s13'>We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that, under standard assumptions of Lambertian reflection and static illumination, the task can be posed as a convex optimization problem. Therefore, the solution, computed using efficient algorithms, is guaranteed to be unique and globally optimal, for any number of independently moving objects, and any number of occlusion layers. We test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance, in addition to motion estimation. We also discuss the shortcomings and limitations of our approach.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:128px;'>.</td><td  class='s14'>2:30<td  class='s13'>Julian Yarkony <td  class='s13'>UCI<td  class='s13'>Planar Cycle Covering Graphs<td  class='s13'>Julian Yarkony, Charless Fowlkes, Alexander Ihler<td  class='s13'>We describe a new variational lower-bound on the minimum energy configuration of a planar binary MRF. Our method is based on adding auxiliary nodes to every face of a planar embedding of the graph in order to capture the effect of unary potentials. A ground state of the resulting approximation can be computed effeciently by reduction to minimum-weight perfect matching. We show that optimization of variational parameters achieves as tight a bound as dual-decomposition approaches that use the set of all cycles or all outer-planar subproblems. We demonstrate that our variational optimization converges quickly and provides high-quality solutions to hard combinatorial problems.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:83px;'>.</td><td  class='s14'>2:50<td  class='s13'>Piotr Slomka<td  class='s13'>Cedars-Sinai Medical Center<td  class='s13'>Computer analysis of modern cardiac imaging data<td  class='s13'>Piotr Slomka<td  class='s13'>The goal of our research is to automate quantitative analysis of complex cardiac imaging data.  The images  are obtained by Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), and Single Photon Emission Computed Tomography (SPECT) modalities.  New advances in these technologies allow 3D acquisition of the beating heart with high temporal and spatial resolution.  Our work involves segmentation of the heart, detection and analysis of lesions in the coronary arteries and in the myocardium (heart muscle), automatic registration of multimodality data, inter-scan comparison of image changes, and quantification of physiologically important parameters.  We have demonstrated that our methods result in more reproducible testing, and in some cases, a more accurate diagnosis than that provided by a visual analysis performed by experienced readers.  I will provide a brief overview of the cardiac imaging methods, our latest results, and computing challenges associated with automated analysis of cardiac data.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:448px;'>.</td><td  class='s14'>3:10<td  class='s13'>Tali Treibitz<td  class='s13'>UCSD<td  class='s13'>Resolution Loss without Blur: Recovery Limits in Pointwise Degradation<td  class='s13'>Tali Treibitz and Yoav Schechner<td  class='s13'>Pointwise image formation models appear in a variety of photography<br/>and computer vision problems, such as: specular/diffuse reflection,<br/>irradiance falloff from point sources, attenuation and veiling in<br/>haze, direct/indirect illumination, dirty windows, semireflections,<br/>vignetting and more. An expanding array of methods has been devised to<br/>handle such problems. However, what is the recovery limit? Are there<br/>object features that cannot be effectively recovered under the<br/>pointwise degradation, despite the best efforts made by the recovery<br/>method? Can we quantitatively assess the recoverability of an object<br/>of a certain size and contrast? If some objects are not recovered, is<br/>there a point in trying to develop a new method to salvage them, or is<br/>their loss fundamental? We derive bounds to recovery from pointwise<br/>degradation, even if the parameters of an algorithm are perfectly<br/>set. The analysis uses a  physical model for the acquired signal and<br/>noise, and also accounts for potential post-acquisition noise<br/>filtering. The analysis yields an effective cutoff-frequency, which is<br/>induced by noise, despite having no optical blur in the imaging model.<br/><br/><br/>The talk is based on works from ICCP&#39;09 and CVPR&#39;09.<br/><td  class='s15'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'><td  class='s16'></tr><tr><td class=hd><p style='height:36px;'>.</td><td  class='s17'>3:30<td  class='s9'>Coffee Break<td  class='s18'><td  class='s18'><td  class='s18'><td  class='s19'><td  class='s20'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'><td  class='s21'></tr><tr><td class=hd><p style='height:203px;'>.</td><td  class='s3'>4:00<td  class='s4'>Marco Andreetti<td  class='s4'>California Institute of Technology<td  class='s4'>Unsupervised Learning of Categorical Segments in Image Collections<td  class='s4'>Marco Andreetto<br/>Lihi Zelnik-Manor<br/>Pietro Perona<td  class='s4'>Which one comes first: segmentation or recognition? We propose a unified framework for carrying out the two simultaneously and without  supervision. The framework combines a flexible probabilistic model, for representing the shape and appearance of each segment, with the popular  ``bag of visual words&#39;&#39; model for recognition.<br/>If applied to a collection of images, our framework can simultaneously discover the segments of each image, and the correspondence between such segments, without supervision.Such recurring segments may be thought of as the `parts&#39; of corresponding<br/>objects that appear multiple times in the image collection. Thus, the model may be used for learning new categories, detecting/classifying objects, and segmenting images, without <br/>using expensive human annotation. <td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:168px;'>.</td><td  class='s3'>4:20<td  class='s4'>Mohsen Hejrati<td  class='s4'>University of California at Irvine<td  class='s4'>Every Picture Tells a Story: Generating Sentences from Images<td  class='s4'>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, David Forsyth<td  class='s4'>Humans can prepare concise descriptions of pictures, focus- ing on what they find important. We demonstrate that automatic meth- ods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning ob- tained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned us- ing data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:383px;'>.</td><td  class='s3'>4:40<td  class='s4'>Steven Branson<td  class='s4'>UCSD<td  class='s4'>Visual Recognition with Humans in the Loop<td  class='s4'>Steve Branson , Catherine Wah, Florian Schro\ufb00, Boris Babenko, Peter<br/>Welinder, Pietro Perona, and Serge Belongie<br/><td  class='s4'>Abstract. We present an interactive, hybrid human-computer method<br/>for object classi\ufb01cation. The method applies to classes of objects that are<br/>recognizable by people with appropriate expertise (e.g., animal species or<br/>airplane model), but not (in general) by people without such expertise. It<br/>can be seen as a visual version of the 20 questions game, where questions<br/>based on simple visual attributes are posed interactively. The goal is to<br/>identify the true class while minimizing the number of questions asked,<br/>using the visual content of the image. We introduce a general framework<br/>for incorporating almost any o\ufb00-the-shelf multi-class object recognition<br/>algorithm into the visual 20 questions game, and provide methodologies<br/>to account for imperfect user responses and unreliable computer vision<br/>algorithms. We evaluate our methods on Birds-200, a difficult dataset<br/>of 200 tightly-related bird species, and on the Animals With Attributes<br/>dataset. Our results demonstrate that incorporating user input drives up<br/>recognition accuracy to levels that are good enough for practical appli-<br/>cations, while at the same time, computer vision reduces the amount of<br/>human interaction required.<br/><td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:343px;'>.</td><td  class='s3'>4:10<td  class='s4'>Carl Vondrick<td  class='s4'>UC Irvine<td  class='s4'>Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces<td  class='s4'>Carl Vondrick, Deva Ramanan, Donald Patterson<td  class='s4'>Accurately annotating entities in video is labor intensive and<br/>expensive. As the quantity of online video grows, traditional solutions to<br/>this task are unable to scale to meet the needs of researchers with lim-<br/>ited budgets. Current practice provides a temporary solution by paying<br/>dedicated workers to label a fraction of the total frames and otherwise<br/>settling for linear interpolation. As budgets and scale require sparser key<br/>frames, the assumption of linearity fails and labels become inaccurate.<br/>To address this problem we have created a public framework for dividing<br/>the work of labeling video data into micro-tasks that can be completed<br/>by huge labor pools available through crowdsourced marketplaces. By<br/>extracting pixel-based features from manually labeled entities, we are<br/>able to leverage more sophisticated interpolation between key frames to<br/>maximize performance given a budget. Finally, by validating the power<br/>of our framework on dicult, real-world data sets we demonstrate an<br/>inherent trade-o between the mix of human and cloud computing used<br/>vs. the accuracy and cost of the labeling.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:138px;'>.</td><td  class='s3'>5:30<td  class='s4'>Peter Welinder<td  class='s4'>Caltech<td  class='s4'>The Multidimensional Wisdom of Crowds<td  class='s4'>Peter Welinder, Steve Branson, Serge Belongie, Pietro Perona<td  class='s4'>Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a model for the annotation process that can be used to estimate annotator skill, bias and data difficulty.  Both annotator skill and data difficulty are modeled as multidimensional quantities, which allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, and data that differ qualitatively. Focusing on binary image labels, we demonstrate that the model predicts ground truth labels on both synthetic and real data better than the current state of the art methods. Furthermore, we show that the model is able to discriminate between different groups of annotators based only on the labels they provide.<td  class='s5'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr><tr><td class=hd><p style='height:16px;'>.</td><td  class='s22'><td  class='s23'><td  class='s23'><td  class='s23'><td  class='s23'><td  class='s23'><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ><td ></tr></table></table></body></html>\n", "encoding": "utf-8"}