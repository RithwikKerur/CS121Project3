{"url": "http://vision.ics.uci.edu/sccv/2009/schedule.html", "content": "<html><head><meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"><meta name=\"lsq\" content=\"85\"><meta name=\"trixrows\" content=\"28\"><meta name=\"trixr1\" content=\"0\"><meta name=\"trixr2\" content=\"28\"><meta name=\"trixdiv\" content=\"20\"><meta name=\"trixcnt\" content=\"2\"><meta name=\"trixlast\" content=\"8\"><link rel=\"stylesheet\" href=\"speakers_files/3457500453-trix_main.css\" type=\"text/css\"><style>.tblGenFixed td {padding:0 3px;overflow:hidden;white-space:normal;letter-spacing:0;word-spacing:0;background-color:#fff;z-index:1;border-top:0px none;border-left:0px none;border-bottom:1px solid #CCC;border-right:1px solid #CCC;} .dn {display:none} .tblGenFixed td.s0 {background-color:#99ccff;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid #CCC;border-left:1px solid #CCC;} .tblGenFixed td.s2 {background-color:white;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s1 {background-color:#DDDDDD;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:bold;font-style:normal;color:#000000;text-decoration:none;text-align:center;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-top:1px solid #CCC;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s9 {background-color:#99ccff;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;border-left:1px solid #CCC;} .tblGenFixed td.s7 {background-color:#ffffff;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s8 {background-color:#ffffff;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s5 {background-color:#b3d580;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;text-decoration:none;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} .tblGenFixed td.s6 {background-color:#99ccff;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;border-left:1px solid #CCC;} .tblGenFixed td.s3 {background-color:#b3d580;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;border-left:1px solid #CCC;} .tblGenFixed td.s4 {background-color:#b3d580;font-family:arial,sans,sans-serif;font-size:100.0%;font-weight:normal;font-style:normal;color:#000000;text-decoration:none;text-align:left;vertical-align:bottom;white-space:normal;overflow:hidden;text-indent:0px;padding-left:3px;border-right:1px solid #CCC;border-bottom:1px solid #CCC;} </style></head><body style=\"border: 0px none ; margin: 0px;\"><table id=\"tblMain\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\"><tbody><tr><td><table class=\"tblGenFixed\" id=\"tblMain_0\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\"><tbody><tr class=\"rShim\"><td class=\"rShim\" style=\"width: 0pt;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 846px;\"></td><td class=\"rShim\" style=\"width: 820px;\"></td><td class=\"rShim\" style=\"width: 8513px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td><td class=\"rShim\" style=\"width: 120px;\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s0\"></td><td class=\"s1\">Name</td><td class=\"s1\">Affiliation</td><td class=\"s1\">Title</td><td class=\"s1\">Authors</td><td class=\"s1\">Abstract</td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td><td class=\"s2\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s3\">10:15-10:30</td><td class=\"s4\">Arrival and coffee</td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">10:30-10:55</td><td class=\"s7\">Piotr Dollar</td><td class=\"s7\">Caltech</td><td class=\"s7\">Evaluation of State-of-the-Art Pedestrian Detection</td><td class=\"s7\">P. Doll\u00e1r, C. Wojek, B. Schiele and P. Perona </td><td class=\"s7\">Pedestrian\ndetection is a key problem in computer vision, with several\napplications including robotics, surveillance and automotive safety. We\nintroduce a new, more realistic dataset two orders of magnitude larger\nthan existing datasets. The dataset contains richly annotated video,\nrecorded from a moving vehicle, with challenging images of low\nresolution, frequently occluded people. We propose improved evaluation\nmetrics, demonstrating that commonly used per-window measures are\nflawed and can fail to predict system performance on full images. We\nalso benchmark several promising detection systems, providing an\noverview of state-of-the-art performance and a direct, unbiased\ncomparison of existing methods. Finally, by analyzing common failure\ncases, we help identify future research directions for the field.</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">11:00-11:25</td><td class=\"s7\">Jan Prokaj</td><td class=\"s7\">USC</td><td class=\"s7\">3-D Model Based Vehicle Recognition<br></td><td class=\"s7\">Jan Prokaj and Gerard Medioni</td><td class=\"s7\">We\npresent a method for recognizing a vehicle\u2019s make and model in a video\nclip taken from an arbitrary viewpoint. This is an improvement over\nexisting methods which require a front view. In addition, we present a\nBayesian approach for establishing accurate correspondences in multiple\nview geometry. We take a model-based, top-down approach to classify\nvehicles. First, the vehicle pose is estimated in every frame by\ncalculating its 3-D motion on a plane using a structure from motion\nalgorithm. Then, exemplars from a database of 3-D models are rotated to\nthe same pose as the vehicle in the video, and projected to the image.\nFeatures in the model images and the vehicle image are matched, and a\nmodel matching score is computed. The model with the best score is\nidenti\ufb01ed as the model of the vehicle in the video. Results on real\nvideo sequences are presented.<br></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">11:30-11:55</td><td class=\"s7\">Hamed Pirsiavash</td><td class=\"s7\">UCI</td><td class=\"s7\">Bilinear classifiers for visual recognition</td><td class=\"s7\">Hamed Pirsiavash, Deva Ramanan, Charless Fowlkes</td><td class=\"s7\">We\ndescribe an algorithm for learning bilinear SVMs. Bilinear classifiers\nare a discriminative variant of bilinear models, which capture the\ndependence of data on multiple factors. Such models are particularly\nappropriate for visual data that is better represented as a matrix or\ntensor, rather than a vector. Matrix encodings allow for more natural\nregularization through rank restriction. For example, a rank-one\nscanning-window classifier yields a separable filter. Low-rank models\nhave fewer parameters and so are easier to regularize and faster to\nscore at run-time. We learn low-rank models with bilinear classifiers.\nWe also use bilinear classifiers for transfer learning by sharing\nlinear factors between different classification tasks. Bilinear\nclassifiers are trained with biconvex programs. Such programs are\noptimized with coordinate descent, where each coordinate step requires\nsolving a convex program - in our case, we use a standard off-the-shelf\nSVM solver. We demonstrate bilinear SVMs on difficult problems of\npeople detection in video sequences and action classification of video\nsequences, achieving state-of-the-art results in both.</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">12:00-12:25</td><td class=\"s7\">Larry Matthies</td><td class=\"s7\">JPL</td><td class=\"s7\">\nReal-time pedestrian detection and tracking for mobile robots</td><td class=\"s7\"></td><td class=\"s7\">Safe operation of mobile robots around people is a paramount concern, which has led DoD sponsors of mobile robot research to shift the focus of robot perception research from terrain-understanding for obstacle detection to classifying which potential obstacles are people. Unlike much research on pedestrian detection, which uses monocular imagery, in this application the availability of 3-D sensors is a given, so it makes most sense to incorporate 3-D perception in the detection and tracking process. I will present an update on our work in this area, which uses real-time stereo vision to create a \u201cpolar-perspective\u201d map, segments candidate blobs from this map, applies a classifier to image-based and 3-D features of these blobs, and tracks map blobs over time to suppress false alarms. I will also summarize recently-started extensions to this work to detect and track cars as well and to estimate the head pose of detected pedestrians as an aid to robot path planning. </td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s3\">12:30-1:30</td><td class=\"s4\">Lunch on 6th floor balcony</td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">1:30-1:55</td><td class=\"s7\">Oscar Beijbom</td><td class=\"s7\">UCSD</td><td class=\"s7\">Single image focus level assessment using SVM</td><td class=\"s7\">Oscar Beijbom</td><td class=\"s7\">Di\ufb00erential white blood cell count is the process of counting and classifying white blood cells in blood smears. It is one of the most common clinical tests which is performed in order to make diagnoses in conjunction with medical examinations.\nThese tests indicate deceases such as infections, allergies, and blood\ncancer and approximately 200-300 million are done yearly around the\nworld. Cellavision AB has developed machines that automate this work and is the global leader in this market. The method developed in this thesis will replace and improve the auto focus routine in these machines. It makes it possible to capture a focused image in only two steps instead of using an iterative multi step algorithm like those used today in most auto focus systems, including the one currently used at Cellavision. In the proposed method a Support Vector Machine, SVM, is trained to assess quantitatively, from a singel image, the level of defocus as well as the direction of defocus for that image. The SVM is trained on features that measure both the image contrast and the image content. High precision is made possible through extracting features from the di\ufb00erent parts of the image as well as from the image as a whole. This requires the image to be segmented and a method for doing this is proposed. Using this method 99.5% of the images in the test data\u2019s distances to focus were classi\ufb01ed less or equal to 5\u00b5m wrong while over 85% were classi\ufb01ed completely correctly. A 5\u00b5m defocus is borderline to what the human eye perceives as defocused.<br><br></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">2:00-2:25</td><td class=\"s7\">Chaitanya Desai</td><td class=\"s7\">UCI</td><td class=\"s7\">Discriminative models for multi-class object layout</td><td class=\"s7\">Chaitanya Desai, Deva Ramanan, Charless Fowlkes<br></td><td class=\"s7\">Many\nstate-of-the-art approaches for object recognition reduce the problem\nto a 0-1 classi\ufb01cation task. Such reductions allow one to leverage\nsophisticated classi\ufb01ers for learning. These models are typically\ntrained independently for each class using positive and negative\nexamples cropped from images. At test-time, various post-processing\nheuristics such as non-maxima suppression (NMS) are required to\nreconcile multiple detections within and between different classes for\neach image. Though crucial to good performance on benchmarks, this\npost-processing is usually de\ufb01ned heuristically. We introduce a uni\ufb01ed\nmodel for multi-class object recognition that casts the problem as a\nstructured prediction task. Rather than predicting a binary label for\neach image window independently, our model simultaneously predicts a\nstructured labeling of the entire image. Our model learns statistics\nthat capture the spatial arrangements of various object classes in real\nimages, both in terms of which arrangements to suppress through NMS and\nwhich arrangements to favor through spatial co-occurrence statistics.\nWe formulate parameter estimation in our model as a max-margin learning\nproblem. Given training images with ground-truth object locations, we\nshow how to formulate learning as a convex optimization problem. We\nemploy a cutting plane algorithm to ef\ufb01ciently learn a model from\nthousands of training images. We show state-of-the-art results on the\nPASCAL VOC benchmark that indicate the bene\ufb01ts of learning a global\nmodel encapsulating the spatial layout of multiple object classes.</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">2:30-2:55</td><td class=\"s7\">Baback Moghaddam</td><td class=\"s7\">JPL</td><td class=\"s7\">Low-Level Vision for Planetary Change Detection</td><td class=\"s7\"> </td><td class=\"s7\">\nI will present a prototype automatic vision system for planetary image change\ndetection. Applications include finding new craters and \"gullies\" on Mars from\ncurrent orbiting platforms, data-mining multi-mission legacy image databases\nfor undiscovered geologic phenomena, and automating the search for \"lost\"\nspacecraft.</td><td class=\"s8\"> </td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">3:00-3:25</td><td class=\"s7\">Hartmut Neven</td><td class=\"s7\">Google</td><td class=\"s7\">Which capabilities are missing when trying to design a comprehensive visual search engine?</td><td class=\"s7\"></td><td class=\"s7\">Computer vision has made significant advances during the last decade. Many capabilities such as the detection of faces or the recognition of rigid textured objects such as landmarks are now working to very satisfying levels. Across the various products and services offered by Google we are interested in analyzing an image crawled on the web in all its aspects. When designing such a comprehensive system it becomes obvious however that important abilities are still lacking. One example is object class recognition that scales to thousands or even millions of classes. Another area where we are still facing obstacles is the reliable recognition of objects that have little surface texture and which are largely contour defined. Even a seemingly simple task such as reading text in a photo is still lacking the accuracy we need. The talk describes our efforts in designing a large scale image recognition system that can analyze any given image on the web with respect to many dimensions. We report on the recognition disciplines in which we made good progress but more importantly call out areas which still require additional work to reach production ready solutions.\n</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s3\">3:30-4:00</td><td class=\"s4\">Coffee Break</td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">4:00-4:25</td><td class=\"s7\">Luis Goncalves</td><td class=\"s7\">Evolution Robotics Retail</td><td class=\"s7\">Computer Vision for Retail Fraud</td><td class=\"s7\">TBA</td><td class=\"s7\">A brief description of the work done at Evolution Robotics Retail, using Computer Vision to prevent retail fraud.</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">4:30-4:55</td><td class=\"s7\">Ricky Sethi</td><td class=\"s7\">UCR</td><td class=\"s7\">Activity Recognition Using a Physics-Based Data Driven Hamiltonian Monte Carlo</td><td class=\"s7\">Ricky J. Sethi<br>Amit K. Roy-Chowdhury<br>Brian E. Moore</td><td class=\"s7\">Motion\nand image analysis are both important for activity recognition in\nvideo. We present a new approach that extends the Hamiltonian Monte\nCarlo (HMC) to allow us to simultaneously search over the combined\nmotion and image space in a concerted manner using well-known Markov\nChain Monte Carlo (MCMC) techniques. For motion analysis in video, we\nuse tracks generated from the video to calculate the Hamiltonian\nequations of motion for the systems under study, thus utilizing\nanalytical Hamiltonian dynamics to derive a physically significant HMC\nalgorithm which can be used for activity analysis. We then use image\nanalysis to help explore both the motion energy space and the image\nspace by integrating the Hamiltonian energy-based approach with an\nimage-based data-driven proposal to drive the HMC, thereby yielding a\nData Driven HMC (DDHMC). We reduce the enormity of the search space by\ndriving the Hamiltonian dynamics-based MCMC with image data in this\nDDHMC. We also develop the reverse algorithm, which uses motion energy\nproposals to search the image space. Experimental validation of the\ntheory is provided on the well-known USF Gait and Weizmann datasets.\nWhile HMC has been used in other contexts, this is possibly the first\npaper that shows how it can be used for activity recognition in video\ntaking into account the image analysis results and using the physical\nmotion information of the system. In addition, the DDHMC framework has\npotential application to other domains where statistical sampling\ntechniques are useful, as we outline in the section on future work.</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">5:00-5:25</td><td class=\"s7\">Anup Doshi</td><td class=\"s7\">UCSD</td><td class=\"s7\">Vision-based Driver Attention and Behavior Inference</td><td class=\"s7\">Anup Doshi and Mohan M. Trivedi</td><td class=\"s7\">We\nintroduce a new approach to analyzing the attentive and behavioral\nstate of a human subject, given cameras focused on the subject and\ntheir environment. In particular, the task of analyzing the focus of\nattention of a human driver is of primary concern. Up to 80\\% of\ncrashes are related to driver inattention; thus it is important for an\nIntelligent Driver Assistance System (IDAS) to be aware of the driver\nstate. We present a new Bayesian paradigm for estimating human\nattention specifically addressing the problems arising in live driving\nsituations. We will then discuss several novel findings about driver\nbehavior and how those can affect the design of a vision-based HCI\ninterface for driver assistance. </td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s6\">5:30-5:55</td><td class=\"s7\">Pietro Perona</td><td class=\"s7\">Caltech</td><td class=\"s7\">Towards the visual analysis of animal behavior</td><td class=\"s7\">TBA</td><td class=\"s7\">TBA</td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td><td class=\"s8\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s3\">6:00-8:00</td><td class=\"s4\">Dinner</td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td><td class=\"s5\"></td></tr><tr><td class=\"hd\"><p style=\"height: 16px;\">.</p></td><td class=\"s9\"></td>\n</tbody></table></td></tr></tbody></table></body></html>\n", "encoding": "utf-8"}