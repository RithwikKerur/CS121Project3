{"url": "http://www-db.ics.uci.edu/pages/software/htree/README_relfeed", "content": "\n------------------------------------------------------------\nCopyright 1999, The University of California at Irvine\n------------------------------------------------------------\n\nPlease read the README file first which describes the hybrid tree code.\nThis file (called README_relfeed) describes the changes/additions you need\nto the hybrid code as described in README to run relevance feedback\non top of the hybrid tree (also see the papers [1], [2]).\n\nDESCRIPTION OF SOURCE CODE FILES\n================================\n\nThis distribution has 12 files.\n\n(1) It contains the 8 new source/header files which you would need \nin addition to the hybrid tree files described in README:\n\nCluster.h\nCluster.C \nqpm.h\nqex.h\nqpm.C\nqex.C\nnaive.C\nrefinement.C\n\n(2) It also contains 3 files which should replace the original\nhybrid tree files with the same names:\n\nHTree.h\nHTreeSearch.C\nmakefile\n\n(3) It also contains this README_relfeed file.\n\nCOMPILING THE CODE\n==================\n\nRun \"make\" (using the new makefile).\n\n\nRUNNING THE CODE\n================\n\nThe makefile will generate 3 executable programs:\n(1) HTreeTest\n(2) Refine_naive\n(3) Refine_FR\n\nHTreeTest is the same as the HTreeTest described in the original code.\nUse HTreeTest to construct the hybrid tree from a data file and dump\nit into a binary file. For example, to construct a  hybrid tree from the\nincluded data file (CH.16d.asc) and dump it into \"CH.dump\", \nrun \"HTreeTest 0 CH.16d.asc HTdump.color\". See README for detailed \ninstructions. You can also use HTreeTest to run ``no feedback'' queries\n(see README for detailed instructions).\n\nTo run feedback queries, we have 2 test programs:\n(1) naive.C\n(2) refinement.C\n\nThe executables corresponding to the above test files are\nRefine_naive and Refine_FR. For the same query and the same\nrefinement model, the answers generated by the \ntest programs are identical. (The difference lies\nin the speed of the techniques. The naive takes longer time\nwhile the FR (full reconstruction) approach takes much less time.\nThe reason is that the naive approach executes each\nfeedback query just like a initial query i.e. do the whole\nthing from scratch. On the other hand, FR reuses the work done \nduring previous feedback iterations to answer the query \nin subsequent feedback iterations. See [2] for details.)\nBoth test programs support multiple refinement models:\n(1) query expansion (2) query point movement (3) query reweighting\nand (4) both query point movement and query reweighting.\nSee [1] and [2] for details on these refinement models.\nTo run the naive approach, type in:\n\nRefine_naive CH.16d.Query  model CH.dump\nwhere model = 1 for query expansion \n            = 2 for query point movement \n            = 3 for query reweighting \n            = 4 for both query point movement and query reweighting\n\nYou can use any query file instead of CH.16d.Query (as long as it is \nin the same format as CH.16d.Query, see README for details on the \nformat) and whatever hybrid tree dump file in place of CH.dump.\n\nIMPORTANT: BEFORE YOU RUN THE ABOVE COMMAND, CREATE A SUBDIRECTORY CALLED\nresult UNDER YOUR CURRENT DIRECTORY. THE refine_naive PROGRAM WILL\nGENERATE SOME RESULT FILES INSIDE THE result DIRECTORY. THE PROGRAM\nWILL CRASH IF THERE IS NO result SUBDIRECTORY UNDER THE CURRENT DIRECTORY.\n\nThe above command will generate the following output:\nQuery Expansion\nNumber of empty nodes = 0 \nLoaded 1879 nodes from disk \nLoaded 68041 objects from disk \nStarting query\nQuery 0\nQuery 1\nQuery 2\nQuery 3\nQuery 4\nQuery 5\nQuery 6\nQuery 7\nQuery 8\nQuery 9\nQuery 10\nQuery 11\nQuery 12\nQuery 13\n.\n.\n.\nQuery 86\nQuery 87\nQuery 88\nQuery 89\nQuery 90\nQuery 91\nQuery 92\nQuery 93\nQuery 94\nQuery 95\nQuery 96\nQuery 97\nQuery 98\nQuery 99\n\nQuery file: CH.16d.Query\nData file: CH.dump\nQuery Expansion\nHistNaive\nComputing Precision-Recall Graph\n Iteration 0 recall = 0.420667  DiskAcc 317.05  CPU time 0.212335\n Iteration 1 recall = 0.660191  DiskAcc 322.91  CPU time 0.296015\n Iteration 2 recall = 0.714     DiskAcc 328.61  CPU time 0.362086\n Iteration 3 recall = 0.734667  DiskAcc 332.66  CPU time 0.441674\n Iteration 4 recall = 0.748952  DiskAcc 335.86  CPU time 0.515103\n Iteration 5 recall = 0.752952  DiskAcc 337.47  CPU time 0.515009\ndone\n\nIt will also generate a bunch of result files under the result directory.\nBefore we explain the above result and the result files, I will\nbriefly explain what the test program is doing. You can to go \nthrough naive.C for further details.\n\n\nFirst, we choose GROUND_TRUTH (set to 3) number of query points\nat a time (from the query file) to generate multipoint queries \n(see [2] for details on multipoint queries).\nFor each multipoint query, we construct a graded set of \nits RELEVANT (set to 50) neighbors (based on L1 distance)\ni.e. the top 10 answers have the highest grades,\nthe next 10 have slightly lower grades etc.\nWe refer to this set as the relevant set or ground truth of the\nmultipoint query. We construct the starting query by NOT choosing\nthe multipoint query we used to generate the ground truth\nbut a minor variation of it. We construct the starting \nquery by using only the first STARTING_POINTS (set to 1) number\nof points among the GROUND_TRUTH (set to 3) number of points.\nThe idea is to start with an approximation of the ideal\nquery (the one used to generate the ground truth) and \nkeep refining the query till we are close to the ground truth \n(the ideal set of answers). We request for RETRIEVED \n(set to 100) number of top answers to the starting query (iteration 0).\nWe refer to the set of answers returned as the retrieved set.\nWe obtain the refined query by taking the graded intersection \nof the retrieved and relevant sets i.e. \nif an object $O$ in the retrieved set is also present\nin relevant set, it is added to the new multipoint query\nwith its grade as in relevant set. See [1] and naive.C\nfor further details on the construction of the refined query.\nThe goal here is to refine the query such that the retrieved set \ngets as close as possible to relevant set over a small number \nof refinement iterations. \n\nIn the output above, the recall printed quantifies the \n``the graded intersection of the retrieved and relevant sets''\ni.e. higher the recall, the closer the retrieved set is\nto the relevance set (see the code for details).\nSo as we can see, the retrieved set is getting closer\nand closer to the relevant set in successive iterations\n(which means the refinement is working :)).\nThe program also prints the average number of (random) disk \naccesses performed the hybrid tree (as well as the CPU time) \nto return the top RETRIEVED (set to 100) for each iteration.\n\nThe program also outputs precision recall graphs, one for each iteration,\nin the result directory. \n\n\nTo run the full reconstruction approach, type in:\n\nRefine_FR CH.16d.Query  model CH.dump\nwhere model = 1 for query expansion \n            = 2 for query point movement \n            = 3 for query reweighting \n            = 4 for both query point movement and query reweighting\n\nIMPORTANT: BEFORE YOU RUN THE ABOVE COMMAND, CREATE A SUBDIRECTORY CALLED\nresult UNDER YOUR CURRENT DIRECTORY. \n\nThe above command will generate the following output:\nQuery Expansion\nNumber of empty nodes = 0 \nLoaded 1879 nodes from disk \nLoaded 68041 objects from disk \nStarting query\nQuery 0\nQuery 1\nQuery 2\nQuery 3\nQuery 4\nQuery 5\nQuery 6\nQuery 7\nQuery 8\nQuery 9\nQuery 10\nQuery 11\nQuery 12\nQuery 13\n.\n.\n.\nQuery 86\nQuery 87\nQuery 88\nQuery 89\nQuery 90\nQuery 91\nQuery 92\nQuery 93\nQuery 94\nQuery 95\nQuery 96\nQuery 97\nQuery 98\nQuery 99\n\nQuery file: CH.16d.Query\nData file: CH.dump\nQuery Expansion\nHist#\nComputing Precision-Recall Graph\n Iteration 0 recall = 0.420667  DiskAcc 317.05  CPU time 0.215482\n Iteration 1 recall = 0.660191  DiskAcc 45.78   CPU time 0.524043\n Iteration 2 recall = 0.714     DiskAcc 17.77   CPU time 0.612719\n Iteration 3 recall = 0.734667  DiskAcc 8.71    CPU time 0.735577\n Iteration 4 recall = 0.748952  DiskAcc 5.57    CPU time 0.793892\n Iteration 5 recall = 0.752952  DiskAcc 3.22    CPU time 0.819333\ndone\n\nNote that the only part of the output that is different from the \none generated by Refine_naive is the disk accesses and the CPU time.\nThe recall remains the same because the both approaches \ngenerate exactly the same retrieved set in each iteration\n(since we are using the same queries and same refinement model).\nThe disk accesses of the FR approach falls drastically with each \niteration i.e. you can perform the later iterations of feedback at almost\nzero cost. That is exactly the benefit of using the FR approach\ninstead of the naive approach. See [2] for more details.\n\nThe program, like the naive approach, will also generate \na precision recall graph for each iteration in the result directory. \nThese graphs are identical to the graph generated by\nthe naive approach (because the retrieved sets are the same).\n\n\nReferences\n==========\n\n[1] @article{\nauthor=\"K. Porkaew, K. Chakrabarti and S. Mehrotra\",\ntitle=\"Query Refinement for Multimedia Similarity Retrieval in MARS\",\njournal=\"1999 ACM Multimedia Conference\",\nmonth=\"November\",\nyear=\"1999\"\n}\n\n\n[2] @article{\nauthor=\"K. Chakrabarti, K.Porkaew, M. Ortega and S. Mehrotra\",\ntitle=\"Evaluating Refined Queries in Top-$k$ Retrieval Systems\",\njournal=\"Submitted for publication. Available online at http://www-db.ics.uci.edu/pages/publications/index.shtml#TR-MARS-00-05\",\nmonth=\"July\",\nyear=\"2000\"\n}\n\n\n===================================================================\nContact kaushik@ics.uci.edu in case of any problems.\n", "encoding": "ascii"}