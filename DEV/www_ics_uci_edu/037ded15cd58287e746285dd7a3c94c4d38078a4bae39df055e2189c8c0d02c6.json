{"url": "https://www.ics.uci.edu/~eppstein/280/point.html", "content": "<HTML><HEAD>\n<TITLE>Computational Statistics: Single Point Estimation</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Single Point Estimators</H2>\n\nThe simplest data model is one that just passes its parameters as data\nvalues, so that all the variation in the observed data is actually noise.\nWhat one wants then is some kind of average that removes the noise and\nreturns the original value.\n\n<P>For a nontrivial elaboration of this model, consider the problem of\nsubpixel-resolution imaging: if you take one digital image of a scene,\nyou're pretty much stuck with the pixels of the image itself\n(well, you can use Bayesian methods to get some subpixel resolution, but\nonly if you already have a good idea what you're looking at).\nBut if you take several images, you can imagine their pixelations as\nbeing different applications of some kind of noise to the real image,\nand average them to get a higher-resolution image.\n\n<P>A related problem is consensus sequence estimation in molecular biology.\nIn this problem, the data consists of a long sequence of characters\ndrawn from a small alphabet; the noise model forms observed DNA sequences\nfrom this prototype by sequences of mutations.\nAnother DNA single-point estimation problem is reconstruction of a whole\nsequence from fragments, as used in gene mapping systems.\n\n<P>Anyway, those examples have a very high dimension.\nI'm more interested in very low dimension cases where the parameter\nvalues really are geometric coordinates.\n\n<P>In one dimension, there are basically two choices for an estimator:\nthe mean and the median.  The median is obviously much less sensitive to\noutliers.  As far as I can tell, even for the most well-behaved error model\n(Gaussian noise), both the median and the mean have the same accuracy\n(distance of estimation from original value):\nO(s/sqrt(n)) where s is the variance of the Gaussian.\nFurther the median has better invariance properties (is distance-free).\nBut it is not hard to come up with noise models where the mean is the\nbetter choice of the two.  For instance, if you know nothing about the noise\nother than an a priori assumption that lower variances are more likely,\nthen the mean is the max likelihood estimator (it minimizes the variance\nof the differences between the observation and the estimate,\nas can easily seen by noting that the variance is a unimodular quadratic\nfunction of the estimate and by computing its derivative).\n\n<P>There are various ways of generalizing these ideas to higher dimensions:\n\n<H3>Centroid</H3>\n\nThis is the most natural generalization of the mean,\nand is found simply by computing the mean separately for each data coordinate.\nAn alternate way of defining it is the <I>least mean squares</I> estimate:\nthat is, it minimizes the mean (or equivalently sum)\nof the squared distances from the observation to the estimate.\nThis can be seen from the fact that in the sum of squared distances,\neach coordinate can be treated independently of each other coordinate,\nand we have already seen that the mean has this property in one dimension.\n\n<P>Therefore the centroid is the max likelihood estimator for settings\nwith unknown noise and an a priori assumption that lower variance is\nmore likely.  It also inherits the accuracy of the one-dimensional mean\nfor a Gaussian noise model (because Gaussian noise has the nice property\nthat it can be generated by treating each coordinate independently).\n\n<P>There is nothing interesting to say about algorithms for computing\ncentroids.\n\n<A NAME=\"FW\"><H3>Fermat-Weber Point</H3></A>\n\nMore generally, one can define an L<sub>p</sub> estimate to be one\nthat minimizes the sum of pth powers of distances of observations from\nthe estimate.  The centroid is the L<sub>2</sub> estimate;\nthe next most commonly used estimate is the <L<sub>1</sub> estimate,\nalso known as the Fermat-Weber point.\nIt generalizes the median (since the median minimizes the sum of\ndistances in one dimension, as can easily be seen by considering derivatives).\n\n<P>The uniqueness of\nany L<sub>p</sub> estimate for p&gt;1\nfollows since the pth power of distance from each observation is a\nstrictly convex function,\nthe sum of strictly convex functions is convex, and a convex function has a\nunique minimum.  Unfortunately this argument doesn't quite work for p=1:\nif all the points are colinear, and there is an even number of points,\nthen any point on a line segment between the two middle points\nhas an equal sum of distances.  But this is the only exception.\n\n<P>I'm not sure of a noise model for\nwhich this is optimal, but since it uses a lower power of distance\nit's less sensitive than the centroid to outliers.\nIt's also commonly used in management science for facility location e.g.\nof a central store location minimizing the travel distance to each of a\npopulation of customers.\n\n<P>There is no good combinatorial algorithm for the Fermat-Weber point,\nsince its exact location is the solution to a high-degree polynomial\nin the observation's coordinates\n[<A HREF=\"bib.html#B88\">B88</A>,<A HREF=\"bib.html#CM69\">CM69</A>].\nHowever, gradient descent methods\nconverge very rapidly and give good numerical approximations to its\nlocation [<A HREF=\"bib.html#W37\">W37</A>].\n\n<H3><A NAME=\"circ\">Circumcenter</A></H3>\n\nThe limit as p goes to infinity of the pth root of the sum of pth powers\nof distances is just the maximum of the distances.  And so, the limit of\nthe L<sub>p</sub> estimators is the L<sub>infinity</sub> estimator,\nwhich minimizes the maximum distance from the observation to any data\npoint.  If one draws a circle or sphere around the estimate, with radius\nequal to that distance, it contains all the data points, and no smaller\ncircle or sphere contains the same points, so this estimate is also\ncalled the circumcenter.\n\n<P>This is extremely sensitive to outliers and usually does not provide\na good estimator.  But one can imagine conditions under which it would\nbe the appropriate choice; e.g. if we know nothing about the noise other\nthan that each noisy observation is within some (unknown) bounded radius\nof the true estimate, then the circumcenter is the most accurate\nestimator (its maximum distance from the true data value is\nat most half the noise radius).\n\n<P>There exist several linear time algorithms for computing the circumcenter\nof a collection of data points, based on linear-programming type techniques\n(the problem is not itself a linear program, but can be solved with\nmuch the same algorithms&nbsp;[<A HREF=\"bib.html#G95\">G95</A>]).\nThe current best time bounds have the form\nO(d<sup>2</sup>&nbsp;n&nbsp;+&nbsp;f(d)&nbsp;log&nbsp;n) where f is some\nsubexponential function of the dimension and does not affect the main\nterm of the running time.\nBernd G&auml;rtner\nhas a good and well-documented C++ implementation available\n[<A HREF=\"bib.html#G99\">G99</A>]\nwhich can solve problems with hundreds of thousands of points,\nup to dimension 20 or 30.\n\n<A NAME=\"ctrpt\"><H3>Centerpoint</H3></A>\n\nAny point set has a <I>center point</I>,\nthat is, a point x such that any halfspace that doesn't contain x\ncan contain at most a constant fraction dn/(d+1) of the observations.\nEquivalently, any halfspace containing x contains at least n/(d+1)\nobservations.\nFor a proof, see [<A HREF=\"bib.html#ABET98\">ABET98</A>, section 2.5].\n\n<P>This provides a robust distance-free method of estimation:\nif at most n/(d+1)-1 of the observations are outliers\nchosen by an adversary to fool our algorithm, then its estimate\nmust be within the convex hull of the remaining points and is therefore\nnot strongly affected by those outliers.\n\n<P>Center points also have applications within computational geometry,\ne.g. in finding graph separators, which have further\napplications in mesh partitioning, intersection graph construction,\netc [<A HREF=\"bib.html#EMT95\">EMT95</A>].\n\n<P>For points in the plane, a centerpoint can be computed in linear\ntime, and for points in three dimensions the time is O(n polylog(n))\n[Citations to be filled in later].\nFor higher dimensions, the best known time bound is much slower:\nO(n<sup>d+1</sup>).  However, there exists a fast approximation\nalgorithm [Citation to be filled in later].\n\n<H3><A NAME=\"LMS\">Smallest subset methods<BR>\n(least median of squares)</A></H3>\n\nIf one takes as the primary aim robustness, one can do even\nbetter than the centerpoint. The maximum number of outliers one could hope\nto detect is floor((n-1)/2), because if more than half the observations could\nbe outliers, there wouldn't be any way in principle to distinguish the\noutlying half of the data from the unperturbed half.\nBut one can tolerate exactly this many outliers, if one uses a method\nthat finds the subset of ceiling((n+1)/2) points that minimizes\nany of various functionals: e.g. the circumradius, variance, or convex\nhull area.  The non-outlying observations form a small subset under each of\nthese functionals, and any other subset must include at least one\nnon-outlier and so can only be as small if it is also near the\nunperturbed data.\n\n<P>A good question for discussion: what functional is most appropriate\nto optimize (for what noise models)?\n\n<P>The version in which one minimizes the circumradius\nis also called the <I>least median of squares</I>\nbecause it minimizes the median (squared) distance of any observation\nfrom the estimate.\n\n<P>Various algorithms are known for these problems.\nMost such work has gone into variants where the cardinality of the\nselected subset is very small (in which case this is better viewed\nas a form of clustering than of robust estimation)\n[<A HREF=\"bib.html#EE94\">EE94</A>]\nor very large (only tolerant of a small number of outliers).\n\n<P>It is often possible to show that, for problems like this,\nthe optimal solution consists of the k nearest neighbors of some\nestimated center point.  For instance, in the least median of squares\nproblem, the optimal subset consists of the nearest neighbors\nto the optimal subset's circumcenter.  Similarly, the k-element subset\nminimizing the variance consists of the k points nearest\nthe optimal subset's median.  Obviously, we don't know this center\npoint in advance, or we wouldn't need to use an estimation algorithm.\nBut, one can use this idea to limit the number of subsets to examine.\nThe \"order-k Voronoi diagram\" essentially encapsulates the family of\npossible subsets: it is defined as a partition of space into cells,\nsuch that any two points in the same cell have the same set of k nearest\nneighbors.  In the plane, this diagram is known to have O(kn) cells,\nand can be constructed in time\nO(n&nbsp;log&nbsp;n&nbsp;+&nbsp;kn&nbsp;2<sup>O(log*&nbsp;n)</sup>)\n[<A HREF=\"bib.html#R99\">R99</A>].\nThe variance of any one cell can be updated from its neighbor in\nconstant time, so the minimum variance k-element subset can\nbe found in the same time bound -- nearly quadratic for the\nmost statistically relevant case, when k is roughly n/2.\nBy only computing these diagrams for certain subsets of points,\none can remove the n&nbsp;2<sup>O(log*&nbsp;n)</sup> factor\nwhen k is a little smaller than n [<A HREF=\"bib.html#EE94\">EE94</A>].\n\n<P>The same idea works for least median of squares, but is slower\nbecause it's more difficult to update the circumradius as one moves\nfrom cell to cell \n[<A HREF=\"bib.html#E92\">E92</A>].\nInstead, the fastest known algorithms\nuse parametric search (a complicated variant of binary search)\nbased on a decision algorithm which tests a given circle radius\nby finding the largest subset of points that fit within that radius.\nThe decision problem can be solved by drawing a circle of the given\nradius around each point, building the arrangement of these circles,\nand computing how many circles surround each cell\n(in constant time per cell by adding or subtracting one to the value\nfrom a neighboring cell).\nThe best time bounds (again, for k roughly n/2, with some improvement\npossible for smaller k)\nare O(n<sup>2</sup>&nbsp;log&nbsp;n) for d=2\nand O(n<sup>d</sup>&nbsp;log<sup>2</sup>n) for d&gt;2\n[<A HREF=\"bib.html#EE94\">EE94</A>].\n\n<A NAME=\"dist\"><H3>What distance function to use?</H3></A>\n\nAll this has assumed that the distance between two points should\nbe measured using the normal Euclidean distance,\nin which we square the difference in each coordinate,\nsum the squares, and take the square root of the sum.\nBut it seems that this assumption should be based on our choice of noise\nmodel.\n\n<P>If we assume a priori some centrally symmetric noise\ndistribution, or if we assume that the noise in each coordinate\nis an independent Gaussian (in which case the overall distribution ends\nup being centrally symmetric), then Euclidean distance makes sense.\nOtherwise, it seems that we should choose the distance function\nfrom p to q\nto be determined by the probability that a data point actually at p\ncould be perturbed so that we measure it at q.\nThere is no reason to expect such a distance function to obey\nthe axioms of a metric or in any other way be well behaved.\n\n<P>Perhaps the fact that they avoid having to choose a distance\nfunction is one of the things that makes distance-free methods\nsuch as the centerpoint attractive.\n\n<H2><A HREF=\"regress.html\">NEXT: Linear regression</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"point.html\" --></SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}