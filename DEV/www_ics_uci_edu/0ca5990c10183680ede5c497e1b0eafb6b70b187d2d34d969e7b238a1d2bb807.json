{"url": "https://www.ics.uci.edu/~dan/pubs/DC-Sec3.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 3</TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_3\">\n<H2> 3.  STATIC DEFINED-WORD SCHEMES</H2> </a>\n\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tThe classic defined-word scheme was developed over 30 years\nago in Huffman's well-known paper on minimum-redundancy coding \n[Huffman 1952].  Huffman's algorithm provided the first solution \nto the problem of constructing minimum-redundancy codes.  \nMany people believe that Huffman coding cannot be improved upon, that\nis, that it is guaranteed to achieve the best possible compression \nratio.  This is only\ntrue, however, under the constraints that each source message is\nmapped to a unique codeword and that the compressed text is the\nconcatenation of the codewords for the source messages.\nAn earlier\nalgorithm, due independently to Shannon and Fano [Shannon and Weaver\n1949; Fano 1949], is not guaranteed to provide optimal codes, but \napproaches optimal behavior as the number of messages approaches\ninfinity.\nThe Huffman algorithm is also of importance because it has provided\na foundation upon which other data compression techniques \nhave built and a benchmark to which they may be compared.  \nWe classify the codes generated by the Huffman and \nShannon-Fano algorithms as variable-variable\nand note that they include block-variable codes as a special case,\ndepending upon how the source messages are defined.\n<P>\nIn <a href=\"#Sec_3.3\">Section 3.3</a> codes which map the integers onto binary codewords\nare discussed.  Since any finite alphabet may be enumerated, this type\nof code has general-purpose utility.  However, a more common use of these\ncodes (called universal codes) is in conjunction with an adaptive \nscheme.  This connection is discussed in\n<a href=\"DC-Sec5.html#Sec_5.2\">Section 5.2</a>.\n<P>\nArithmetic coding, presented in\n<a href=\"#Sec_3.4\">Section 3.4</a>, takes a significantly \ndifferent approach to data compression from that of the other static\nmethods.  It does not construct a code, in the sense of a mapping from\nsource messages to codewords.  Instead, arithmetic coding replaces\nthe source ensemble by a code string which, unlike all of the other\ncodes discussed here, is not the concatenation of codewords \ncorresponding to individual source messages.  Arithmetic coding is\ncapable of achieving compression results which are arbitrarily close\nto the entropy of the source.\n\n<a name=\"Sec_3.1\">\n<H3> 3.1  Shannon-Fano Coding</H3> </a>\n\n\tThe Shannon-Fano technique has as an advantage its simplicity.\nThe code is constructed as follows:  the source messages <VAR>a</VAR>(<VAR>i</VAR>) and their\nprobabilities <VAR>p</VAR>( <VAR>a</VAR>(<VAR>i</VAR>) ) are listed in order of nonincreasing probability.\nThis list is then divided in such a way as to form two groups of as\nnearly equal total probabilities as possible.  Each message in the first\ngroup receives 0 as the first digit of its codeword; the messages\nin the second half have codewords beginning with 1.  Each of these\ngroups is then divided according to the same criterion and\nadditional code digits are appended.  The process is continued until\neach subset contains only one message.  Clearly the Shannon-Fano\nalgorithm yields a minimal prefix code.  \n\n<PRE>\n<VAR>a</VAR>    1/2     0\n<VAR>b</VAR>    1/4     10\n<VAR>c</VAR>    1/8     110\n<VAR>d</VAR>    1/16    1110\n<VAR>e</VAR>    1/32    11110\n<VAR>f</VAR>    1/32    11111\n\nFigure 3.1 -- A Shannon-Fano Code.\n</PRE>\n\n\tFigure 3.1 shows the application of the method to a particularly\nsimple probability distribution.  The length of each\ncodeword <VAR>x</VAR> is equal to -lg p(<VAR>x</VAR>).  This is true as long as it\nis possible to divide the list into subgroups of exactly equal\nprobability.  When this is not possible, some codewords may be\nof length -lg <VAR>p</VAR>(<VAR>x</VAR>)+1.  The Shannon-Fano algorithm yields\nan average codeword length <VAR>S</VAR> which satisfies  H &lt;= S &lt;= H + 1.\nIn Figure 3.2, the Shannon-Fano code for ensemble <VAR>EXAMPLE</VAR> is\ngiven.  As is often the case, the average codeword length is the\nsame as that achieved by the Huffman code (see Figure 1.3). \nThat the Shannon-Fano algorithm is not guaranteed to produce\nan optimal code is demonstrated by the following set of probabilities:\n{ .35, .17, .17, .16, .15 }.  The Shannon-Fano code for\nthis distribution is compared with the Huffman code in\n<a href=\"#Sec_3.2\">Section 3.2</a>.\n\n<PRE>\n<VAR>g</VAR>      8/40    00\n<VAR>f</VAR>      7/40    010\n<VAR>e</VAR>      6/40    011\n<VAR>d</VAR>      5/40    100\n<VAR>space</VAR>  5/40    101\n<VAR>c</VAR>      4/40    110\n<VAR>b</VAR>      3/40    1110\n<VAR>a</VAR>      2/40    1111\n\nFigure 3.2 -- A Shannon-Fano Code for <VAR>EXAMPLE</VAR>\n              (code length=117).\n</PRE>\n\n<a name=\"Sec_3.2\">\n<H3> 3.2.  Static Huffman Coding</H3> </a>\n\n\tHuffman's algorithm, expressed graphically, takes as input\na list of nonnegative weights {<VAR>w</VAR>(1), ... ,<VAR>w</VAR>(<VAR>n</VAR>) } and constructs\na full binary tree [a binary tree is full if every\nnode has either zero or two children]\nwhose leaves are labeled with \nthe weights.  When the Huffman algorithm is used to construct a code,\nthe weights represent the probabilities associated with\nthe source letters.  Initially there is a set of singleton trees, one for\neach weight in the list.  At each step in the algorithm the trees\ncorresponding to the two \nsmallest weights, <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>), are merged into a new tree whose\nweight is <VAR>w</VAR>(<VAR>i</VAR>)+<VAR>w</VAR>(<VAR>j</VAR>) and whose root has two children which\nare the subtrees represented \nby <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>).  The weights <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>) are removed from the\nlist and <VAR>w</VAR>(<VAR>i</VAR>)+<VAR>w</VAR>(<VAR>j</VAR>) is inserted into the list.  This process\ncontinues until the weight list contains a single value.  If, at\nany time, there is more than one way to choose a smallest pair\nof weights, any such pair may be chosen.  In Huffman's paper, \nthe process begins with a nonincreasing list of weights.  This\ndetail is not important to the correctness of the\nalgorithm, but it does provide a more efficient implementation [Huffman 1952].\nThe Huffman algorithm is demonstrated in Figure 3.3.\n<P>\n<IMG SRC=\"DC-fig33.gif\" ALT=\"[FIGURE 3.3]\">\n<P>\nFigure 3.3 -- The Huffman process: (a) The list; (b) the tree.\n<P>\n\tThe Huffman algorithm determines the lengths of the codewords\nto be mapped to each of the source letters <VAR>a</VAR>(<VAR>i</VAR>).  There are many\nalternatives for specifying the actual digits; it is necessary only\nthat the code have the prefix property.  The usual assignment\nentails labeling the edge from each parent to its left child with\nthe digit 0 and the edge to the right child with 1.  The codeword for\neach source letter is the sequence of labels along the path from\nthe root to the leaf node representing that letter.\nThe codewords for the source of Figure 3.3,\nin order of decreasing probability, are\n{01,11,001,100,101,000,0001}.\nClearly, this\nprocess yields a minimal prefix code.  Further, the algorithm is \nguaranteed to produce an <EM>optimal</EM> (minimum redundancy) code [Huffman 1952].  Gallager \nhas proved an upper bound on the redundancy of a Huffman code of\n<VAR>p</VAR>(<VAR>n</VAR>) + lg [(2 lg <VAR>e</VAR>)/<VAR>e</VAR>] which is approximately <VAR>p</VAR>(<VAR>n</VAR>) + 0.086,\nwhere <VAR>p</VAR>(<VAR>n</VAR>) is the probability of the least likely source message\n[Gallager 1978].  In a recent paper, Capocelli et al. provide new\nbounds which are tighter than those of Gallagher for some probability\ndistributions [Capocelli et al. 1986].  \nFigure 3.4 shows a distribution for which the\nHuffman code is optimal while the Shannon-Fano code is not.\n<P>\n\tIn addition to the fact that there are\nmany ways of forming codewords of appropriate lengths, there are\ncases in which the Huffman algorithm does not uniquely determine\nthese lengths due to the arbitrary choice among equal\nminimum weights.\nAs an example, codes with codeword lengths of {1,2,3,4,4}\nand of {2,2,2,3,3} both yield the same average codeword length\nfor a source with probabilities {.4,.2,.2,.1,.1}.\nSchwartz defines a variation of the Huffman algorithm which\nperforms \"bottom merging\"; that is, orders a new parent node\nabove existing nodes of the same weight and always merges the last\ntwo weights in the list.  The code constructed is the Huffman code\nwith minimum values of maximum codeword length (MAX{ <VAR>l</VAR>(<VAR>i</VAR>) }) and \ntotal codeword length (SUM{ <VAR>l</VAR>(<VAR>i</VAR>) }) [Schwartz 1964]. Schwartz\nand Kallick describe an implementation of Huffman's algorithm with\nbottom merging [Schwartz and Kallick 1964].  The Schwartz-Kallick\nalgorithm and a later algorithm by Connell [Connell 1973]\nuse Huffman's procedure to determine the lengths of the codewords,\nand actual digits are assigned so that the code has the <EM>numerical\nsequence property</EM>.  That is, codewords of equal length form a \nconsecutive sequence of binary numbers.\nShannon-Fano codes also have the numerical\nsequence property.  This property can be exploited to achieve a compact representation\nof the code and rapid encoding and decoding. \n\n<PRE>\n                        S-F      Huffman\n\n<VAR>a</VAR>(1)        0.35        00       1\n<VAR>a</VAR>(2)        0.17        01       011\n<VAR>a</VAR>(3)        0.17        10       010\n<VAR>a</VAR>(4)        0.16        110      001\n<VAR>a</VAR>(5)        0.15        111      000\n\nAverage codeword length 2.31     2.30\n\nFigure 3.4 -- Comparison of Shannon-Fano and Huffman Codes.\n</PRE>\n\n  Both the Huffman and the Shannon-Fano mappings can be \ngenerated in <VAR>O</VAR>(<VAR>n</VAR>) time,\nwhere <VAR>n</VAR> is the number of messages in the\nsource ensemble (assuming that the weights have been presorted).  \nEach of these algorithms maps a source \nmessage <VAR>a</VAR>(<VAR>i</VAR>) with probability <VAR>p</VAR> to a codeword of length <VAR>l</VAR> \n(-lg <VAR>p</VAR> &lt;= l &lt;= - lg <VAR>p</VAR> + 1).  Encoding and decoding times\ndepend upon the representation of the mapping.  If the\nmapping is stored as a binary tree, then decoding the codeword for\n<VAR>a</VAR>(<VAR>i</VAR>) involves following a path of length <VAR>l</VAR> in the tree.  \nA table indexed by the source messages could be used for\nencoding; the code for <VAR>a</VAR>(<VAR>i</VAR>) would be stored in position <VAR>i</VAR> of\nthe table and encoding time would be <VAR>O</VAR>(<VAR>l</VAR>).  \nConnell's algorithm makes use of the <EM>index</EM>\nof the Huffman code, a representation of the distribution of\ncodeword lengths, to encode and decode in <VAR>O</VAR>(<VAR>c</VAR>) time where <VAR>c</VAR> is the\nnumber of different codeword lengths.  Tanaka presents an implementation\nof Huffman coding based on finite-state machines which can be realized\nefficiently in either hardware or software [Tanaka 1987].\n<P>\n\tAs noted earlier, the \nredundancy bound for Shannon-Fano codes is 1 and the bound for \nthe Huffman method is <VAR>p</VAR>(<VAR>n</VAR> + 0.086 where <VAR>p</VAR>(<VAR>n</VAR>) is the probability of the \nleast likely source message (so <VAR>p</VAR>(<VAR>n</VAR>) is less than or equal to .5, \nand generally much less).  It is important to note that in defining \nredundancy to be average codeword length minus\nentropy, the cost of transmitting the code mapping computed by \nthese algorithms is ignored.   The overhead cost for any method\nwhere the source alphabet has not been established prior to \ntransmission includes <VAR>n</VAR> lg <VAR>n</VAR> bits for sending the <VAR>n</VAR> source\nletters.  For a Shannon-Fano code, a list of codewords\nordered so as to correspond to the source letters could be \ntransmitted.  The additional time required is then SUM <VAR>l</VAR>(<VAR>i</VAR>),\nwhere the <VAR>l</VAR>(<VAR>i</VAR>) are the lengths of\nthe codewords.  For Huffman coding, an encoding of the shape of\nthe code tree might be transmitted.  Since any full binary tree may \nbe a legal Huffman\ncode tree, encoding tree shape may require as many as lg 4^<VAR>n</VAR> = 2<VAR>n</VAR> bits.\nIn most cases the message ensemble is very large, so that the\nnumber of bits of overhead is minute by comparison to the total\nlength of the encoded transmission.  However, it is imprudent to ignore\nthis cost.\n<P>\n\tIf a less-than-optimal code is acceptable, the overhead costs\ncan be avoided through a prior agreement by sender and receiver as to\nthe code mapping.  Rather than using a Huffman code based upon the\ncharacteristics of the current message ensemble, the code\nused could be based on statistics for a class of\ntransmissions to which the current ensemble is assumed\nto belong.\nThat is, both sender and receiver could have access to\na <EM>codebook</EM> with <VAR>k</VAR> mappings in it; one for Pascal source, one for\nEnglish text, etc.  The sender would then simply alert the receiver\nas to which of the common codes he is using.  This requires only\nlg <VAR>k</VAR> bits of overhead.  Assuming that classes of transmission with\nrelatively stable characteristics could be identified, this hybrid approach\nwould greatly reduce the redundancy due to overhead without\nsignificantly increasing expected codeword length.\nIn addition, the cost of computing the mapping would be amortized \nover all files of a given class.  That is, the mapping would be \ncomputed once on a statistically significant sample and then \nused on a great number of files for which the sample is \nrepresentative.  There is clearly a substantial risk associated \nwith assumptions about file characteristics and great care would \nbe necessary in choosing both the sample from which the mapping \nis to be derived and the categories into which to partition \ntransmissions.  An extreme example of the risk associated \nwith the codebook approach is provided by author Ernest V. \nWright who wrote a novel <EM>Gadsby</EM> (1939) containing no \noccurrences of the letter E.  Since E is the most commonly used \nletter in the English language, an encoding based upon a sample \nfrom <EM>Gadsby</EM> would be disastrous if used with \"normal\"\nexamples of English text.  Similarly, the \"normal\" encoding\nwould provide poor compression of <EM>Gadsby</EM>.\n<P>\n\tMcIntyre and Pechura describe an experiment in which \nthe codebook approach is compared to static Huffman coding\n[McIntyre and Pechura 1985].  The sample used for comparison\nis a collection of 530 source programs in four languages.\nThe codebook contains a Pascal code tree, a FORTRAN code tree,\na COBOL code tree, a PL/1 code tree, and an ALL code tree.\nThe Pascal code tree is the result of applying the static Huffman\nalgorithm to the combined character \nfrequencies of all of the Pascal programs in the sample.\nThe ALL code tree is based upon the combined character frequencies\nfor all of the programs.  The experiment involves encoding each\nof the programs using the five codes in the codebook and the static \nHuffman algorithm.  The data reported for each of the 530 programs\nconsists of the size of the coded program for each of the five\npredetermined codes, and the size of the coded program plus the\nsize of the mapping (in table form) for the static Huffman method. \nIn every case, the code tree for the language class to which the \nprogram belongs generates the most compact encoding.  Although using\nthe Huffman algorithm on the program itself yields an optimal\nmapping, the overhead cost is greater than the added redundancy\nincurred by the less-than-optimal code.\nIn many cases, the ALL code tree also generates a more compact encoding\nthan the static Huffman algorithm.  In the worst case, an\nencoding constructed from the codebook is only 6.6% larger\nthan that constructed by the Huffman algorithm.  These results\nsuggest that, for files of source code, the codebook approach\nmay be appropriate.\n<P>\n\tGilbert discusses the construction of Huffman codes\nbased on inaccurate source probabilities [Gilbert 1971].\nA simple solution to the problem of incomplete knowledge of\nthe source is to avoid long codewords, thereby minimizing the\nerror of underestimating badly the probability of a message.\nThe problem becomes one of constructing the optimal binary\ntree subject to a height restriction (see [Knuth 1971; Hu and \nTan 1972; Garey 1974]).  Another approach involves collecting \nstatistics for several sources and then constructing a code based \nupon some combined criterion.  This approach could be applied to the\nproblem of designing a single code for use with English, French, German,\netc., sources.  To accomplish this, Huffman's algorithm could be used to\nminimize either the average codeword length for the combined source\nprobabilities; or the average codeword length for English,\nsubject to constraints on average codeword lengths\nfor the other sources. \n\n<a name=\"Sec_3.3\">\n<H3> 3.3  Universal Codes and Representations of the Integers</H3> </a>\n\nA code is <EM>universal</EM> if it maps source messages to codewords\nso that the resulting average codeword length is bounded by <VAR>c1</VAR>(<VAR>H</VAR> + <VAR>c2</VAR>).\nThat is, given an arbitrary source with nonzero entropy, a universal\ncode achieves average codeword length which is at most a constant times\nthe optimal possible for that source.  The potential compression offered\nby a universal code clearly depends on the magnitudes of the constants\n<VAR>c1</VAR> and <VAR>c2</VAR>.  We recall the definition of an asymptotically optimal\ncode as one for which average codeword length approaches entropy and remark\nthat a universal code with <VAR>c1</VAR>=1 is asymptotically optimal.\n<P>\nAn advantage of universal codes over Huffman codes\nis that it is not necessary to know the exact probabilities with\nwhich the source messages appear.  While Huffman coding is not\napplicable unless the probabilities are known, it is sufficient\nin the case of universal coding to know the probability distribution\nonly to the extent that the source messages can be ranked in probability\norder.  By mapping messages in order of decreasing\nprobability to codewords in order of increasing length, universality\ncan be achieved.  Another advantage to universal codes is that\nthe codeword sets are fixed.  It is not necessary to compute a codeword\nset based upon the statistics of an ensemble; any universal codeword\nset will suffice as long as the source messages are ranked.  The\nencoding and decoding processes are thus simplified. \nWhile universal codes can be used instead of\nHuffman codes as general-purpose static schemes, the more common \napplication is as an adjunct to a dynamic scheme.  This type of\napplication will be demonstrated in\n<a href=\"DC-Sec5.html#Sec_5\">Section 5</a>.\n<P>\nSince the ranking of source messages is the essential parameter\nin universal coding, we may think of a universal code as \nrepresenting an enumeration of the source messages, or as\nrepresenting the integers, which provide an enumeration.\nElias defines a sequence of universal coding schemes which map the\nset of positive\nintegers onto the set of binary codewords [Elias 1975].  \n\n<PRE>\n      <VAR>gamma</VAR>          <VAR>delta</VAR>\n\n1     1              1\n2     010            0100\n3     011            0101\n4     00100          01100\n5     00101          01101\n6     00110          01110\n7     00111          01111\n8     0001000        00100000\n16    000010000      001010000\n17    000010001      001010001\n32    00000100000    0011000000\n\nFigure 3.5 -- Elias Codes.\n</PRE>\n\n\tThe first Elias code is one which\nis simple but not optimal.  This code, <VAR>gamma</VAR>, maps an integer\n<VAR>x</VAR> onto the binary value of <VAR>x</VAR> prefaced by floor( lg <VAR>x</VAR>)\nzeros.  The binary value of <VAR>x</VAR> is expressed in as few bits as possible,\nand therefore begins with a 1, which serves to\ndelimit the prefix.  The result is an instantaneously decodable code \nsince the total length of a codeword is exactly one greater than \ntwice the number of zeros in the prefix; therefore, as soon as the first\n1 of a codeword is encountered, its length is known.  \nThe code is not a minimum redundancy code since \nthe ratio of expected codeword length to entropy goes to 2 as \nentropy approaches infinity.  The second code, <VAR>delta</VAR>, maps\nan integer <VAR>x</VAR> to a codeword consisting of\n<VAR>gamma</VAR> (floor[lg <VAR>x</VAR>] +1)\nfollowed by the binary value of <VAR>x</VAR> with the leading\n1 deleted.  The resulting codeword has length\nfloor[lg <VAR>x</VAR>] + 2 floor[lg (1 + floor[lg <VAR>x</VAR>] ) ] + 1.  This\nconcept can be applied recursively to shorten the codeword lengths,\nbut the benefits decrease rapidly.  The code <VAR>delta</VAR> is asymptotically\noptimal since the limit of the ratio of expected codeword length to entropy\nis 1.  Figure 3.5 lists the values of <VAR>gamma</VAR> and <VAR>delta</VAR> for a\nsampling of the integers.  Figure 3.6 shows an Elias code for string\n<VAR>EXAMPLE</VAR>.  The number of bits transmitted using this mapping would\nbe 161, which does not compare well with the 117 bits transmitted by\nthe Huffman code of Figure 1.3.  Huffman coding is optimal under the static\nmapping model.  Even an asymptotically optimal universal code cannot\ncompare with static Huffman coding on a source for which the \nprobabilities of the messages are known.\n\n<PRE>\nSource   Frequency   Rank       Codeword\nmessage\n\n<VAR>g</VAR>           8          1        <VAR>delta</VAR>(1)=1\n<VAR>f</VAR>           7          2        <VAR>delta</VAR>(2)=0100\n<VAR>e</VAR>           6          3        <VAR>delta</VAR>(3)=0101\n<VAR>d</VAR>           5          4        <VAR>delta</VAR>(4)=01100\n<VAR>space</VAR>       5          5        <VAR>delta</VAR>(5)=01101\n<VAR>c</VAR>           4          6        <VAR>delta</VAR>(6)=01110\n<VAR>b</VAR>           3          7        <VAR>delta</VAR>(7)=01111\n<VAR>a</VAR>           2          8        <VAR>delta</VAR>(8)=00100000\n\nFigure 3.6 -- An Elias Code for <VAR>EXAMPLE</VAR> (code length=161).\n</PRE>\n\nA second sequence of universal coding schemes, based on the Fibonacci\nnumbers, is defined by  Apostolico and Fraenkel [Apostolico and \nFraenkel 1985].   While the Fibonacci codes are not asymptotically\noptimal, they compare well to the Elias codes as long as the number\nof source messages is not too large.  Fibonacci codes have the\nadditional attribute of robustness, which manifests itself by the\nlocal containment of errors.  This aspect of Fibonacci codes will\nbe discussed further in\n<a href=\"DC-Sec678.html#Sec_7\">Section 7</a>.\n<P>\nThe sequence of Fibonacci codes described by Apostolico and Fraenkel\nis based on the Fibonacci numbers of order <VAR>m</VAR> &gt;= 2, where the\nFibonacci numbers of order 2 are the standard Fibonacci numbers:\n1, 1, 2, 3, 5, 8, 13, ....  In general, the Fibonnaci numbers\nof order <VAR>m</VAR> are defined by the recurrence:  Fibonacci numbers <VAR>F</VAR>(-<VAR>m</VAR>+1)\nthrough <VAR>F</VAR>(0) are equal to 1; the <VAR>k</VAR>th number for <VAR>k</VAR> &gt;= 1 is \nthe sum of the preceding <VAR>m</VAR> numbers.  We describe only the order \n2 Fibonacci code; the extension to higher orders is straightforward.\n\n<PRE>\n<VAR>N</VAR>             <VAR>R</VAR>(<VAR>N</VAR>)               <VAR>F</VAR>(<VAR>N</VAR>)\n\n 1                           1   11\n 2                       1   0   011\n 3                   1   0   0   0011\n 4                   1   0   1   1011\n 5               1   0   0   0   00011\n 6               1   0   0   1   10011\n 7               1   0   1   0   01011\n 8           1   0   0   0   0   000011\n16       1   0   0   1   0   0   0010011\n32   1   0   1   0   1   0   0   00101011\n\n    21  13   8   5   3   2   1\n\nFigure 3.7 -- Fibonacci Representations and Fibonacci Codes.\n</PRE>\n\nEvery nonnegative integer <VAR>N</VAR> has precisely one binary representation\nof the form <VAR>R</VAR>(<VAR>N</VAR>) = SUM(<VAR>i</VAR>=0 to <VAR>k</VAR>) <VAR>d</VAR>(<VAR>i</VAR>) <VAR>F</VAR>(<VAR>i</VAR>) (where <VAR>d</VAR>(<VAR>i</VAR>) is in {0,1},\n<VAR>k</VAR> &lt;= <VAR>N</VAR>, and the <VAR>F</VAR>(<VAR>i</VAR>)\nare the order 2 Fibonacci numbers as \ndefined above) such that there are no adjacent ones in the representation.\nThe Fibonacci representations for a small sampling of the integers\nare shown in Figure 3.7, using the standard bit sequence, from high\norder to low.  The bottom row of the figure gives the values of the\nbit positions.  It is immediately obvious that this Fibonacci \nrepresentation does not constitute a prefix code.  The order 2 \nFibonacci code for <VAR>N</VAR> is defined to be:\n<VAR>F</VAR>(<VAR>N</VAR>)=<VAR>D</VAR>1 where <VAR>D</VAR>=d(0)d(1)d(2)...d(<VAR>k</VAR>)\n(the <VAR>d</VAR>(<VAR>i</VAR>) defined above).  That is, the Fibonacci representation is\nreversed and 1 is appended.  The Fibonacci code values for a small\nsubset of the integers are given in Figure 3.7.  These binary\ncodewords form a prefix code since every codeword now terminates\nin two consecutive ones, which cannot appear anywhere else in a\ncodeword.  \n<P>\nFraenkel and Klein prove that the Fibonacci code of order 2 is universal,\nwith <VAR>c1</VAR>=2 and <VAR>c2</VAR>=3 [Fraenkel and Klein 1985].  It is not asymptotically\noptimal since <VAR>c1</VAR>&gt;1.  Fraenkel and Klein also show that Fibonacci\ncodes of higher order compress better than the order 2 code if the\nsource language is large enough (i.e., the number of distinct source messages\nis large) and the probability distribution is nearly uniform.\nHowever, no Fibonacci code is asymptotically optimal.  The Elias \ncodeword <VAR>delta</VAR>(<VAR>N</VAR>) is asymptotically shorter than any Fibonacci\ncodeword for <VAR>N</VAR>, but the integers in a very large initial range have\nshorter Fibonacci codewords.  For <VAR>m</VAR>=2, for example, the \ntransition point is <VAR>N</VAR>=514,228 [Apostolico and Fraenkel 1985].  \nThus, a Fibonacci code provides better compression\nthan the Elias code until the size of the source language becomes very\nlarge.  Figure 3.8 shows a Fibonacci code for string <VAR>EXAMPLE</VAR>.\nThe number of bits transmitted using this mapping would be 153, which\nis an improvement over the Elias code of Figure 3.6 but still \ncompares poorly with the Huffman code of Figure 1.3.\n\n<PRE>\nSource   Frequency   Rank       Codeword\nmessage\n\n<VAR>g</VAR>           8          1        <VAR>F</VAR>(1)=11\n<VAR>f</VAR>           7          2        <VAR>F</VAR>(2)=011\n<VAR>e</VAR>           6          3        <VAR>F</VAR>(3)=0011\n<VAR>d</VAR>           5          4        <VAR>F</VAR>(4)=1011\n<VAR>space</VAR>       5          5        <VAR>F</VAR>(5)=00011\n<VAR>c</VAR>           4          6        <VAR>F</VAR>(6)=10011\n<VAR>b</VAR>           3          7        <VAR>F</VAR>(7)=01011\n<VAR>a</VAR>           2          8        <VAR>F</VAR>(8)=000011\n\nFigure 3.8 -- A Fibonacci Code for <VAR>EXAMPLE</VAR> (code length=153).\n</PRE>\n\n<a name=\"Sec_3.4\">\n<H3> 3.4 Arithmetic Coding</H3> </a>\n\nThe method of arithmetic coding was suggested by Elias, and presented\nby Abramson in his text on Information Theory [Abramson 1963].\nImplementations of Elias' technique were developed by Rissanen, Pasco,\nRubin, and, most recently, Witten et al. [Rissanen 1976; Pasco 1976;\nRubin 1979; Witten et al. 1987].  We present the concept of arithmetic\ncoding first and follow with a discussion of implementation details \nand performance.\n<P>\nIn arithmetic coding a source ensemble is represented by an interval\nbetween 0 and 1 on the real number line.  Each symbol of the ensemble\nnarrows this interval.  As the interval becomes smaller, the number of\nbits needed to specify it grows.  Arithmetic coding assumes an explicit\nprobabilistic model of the source.  It is a defined-word scheme which\nuses the probabilities of the source messages to successively narrow\nthe interval used to represent the ensemble.  A high probability\nmessage narrows the interval less than a low probability message, so\nthat high probability messages contribute fewer bits to the coded ensemble.\nThe method begins with an unordered list of source messages and their\nprobabilities.  The number line is partitioned into subintervals based on\ncumulative probabilities.  \n<P>\nA small example will be used to illustrate the idea of arithmetic coding.\nGiven source messages {<VAR>A,B,C,D,#</VAR>} with probabilities \n{.2, .4, .1, .2, .1}, Figure 3.9 demonstrates the initial partitioning\nof the number line.  The symbol <VAR>A</VAR> corresponds to the first 1/5 of\nthe interval [0,1); <VAR>B</VAR> the next 2/5; <VAR>D</VAR> the subinterval of size\n1/5 which begins 70% of the way from the left endpoint to the right.\nWhen encoding begins, the source ensemble is represented by the entire \ninterval [0,1).  For the ensemble <VAR>AADB#</VAR>, the first <VAR>A</VAR> reduces the\ninterval to [0,.2) and the second <VAR>A</VAR> to [0,.04) (the first 1/5\nof the previous interval).  The <VAR>D</VAR> further narrows the interval to\n[.028,.036) (1/5 of the previous size, beginning 70% of the distance\nfrom left to right).  The <VAR>B</VAR> narrows the interval to [.0296,.0328), \nand the <VAR>#</VAR> yields a final interval of [.03248,.0328).  The interval,\nor alternatively any number <VAR>i</VAR> within the interval, may now be used to\nrepresent the source ensemble.\n\n<PRE>\nSource    Probability   Cumulative    Range\nmessage                 probability\n\n   <VAR>A</VAR>            .2          .2        [0,.2)\n   <VAR>B</VAR>            .4          .6        [.2,.6)\n   <VAR>C</VAR>            .1          .7        [.6,.7)\n   <VAR>D</VAR>            .2          .9        [.7,.9)\n   <VAR>#</VAR>            .1         1.0        [.9,1.0)\n\nFigure 3.9 -- The Arithmetic coding model.\n</PRE>\n\nTwo equations may be used to define the narrowing process described above: <BR>\n<PRE>\n   newleft = prevleft + msgleft*prevsize      (1)\n   newsize = prevsize * msgsize               (2)\n</PRE>\n\nThe first equation states that the left endpoint of the new interval is\ncalculated from the previous interval and the current source message.\nThe left endpoint of the range associated with the current message\nspecifies what percent of the previous interval to remove from the left\nin order to form the new interval.  For <VAR>D</VAR> in the above example, the new left\nendpoint is moved over by .7 * .04 (70% of the size of the previous\ninterval).  The second equation computes the\nsize of the new interval from the previous interval size and the probability\nof the current message (which is equivalent to the size of its associated\nrange).  Thus, the size of the interval determined by <VAR>D</VAR> is .04*.2, and the\nright endpoint is .028+.008=.036 (left endpoint + size).\n<P>\nThe size of the final subinterval determines the number of bits needed to\nspecify a number in that range.  The number of bits needed to specify a\nsubinterval of [0,1) of size <VAR>s</VAR> is -lg <VAR>s</VAR>.  Since the size of the final\nsubinterval is the product of the probabilities of the source messages\nin the ensemble (that is, <VAR>s</VAR>=PROD{<VAR>i</VAR>=1 to <VAR>N</VAR>} <VAR>p</VAR>(source message <VAR>i</VAR>) where \n<VAR>N</VAR> is the length of the ensemble), we have \n-lg <VAR>s</VAR> = -SUM{<VAR>i</VAR>=1 to <VAR>N</VAR> lg <VAR>p</VAR>(source message <VAR>i</VAR>) =\n- SUM{<VAR>i</VAR>=1 to <VAR>n</VAR>} <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)), where <VAR>n</VAR> is the number of unique\nsource messages <VAR>a</VAR>(1), <VAR>a</VAR>(2), ... <VAR>a</VAR>(<VAR>n</VAR>).\nThus, the number of bits generated\nby the arithmetic coding technique is exactly equal to entropy, <VAR>H</VAR>.\nThis demonstrates the fact that arithmetic coding achieves compression\nwhich is almost exactly that predicted by the entropy of the source.\n<P>\nIn order to recover the original ensemble, the decoder must\nknow the model of the source used by the encoder (eg., the source messages\nand associated ranges) and a single number within the interval determined\nby the encoder.  Decoding consists of a series of comparisons of the number\n<VAR>i</VAR> to the ranges representing the source messages.  For this example,\n<VAR>i</VAR> might be .0325 (.03248, .0326, or .0327 would all do just as well).\nThe decoder uses <VAR>i</VAR> to simulate the actions of the encoder.  Since <VAR>i</VAR> lies \nbetween 0 and .2, he deduces that the first letter was <VAR>A</VAR> (since the range\n[0,.2) corresponds to source message <VAR>A</VAR>).  This narrows the interval\nto [0,.2).  The decoder can now deduce that the next message will further\nnarrow the interval in one of the following ways:  to [0,.04) for <VAR>A</VAR>,\nto [.04,.12) for <VAR>B</VAR>, to [.12,.14) for <VAR>C</VAR>, to [.14,.18) for <VAR>D</VAR>,\nand to [.18,.2) for <VAR>#</VAR>.  Since <VAR>i</VAR> falls into the interval [0,.04),\nhe knows that the second message is again <VAR>A</VAR>.  This process continues until\nthe entire ensemble has been recovered.\n<P>\nSeveral difficulties become evident when implementation of arithmetic coding\nis attempted.  The first is that the decoder needs some way of knowing\nwhen to stop.  As evidence of this, the number 0 could represent any of\nthe source ensembles <VAR>A, AA, AAA,</VAR> etc.  Two solutions to this problem\nhave been suggested.  One is that the encoder transmit the size of the\nensemble as part of the description of the model.  Another is that\na special symbol be included in the model for the purpose of signaling\nend of message.  The <VAR>#</VAR> in the above example serves this purpose.\nThe second alternative is preferable for several reasons.  First, sending\nthe size of the ensemble requires a two-pass process and precludes the use\nof arithmetic coding as part of a hybrid codebook scheme (see\n<a href=\"DC-Sec1.html#Sec_1.2\">Sections 1.2</a> and <a href=\"#Sec_3.2\">3.2</a>).\nSecondly, adaptive methods of arithmetic coding are easily\ndeveloped and a first pass to determine ensemble size is inappropriate in\nan on-line adaptive scheme.\n<P>\nA second issue left unresolved by the fundamental concept of arithmetic\ncoding is that of incremental transmission and reception.  It appears\nfrom the above discussion that the encoding algorithm transmits nothing\nuntil the final interval is determined.  However, this delay is not\nnecessary.  As the interval narrows, the leading bits  of the left\nand right endpoints become the same.  Any leading bits that are the same may be\ntransmitted immediately, as they will not be affected by further narrowing.\nA third issue is that of precision.  From the description of arithmetic\ncoding it appears that the precision required grows without bound as the\nlength of the ensemble grows.  Witten et al. and Rubin address this issue\n[Witten et al. 1987; Rubin 1979].  Fixed precision registers may be used\nas long as underflow and overflow are detected and managed.  The degree\nof compression achieved by an implementation of arithmetic coding is\nnot exactly <VAR>H</VAR>, as implied by the concept of arithmetic coding.  Both the use\nof a message terminator and the use of fixed-length arithmetic reduce\ncoding effectiveness.  However, it is clear that an end-of-message symbol\nwill not have a significant effect on a large source ensemble.\nWitten et al. approximate the overhead due to the use of fixed\nprecision at 10^-4 bits per source message, which is also negligible.\n<P>\nThe arithmetic coding model for ensemble <VAR>EXAMPLE</VAR> is given in\nFigure 3.10.  The final interval size is\n<VAR>p</VAR>(a)^2*<VAR>p</VAR>(b)^3*<VAR>p</VAR>(c)^4*<VAR>p</VAR>(d)^5*<VAR>p</VAR>(e)^6*<VAR>p</VAR>(f)^7*<VAR>p</VAR>(g)^8*<VAR>p</VAR>(<VAR>space</VAR>)^5.\nThe number of bits needed to specify a value in the interval \nis -lg(1.44*10^-35)=115.7.  So excluding overhead, arithmetic coding\ntransmits <VAR>EXAMPLE</VAR> in 116 bits, one less bit than static Huffman coding.\n\n<PRE>\nSource    Probability   Cumulative    Range\nmessage                 probability\n\n   <VAR>a</VAR>            .05         .05       [0,.05)\n   <VAR>b</VAR>            .075        .125      [.05,.125)\n   <VAR>c</VAR>            .1          .225      [.125,.225)\n   <VAR>d</VAR>            .125        .35       [.225,.35)\n   <VAR>e</VAR>            .15         .5        [.35,.5)\n   <VAR>f</VAR>            .175        .675      [.5,.675)\n   <VAR>g</VAR>            .2          .875      [.675,.875)\n   <VAR>space</VAR>        .125       1.0        [.875,1.0)\n\nFigure 3.10 -- The Arithmetic coding model of <VAR>EXAMPLE</VAR>.\n</PRE>\n\nWitten et al. provide an implementation of arithmetic coding, written in C,\nwhich separates the model of the source from the coding process (where the\ncoding process is defined by Equations 3.1 and 3.2) [Witten et al. 1987].\nThe model is in a separate program module and is consulted by the encoder\nand by the decoder at every step in the processing.  The fact that the\nmodel can be separated so easily renders the classification static/adaptive\nirrelevent for this technique.  Indeed, the fact that the coding method\nprovides compression efficiency nearly equal to the entropy of the source\nunder any model allows arithmetic coding to be coupled with any static or\nadaptive method for computing the probabilities (or frequencies) of the\nsource messages.  Witten et al. implement an adaptive model similar to the\ntechniques described in <a href=\"DC-Sec4.html#Sec_4\">Section 4</a>.  The performance\nof this implementation is discussed in <a href=\"DC-Sec678.html#Sec_6\">Section 6</a>.\n\n<P>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}