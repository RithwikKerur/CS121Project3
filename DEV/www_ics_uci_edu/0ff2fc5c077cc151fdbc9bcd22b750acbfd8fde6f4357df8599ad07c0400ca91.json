{"url": "https://www.ics.uci.edu/~theory/269/190524b.html", "content": "<!DOCTYPE html>\n<html>\n<head>\n<title>Theory Seminar, May 24, 2019</title>\n<link rel=\"stylesheet\" href=\"../stylesheet.css\" type=\"text/css\">\n<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n  tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n</head>\n<body>\n<a href=\"/~theory/\"><img src=\"http://www.ics.uci.edu/~theory/logo/CATOC2.jpg\"\nalt=\"Center for Algorithms and Theory of Computation\"></a>\n<h2><a href=\"/~theory/269/\">CS 269S, Spring 2019: Theory Seminar</a><br>\nBren Hall, Room 1423, 1pm\n</h2>\n<hr />\n<h2>May 24, 2019:</h2>\n<h1>\nOn the capacity of FeedForward Neural Networks\n</h1>\n<h2>\nCeasar Aguma\n</h2>\n\n<p>\nWhile Artificial Neural Networks have become a quite popular problem-solving tool,\nmuch of their theory is unknown or ambiguous. There has been no clear framework\nfor quantifying the theoretical capabilities of Neural Networks. \nThis paper by Baldi and Versynin provides such a framework for theoretically\nanalyzing the capacity of FeedForward Neural Networks.\nMy talk will summarize the core result presented in the paper, that it, a\ndefinition of \"the capacity of an architecture by the binary logarithm of the\nnumber of functions it can compute, as the synaptic weights are varied.\" While the\ntopic of Artificial Neural Networks is synonymous with Deep Learning,\nI hope this talk can justify the need to look at Artificial Neural Networks from a\ntheoretical perspective.\n\n<p>\nPaper by Pierre Baldi and Roman Vershynin\n\n</body></html>\n", "encoding": "ascii"}