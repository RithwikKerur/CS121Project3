{"url": "https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/paper1", "content": ".pn 0\n.ls1\n.EQ\ndelim $$\n.EN\n.ev1\n.ps-2\n.vs-2\n.ev\n\\&\n.sp 5\n.ps+4\n.ce\nARITHMETIC CODING FOR DATA COMPRESSION\n.ps-4\n.sp4\n.ce\nIan H. Witten, Radford M. Neal, and John G. Cleary\n.sp2\n.ce4\nDepartment of Computer Science\nThe University of Calgary\n2500 University Drive NW\nCalgary, Canada T2N 1N4\n.sp2\n.ce\nAugust, 1986, revised January 1987\n.sp 8\n.in+1i\n.ll-1i\nThe state of the art in data compression is arithmetic coding, not\nthe better-known Huffman method.\nArithmetic coding gives greater compression, is faster for adaptive models,\nand clearly separates the model from the channel encoding.\nThis paper presents a practical implementation of the technique.\n.sp 3\n.in +0.5i\n.ti -0.5i\n\\fICR Categories and subject descriptors:\\fR\n.br\nE.4 [DATA] Coding and Information Theory \\(em Data Compaction and Compression\n.br\nH.1.1 [Models and Principles] Systems and Information Theory \\(em Information Theory\n.sp\n.ti -0.5i\n\\fIGeneral terms:\\fR  algorithms, performance\n.sp\n.ti -0.5i\n\\fIAdditional key words and phrases:\\fR  arithmetic coding, Huffman coding, adaptive modeling\n.ll+1i\n.in 0\n.bp\n.sh \"Introduction\"\n.pp\nArithmetic coding is superior in most respects to the better-known Huffman\n(1952) method.\n.[\nHuffman 1952 method construction minimum-redundancy codes\n.]\nIt represents information at least as compactly, sometimes considerably more\nso.\nIts performance is optimal without the need for blocking of input data.\nIt encourages a clear separation between the model for representing data and\nthe encoding of information with respect to that model.\nIt accommodates adaptive models easily.\nIt is computationally efficient.\nYet many authors and practitioners seem unaware of the technique.\nIndeed there is a widespread belief that Huffman coding cannot be improved\nupon.\n.pp\nThis paper aims to rectify the situation by presenting an accessible\nimplementation of arithmetic coding, and detailing its performance\ncharacteristics.\nThe next section briefly reviews basic concepts of data compression and\nintroduces the model-based approach which underlies most modern techniques.\nWe then outline the idea of arithmetic coding using a simple example.\nFollowing that programs are presented for both encoding and decoding, in which\nthe model occupies a separate module so that different ones can easily be\nused.\nNext we discuss the construction of fixed and adaptive models.\nThe subsequent section details the compression efficiency and execution time\nof the programs, including the effect of different arithmetic word lengths on\ncompression efficiency.\nFinally, we outline a few applications where arithmetic coding is appropriate.\n.sh \"Data compression\"\n.pp\nTo many, data compression conjures up an assortment of \\fIad hoc\\fR\ntechniques such as converting spaces in text to tabs, creating special codes\nfor common words, or run-length coding of picture data (eg see Held, 1984).\n.[\nHeld 1984 data compression techniques applications\n.]\nThis contrasts with the more modern model-based paradigm for\ncoding, where from an \\fIinput string\\fR of symbols and a \\fImodel\\fR, an\n\\fIencoded string\\fR is produced which is (usually) a compressed version of\nthe input.\nThe decoder, which must have access to the same model,\nregenerates the exact input string from the encoded string.\nInput symbols are drawn from some well-defined set such as the ASCII or\nbinary alphabets;\nthe encoded string is a plain sequence of bits.\nThe model is a way of calculating, in any given context, the distribution of\nprobabilities for the next input symbol.\nIt must be possible for the decoder to produce exactly the same probability\ndistribution in the same context.\nCompression is achieved by transmitting the more probable symbols in fewer\nbits than the less probable ones.\n.pp\nFor example, the model may assign a predetermined probability to each symbol\nin the ASCII alphabet.\nNo context is involved.\nThese probabilities may be determined by counting frequencies in\nrepresentative samples of text to be transmitted.\nSuch a \\fIfixed\\fR model is communicated in advance to both encoder and\ndecoder, after which it is used for many messages.\n.pp\nAlternatively, the probabilities the model assigns may change as each symbol\nis transmitted, based on the symbol frequencies seen \\fIso far\\fR in this\nmessage.\nThis is an \\fIadaptive\\fR model.\nThere is no need for a representative sample of text, because each message\nis treated as an independent unit, starting from scratch.\nThe encoder's model changes with each symbol transmitted, and the decoder's\nchanges with each symbol received, in sympathy.\n.pp\nMore complex models can provide more accurate probabilistic predictions and\nhence achieve greater compression.\nFor example, several characters of previous context could condition the\nnext-symbol probability.\nSuch methods have enabled mixed-case English text to be encoded in around\n2.2\\ bit/char with two quite different kinds of model.\n(Cleary & Witten, 1984b; Cormack & Horspool, 1985).\n.[\nCleary Witten 1984 data compression\n%D 1984b\n.]\n.[\nCormack Horspool 1985 dynamic Markov\n%O April\n.]\nTechniques which do not separate modeling from coding so distinctly, like\nthat of Ziv & Lempel (1978), do not seem to show such great potential for\ncompression, although they may be appropriate when the aim is raw speed rather\nthan compression performance (Welch, 1984).\n.[\nZiv Lempel 1978 compression of individual sequences\n.]\n.[\nWelch 1984 data compression\n.]\n.pp\nThe effectiveness of any model can be measured by the \\fIentropy\\fR of the\nmessage with respect to it, usually expressed in bits/symbol.\nShannon's fundamental theorem of coding states that given messages randomly\ngenerated from a model, it is impossible to encode them into less bits\n(on average) than the entropy of that model (Shannon & Weaver, 1949).\n.[\nShannon Weaver 1949\n.]\n.pp\nA message can be coded with respect to a model using either Huffman or\narithmetic coding.\nThe former method is frequently advocated as the best possible technique for\nreducing the encoded data rate.\nBut it is not.\nGiven that each symbol in the alphabet must translate into an integral number\nof bits in the encoding, Huffman coding indeed achieves ``minimum\nredundancy''.\nIn other words, it performs optimally if all symbol probabilities are\nintegral powers of 1/2.\nBut this is not normally the case in practice;\nindeed, Huffman coding can take up to one extra bit per symbol.\nThe worst case is realized by a source in which one symbol has probability\napproaching unity.\nSymbols emanating from such a source convey negligible information on average,\nbut require at least one bit to transmit (Gallagher, 1978).\n.[\nGallagher 1978 variations on a theme by Huffman\n.]\nArithmetic coding dispenses with the restriction that each symbol translates\ninto an integral number of bits, thereby coding more efficiently.\nIt actually achieves the theoretical entropy bound to compression efficiency\nfor any source, including the one just mentioned.\n.pp\nIn general, sophisticated models expose the deficiencies of Huffman coding\nmore starkly than simple ones.\nThis is because they more often predict symbols with probabilities close to\none, the worst case for Huffman coding.\nFor example, the techniques mentioned above which code English text in\n2.2\\ bit/char both use arithmetic coding as the final step, and performance\nwould be impacted severely if Huffman coding were substituted.\nNevertheless, since our topic is coding and not modeling, the illustrations in\nthis paper all employ simple models.\nEven then, as we shall see, Huffman coding is inferior to arithmetic coding.\n.pp\nThe basic concept of arithmetic coding can be traced back to Elias in the\nearly 1960s (see Abramson, 1963, pp 61-62).\n.[\nAbramson 1963\n.]\nPractical techniques were first introduced by Rissanen (1976) and\nPasco (1976), and developed further in Rissanen (1979).\n.[\nRissanen 1976 Generalized Kraft Inequality\n.]\n.[\nPasco 1976\n.]\n.[\nRissanen 1979 number representations\n.]\n.[\nLangdon 1981 tutorial arithmetic coding\n.]\nDetails of the implementation presented here have not appeared in the\nliterature before; Rubin (1979) is closest to our approach.\n.[\nRubin 1979 arithmetic stream coding\n.]\nThe reader interested in the broader class of arithmetic codes is referred\nto Rissanen & Langdon (1979);\n.[\nRissanen Langdon 1979 Arithmetic coding\n.]\na tutorial is available in Langdon (1981).\n.[\nLangdon 1981 tutorial arithmetic coding\n.]\nDespite these publications, the method is not widely known.\nA number of recent books and papers on data compression mention it only in\npassing, or not at all.\n.sh \"The idea of arithmetic coding\"\n.pp\nIn arithmetic coding a message is represented by an\ninterval of real numbers between 0 and 1.\nAs the message becomes longer, the interval needed to represent it becomes\nsmaller, and the number of bits needed to specify that interval grows.\nSuccessive symbols of the message reduce the size of the\ninterval in accordance with the symbol probabilities generated by the\nmodel.\nThe more likely symbols reduce the range by less than the unlikely symbols,\nand hence add fewer bits to the message.\n.pp\nBefore anything is transmitted, the range for the message is the entire\ninterval [0,\\ 1)\\(dg.\n.FN\n\\(dg [0,\\ 1) denotes the half-open interval 0\\(<=\\fIx\\fR<1.\n.EF\nAs each symbol is processed, the range is narrowed to that portion of it\nallocated to the symbol.\nFor example, suppose the alphabet is {\\fIa,\\ e,\\ i,\\ o,\\ u,\\ !\\fR}, and a\nfixed model is used with probabilities shown in Table\\ 1.\nImagine transmitting the message \\fIeaii!\\fR.\nInitially, both encoder and decoder know that the range is [0,\\ 1).\nAfter seeing the first symbol, \\fIe\\fR, the encoder narrows it to\n[0.2,\\ 0.5), the range the model allocates to this symbol.\nThe second symbol, \\fIa\\fR, will narrow this new range to the first 1/5 of it,\nsince \\fIa\\fR has been allocated [0,\\ 0.2).\nThis produces [0.2,\\ 0.26) since the previous range was 0.3 units long and\n1/5 of that is 0.06.\nThe next symbol, \\fIi\\fR, is allocated [0.5,\\ 0.6), which when applied to\n[0.2,\\ 0.26) gives the smaller range [0.23,\\ 0.236).\nProceeding in this way, the encoded message builds up as follows:\n.LB\n.nf\n.ta \\w'after seeing   'u +0.5i +\\w'[0.23354, 'u\ninitially\t\t[0,\t1)\nafter seeing\t\\fIe\\fR\t[0.2,\t0.5)\n\t\\fIa\\fR\t[0.2,\t0.26)\n\t\\fIi\\fR\t[0.23,\t0.236)\n\t\\fIi\\fR\t[0.233,\t0.2336)\n\t\\fI!\\fR\t[0.23354,\t0.2336)\n.fi\n.LE\nFigure\\ 1 shows another representation of the encoding process.\nThe vertical bars with ticks represent the symbol probabilities stipulated\nby the model.\nAfter the first symbol, \\fIe\\fR, has been processed, the model is scaled\ninto the range [0.2,\\ 0.5), as shown in part (a).\nThe second symbol, \\fIa\\fR, scales it again into the range [0.2,\\ 0.26).\nBut the picture cannot be continued in this way without a magnifying glass!\nConsequently Figure\\ 1(b) shows the ranges expanded to full height at every\nstage, and marked with a scale which gives the endpoints as numbers.\n.pp\nSuppose all the decoder knows about the message is the final range,\n[0.23354,\\ 0.2336).\nIt can immediately deduce that the first character was \\fIe\\fR, since the\nrange lies entirely within the space the model of Table\\ 1 allocates for\n\\fIe\\fR.\nNow it can simulate the operation of the \\fIen\\fR\\^coder:\n.LB\n.nf\n.ta \\w'after seeing   'u +0.5i +\\w'[0.2, 'u\ninitially\t\t[0,\t1)\nafter seeing\t\\fIe\\fR\t[0.2,\t0.5)\n.fi\n.LE\nThis makes it clear that the second character of the message is \\fIa\\fR,\nsince this will produce the range\n.LB\n.nf\n.ta \\w'after seeing   'u +0.5i +\\w'[0.2, 'u\nafter seeing\t\\fIa\\fR\t[0.2,\t0.26)\n.fi\n.LE\nwhich entirely encloses the given range [0.23354,\\ 0.2336).\nProceeding like this, the decoder can identify the whole message.\n.pp\nIt is not really necessary for the decoder to know both ends of the range\nproduced by the encoder.\nInstead, a single number within the range \\(em for example, 0.23355 \\(em will\nsuffice.\n(Other numbers, like 0.23354, 0.23357, or even 0.23354321, would do just as\nwell.)  \\c\nHowever, the decoder will face the problem of detecting the end of the\nmessage, to determine when to stop decoding.\nAfter all, the single number 0.0 could represent any of \\fIa\\fR, \\fIaa\\fR,\n\\fIaaa\\fR, \\fIaaaa\\fR, ...\\ .\nTo resolve the ambiguity we ensure that each message ends with a special\nEOF symbol known to both encoder and decoder.\nFor the alphabet of Table\\ 1, \\fI!\\fR will be used to terminate messages,\nand only to terminate messages.\nWhen the decoder sees this symbol it stops decoding.\n.pp\nRelative to the fixed model of Table\\ 1, the entropy of the 5-symbol message\n\\fIeaii!\\fR is\n.LB\n$- ~ log ~ 0.3 ~ - ~ log ~ 0.2 ~ - ~ log ~ 0.1 ~ - ~ log ~ 0.1 ~ - ~ log ~ 0.1 ~~=~~ - ~ log ~ 0.00006 ~~ approx ~~ 4.22$\n.LE\n(using base 10, since the above encoding was performed in decimal).\nThis explains why it takes 5\\ decimal digits to encode the message.\nIn fact, the size of the final range is $0.2336 ~-~ 0.23354 ~~=~~ 0.00006$,\nand the entropy is the negative logarithm of this figure.\nOf course, we normally work in binary, transmitting binary digits and\nmeasuring entropy in bits.\n.pp\nFive decimal digits seems a lot to encode a message comprising four vowels!\nIt is perhaps unfortunate that our example ended up by expanding rather than\ncompressing.\nNeedless to say, however, different models will give different entropies.\nThe best single-character model of the message \\fIeaii!\\fR is the set of\nsymbol frequencies\n{\\fIe\\fR\\ (0.2),  \\fIa\\fR\\ (0.2),  \\fIi\\fR\\ (0.4),  \\fI!\\fR\\ (0.2)},\nwhich gives an entropy for \\fIeaii!\\fR of 2.89\\ decimal digits.\nUsing this model the encoding would be only 3\\ digits long.\nMoreover, as noted earlier more sophisticated models give much better\nperformance in general.\n.sh \"A program for arithmetic coding\"\n.pp\nFigure\\ 2 shows a pseudo-code fragment which summarizes the encoding and\ndecoding procedures developed in the last section.\nSymbols are numbered 1, 2, 3, ...\nThe frequency range for the $i$th symbol is from $cum_freq[i]$ to\n$cum_freq[i-1]$.\n$cum_freq[i]$ increases as $i$ decreases, and $cum_freq[0] = 1$.\n(The reason for this ``backwards'' convention is that later, $cum_freq[0]$\nwill contain a normalizing factor, and it will be convenient to have it\nbegin the array.)  \\c\nThe ``current interval'' is [$low$,\\ $high$); and for both encoding and\ndecoding this should be initialized to [0,\\ 1).\n.pp\nUnfortunately, Figure\\ 2 is overly simplistic.\nIn practice, there are several factors which complicate both encoding and\ndecoding.\n.LB\n.NP\nIncremental transmission and reception.\n.br\nThe encode algorithm as described does not transmit anything until the entire\nmessage has been encoded; neither does the decode algorithm begin decoding\nuntil it has received the complete transmission.\nIn most applications, an incremental mode of operation is necessary.\n.sp\n.NP\nThe desire to use integer arithmetic.\n.br\nThe precision required to represent the [$low$, $high$) interval grows with\nthe length of the message.\nIncremental operation will help overcome this, but the potential for overflow\nand underflow must still be examined carefully.\n.sp\n.NP\nRepresenting the model so that it can be consulted efficiently.\n.br\nThe representation used for the model should minimize the time required for\nthe decode algorithm to identify the next symbol.\nMoreover, an adaptive model should be organized to minimize the\ntime-consuming task of maintaining cumulative frequencies.\n.LE\n.pp\nFigure\\ 3 shows working code, in C, for arithmetic encoding and decoding.\nIt is considerably more detailed than the bare-bones sketch of Figure\\ 2!\nImplementations of two different models are given in Figure\\ 4;\nthe Figure\\ 3 code can use either one.\n.pp\nThe remainder of this section examines the code of Figure\\ 3 more closely,\nincluding a proof that decoding is still correct in the integer\nimplementation and a review of constraints on word lengths in the program.\n.rh \"Representing the model.\"\nImplementations of models are discussed in the next section; here we\nare concerned only with the interface to the model (lines 20-38).\nIn C, a byte is represented as an integer between 0 and 255 (call this a\n$char$).\nInternally, we represent a byte as an integer between 1 and 257 inclusive\n(call this an $index$), EOF being treated as a 257th symbol.\nIt is advantageous to sort the model into frequency order, to minimize the\nnumber of executions of the decoding loop (line 189).\nTo permit such reordering, the $char$/$index$ translation is implemented as\na pair of tables, $index_to_char[ \\| ]$ and $char_to_index[ \\| ]$.\nIn one of our models, these tables simply form the $index$ by adding 1 to the\n$char$, but another implements a more complex translation which assigns small\nindexes to frequently-used symbols.\n.pp\nThe probabilities in the model are represented as integer frequency counts,\nand cumulative counts are stored in the array $cum_freq[ \\| ]$.\nAs previously, this array is ``backwards,'' and the total frequency count \\(em\nwhich is used to normalize all frequencies \\(em appears in $cum_freq[0]$.\nCumulative counts must not exceed a predetermined maximum, $Max_frequency$,\nand the model implementation must prevent overflow by scaling appropriately.\nIt must also ensure that neighboring values in the $cum_freq[ \\| ]$ array\ndiffer by at least 1; otherwise the affected symbol could not be transmitted.\n.rh \"Incremental transmission and reception.\"\nUnlike Figure\\ 2, the program of Figure\\ 3 represents $low$ and $high$ as\nintegers.\nA special data type, $code_value$, is defined for these quantities,\ntogether with some useful constants:  \\c\n$Top_value$, representing the largest possible $code_value$, and\n$First_qtr$, $Half$, and $Third_qtr$, representing parts of the range\n(lines 6-16).\nWhereas in Figure\\ 2 the current interval is represented by\n[$low$,\\ $high$), in Figure\\ 3 it is [$low$,\\ $high$]; that is, the range now\nincludes the value of $high$.\nActually, it is more accurate (though more confusing) to say that in the\nprogram of Figure\\ 3 the interval represented is\n[$low$,\\ $high + 0.11111 ...$).\nThis is because when the bounds are scaled up to increase the precision, 0's\nare shifted into the low-order bits of $low$ but 1's are shifted into $high$.\nWhile it is possible to write the program to use a different convention,\nthis one has some advantages in simplifying the code.\n.pp\nAs the code range narrows, the top bits of $low$ and $high$ will become the\nsame.\nAny bits which are the same can be transmitted immediately, since they cannot\nbe affected by future narrowing.\nFor encoding, since we know that $low ~ <= ~ high$, this requires code like\n.LB \"nnnn\"\n.nf\n.ta \\w'nnnn'u +\\w'if (high < 'u +\\w'Half) { 'u +\\w'output_bit(1); low = 2*(low\\-Half); high = 2*(high\\-Half)+1; 'u\n.ne 4\nfor (;;) {\n\tif (high <\tHalf) {\toutput_bit(0); low = 2*low; high = 2*high+1;\t}\n\tif (low $>=$\tHalf) {\toutput_bit(1); low = 2*(low\\-Half); high = 2*(high\\-Half)+1;\t}\n}\n.fi\n.LE \"nnnn\"\nwhich ensures that, upon completion, $low ~ < ~ Half ~ <= ~ high$.\nThis can be found in lines 95-113 of $encode_symbol( \\| )$,\nalthough there are some extra complications caused by underflow possibilities\n(see next subsection).\nCare is taken care to shift 1's in at the bottom when $high$ is scaled, as\nnoted above.\n.pp\nIncremental reception is done using a number called $value$ as in Figure\\ 2,\nin which processed bits flow out the top (high-significance) end and\nnewly-received ones flow in the bottom.\n$start_decoding( \\| )$ (lines 168-176) fills $value$ with received bits initially.\nOnce $decode_symbol( \\| )$ has identified the next input symbol, it shifts out\nnow-useless high-order bits which are the same in $low$ and $high$, shifting\n$value$ by the same amount (and replacing lost bits by fresh input bits at the\nbottom end):\n.LB \"nnnn\"\n.nf\n.ta \\w'nnnn'u +\\w'if (high < 'u +\\w'Half) { 'u +\\w'value = 2*(value\\-Half)+input_bit(\\|); low = 2*(low\\-Half); high = 2*(high\\-Half)+1; 'u\n.ne 4\nfor (;;) {\n\tif (high <\tHalf) {\tvalue = 2*value+input_bit(\\|); low = 2*low; high = 2*high+1;\t}\n\tif (low $>=$\tHalf) {\tvalue = 2*(value\\-Half)+input_bit(\\|); low = 2*(low\\-Half); high = 2*(high\\-Half)+1;\t}\n}\n.fi\n.LE \"nnnn\"\n(see lines 194-213, again complicated by precautions against underflow\ndiscussed below).\n.rh \"Proof of decoding correctness.\"\nAt this point it is worth checking that the identification of the next\nsymbol by $decode_symbol( \\| )$ works properly.\nRecall from Figure\\ 2 that $decode_symbol( \\| )$ must use $value$ to find that\nsymbol which, when encoded, reduces the range to one that still includes\n$value$.\nLines 186-188 in $decode_symbol( \\| )$ identify the symbol for which\n.LB\n$cum_freq[symbol] ~ <= ~~\nleft f {(value-low+1)*cum_freq[0] ~-~ 1} over {high-low+1} right f\n~~ < ~ cum_freq[symbol-1]$,\n.LE\nwhere $left f ~ right f$ denotes the ``integer part of'' function that\ncomes from integer division with truncation.\nIt is shown in the Appendix that this implies\n.LB \"nnnn\"\n$low ~+~~ left f {(high-low+1)*cum_freq[symbol]} over cum_freq[0] right f\n~~ <= ~ v ~ <=  ~~\nlow ~+~~ left f {(high-low+1)*cum_freq[symbol-1]} over cum_freq[0] right f ~~ - ~ 1$,\n.LE \"nnnn\"\nso $value$ lies within the new interval that $decode_symbol( \\| )$\ncalculates in lines 190-193.\nThis is sufficient to guarantee that the decoding operation identifies each\nsymbol correctly.\n.rh \"Underflow.\"\nAs Figure\\ 1 shows, arithmetic coding works by scaling the cumulative\nprobabilities given by the model into the interval [$low$,\\ $high$] for\neach character transmitted.\nSuppose $low$ and $high$ are very close together, so close that this scaling\noperation maps some different symbols of the model on to the same integer in\nthe [$low$,\\ $high$] interval.\nThis would be disastrous, because if such a symbol actually occurred it would\nnot be possible to continue encoding.\nConsequently, the encoder must guarantee that the interval [$low$,\\ $high$] is\nalways large enough to prevent this.\nThe simplest way is to ensure that this interval is at least as large as\n$Max_frequency$, the maximum allowed cumulative frequency count (line\\ 36).\n.pp\nHow could this condition be violated?\nThe bit-shifting operation explained above ensures that $low$ and $high$ can\nonly become close together when they straddle $Half$.\nSuppose in fact they become as close as\n.LB\n$First_qtr ~ <= ~ low ~<~ Half ~ <= ~ high ~<~ Third_qtr$.\n.LE\nThen the next two bits sent will have opposite polarity, either 01 or 10.\nFor example, if the next bit turns out to be 0 (ie $high$ descends below\n$Half$ and [0,\\ $Half$] is expanded to the full interval) the bit after\nthat will be 1 since the range has to be above the midpoint of the expanded\ninterval.\nConversely if the next bit happens to be 1 the one after that will be 0.\nTherefore the interval can safely be expanded right now, if only we remember\nthat whatever bit actually comes next, its opposite must be transmitted\nafterwards as well.\nThus lines 104-109 expand [$First_qtr$,\\ $Third_qtr$] into the whole interval,\nremembering in $bits_to_follow$ that the bit that is output next must be\nfollowed by an opposite bit.\nThis explains why all output is done via $bit_plus_follow( \\| )$\n(lines 128-135) instead of directly with $output_bit( \\| )$.\n.pp\nBut what if, after this operation, it is \\fIstill\\fR true that\n.LB\n$First_qtr ~ <= ~ low ~<~ Half ~ <= ~ high ~<~ Third_qtr$?\n.LE\nFigure\\ 5 illustrates this situation, where the current [$low$,\\ $high$]\nrange (shown as a thick line) has been expanded a total of three times.\nSuppose the next bit will turn out to be 0, as indicated by the arrow in\nFigure\\ 5(a) being below the halfway point.\nThen the next \\fIthree\\fR bits will be 1's, since not only is the arrow in the\ntop half of the bottom half of the original range, it is in the top quarter,\nand moreover the top eighth, of that half \\(em that is why the expansion\ncan occur three times.\nSimilarly, as Figure\\ 5(b) shows, if the next bit turns out to be a 1 it will\nbe followed by three 0's.\nConsequently we need only count the number of expansions and follow the next\nbit by that number of opposites (lines 106 and 131-134).\n.pp\nUsing this technique, the encoder can guarantee that after the shifting\noperations, either\n.LB\n.ta \\n(.lu-\\n(.iuR\n$low ~<~ First_qtr ~<~ Half ~ <= ~ high$\t(1a)\n.LE\nor\n.LB\n.ta \\n(.lu-\\n(.iuR\n$low ~<~ Half ~<~ Third_qtr ~ <= ~ high$.\t(1b)\n.LE\nTherefore as long as the integer range spanned by the cumulative frequencies\nfits into a quarter of that provided by $code_value$s, the underflow problem\ncannot occur.\nThis corresponds to the condition\n.LB\n$Max_frequency ~ <= ~~ {Top_value+1} over 4 ~~ + ~ 1$,\n.LE\nwhich is satisfied by Figure\\ 3 since $Max_frequency ~=~ 2 sup 14 - 1$ and\n$Top_value ~=~ 2 sup 16 - 1$ (lines\\ 36, 9).\nMore than 14\\ bits cannot be used to represent cumulative frequency\ncounts without increasing the number of bits allocated to $code_value$s.\n.pp\nWe have discussed underflow in the encoder only.\nSince the decoder's job, once each symbol has been decoded, is to track the\noperation of the encoder, underflow will be avoided if it performs the same\nexpansion operation under the same conditions.\n.rh \"Overflow.\"\nNow consider the possibility of overflow in the integer multiplications\ncorresponding to those of Figure\\ 2, which occur in lines 91-94 and 190-193\nof Figure\\ 3.\nOverflow cannot occur provided the product\n.LB\n$range * Max_frequency$\n.LE\nfits within the integer word length available, since cumulative frequencies\ncannot exceed $Max_frequency$.\n$Range$ might be as large as $Top_value ~+~1$, so the largest possible product\nin Figure 3 is $2 sup 16 ( 2 sup 14 - 1 )$ which is less than $2 sup 30$.\n$Long$ declarations are used for $code_value$ (line\\ 7) and $range$\n(lines\\ 89, 183) to ensure that arithmetic is done to 32-bit precision.\n.rh \"Constraints on the implementation.\"\nThe constraints on word length imposed by underflow and overflow can\nbe simplified by assuming frequency counts are represented in $f$\\ bits, and\n$code_value$s in $c$\\ bits.\nThe implementation will work correctly provided\n.LB\n$f ~ <= ~ c ~ - ~2$\n.br\n$f ~+~ c ~ <= ~ p$, the precision to which arithmetic is performed.\n.LE\nIn most C implementations, $p=31$ if $long$ integers are used, and $p=32$\nif they are $unsigned ~ long$.\nIn Figure\\ 3, $f=14$ and $c=16$.\nWith appropriately modified declarations, $unsigned ~ long$ arithmetic with\n$f=15$, $c=17$ could be used.\nIn assembly language $c=16$ is a natural choice because it expedites some\ncomparisons and bit manipulations (eg those of lines\\ 95-113 and 194-213).\n.pp\nIf $p$ is restricted to 16\\ bits, the best values possible are $c=9$ and\n$f=7$, making it impossible to encode a full alphabet of 256\\ symbols, as each\nsymbol must have a count of at least 1.\nA smaller alphabet (eg the letters, or 4-bit nibbles) could still be\nhandled.\n.rh \"Termination.\"\nTo finish the transmission, it is necessary to send a unique terminating\nsymbol ($EOF_symbol$, line 56) and then follow it by enough bits to ensure\nthat the encoded string falls within the final range.\nSince $done_encoding( \\| )$ (lines 119-123) can be sure that\n$low$ and $high$ are constrained by either (1a) or (1b) above, it need only\ntransmit $01$ in the first case or $10$ in the second to remove the remaining\nambiguity.\nIt is convenient to do this using the $bit_plus_follow( \\| )$ procedure\ndiscussed earlier.\nThe $input_bit( \\| )$ procedure will actually read a few more bits than were\nsent by $output_bit( \\| )$ as it needs to keep the low end of the buffer full.\nIt does not matter what value these bits have as the EOF is uniquely\ndetermined by the last two bits actually transmitted.\n.sh \"Models for arithmetic coding\"\n.pp\nThe program of Figure\\ 3 must be used with a model which provides\na pair of translation tables $index_to_char[ \\| ]$ and $char_to_index[ \\| ]$,\nand a cumulative frequency array $cum_freq[ \\| ]$.\nThe requirements on the latter are that\n.LB\n.NP\n$cum_freq[ i-1 ] ~ >= ~ cum_freq[ i ]$;\n.NP\nan attempt is never made to encode a symbol $i$ for which\n$cum_freq[i-1] ~=~ cum_freq[i]$;\n.NP\n$cum_freq[0] ~ <= ~ Max_frequency$.\n.LE\nProvided these conditions are satisfied the values in the array need bear\nno relationship to the actual cumulative symbol frequencies in messages.\nEncoding and decoding will still work correctly, although encodings will\noccupy less space if the frequencies are accurate.\n(Recall our successfully encoding \\fIeaii!\\fR according to the model of\nTable\\ 1, which does not actually reflect the frequencies in the message.)  \\c\n.rh \"Fixed models.\"\nThe simplest kind of model is one in which symbol frequencies are fixed.\nThe first model in Figure\\ 4 has symbol frequencies which approximate those\nof English (taken from a part of the Brown Corpus, Kucera & Francis, 1967).\n.[\n%A Kucera, H.\n%A Francis, W.N.\n%D 1967\n%T Computational analysis of present-day American English\n%I Brown University Press\n%C Providence, RI\n.]\nHowever, bytes which did not occur in that sample have been given frequency\ncounts of 1 in case they do occur in messages to be encoded\n(so, for example, this model will still work for binary files in which all\n256\\ bytes occur).\nFrequencies have been normalized to total 8000.\nThe initialization procedure $start_model( \\| )$ simply computes a cumulative\nversion of these frequencies (lines 48-51), having first initialized the\ntranslation tables (lines 44-47).\nExecution speed would be improved if these tables were used to re-order\nsymbols and frequencies so that the most frequent came first in the\n$cum_freq[ \\| ]$ array.\nSince the model is fixed, the procedure $update_model( \\| )$, which is\ncalled from both $encode.c$ and $decode.c$, is null.\n.pp\nAn \\fIexact\\fR model is one where the symbol frequencies in the message are\nexactly as prescribed by the model.\nFor example, the fixed model of Figure\\ 4 is close to an exact model\nfor the particular excerpt of the Brown Corpus from which it was taken.\nTo be truly exact, however, symbols that did not occur in the excerpt would\nbe assigned counts of 0, not 1 (sacrificing the capability of\ntransmitting messages containing those symbols).\nMoreover, the frequency counts would not be scaled to a predetermined\ncumulative frequency, as they have been in Figure\\ 4.\nThe exact model may be calculated and transmitted before sending the message.\nIt is shown in Cleary & Witten (1984a) that, under quite general conditions,\nthis will \\fInot\\fR give better overall compression than adaptive coding,\ndescribed next.\n.[\nCleary Witten 1984 enumerative adaptive codes\n%D 1984a\n.]\n.rh \"Adaptive models.\"\nAn adaptive model represents the changing symbol frequencies seen \\fIso far\\fR\nin the message.\nInitially all counts might be the same (reflecting no initial information),\nbut they are updated as each symbol is seen, to approximate the observed\nfrequencies.\nProvided both encoder and decoder use the same initial values (eg equal\ncounts) and the same updating algorithm, their models will remain in step.\nThe encoder receives the next symbol, encodes it, and updates its model.\nThe decoder identifies it according to its current model, and then updates its\nmodel.\n.pp\nThe second half of Figure\\ 4 shows such an adaptive model.\nThis is the type of model recommended for use with Figure\\ 3, for in practice\nit will outperform a fixed model in terms of compression efficiency.\nInitialization is the same as for the fixed model, except that all frequencies\nare set to 1.\nThe procedure $update_model(symbol)$ is called by both $encode_symbol( \\| )$\nand $decode_symbol( \\| )$ (Figure\\ 3 lines 54 and 151) after each symbol is\nprocessed.\n.pp\nUpdating the model is quite expensive, because of the need to maintain\ncumulative totals.\nIn the code of Figure\\ 4, frequency counts, which must be maintained anyway,\nare used to optimize access by keeping the array in frequency order \\(em an\neffective kind of self-organizing linear search (Hester & Hirschberg, 1985).\n.[\nHester Hirschberg 1985\n.]\n$Update_model( \\| )$ first checks to see if the new model will exceed\nthe cumulative-frequency limit, and if so scales all frequencies down by a\nfactor of 2 (taking care to ensure that no count scales to zero) and\nrecomputes cumulative values (Figure\\ 4, lines\\ 29-37).\nThen, if necessary, $update_model( \\| )$ re-orders the symbols to place the\ncurrent one in its correct rank in the frequency ordering, altering the\ntranslation tables to reflect the change.\nFinally, it increments the appropriate frequency count and adjusts cumulative\nfrequencies accordingly.\n.sh \"Performance\"\n.pp\nNow consider the performance of the algorithm of Figure\\ 3, both\nin compression efficiency and execution time.\n.rh \"Compression efficiency.\"\nIn principle, when a message is coded using arithmetic coding, the number of\nbits in the encoded string is the same as the entropy of that message with\nrespect to the model used for coding.\nThree factors cause performance to be worse than this in practice:\n.LB\n.NP\nmessage termination overhead\n.NP\nthe used of fixed-length rather than infinite-precision arithmetic\n.NP\nscaling of counts so that their total is at most $Max_frequency$.\n.LE\nNone of these effects is significant, as we now show.\nIn order to isolate the effect of arithmetic coding the model will be\nconsidered to be exact (as defined above).\n.pp\nArithmetic coding must send extra bits at the end of each message, causing a\nmessage termination overhead.\nTwo bits are needed, sent by $done_encoding( \\| )$ (Figure\\ 3 lines 119-123),\nin order to disambiguate the final symbol.\nIn cases where a bit-stream must be blocked into 8-bit characters before\nencoding, it will be necessary to round out to the end of a block.\nCombining these, an extra 9\\ bits may be required.\n.pp\nThe overhead of using fixed-length arithmetic\noccurs because remainders are truncated on division.\nIt can be assessed by comparing the algorithm's performance with\nthe figure obtained from a theoretical entropy calculation\nwhich derives its frequencies from counts scaled exactly as for coding.\nIt is completely negligible \\(em on the order of $10 sup -4$ bits/symbol.\n.pp\nThe penalty paid by scaling counts is somewhat larger, but still very\nsmall.\nFor short messages (less than $2 sup 14$ bytes) no scaling need be done.\nEven with messages of $10 sup 5$ to $10 sup 6$ bytes, the overhead was found\nexperimentally to be less than 0.25% of the encoded string.\n.pp\nThe adaptive model of Figure\\ 4 scales down all counts whenever the total\nthreatens to exceed $Max_frequency$.\nThis has the effect of weighting recent events more heavily compared with\nthose earlier in the message.\nThe statistics thus tend to track changes in the input sequence, which can be\nvery beneficial.\n(For example, we have encountered cases where limiting counts to 6 or 7\\ bits\ngives better results than working to higher precision.)  \\c\nOf course, this depends on the source being modeled.\nBentley \\fIet al\\fR (1986) consider other, more explicit, ways of\nincorporating a recency effect.\n.[\nBentley Sleator Tarjan Wei 1986 locally adaptive\n%J Communications of the ACM\n.]\n.rh \"Execution time.\"\nThe program in Figure\\ 3 has been written for clarity, not execution speed.\nIn fact, with the adaptive model of Figure\\ 4, it takes about 420\\ $mu$s per\ninput byte on a VAX-11/780 to encode a text file, and about the same for\ndecoding.\nHowever, easily avoidable overheads such as procedure calls account for much\nof this, and some simple optimizations increase speed by a factor of 2.\nThe following alterations were made to the C version shown:\n.LB\n.NP\nthe procedures $input_bit( \\| )$, $output_bit( \\| )$, and\n$bit_plus_follow( \\| )$ were converted to macros to eliminate\nprocedure-call overhead;\n.NP\nfrequently-used quantities were put in register variables;\n.NP\nmultiplies by two were replaced by additions (C ``+='');\n.NP\narray indexing was replaced by pointer manipulation in the loops\nat line 189 of Figure\\ 3 and lines 49-52 of the adaptive model in Figure\\ 4.\n.LE\n.pp\nThis mildly-optimized C implementation has an execution time of\n214\\ $mu$s/262\\ $mu$s, per input byte,\nfor encoding/decoding 100,000\\ bytes of English text on a VAX-11/780, as shown\nin Table\\ 2.\nAlso given are corresponding figures for the same program on an\nApple Macintosh and a SUN-3/75.\nAs can be seen, coding a C source program of the same length took slightly\nlonger in all cases, and a binary object program longer still.\nThe reason for this will be discussed shortly.\nTwo artificial test files were included to allow readers to replicate the\nresults.\n``Alphabet'' consists of enough copies of the 26-letter alphabet to fill\nout 100,000\\ characters (ending with a partially-completed alphabet).\n``Skew-statistics'' contains 10,000 copies of the string\n\\fIaaaabaaaac\\fR\\^; it demonstrates that files may be encoded into less than\n1\\ bit per character (output size of 12,092\\ bytes = 96,736\\ bits).\nAll results quoted used the adaptive model of Figure\\ 4.\n.pp\nA further factor of 2 can be gained by reprogramming in assembly language.\nA carefully optimized version of Figures\\ 3 and 4 (adaptive model) was\nwritten in both VAX and M68000 assembly language.\nFull use was made of registers.\nAdvantage was taken of the 16-bit $code_value$ to expedite some crucial\ncomparisons and make subtractions of $Half$ trivial.\nThe performance of these implementations on the test files is also shown in\nTable\\ 2 in order to give the reader some idea of typical execution speeds.\n.pp\nThe VAX-11/780 assembly language timings are broken down in Table\\ 3.\nThese figures were obtained with the U\\s-2NIX\\s+2 profile facility and are\naccurate only to within perhaps 10%\\(dg.\n.FN\n\\(dg This mechanism constructs a histogram of program counter values at\nreal-time clock interrupts, and suffers from statistical variation as well as\nsome systematic errors.\n.EF\n``Bounds calculation'' refers to the initial part of $encode_symbol( \\| )$\nand $decode_symbol( \\| )$ (Figure\\ 3 lines 90-94 and 190-193)\nwhich contain multiply and divide operations.\n``Bit shifting'' is the major loop in both the encode and decode routines\n(lines 95-113 and 194-213).\nThe $cum$ calculation in $decode_symbol( \\| )$, which requires a\nmultiply/divide, and the following loop to identify the next symbol\n(lines\\ 187-189), is ``Symbol decode''.\nFinally, ``Model update'' refers to the adaptive\n$update_model( \\| )$ procedure of Figure\\ 4 (lines\\ 26-53).\n.pp\nAs expected, the bounds calculation and model update take the same time for\nboth encoding and decoding, within experimental error.\nBit shifting was quicker for the text file than for the C program and object\nfile because compression performance was better.\nThe extra time for decoding over encoding is due entirely to the symbol\ndecode step.\nThis takes longer in the C program and object file tests because the loop of\nline\\ 189 was executed more often (on average 9\\ times, 13\\ times, and\n35\\ times respectively).\nThis also affects the model update time because it is the number of cumulative\ncounts which must be incremented in Figure\\ 4 lines\\ 49-52.\nIn the worst case, when the symbol frequencies are uniformly distributed,\nthese loops are executed an average of 128 times.\nWorst-case performance would be improved by using a more complex tree\nrepresentation for frequencies, but this would likely be slower for text\nfiles.\n.sh \"Some applications\"\n.pp\nApplications of arithmetic coding are legion.\nBy liberating \\fIcoding\\fR with respect to a model from the \\fImodeling\\fR\nrequired for prediction, it encourages a whole new view of data compression\n(Rissanen & Langdon, 1981).\n.[\nRissanen Langdon 1981 Universal modeling and coding\n.]\nThis separation of function costs nothing in compression performance, since\narithmetic coding is (practically) optimal with respect to the entropy of\nthe model.\nHere we intend to do no more than suggest the scope of this view\nby briefly considering\n.LB\n.NP\nadaptive text compression\n.NP\nnon-adaptive coding\n.NP\ncompressing black/white images\n.NP\ncoding arbitrarily-distributed integers.\n.LE\nOf course, as noted earlier, greater coding efficiencies could easily be\nachieved with more sophisticated models.\nModeling, however, is an extensive topic in its own right and is beyond the\nscope of this paper.\n.pp\n.ul\nAdaptive text compression\nusing single-character adaptive frequencies shows off arithmetic coding to\ngood effect.\nThe results obtained using the program of Figures\\ 3 and 4 vary from\n4.8\\-5.3\\ bit/char for short English text files ($10 sup 3$\\ to $10 sup 4$\nbytes) to 4.5\\-4.7\\ bit/char for long ones ($10 sup 5$ to $10 sup 6$ bytes).\nAlthough adaptive Huffman techniques do exist (eg Gallagher, 1978;\nCormack & Horspool, 1984) they lack the conceptual simplicity of\narithmetic coding.\n.[\nGallagher 1978 variations on a theme by Huffman\n.]\n.[\nCormack Horspool 1984 adaptive Huffman codes\n.]\nWhile competitive in compression efficiency for many files, they are slower.\nFor example, Table\\ 4 compares the performance of the mildly-optimized C\nimplementation of arithmetic coding with that of the U\\s-2NIX\\s+2\n\\fIcompact\\fR program which implements adaptive Huffman coding using\na similar model\\(dg.\n.FN\n\\(dg \\fICompact\\fR's model is essentially the same for long files (like those\nof Table\\ 4) but is better for short files than the model used as an example\nin this paper.\n.EF\nCasual examination of \\fIcompact\\fR indicates that the care taken in\noptimization is roughly comparable for both systems, yet arithmetic coding\nhalves execution time.\nCompression performance is somewhat better with arithmetic coding on all the\nexample files.\nThe difference would be accentuated with more sophisticated models that\npredict symbols with probabilities approaching one under certain circumstances\n(eg letter ``u'' following ``q'').\n.pp\n.ul\nNon-adaptive coding\ncan be performed arithmetically using fixed, pre-specified models like that in\nthe first part of Figure\\ 4.\nCompression performance will be better than Huffman coding.\nIn order to minimize execution time, the total frequency count,\n$cum_freq[0]$, should be chosen as a power of two so the divisions\nin the bounds calculations (Figure\\ 3 lines 91-94 and 190-193) can be done\nas shifts.\nEncode/decode times of around 60\\ $mu$s/90\\ $mu$s should then be possible\nfor an assembly language implementation on a VAX-11/780.\nA carefully-written implementation of Huffman coding, using table look-up for\nencoding and decoding, would be a bit faster in this application.\n.pp\n.ul\nCompressing black/white images\nusing arithmetic coding has been investigated by Langdon & Rissanen (1981),\nwho achieved excellent results using a model which conditioned the probability\nof a pixel's being black on a template of pixels surrounding it.\n.[\nLangdon Rissanen 1981 compression of black-white images\n.]\nThe template contained a total of ten pixels, selected from those above and\nto the left of the current one so that they precede it in the raster scan.\nThis creates 1024 different possible contexts, and for each the probability of\nthe pixel being black was estimated adaptively as the picture was transmitted.\nEach pixel's polarity was then coded arithmetically according to this\nprobability.\nA 20%\\-30% improvement in compression was attained over earlier methods.\nTo increase coding speed Langdon & Rissanen used an approximate method\nof arithmetic coding which avoided multiplication by representing\nprobabilities as integer powers of 1/2.\nHuffman coding cannot be directly used in this application, as it never\ncompresses with a two-symbol alphabet.\nRun-length coding, a popular method for use with two-valued alphabets,\nprovides another opportunity for arithmetic coding.\nThe model reduces the data to a sequence of lengths of runs of the same symbol\n(eg for picture coding, run-lengths of black followed by white followed by\nblack followed by white ...).\nThe sequence of lengths must be transmitted.\nThe CCITT facsimile coding standard (Hunter & Robinson, 1980), for example,\nbases a Huffman code on the frequencies with which black and white runs of\ndifferent lengths occur in sample documents.\n.[\nHunter Robinson 1980 facsimile\n.]\nA fixed arithmetic code using these same frequencies would give better\nperformance; adapting the frequencies to each particular document would be\nbetter still.\n.pp\n.ul\nCoding arbitrarily-distributed integers\nis often called for when using more sophisticated models of text, image,\nor other data.\nConsider, for instance, Bentley \\fIet al\\fR's (1986) locally-adaptive data\ncompression scheme, in which the encoder and decoder cache the last $N$\ndifferent words seen.\n.[\nBentley Sleator Tarjan Wei 1986 locally adaptive\n%J Communications of the ACM\n.]\nA word present in the cache is transmitted by sending the integer cache index.\nWords not in the cache are transmitted by sending a new-word marker followed\nby the characters of the word.\nThis is an excellent model for text in which words are used frequently over\nshort intervals and then fall into long periods of disuse.\nTheir paper discusses several variable-length codings for the integers used\nas cache indexes.\nArithmetic coding allows \\fIany\\fR probability distribution to be used as the\nbasis for a variable-length encoding, including \\(em amongst countless others\n\\(em the ones implied by the particular codes discussed there.\nIt also permits use of an adaptive model for cache\nindexes, which is desirable if the distribution of cache hits is\ndifficult to predict in advance.\nFurthermore, with arithmetic coding, the code space allotted to the cache\nindexes can be scaled down to accommodate any desired probability for the\nnew-word marker.\n.sh \"Acknowledgement\"\n.pp\nFinancial support for this work has been provided by the\nNatural Sciences and Engineering Research Council of Canada.\n.sh \"References\"\n.sp\n.in+4n\n.[\n$LIST$\n.]\n.in 0\n.bp\n.sh \"APPENDIX: Proof of decoding inequality\"\n.sp\nUsing 1-letter abbreviations for $cum_freq$, $symbol$, $low$, $high$, and\n$value$, suppose\n.LB\n$c[s] ~ <= ~~ left f {(v-l+1) times c[0] ~-~ 1} over {h-l+1} right f ~~ < ~\nc[s-1]$;\n.LE\nin other words,\n.LB\n.ta \\n(.lu-\\n(.iuR\n$c[s] ~ <= ~~ {(v-l+1) times c[0] ~-~ 1} over {r} ~~-~~ epsilon ~~ <= ~\nc[s-1] ~-~1$, \t(1)\n.LE\n.ta 8n\nwhere\t$r ~=~ h-l+1$,  $0 ~ <= ~ epsilon ~ <= ~ {r-1} over r $.\n.sp\n(The last inequality of (1) derives from the fact that $c[s-1]$ must be an\ninteger.)  \\c\nThen we wish to show that  $l' ~ <= ~ v ~ <= ~ h'$,  where $l'$ and $h'$\nare the updated values for $low$ and $high$ as defined below.\n.sp\n.ta \\w'(a)    'u\n(a)\t$l' ~ == ~~ l ~+~~ left f {r times c[s]} over c[0] right f ~~ mark\n<= ~~ l ~+~~ {r} over c[0] ~ left [ ~ {(v-l+1) times c[0] ~-~ 1} over {r}\n                          ~~ - ~ epsilon ~ right ]$    from (1),\n.sp 0.5\n$lineup <= ~~ v ~ + ~ 1 ~ - ~ 1 over c[0]$ ,\n.sp 0.5\n\tso   $l' ~ <= ~~ v$   since both $v$ and $l'$ are integers\nand $c[0] > 0$.\n.sp\n(b)\t$h' ~ == ~~ l ~+~~ left f {r times c[s-1]} over c[0] right f ~~-~1~~ mark\n>= ~~ l ~+~~  {r} over c[0] ~ left [ ~ {(v-l+1) times c[0] ~-~ 1} over {r}\n                          ~~ + ~ 1 ~ - ~ epsilon ~ right ] ~~ - ~ 1\n$    from (1),\n.sp 0.5\n$lineup >= ~~ v ~ + ~~ r over c[0] ~ left [ ~ - ~ 1 over r ~+~ 1\n                                                  ~-~~ r-1 over r right ]\n~~ = ~~ v$.\n.bp\n.sh \"Captions for tables\"\n.sp\n.nf\n.ta \\w'Figure 1  'u\nTable 1\tExample fixed model for alphabet {\\fIa, e, i, o, u, !\\fR}\nTable 2\tResults for encoding and decoding 100,000-byte files\nTable 3\tBreakdown of timings for VAX-11/780 assembly language version\nTable 4\tComparison of arithmetic and adaptive Huffman coding\n.fi\n.sh \"Captions for figures\"\n.sp\n.nf\n.ta \\w'Figure 1  'u\nFigure 1\t(a) Representation of the arithmetic coding process\n\t(b) Like (a) but with the interval scaled up at each stage\nFigure 2\tPseudo-code for the encoding and decoding procedures\nFigure 3\tC implementation of arithmetic encoding and decoding\nFigure 4\tFixed and adaptive models for use with Figure 3\nFigure 5\tScaling the interval to prevent underflow\n.fi\n.bp 0\n.ev2\n.nr x2 \\w'symbol'/2\n.nr x3 (\\w'symbol'/2)+0.5i+(\\w'probability'/2)\n.nr x4 (\\w'probability'/2)+0.5i\n.nr x5 (\\w'[0.0, '\n.nr x1 \\n(x2+\\n(x3+\\n(x4+\\n(x5+\\w'0.0)'\n.nr x0 (\\n(.l-\\n(x1)/2\n.in \\n(x0u\n.ta \\n(x2uC +\\n(x3uC +\\n(x4u +\\n(x5u\n\\l'\\n(x1u'\n.sp\n\tsymbol\tprobability\t\\0\\0range\n\\l'\\n(x1u'\n.sp\n\t\\fIa\\fR\t0.2\t[0,\t0.2)\n\t\\fIe\\fR\t0.3\t[0.2,\t0.5)\n\t\\fIi\\fR\t0.1\t[0.5,\t0.6)\n\t\\fIo\\fR\t0.2\t[0.6,\t0.8)\n\t\\fIu\\fR\t0.1\t[0.8,\t0.9)\n\t\\fI!\\fR\t0.1\t[0.9,\t1.0)\n\\l'\\n(x1u'\n.sp\n.in 0\n.FE \"Table 1  Example fixed model for alphabet {\\fIa, e, i, o, u, !\\fR}\"\n.bp 0\n.ev2\n.nr x1 0.5i+\\w'\\fIVAX object program\\fR      '+\\w'100,000      '+\\w'time ($mu$s)  '+\\w'time ($mu$s)    '+\\w'time ($mu$s)  '+\\w'time ($mu$s)    '+\\w'time ($mu$s)  '+\\w'time ($mu$s)'\n.nr x0 (\\n(.l-\\n(x1)/2\n.in \\n(x0u\n.ta 0.5i +\\w'\\fIVAX object program\\fR      'u +\\w'100,000      'u +\\w'time ($mu$s)  'u +\\w'time ($mu$s)    'u +\\w'time ($mu$s)  'u +\\w'time ($mu$s)    'u +\\w'time ($mu$s)  'u\n\\l'\\n(x1u'\n.sp\n\t\t\t\\0\\0VAX-11/780\t\\0\\0\\0Macintosh\t\\0\\0\\0\\0SUN-3/75\n\t\toutput\t encode\t decode\t encode\t decode\t encode\t decode\n\t\t(bytes)\ttime ($mu$s)\ttime ($mu$s)\ttime ($mu$s)\ttime ($mu$s)\ttime ($mu$s)\ttime ($mu$s)\n\\l'\\n(x1u'\n.sp\nMildly optimized C implementation\n.sp\n\t\\fIText file\\fR\t\\057718\t\\0\\0214\t\\0\\0262\t\\0\\0687\t\\0\\0881\t\\0\\0\\098\t\\0\\0121\n\t\\fIC program\\fR\t\\062991\t\\0\\0230\t\\0\\0288\t\\0\\0729\t\\0\\0950\t\\0\\0105\t\\0\\0131\n\t\\fIVAX object program\\fR\t\\073501\t\\0\\0313\t\\0\\0406\t\\0\\0950\t\\01334\t\\0\\0145\t\\0\\0190\n\t\\fIAlphabet\\fR\t\\059292\t\\0\\0223\t\\0\\0277\t\\0\\0719\t\\0\\0942\t\\0\\0105\t\\0\\0130\n\t\\fISkew-statistics\\fR\t\\012092\t\\0\\0143\t\\0\\0170\t\\0\\0507\t\\0\\0645\t\\0\\0\\070\t\\0\\0\\085\n.sp\nCarefully optimized assembly language implementation\n.sp\n\t\\fIText file\\fR\t\\057718\t\\0\\0104\t\\0\\0135\t\\0\\0194\t\\0\\0243\t\\0\\0\\046\t\\0\\0\\058\n\t\\fIC program\\fR\t\\062991\t\\0\\0109\t\\0\\0151\t\\0\\0208\t\\0\\0266\t\\0\\0\\051\t\\0\\0\\065\n\t\\fIVAX object program\\fR\t\\073501\t\\0\\0158\t\\0\\0241\t\\0\\0280\t\\0\\0402\t\\0\\0\\075\t\\0\\0107\n\t\\fIAlphabet\\fR\t\\059292\t\\0\\0105\t\\0\\0145\t\\0\\0204\t\\0\\0264\t\\0\\0\\051\t\\0\\0\\065\n\t\\fISkew-statistics\\fR\t\\012092\t\\0\\0\\063\t\\0\\0\\081\t\\0\\0126\t\\0\\0160\t\\0\\0\\028\t\\0\\0\\036\n\n\\l'\\n(x1u'\n.sp 2\n.nr x0 \\n(.l\n.ll \\n(.lu-\\n(.iu\n.fi\n.in \\w'\\fINotes:\\fR  'u\n.ti -\\w'\\fINotes:\\fR  'u\n\\fINotes:\\fR\\ \\ \\c\nTimes are measured in $mu$s per byte of uncompressed data.\n.sp 0.5\nThe VAX-11/780 had a floating-point accelerator, which reduces integer\nmultiply and divide times.\n.sp 0.5\nThe Macintosh uses an 8\\ MHz MC68000 with some memory wait states.\n.sp 0.5\nThe SUN-3/75 uses a 16.67\\ MHz MC68020.\n.sp 0.5\nAll times exclude I/O and operating system overhead in support of I/O.\nVAX and SUN figures give user time from the U\\s-2NIX\\s+2 \\fItime\\fR\ncommand; on the Macintosh I/O was explicitly directed to an array.\n.sp 0.5\nThe 4.2BSD C compiler was used for VAX and SUN; Aztec C 1.06g for Macintosh.\n.sp\n.ll \\n(x0u\n.nf\n.in 0\n.FE \"Table 2  Results for encoding and decoding 100,000-byte files\"\n.bp 0\n.ev2\n.nr x1 \\w'\\fIVAX object program\\fR        '+\\w'Bounds calculation        '+\\w'time ($mu$s)    '+\\w'time ($mu$s)'\n.nr x0 (\\n(.l-\\n(x1)/2\n.in \\n(x0u\n.ta \\w'\\fIVAX object program\\fR        'u +\\w'Bounds calculation        'u +\\w'time ($mu$s)    'u +\\w'time ($mu$s)'u\n\\l'\\n(x1u'\n.sp\n\t\t encode\t decode\n\t\ttime ($mu$s)\ttime ($mu$s)\n\\l'\\n(x1u'\n.sp\n\\fIText file\\fR\tBounds calculation\t\\0\\0\\032\t\\0\\0\\031\n\tBit shifting\t\\0\\0\\039\t\\0\\0\\030\n\tModel update\t\\0\\0\\029\t\\0\\0\\029\n\tSymbol decode\t\\0\\0\\0\\(em\t\\0\\0\\045\n\tOther\t\\0\\0\\0\\04\t\\0\\0\\0\\00\n\t\t\\0\\0\\l'\\w'100'u'\t\\0\\0\\l'\\w'100'u'\n\t\t\\0\\0104\t\\0\\0135\n.sp\n\\fIC program\\fR\tBounds calculation\t\\0\\0\\030\t\\0\\0\\028\n\tBit shifting\t\\0\\0\\042\t\\0\\0\\035\n\tModel update\t\\0\\0\\033\t\\0\\0\\036\n\tSymbol decode\t\\0\\0\\0\\(em\t\\0\\0\\051\n\tOther\t\\0\\0\\0\\04\t\\0\\0\\0\\01\n\t\t\\0\\0\\l'\\w'100'u'\t\\0\\0\\l'\\w'100'u'\n\t\t\\0\\0109\t\\0\\0151\n.sp\n\\fIVAX object program\\fR\tBounds calculation\t\\0\\0\\034\t\\0\\0\\031\n\tBit shifting\t\\0\\0\\046\t\\0\\0\\040\n\tModel update\t\\0\\0\\075\t\\0\\0\\075\n\tSymbol decode\t\\0\\0\\0\\(em\t\\0\\0\\094\n\tOther\t\\0\\0\\0\\03\t\\0\\0\\0\\01\n\t\t\\0\\0\\l'\\w'100'u'\t\\0\\0\\l'\\w'100'u'\n\t\t\\0\\0158\t\\0\\0241\n\\l'\\n(x1u'\n.in 0\n.FE \"Table 3  Breakdown of timings for VAX-11/780 assembly language version\"\n.bp 0\n.ev2\n.nr x1 \\w'\\fIVAX object program\\fR      '+\\w'100,000      '+\\w'time ($mu$s)  '+\\w'time ($mu$s)    '+\\w'100,000      '+\\w'time ($mu$s)  '+\\w'time ($mu$s)'\n.nr x0 (\\n(.l-\\n(x1)/2\n.in \\n(x0u\n.ta \\w'\\fIVAX object program\\fR      'u +\\w'100,000      'u +\\w'time ($mu$s)  'u +\\w'time ($mu$s)    'u +\\w'100,000      'u +\\w'time ($mu$s)  'u +\\w'time ($mu$s)'u\n\\l'\\n(x1u'\n.sp\n\t\\0\\0\\0\\0\\0\\0Arithmetic coding\t\\0\\0\\0Adaptive Huffman coding\n\toutput\t encode\t decode\toutput\t encode\t decode\n\t(bytes)\ttime ($mu$s)\ttime ($mu$s)\t(bytes)\ttime ($mu$s)\ttime ($mu$s)\n\\l'\\n(x1u'\n.sp\n\\fIText file\\fR\t\\057718\t\\0\\0214\t\\0\\0262\t\\057781\t\\0\\0550\t\\0\\0414\n\\fIC program\\fR\t\\062991\t\\0\\0230\t\\0\\0288\t\\063731\t\\0\\0596\t\\0\\0441\n\\fIVAX object program\\fR\t\\073546\t\\0\\0313\t\\0\\0406\t\\076950\t\\0\\0822\t\\0\\0606\n\\fIAlphabet\\fR\t\\059292\t\\0\\0223\t\\0\\0277\t\\060127\t\\0\\0598\t\\0\\0411\n\\fISkew-statistics\\fR\t\\012092\t\\0\\0143\t\\0\\0170\t\\016257\t\\0\\0215\t\\0\\0132\n\\l'\\n(x1u'\n.sp 2\n.nr x0 \\n(.l\n.ll \\n(.lu-\\n(.iu\n.fi\n.in +\\w'\\fINotes:\\fR  'u\n.ti -\\w'\\fINotes:\\fR  'u\n\\fINotes:\\fR\\ \\ \\c\nMildly optimized C implementation used for arithmetic coding\n.sp 0.5\nU\\s-2NIX\\s+2 \\fIcompact\\fR used for adaptive Huffman coding\n.sp 0.5\nTimes are for a VAX-11/780, and exclude I/O and operating system overhead in\nsupport of I/O.\n.sp\n.ll \\n(x0u\n.nf\n.in 0\n.FE \"Table 4  Comparison of arithmetic and adaptive Huffman coding\"\n", "encoding": "ascii"}