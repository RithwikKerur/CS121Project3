{"url": "https://www.ics.uci.edu/~eppstein/161/960116.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Sorting</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 16, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Sorting</h1>\n\nHow to alphabetize a list of words? Sort a list of numbers? Some\nother information? We saw last time one reason for doing this (so\nwe can apply binary search) and the same problem comes up over and\nover again in programming. \n\n<h2>Comparison sorting</h2>\n\nThis is a very abstract model of sorting. We assume we are given a\nlist of objects to sort, and that there is some particular order in\nwhich they should be sorted. What is the minimum amount of\ninformation we can get away with and still be able to sort them? \n\n<p>As a particular case of the sorting problem, we should be able\nto sort lists of two objects. But this is the same as comparing any\ntwo objects, to determine which comes first in the sorted order.\n(For now, we assume no two objects are equal, so one should always\ngo before the other; most sorting algorithms can also handle\nobjects that are \"the same\" but it complicates the problem.)</p>\n\n<p>Algorithms that sort a list based only on comparisons of pairs\n(and not using other information about what is being sorted, for\ninstance arithmetic on numbers) are called <i>comparison sorting\nalgorithms</i></p>\n\n<p>Why do we care about this abstract and restrictive model of\nsorting?</p>\n\n<ul>\n<li>We only have to write one routine to do sorting, that can be\nused over and over again without having to rewrite it and re-debug\nit for each new sorting problem you need to solve.</li>\n\n<li>In fact we don't even have to write that one routine, it is\nprovided in the <tt>qsort()</tt> routine in the Unix library.</li>\n\n<li>For some problems, it is not obvious how to do anything other\nthan comparisons. (I gave an example from my own research, on a\ngeometric problem of <a href= \n\"http://www.ics.uci.edu/~eppstein/pubs/p-parallel-qt.html\">quadtree\nconstruction</a>, which involved comparing points (represented as\npairs of coordinates) by computing bitwise exclusive ors of the\ncoordinates, comparing those numbers, and using the result to\ndetermine which coordinates to compare).</li>\n\n<li>It's easier to design and analyze algorithms without having to\nthink about unnecessary problem-specific details</li>\n\n<li>Some comparison sorting algorithms work quite well, so there is\nnot so much need to do something else.</li>\n</ul>\n\n<h2>Sorting algorithms</h2>\n\nThere are dozens of sorting algorithms. Baase covers around seven.\nWe'll probably have time only for four: heapsort, merge sort,\nquicksort, and bucket sort. Each of these is useful as an\nalgorithm, but also helps introduce some new ideas: \n\n<ul>\n<li>Heapsort shows how one can start with a slow algorithm\n(selection sort) and by adding some simple data structures\ntransform it into a much better one.</li>\n\n<li>Merge sort and quick sort are different examples of divide and\nconquer, a very general algorithm design technique in which one\npartitions an input into parts, solves the parts recursively, then\nrecombines the subproblem solutions into one overall solution. The\ntwo differ in how they do the partition and recombination; merge\nsort allows any partition, but the result of the recursive solution\nto the parts is two interleaved sorted lists, which we must combine\ninto one in a somewhat complicated way. Quick sort instead does a\nmore complicated partition so that one subproblem contains all\nobjects less than some value, and the other contains all objects\ngreater than that value, but then the recombination stage is\ntrivial (just concatenate).</li>\n\n<li>Quick sort is an example of randomization and average case\nanalysis.</li>\n\n<li>Bucket sort shows how abstraction is not always a good idea --\nwe can derive improved sorting algorithms for both numbers and\nalphabetical words by looking more carefully at the details of the\nobjects being sorted.</li>\n</ul>\n\n<h2>Sorting time bounds</h2>\n\nWhat sort of time bounds should we expect? First, how should we\nmeasure time? If we have a comparison sorting algorithm, we can't\nreally say how many machine instructions it will take, because it\nwill vary depending on how complicated the comparisons are. Since\nthe comparisons usually end up dominating the overall time bound,\nwe'll measure time in terms of the number of comparisons made. \n\n<p>Sorting algorithms have a range of time bounds, but for some\nreason there are two typical time bounds for comparison sorting:\nmergesort, heapsort, and (the average case of) quicksort all take\nO(n log n), while insertion sort, selection sort, and the worst\ncase of quicksort all take O(n^2). As we'll see, O(n log n) is the\nbest you could hope to achieve, while O(n^2) is the worst -- it\ndescribes the amount of time taken by an algorithm that performs\nevery possible comparison it could.</p>\n\n<p>O(n log n) is significantly faster than O(n^2):</p>\n\n<pre>\n    n       n log n         n^2\n    --      -------         ---\n    10      33              100\n    100     665             10K\n    1000    10^4            10^6\n    10^6    2 10^7          10^12\n    10^9    3 10^10         10^18\n</pre>\n\nSo even if you're sorting small lists it pays to use a good\nalgorithm such as quicksort instead of a poor one like bubblesort.\nYou don't even have the excuse that bubblesort is easier, since to\nget a decent sorting algorithm in a program you merely have to call\n<tt>qsort</tt>. \n\n<h2>Lower bounds</h2>\n\nA lower bound is a mathematical argument saying you can't hope to\ngo faster than a certain amount. More precisely, every algorithm\nwithin a certain model of computation has a running time at least\nthat amount. (This is usually proved for worst case running times\nbut you could also do the same sort of thing for average case or\nbest case if you want to.) This doesn't necessarily mean faster\nalgorithms are completely impossible, but only that if you want to\ngo faster, you can't stick with the abstract model, you have to\nlook more carefully at the problem. So the linear time bound we'll\nsee later for bucketsort won't contradict the n log n lower bounds\nwe'll prove now. \n\n<p>Lower bounds are useful for two reasons: First, they give you\nsome idea of how good an algorithm you could expect to find (so you\nknow if there is room for further optimization). Second, if your\nlower bound is slower than the amount of time you want to actually\nspend solving a problem, the lower bound tells you that you'll have\nto break the assumptions of the model of computation somehow.</p>\n\n<p>We'll prove lower bounds for sorting in terms of the number of\ncomparisons. Suppose you have a sorting algorithm that only\nexamines the data by making comparisons between pairs of objects\n(and doesn't use any random numbers; the model we describe can be\nextended to deal with randomized algorithms but it gets more\ncomplicated). We assume that we have some particular comparison\nsorting algorithm A, but that we don't know anything more about how\nit runs. Using that assumption, we'll prove that the worst case\ntime for A has to be at least a certain amount, but since the only\nassumption we make on A is that it's a comparison sorting\nalgorithm, this fact will be true for all such algorithms.</p>\n\n<h2>Decision trees</h2>\n\nGiven a comparison sorting algorithm A, and some particular number\nn, we draw a tree corresponding to the different sequences of\ncomparisons A might make on an input of length n. \n\n<p>If the first comparison the algorithm makes is between the\nobjects at positions a and b, then it will make the same comparison\nno matter what other list of the same length is input, because in\nthe comparison model we do not have any other information than n so\nfar on which to make a decision.</p>\n\n<p>Then, for all lists in which a&lt;b, the second comparison will\nalways be the same, but the algorithm might do something different\nif the result of the first comparison is that a&gt;b.</p>\n\n<p>So we can draw a tree, in which each node represents the\npositions involved at some comparison, and each path in the tree\ndescribes the sequence of comparisons and their results from a\nparticular run of the algorithm. Each node will have two children,\nrepresenting the possible behaviors of the program depending on the\nresult of the comparison at that node. Here is an example for\nn=3.</p>\n\n<pre>\n                  1:2\n                /     \\\n             &lt; /     &gt; \\\n              /         \\\n           2:3           1:3\n           / \\           / \\\n        &lt; / &gt; \\       &lt; / &gt; \\\n         /     \\       /     \\\n      1,2,3    1:3  2,1,3    2:3\n               / \\           / \\\n            &lt; / &gt; \\       &lt; / &gt; \\\n             /     \\       /     \\\n          1,3,2   3,1,2 2,3,1   3,2,1\n</pre>\n\nThis tree describes an algorithm in which the first comparison is\nalways between the first and second positions in the list (this\ninformation is denoted by the \"1:2\" at the root of the tree). If\nthe object in position one is less than the object in position two,\nthe next comparison will always be between the second and third\npositions in the list (the \"2:3\" at the root of the left subtree).\nIf the second is less than the third, we can deduce that the input\nis already sorted, and we write \"1,2,3\" to denote the permutation\nof the input that causes it to be sorted. But if the second is\ngreater than the third, there still remain two possible\npermutations to be distinguished between, so we make a third\ncomparison \"1:3\", and so on. \n\n<p>Any comparison sorting algorithm can always be put in this form,\nsince the comparison it chooses to make at any point in time can\nonly depend on the answers to previously asked comparisons. And\nconversely, a tree like this can be used as a sorting algorithm:\nfor any given list, follow a path in the tree to determine which\ncomparisons to be made and which permutation of the input gives a\nsorted order. This is a reasonable way to represent algorithms for\nsorting very small lists (such as the case n=3 above) but for\nlarger values of n it works better to use pseudo-code. However this\ntree is also useful for discovering various properties of our\noriginal algorithm A.</p>\n\n<ul>\n<li>The worst case number of comparisons made by algorithm A is\njust the longest path in the tree.</li>\n\n<li>One can also determine the average case number of comparisons\nmade, but this is more complicated.</li>\n\n<li>At each leaf in the tree, no more comparisons to be made --\ntherefore we know what the sorted order is. Each possible sorted\norder corresponds to a permutation, so there are at least n!\nleaves. (There might be more if for instance we have a stupid\nalgorithm that tests whether a&lt;c even after it has already\ndiscovered that a&lt;b and b&lt;c).</li>\n</ul>\n\n<h2>The sorting lower bound</h2>\n\nWhat is longest path in binary tree with k leaves? At least log k.\n(Proof: one of the two subtrees has at least half the leaves so\nLP(k) &gt;= 1 + LP(k/2); the result follows by induction.) \n\n<p><a name=\"omega\">So the number of comparisons to sort is at least\nlog n!. This turns out to be roughly n log n; to distinguish lower\nbounds from upper bounds we write them a little differently, with a\nbig Omega rather than a big O, so we write this lower bound as\nOmega(n log n).</a> More precisely,</p>\n\n<pre>\n    log n! = n log n - O(n).\n</pre>\n\nA reasonably simple proof follows: \n\n<pre>\n        n\n    n! = product i\n       i=1\n</pre>\n\nso \n\n<pre>\n          n\n    log n! = sum log i\n         i=1\n\n          n\n       = sum log (n i/n)\n         i=1\n\n          n\n       = sum (log n - log n/i)\n         i=1\n\n            n\n       = n log n - sum log n/i .\n               i=1\n</pre>\n\nLet f(n) be the last term above, sum log(n/i); then we can write\ndown a recurrence bounding f(n): \n\n<pre>\n        n\n    f(n) = sum log n/i\n       i=1\n\n       n/2             n\n    f(n) = sum log n/i +  sum  log n/i\n       i=1          i=n/2+1\n</pre>\n\nAll of the terms in the first sum are equal to log 2((n/2)/i) = 1 +\nlog((n/2)/i), and all of the terms in the second sum are logs of\nnumbers between 1 and 2, and so are themselves numbers between 0\nand 1. So we can simplify this equation to \n\n<pre>\n        n/2\n    f(n) &lt;= n + sum log (n/2)/i\n        i=1\n\n     = n + f(n/2)\n</pre>\n\nwhich solves to 2n and completes the proof that log n! &gt;= n log\nn - 2n. \n\n<p>(Note: in class I got this argument slightly wrong and lost a\nfactor of two in the recurrence for f(n).) We can get a slightly\nmore accurate formula from Sterling's formula (which I won't\nprove):</p>\n\n<pre>\n    n! ~ sqrt(pi/n) (n/e)^n\n</pre>\n\nso \n\n<pre>\n    log n! ~ n log n - 1.4427 n - 1/2 log n + .826\n</pre>\n\nLet's compute a couple examples to see how accurate this is: \n\n<pre>\n        log n!          formula gives\n    n=10    21.8            33.22 - 14.43 ~ 18.8\n    n=100   524.8           664.4 - 144.3 ~ 520.1\n</pre>\n\nEnough math, let's do some actual algorithms. \n\n<h2>Selection sort</h2>\n\nTo understand heap sort, let's start with selection sort. An\nexperiment: I write a list of numbers, once I'm done you tell me\nsorted order. \n\n<pre>\n    5,2,100,19,22,7\n</pre>\n\nHow did you go about finding them? You probably looked through the\nlist for the first number, then looked through it again for the\nnext one, etc. One way of formalizing this process is called <i>\nselection sort</i>: \n\n<pre>\n    selection sort(list L)\n    {\n    list X = empty\n    while L nonempty\n    {\n        remove smallest element of L\n        and add it to X\n    }\n    }\n</pre>\n\nTime analysis: there is one loop, executed n times. But the total\ntime is not O(n). Remember we are counting comparisons. \"Remove the\nsmallest element of L\" could take many comparisons. We need to look\nmore carefully at this part of the loop. (The other part, adding an\nelement to X, also depends on how we store X, but can be done in\nconstant time for most reasonable implementations and in any case\ndoesn't require any comparisons, which is what we're counting.) \n\n<p>The obvious method of finding (and removing) the smallest\nelement: scan L and keep track of the smallest object. So this\nproduces a nested inner loop, time = O(length of L) so total time =\nO(sum i) = O(n^2). This is one of the slow algorithms. In fact it\nis as slow as possible: it always makes every possible comparison.\nWhy am I describing it when there are so many better\nalgorithms?</p>\n\n<h2>Heap sort</h2>\n\nHeap sort (invented by <a href=\"people.html\">J.R.J. Williams</a>)\nlooks exactly like the pseudo-code above for selection sort, and\nsimply uses some data structures to perform the main step of\nselection sort more quickly. \n\n<p>The operations we need to perform are</p>\n\n<ul>\n<li>Starting with a list L and turning it into a copy of whatever\ndata structure we're using,</li>\n\n<li>Finding the smallest object in the data structure, and</li>\n\n<li>Removing the smallest element</li>\n</ul>\n\nThere are many suitable data structures, for instance the AVL trees\nstudied in ICS 23. We'll describe here a structure called a <i>\nbinary heap</i>. A heap also supports other possible operations,\nsuch as adding objects to the list; that's not useful in this\nalgorithm but maybe later. (We will see heaps again when we talk\nabout minimum spanning trees and shortest paths.) \n\n<p>Simple analysis of heap sort: if we can build a data structure\nfrom our list in time X and finding and removing the smallest\nobject takes time Y then the total time will be O(X + nY). In our\ncase X will be O(n) and Y will be O(log n) so total time will be\nO(n + n log n) = O(n log n)</p>\n\n<h2>Heap data structure</h2>\n\nWe form a binary tree with certain properties: \n\n<ul>\n<li>The elements of L are placed on the nodes of the tree; each\nnode holds one element and each element is placed on one node.</li>\n\n<li>The tree is <i>balanced</i> which as far as I'm concerned means\nthat all paths have length O(log n); Baase uses a stronger property\nin which no two paths to a leaf differ in length by more than one.\n<a name=\"heapprop\"></a></li>\n\n<li>(The <i>heap property</i>): If one node is a parent of another,\nthe value at the parent is always smaller than the value at the\nchild.</li>\n</ul>\n\nYou can think of the heap property as being similar to a property\nof family trees -- a parent's birthday is always earlier than his\nor her childrens' birthdays. As another example, in a corporate\nhierarchy, the salary of a boss is (almost) always bigger than that\nof his or her underlings. \n\n<p>You can find the smallest heap element by looking at root of the\ntree (e.g. the boss of whole company has the biggest salary); this\nis easy to see, since any node in a tree has a smaller value than\nall its descendants (by transitivity).</p>\n\n<p>How to remove it? Say the company boss quits. How do we fill his\nplace? We have promote somebody. To satisfy the heap property, that\nwill have to be the person with the biggest salary, but that must\nbe one of his two direct underlings (the one of the two with the\nbigger salary). Promoting this person then leaves a vacancy lower\ndown that we can fill the same sort of way, and so on. In\npseudo-code:</p>\n\n<pre>\n    remove_node(node x):\n    {\n    if (x is a leaf) delete it\n    else if (no right child or left &lt; right)\n    {\n        move value at left child to x\n        remove_node(left child)\n    }\n    else if (no left child or right &lt; left)\n    {\n        move value at right child to x\n        remove_node(right child)\n    }\n    }\n</pre>\n\n(Baase has a more complicated procedure since she wants to maintain\na stronger balanced tree property. Essentially the idea is to pick\nsomeone at the bottom of the tree to be the new root, notice that\nthat violates the heap property, and trade that value with its best\nchild until it no longer causes a violation. This results in twice\nas many comparisons but has some technical advantages in terms of\nbeing able to store the heap in the same space as the sorted list\nyou're constructing.) \n\n<p>The number of comparison steps in this operation is then just\nthe length of the longest path in the tree, O(log n).</p>\n\n<p>This fits into the comparison sorting framework because the only\ninformation we use to determine who should be promoted is to\ncompare pairs of objects..</p>\n\n<p>The total number of comparisons in heapsort is then O(n log n) +\nhow much time it takes to set up the heap.</p>\n\n<h2>How to build heap?</h2>\n\nI ran out of time, we'll have to see this <a href=\"960118.html\">\nnext time</a>. \n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960116.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}