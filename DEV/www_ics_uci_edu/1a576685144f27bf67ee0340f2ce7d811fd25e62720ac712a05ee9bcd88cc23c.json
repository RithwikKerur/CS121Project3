{"url": "https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/paper3", "content": ".pn 0\n.ls1\n.EQ\ndelim $$\n.EN\n.ev1\n.ps-2\n.vs-2\n.ev\n\\&\n.sp 10\n.ps+4\n.ce\nIN SEARCH OF ``AUTONOMY''\n.ps-4\n.sp4\n.ce\nIan H. Witten\n.sp2\n.ce4\nDepartment of Computer Science\nThe University of Calgary\n2500 University Drive NW\nCalgary, Canada T2N 1N4\n.sp2\n.sh \"Abstract\"\n.pp\nThis paper examines the concept of autonomy as it pertains to computer\nsystems.\nTwo rather different strands of meaning are identified.\nThe first regards autonomy as self-government or self-motivation.\nThis is developed by reviewing some recent AI research on representing and\nusing goals, together with physiological, psychological, and philosophical\nviewpoints on motivation and goal-seeking behavior.\nThe second concerns the biological independence of organisms which have the\nability to maintain their own organization in a capricious environment.\nThe advantages of such organisms have been realized recently in a number of\ndifferent computer contexts, and the examples of worm programs,\nself-replicating Trojan horses and viruses are introduced and discussed.\n.bp 1\n.ls2\n.sh \"Introduction\"\n.pp\nWhat does it mean for a machine to be autonomous?\nHas any progress been made towards autonomous machines since Grey Walter's\nfamous \\fIM.\\ Speculatrix\\fR\\u1\\d (Walter, 1953)?\n.[\nWalter 1953 living brain\n.]\n.FN\n1.\\ \\ for the discerning, or ``tortoise'' for the profane, as its inventor\ntook pains to point out.\n.EF\nIn a narrow sense it is clear that there has, as evidenced by the evolution of\nthe \\fIM.\\ Labyrinthea\\fR species (of which Claude Shannon constructed an\nearly example) into the fleet-footed trial-and-error goal\nseeking devices seen in successive generations of the IEEE Micromice\ncompetition.\nHowever, these devices have a predictable course and a predestined end,\nproviding an excellent example of the old argument against artificial\nintelligence that ``reliable computers do only what they are instructed to\ndo''.\nIn this paper we seek autonomy in some deeper sense.\n.pp\nIt is not surprising that dictionary definitions of autonomy concentrate on\nnatural systems.\nAccording to the Oxford dictionary, it has two principal strands of meaning:\n.LB \"\\fBAutonomy\\fR  1.  \\fBa\\fR  \"\n.NI \"\\fBAutonomy\\fR  1.  \\fBa\\fR  \"\n\\fBAutonomy\\fR\\ \\ 1.\\ \\ Of a state, institution, etc\n.NI \"\\fBa\\fR  \"\n\\fBa\\fR\\ \\ The right of self-government, of making its own laws and\nadministering its own affairs\n.NI \"\\fBb\\fR  \"\n\\fBb\\fR\\ \\ Liberty to follow one's will, personal freedom\n.NI \"\\fBc\\fR  \"\n\\fBc\\fR\\ \\ Freedom (of the will): the Kantian doctrine of the Will giving\nitself its own law, apart from any object willed; opposed to \\fIheteronomy\\fR\n.NI \"1.  \\fBa\\fR  \"\n2.\\ \\ \\fIBiol.\\fR  autonomous condition\n.NI \"\\fBa\\fR  \"\n\\fBa\\fR\\ \\ The condition of being controlled only by its own laws, and not\nsubject to any higher one\n.NI \"\\fBb\\fR  \"\n\\fBb\\fR\\ \\ Organic independence\n.LE \"\\fBAutonomy\\fR  1.  \\fBa\\fR  \"\nOur interest here lies in practical aspects of autonomy as opposed to\nphilosophical ones.\nConsequently we will steer clear of the debate on free will and what it means\nfor machines, simply noting in passing that some dismiss the problem out of\nhand.\nFor instance, Minsky (1961) quotes with approval McCulloch (1954) that our\n\\fIfreedom of will\\fR ``presumably means no more than that we can distinguish\nbetween what we intend (ie our \\fIplan\\fR), and some intervention in our\naction''\\u2\\d.\n.FN\n2.\\ \\ This seems to endow free will to a Micromouse which, having mapped the\nmaze, is following its plan the second time round when it finds a new\nobstacle!\n.EF\n.[\nMinsky 1961 steps toward artificial intelligence\n.]\n.[\nMcCulloch 1954\n.]\nWe also refrain from the potentially theological considerations of what is\nmeant by ``higher'' laws in the second part.\n.pp\nHow can we interpret what is left of the definition?\nIn terms of modern AI, the first meaning can best be read as\nself-government through goal-seeking behavior,\nsetting one's own goals, and choosing which way to pursue them.\nThe second meaning, organic independence, has been the subject of major debate\nin the biological and system-theoretic community around the concepts of\n``homeostasis'' and, more recently, ``autopoiesis''.\n.pp\nOur search in this paper will pursue these strands separately.\nGoals and plans have received much attention in AI, both from the point of\nview of understanding (or at least explaining) stories involving human goals\nand how they can be achieved or frustrated, and in purely artificial systems\nwhich learn by discovery.\nBiologists and psychologists have studied goal-seeking behavior in people,\nand come to conclusions which seem to indicate remarkable similarities with\nthe approach taken by current AI systems to setting and pursuing goals.\nOn the other side of the coin, there are strong arguments that these\nsimilarities should be viewed with a good deal of suspicion.\n.pp\nThe second strand of meaning, organic independence, has not been contemplated\nexplicitly in mainstream computer science.\nThere have been a number of well-known developments on the periphery of\nthe subject which do involve self-replicating organisms.\nExamples include games such as ``life'' (Berlekamp \\fIet al\\fR, 1982) and\n``core wars'' (Dewdney, 1984), as well as\ncellular (eg Codd, 1968), self-reproducing (eg von Neumann, 1966),\nand evolutionary (eg Fogel \\fIeg al\\fR, 1966) automata.\n.[\nDewdney 1984\n.]\n.[\nBerlekamp Conway Guy 1982\n.]\n.[\nCodd 1968 cellular automata\n.]\n.[\nvon Neumann 1966 self-reproducing automata\n.]\n.[\nFogel Owens Walsh 1966\n.]\nHowever, these seem artificial and contrived examples of autonomy.\nIn contrast, some autonomous systems have recently arisen naturally in\ncomputer software.\nWe examine the system-theoretic idea of ``autopoiesis'' and then look at these\nsoftware developments in this context.\n.sh \"Goal-seeking \\(em artificial and natural\"\n.pp\nIn a discussion of robots and emotions, Sloman and Croucher (1981) note that\nmany people deny that machines could ever be said to have their own goals.\n``Machines hitherto familiar to us either are not goal-directed at all\n(clocks, etc) or else, like current game-playing computer programs,\nhave a simple hierarchical set of goals, with the highest-level goal put there\nby a programmer''.\n.[\nSloman Croucher 1981 robots emotions\n.]\nThey postulate that robots will need \\fImotive generators\\fR to allow them\nto develop a sufficiently rich structure of goals; unfortunately they do not\nsay how such generators might work.\nTo exemplify how goals are used in existing AI programs, we will briefly\nreview two lines of current research.\n.rh \"Examples of artificial goal-seeking.\"\nThose working on conceptual dependency in natural language understanding have\nlong recognized that stories cannot be understood without knowing about the\ngoal-seeking nature of the actors involved.\nSchank & Abelson (1977) present a taxonomy of human goals, noting that\ndifferent attempts at classification present a confusing array of partially\noverlapping constructs and suggesting that some future researcher might\nsucceed in bringing order out of the chaos using methods such as cluster\nanalysis.\n.[\nSchank Abelson 1977\n.]\nThey postulate the following seven goal forms:\n.LB\n.NP\nSatisfaction goal \\(em a recurring strong biological need\n.br\nExamples:  \\fIhunger\\fR, \\fIsex\\fR, \\fIsleep\\fR\n.NP\nEnjoyment goal \\(em an activity which is optionally pursued for enjoyment or\nrelaxation\n.br\nExamples:  \\fItravel\\fR, \\fIentertainment\\fR, \\fIexercise\\fR\n(in addition, the activities implied by some satisfaction goals may\nalternatively be pursued primarily for enjoyment)\n.NP\nAchievement goal \\(em the realization (often over a long term) of some valued\nacquisition or social position\n.br\nExamples:  \\fIpossessions\\fR, \\fIgood job\\fR, \\fIsocial relationships\\fR\n.NP\nPreservation goal \\(em preserving or improving the health, safety, or good\ncondition of people, position, or property\n.br\nExamples:  \\fIhealth\\fR, \\fIgood eyesight\\fR\n.NP\nCrisis goal \\(em a special class of preservation goal set up to handle serious\nand imminent threats.\n.br\nExamples:  \\fIfire\\fR, \\fIstorm\\fR\n.NP\nInstrumental goal \\(em occurs in the service of any of the above goals to\nrealize a precondition\n.br\nExamples:  \\fIget babysitter\\fR\n.NP\nDelta goal \\(em similar to instrumental goal except that general planning\noperations instead of scripts are involved in its pursuit\n.br\nExamples:  \\fIknow\\fR, \\fIgain-proximity\\fR, \\fIgain-control\\fR.\n.LE\nThe first three involve striving for desired states;\nthe next two, avoidance of undesired states;\nthe last two, intermediate subgoals for any of the other five forms.\nPrograms developed within this framework ``understand'' (ie can answer\nquestions about) stories involving human actors with these goals\n(eg Wilensky, 1983; Dyer, 1983).\n.[\nWilensky 1983 Planning and understanding\n.]\n.[\nDyer 1983 in-depth understanding MIT Press\n.]\nFor example, if John goes to a restaurant it is likely that he is attempting\nto fulfill either a satisfaction goal or an entertainment goal (or both).\nInstrumental or delta goals will be interpreted in the context of the\nprevailing high-level goal.\nIf John takes a cab to the restaurant it will be understood that he is\nachieving the delta goal \\fIgain-proximity\\fR in service of his satisfaction\nor entertainment goal.\n.pp\nOur second example of goal usage in contemporary AI is Lenat's ``discovery''\nprogram \\s-2AM\\s+2, and its successor \\s-2EURISKO\\s+2 (Davis & Lenat, 1982;\nLenat \\fIet al\\fR, 1982).\n.[\nDavis Lenat 1982\n.]\n.[\nLenat Sutherland Gibbons 1982\n.]\nThese pursue interesting lines of research in the domains of\nelementary mathematics and VLSI design heuristics, respectively.\nThey do this by exploring concepts \\(em producing examples, generalizing,\nspecializing, noting similarities, making plausible hypotheses and\ndefinitions, etc.\nThe programs evaluate these discoveries for utility and ``interestingness,''\nand add them to the vocabulary of concepts.\nThey essentially perform exploration in an enormous search space, governed\nby heuristics which evaluate the results and suggest fruitful avenues for\nfuture work.\n.pp\nEach concept in these systems is represented by a frame-like data structure\nwith dozens of different facets or slots.\nThe types of facets in \\s-2AM\\s+2 include\n.LB\n.NP\nexamples\n.NP\ndefinitions\n.NP\ngeneralizations\n.NP\ndomain/range\n.NP\nanalogies\n.NP\ninterestingness.\n.LE\nHeuristics are organized around the facets.\nFor example, the following strategy fits into the \\fIexamples\\fR facet\nof the \\fIpredicate\\fR concept:  \\c\n.sp\n.BQ\nIf, empirically, 10 times as many elements\n.ul\nfail\nsome predicate P as\n.ul\nsatisfy\nit, then some\n.ul\ngeneralization\n(weakened version) of P might be more interesting than P.\n.FQ\n.sp\n\\s-2AM\\s+2 considers this suggestion after trying to fill in examples of each\npredicate.\nFor instance, when the predicate \\s-2SET-EQUALITY\\s+2 is investigated, so few\nexamples are found that \\s-2AM\\s+2 decides to generalize it.\nThe result is the creation of a new predicate which means\n\\s-2HAS-THE-SAME-LENGTH-AS\\s+2 \\(em a rudimentary precursor to the discovery\nof natural numbers.\n.pp\nIn an unusual and insightful retrospective on these programs,\nLenat & Brown (1984) report that the exploration consists of (mere?) syntactic\nmutation of programs expressed in certain representations.\n.[\nLenat Brown 1984\n.]\nThe key element of the approach is to find representations with a high\ndensity of interesting concepts so that many of the random mutations will be\nworth exploring.\nIf the representation is not well matched to the problem domain, most\nexplorations will be fruitless and the method will fail.\n.pp\nWhile the conceptual dependency research reviewed above is concerned with\nunderstanding the goals of actors in stories given to a program, the approach\ntaken seems equally suited to the construction of artificial goal-oriented\nsystems.\nIf a program could really understand or empathize with the motives of people,\nit seems a small technical step to turn it around to create an autonomous\nsimulation with the same motivational structure.\nIndeed, one application of the conceptual dependency framework is in\n\\fIgenerating\\fR coherent stories by inventing goals for the actors, choosing\nappropriate plans, and simulating the frustration or achievement of the goals\n(Meehan, 1977).\n.[\nMeehan 1977 talespin\n.]\nThe ``learning by discovery'' research shows how plausible subgoals can be\ngenerated from an overall goal of maximizing the interestingness of\nthe concepts being developed.\nIt is worth noting that Andreae (1977) chose a similar idea, ``novelty,''\nas the driving force behind a very different learning system.\n.[\nAndreae 1977 thinking with the teachable machine\n.]\nRandom mutation in an appropriate representation seems to be the closest we\nhave come so far to the \\fImotive generator\\fR mentioned at the beginning of\nthis section.\n.rh \"The mechanism and psychology of natural goal-seeking.\"\nNow turn to natural systems.\nThe objection to the above-described use of goals in natural language\nunderstanders and discovery programs is that they are just programmed in.\nThe computer only does what it is told.\nIn the first case, it is told a classification of goals and given\ninformation about their interrelationships, suitable plans for achieving them,\nand so on.\nIn the second case it is told to maximize interestingness by random\nmutation.\nOn the surface, these seem to be a pale reflection of the autonomous\nself-government of natural systems.\nBut let us now look at how goals seem to arise in natural systems.\n.pp\nThe eminent British anatomist J.Z.\\ Young describes the modern biologist's\nhighly mechanistic view of the basic needs of animals.\n.[\nYoung 1978 programs of the brain\n.]\n``Biologists no longer believe that living depends upon some special\nnon-physical agency or spirit,'' he avers (Young, 1978, p.\\ 13), and goes on\nto claim that we now understand how it comes about that organisms behave as if\nall their actions were directed towards an aim or goal\\u3\\d.\n.FN\n3.\\ \\ Others apparently tend to be more reticent \\(em\n``it has been curiously unfashionable among biologists to call attention to\nthis characteristic of living things'' (Young, 1978, p.\\ 16).\n.EF\nThe mechanism for this is the reward system situated in the hypothalamus.\nFor example, the cells of the hypothalamus ensure that the right amount of\nfood and drink are taken and the right amount is incorporated to allow the\nbody to grow to its proper size.\nThese hypothalamic centers stimulate the need for what is lacking, for\ninstance of food, sex, or sleep, and they indicate satisfaction when enough\nhas been obtained.\nMoreover, the mechanism has been traced to a startling level of detail.\nFor example, Young describes how hypothalamic cells can be\nidentified which regulate the amount of water in the body.\n.sp\n.BQ\nThe setting of the level of their sensitivity to salt provides the\ninstruction that determines the quantity of water that is held in the body.\nWe can say that the properties of these cells are physical symbols\n``representing'' the required water content.\nThey do this in fact by actually swelling or shrinking when the salt\nconcentration of the blood changes.\n.FQ \"Young, 1978, p.\\ 135\"\n.sp\nFood intake is regulated in the same way.\nThe hypothalamus ensures propagation of the species by directing reproductive\nbehavior and, along with neighboring regions of the brain, attends to the goal\nof self-preservation by allowing us to defend ourselves if attacked.\n.pp\nNeedless to say, experimental evidence for this is obtained primarily from\nanimals.\nDo people's goals differ?\nThe humanistic psychologist Abraham Maslow propounded a theory of human\nmotivation that distinguishes between different kinds of needs (Maslow, 1954).\n.[\nMaslow 1954\n.]\n\\fIBasic needs\\fR include hunger, affection, security, love, and self-esteem.\n\\fIMetaneeds\\fR include justice, goodness, beauty, order, and unity.\nBasic needs are arranged in a hierarchical order so that some are stronger\nthan others (eg security over love); but all are generally stronger than\nmetaneeds.\nThe metaneeds have equal value and no hierarchy, and one can be substituted\nfor another.\nLike the basic needs, the metaneeds are inherent in man, and when they are not\nfulfilled, the person may become psychologically sick (suffering, for example,\nfrom alienation, anguish, apathy, or cynicism).\n.pp\nIn his later writing, Maslow (1968) talks of a ``single ultimate value for\nmankind, a far goal towards which all men strive''.\nAlthough going under different names (Maslow favors \\fIself-actualization\\fR),\nit amounts to ``realizing the potentialities of the person, that is to say,\nbecoming fully human, everything that the person \\fIcan\\fR become''.\nHowever, the person does not know this.\nAs far as he is concerned, the individual needs are the driving force.\nHe does not know in advance that he will strive on after the current need\nhas been satisfied.\nMaslow produced the list of personality characteristics of the psychologically\nhealthy person shown in Table\\ 1.\n.RF\n.in 0.5i\n.ll -0.5i\n.nr x0 \\n(.l-\\n(.i\n\\l'\\n(x0u'\n.in +\\w'\\(bu 'u\n.fi\n.NP\nThey are realistically oriented.\n.NP\nThey accept themselves, other people, and the natural world for what they are.\n.NP\nThey have a great deal of spontaneity.\n.NP\nThey are problem-centered rather than self-centered.\n.NP\nThey have an air of detachment and a need for privacy.\n.NP\nThey are autonomous and independent.\n.NP\nTheir appreciation of people and things is fresh rather than stereotyped.\n.NP\nMost of them have had profound mystical or spiritual experiences although not\nnecessarily religious in character.\n.NP\nThey identify with mankind.\n.NP\nTheir intimate relationships with a few specially loved people tend to be\nprofound and deeply emotional rather than superficial.\n.NP\nTheir values and attitudes are democratic.\n.NP\nThey do not confuse means with ends.\n.NP\nTheir sense of humor is philosophical rather than hostile.\n.NP\nThey have a great fund of creativeness.\n.NP\nThey resist conformity to the culture.\n.NP\nThey transcend the environment rather than just coping with it.\n.nf\n.in -\\w'\\(bu 'u\n\\l'\\n(x0u'\n.ll +1i\n.in 0\n.FE \"Table 1: Characteristics of self-actualized persons (Maslow, 1954)\"\n.pp\nMaslow's \\fIbasic needs\\fR seem to correspond reasonably closely with those\nidentified by conceptual dependency theory.\nMoreover, there is some similarity to the goals mentioned by Young (1978),\nwhich, as we have seen, are thought to be ``programmed in'' to the brain in an\nastonishingly literal sense.\nConsequently it is not clear how programs in which these goals are embedded\ndiffer in principle from goal-oriented systems in nature.\nThe \\fImetaneeds\\fR are more remote from current computer systems,\nalthough there have been shallow attempts to simulate paranoia in the\n\\s-2PARRY\\s+2 system (Colby, 1973).\n.[\nColby 1973 simulations of belief systems\n.]\nIt is intriguing to read Table\\ 1 in the context of self-actualized computers!\nMoreover, one marvels at the similarity between the single-highest-goal model\nof people in terms of self-actualization, and the architecture for discovery\nprograms sketched earlier in terms of a quest for ``interestingness''.\n.rh \"The sceptical view.\"\nThe philosopher John Haugeland addressed the problem of natural language\nunderstanding and summed up his viewpoint in the memorable aphorism,``the\ntrouble with Artificial Intelligence is that computers don't give a damn''\n(Haugeland, 1979).\n.[\nHaugeland 1979 understanding natural language\n.]\nHe identified four different ways in which brief segments of text cannot be\nunderstood ``in isolation'', which he called four \\fIholisms\\fR.\nTwo of these, concerning \\fIcommon-sense knowledge\\fR and\n\\fIsituational knowledge\\fR,\nare the subject of intensive research in natural language analysis systems.\nAnother, the \\fIholism of intentional interpretation\\fR,\nexpresses the requirement that utterances and descriptions ``make sense'' and\nseems to be at least partially addressed by the goal/plan orientation of some\nnatural language systems.\nIt is the fourth, called \\fIexistential holism\\fR, that is most germane to the\npresent topic.\nHaugeland argues that one must have actually \\fIexperienced\\fR emotions (like\nembarrassment, relief, guilt, shame) to understand\n``the meaning of text that (in a familiar sense) \\fIhas\\fR any meaning''.\nOne can only experience emotions in the context of one's own self-image.\nConsequently, Haugeland concludes that\n``only a being that cares about who it is, as some sort of enduring whole,\ncan care about guilt or folly, self-respect or achievement, life or death.\nAnd only such a being can read.''  Computers just don't give a damn.\n.pp\nAs AI researchers have pointed out repeatedly, however, it is difficult to\ngive such arguments \\fIoperational\\fR meanings.\nHow could one test whether a machine has \\fIexperienced\\fR an emotion like\nembarrassment?\nIf it acts embarrassed, isn't that enough?\nAnd while machines cannot yet behave convincingly as though they do experience\nemotions, it is not clear that fundamental obstacles stand in the way of\nfurther and continued progress.\nThere seems to be no reason in principle why a machine cannot be given a\nself-image.\n.pp\nThis controversy has raged back and forth for decades, a recent resurgence\nbeing Searle's (1980) paper on the Chinese room, and the 28 responses which\nwere published with it.\n.[\nSearle 1980 minds programs\n.]\nSearle considered the following \\fIgedanken\\fP experiment.\nSuppose someone, who knows no Chinese (or any related language), is locked in\na room and given three large batches of Chinese writing, together with a\nset of rules in English which allow her to correlate the apparently\nmeaningless squiggles in the three batches and to produce certain sorts of\nshapes in response to certain sorts of shapes which may appear in the third\nbatch.\nUnknown to her, the experimenters call the first batch a ``script'', the\nsecond batch a ``story'', the third batch ``questions'', and the symbols\nshe produces ``answers''.\nWe will call the English rules a ``program'', and of course the intention is\nthat, when executed, sensible and appropriate Chinese answers, based on the\nChinese script, are generated to the Chinese questions about the Chinese\nstory.\nBut the subject, with no knowledge of Chinese, does not see them that way.\nThe question is, given that with practice the experimenters become so adept\nat writing the rules and the subject so adept at interpreting them\nthat the resulting answers are indistinguishable from those generated by a\nnative Chinese speaker, does the subject ``understand'' the stories?\nTo summarize a large and complex debate in a few words, Searle says no; while\nmany AI researchers say yes, or at least that the subject-plus-rules system\nunderstands.\n.pp\nSearle states his thesis succinctly:  ``such intentionality as computers\nappear to have is solely in the minds of those who program them and those who\nuse them, those who send in the input and those who interpret the output''.\nAnd the antithesis could be caricatured as\n``maybe, but does it \\fImatter?\\fR''.\nThose who find the debate frustrating can always, with\nSloman & Croucher (1981), finesse the issue:  \\c\n``Ultimately, the decision whether to say such machines have motives is a\n\\fImoral\\fR decision, concerned with how we ought to treat them''.\n.[\nSloman Croucher 1981 robots emotions\n.]\n.sh \"Autopoiesis \\(em natural and artificial\"\n.pp\nAutonomy is a striking feature of biological systems.\nNot surprisingly, some biologists have made strenuous attempts to articulate\nwhat it means to them; to pin it down, formalize and study it in a\nsystem-theoretic context.\nHowever, this work is obscure and difficult to assess in terms of its\npredictive power (which must be the fundamental test of any theory).\nEven as a descriptive theory its use is surrounded by controversy.\nConsequently this section attempts to give the flavor of the endeavor, relying\nheavily on quotations from the major participants in the research, and goes on\nto describe some practical computer systems which appear to satisfy the\ncriteria biologists have identified for autonomy.\n.rh \"Homeostasis.\"\nPeople have long expressed wonder at how a living organism maintains its\nidentity in the face of continuous change.\n.sp\n.BQ\nIn an open system, such as our bodies represent, compounded of unstable\nmaterial and subjected continuously to disturbing conditions, constancy is\nin itself evidence that agencies are acting or ready to act, to maintain this\nconstancy.\n.FQ \"Cannon, 1932\"\n.sp\n.[\nCannon 1932 wisdom of the body\n.]\nFollowing Cannon, Ashby (1960) developed the idea of ``homeostasis'' to\naccount for this remarkable ability to preserve stability under conditions of\nchange.\n.[\nAshby 1960 design for a brain\n.]\nThe word has now found its way into North American dictionaries, eg Webster's\n.sp\n.BQ\nHomeostasis is the tendency to maintain, or the maintenance of, normal,\ninternal stability in an organism by coordinated responses of the organ\nsystems that automatically compensate for environmental changes.\n.FQ\n.sp\nThe basis for homeostasis was adaptation by the organism.\nWhen change occurred, the organism adapted to it and thus preserved its\nconstancy.\n.sp\n.BQ\nA form of behavior is \\fIadaptive\\fR if it maintains the essential variables\nwithin physiological limits.\n.FQ \"Ashby, 1960, p. 58\"\n.sp\nThe ``essential variables'' are closely related to survival and linked\ntogether dynamically so that marked changes in any one soon lead to changes in\nthe others.\nExamples are pulse rate, blood pressure, body temperature, number of\nbacteria in the tissue, etc.\nAshby went so far as to construct an artifact, the ``Homeostat'', which\nexhibits this kind of ultrastable equilibrium.\n.pp\nHomeostasis emphasizes the stability of biological systems under external\nchange.\nRecently, a concept called ``autopoiesis'' has been identified, which\ncaptures the essence of biological autonomy in the sense of stability or\npreservation of identity under \\fIinternal\\fR change\n(Maturana, 1975; Maturana & Varela, 1980; Varela, 1979; Zeleny, 1981).\n.[\nMaturana 1975 organization of the living\n.]\n.[\nMaturana Varela 1980 autopoiesis\n.]\n.[\nVarela 1979 biological autonomy\n.]\n.[\nZeleny 1981 Editor Autopoiesis  a theory of living organization\n.]\nThis has aroused considerable interest, and controversy, in the system\ntheoretic research community.\n.rh \"Autopoiesis.\"\nThe neologism ``autopoiesis'' means literally ``self-production'', and a\nstriking example occurs in living cells.\nThese complex systems produce and synthesize macromolecules of proteins,\nlipids, and enzymes, and consist of about $10 sup 5$ macromolecules.\nThe entire population of a given cell is renewed about $10 sup 4$ times\nduring its lifetime (Zeleny, 1981a).\n.[\n%A Zeleny, M.\n%D 1981a\n%T What is autopoiesis?\n%E M.Zeleny\n%B Autopoiesis:  a theory of living organization\n%I North Holland\n%C New York\n%P 4-17\n.]\nDespite this turnover of matter, the cell retains its distinctiveness and\ncohesiveness \\(em in short, its \\fIautonomy\\fR.\nThis maintenance of unity and identity of the whole, despite the fact that\nall the while components are being created and destroyed, is called\n``autopoiesis''.\nA concise definition is\n.sp\n.BQ\nAutopoiesis is the capability of living systems to develop and maintain\ntheir own organization.\nThe organization that is developed and maintained is identical to that\nperforming the development and maintenance.\n.FQ \"Andrew, 1981, p. 156\"\n.sp\n.[\nAndrew 1981\n.]\nOther authors (eg Maturana & Varela, 1980; Zeleny, 1981a) add a corollary:\n.sp\n.BQ\na topological boundary emerges as a result of the processes [of development\nand maintenance].\n.FQ \"Zeleny, 1981a, p. 6\"\n.sp\nThis emphasizes the train of thought ``from self-production to identity''\nthat seems to underly much of the autopoietic literature.\n.pp\nOperating as a system which produces or renews its own components, an\nautopoietic system continuously regenerates its own organization.\nIt does this in an endless turnover of components and despite inevitable\nperturbations.\nTherefore autopoiesis is a form of homeostasis which has its own\norganization as the fundamental variable that remains constant.\nThe principal fascination of the concept lies in the self-reference it\nimplies,\nThis has stimulated a theoretical formulation of the notion of circularity or\nself-reference in Varela's (1975) extension of Brown's\n``calculus of distinctions'' (Brown, 1969).\n.[\n%A Varela, F.J.\n%D 1975\n%K *\n%T A calculus for self-reference\n%J Int J General Systems\n%V 2\n%N 1\n%P 5-24\n.]\n.[\nBrown 1969 Laws of Form\n.]\nAlong with other work on self-reference (eg Hofstadter, 1979), this\nhas an esoteric and obscure, almost mystical, quality.\n.[\nHofstadter 1979 Godel Escher Bach\n.]\nWhile it may yet form the basis of a profound paradigm shift in systems\nscience, it is currently surrounded by controversy and its potential\ncontribution is quite unclear (Gaines, 1981).\n.[\nGaines 1981 Autopoiesis some questions\n.]\nIndeed, it has been noted that an\n``unusual degree of parochialism, defensiveness, and quasi-theological\ndogmatism has arisen around autopoiesis'' (Jantsch, 1981).\n.[\nJantsch 1981 autopoiesis\n.]\n.pp\nThere has been considerable discussion of the relation between autopoiesis and\nconcepts such as purpose and information.\nVarela (1979) claims that\n``notions [of teleology and information] are unnecessary for the\n\\fIdefinition\\fR of the living organization, and that they belong to a\ndescriptive domain distinct from and independent of the domain in which the\nliving system's \\fIoperations\\fR are described'' (p.\\ 63/64).\nIn other words, nature is not about goals and information; we observers invent\nsuch concepts to help classify what we see.\nMaturana (1975) is more outspoken:  \\c\n``descriptions in terms of information transfer, coding and computations of\nadequate states are fallacious because they only reflect the observer's domain\nof purposeful design and not the dynamics of the system as a state-determined\nsystem'';\n.[\nMaturana 1975 organization of the living\n.]\npresumably goals are included too in the list of proscribed terms.\nSome have protested strongly against this hard-line view \\(em which is\nparticularly provocative because of its use of the word ``fallacious'' \\(em\nand attempted to reconcile it with ``the fact that the behavior of people and\nanimals is very readily and satisfactorily described in terms of goals and\nattempts to achieve them'' (Andrew, 1981, p. 158).\nIn his more recent work Varela (1981) diverged further from the hard-line\nview, explaining that he had intended to criticize only ``the \\fInaive\\fR use\nof information and purpose as notions that can enter into the definition of\na system on the same basis as material interactions'' [his emphasis].\n.[\nVarela 1981 describing the logic of the living\n.]\nHe concluded that ``autopoiesis, as an operational explanation, is not quite\nsufficient for a full understanding of the phenomenology of the living,\nand that it needs a carefully constructed complementary symbolic\nexplanation''.\nFor Varela, a symbolic explanation is one that is based on the notions of\ninformation and purpose.\nIt is clear, though, that while some allow that autopoiesis can \\fIcoexist\\fR\nwith purposive interpretations, it will not \\fIcontribute\\fR to them.\n.pp\nIs autopoiesis restricted to \\fIliving\\fR systems?\nSome authors find it attractive to extend the notion to the level of society\nand socio-political evolution (eg Beer, 1980; Zeleny, 1977).\n.[\nBeer 1980\n.]\n.[\nZeleny 1977\n.]\nOthers (eg Varela, 1981) stress the renewal of components through material\nself-production and restrict autopoiesis to chemical processes.\nWithout self-production in a material sense, the support for the corollary\nabove becomes unclear, and consequently the whole relevance of autopoiesis\nto identity and autonomy comes under question.\n.rh \"Artificial autopoiesis.\"\nAlthough one can point to computer simulations of very simple autopoietic\nsystems (eg Varela \\fIet al\\fR, 1974; Zeleny, 1978; Uribe, 1981), there seems\nto have been little study of artificially autopoietic systems in their own\nright.\n.[\nVarela Maturana Uribe 1974 autopoiesis characterization and model\n.]\n.[\nZeleny 1978 experiments in self-organization of complexity\n.]\nHowever there are examples of computer systems which are autopoietic and\nwhich have arisen ``naturally'', that is to say, were developed for other\npurposes and not as illustrations of autopoiesis.\nIt is probably true that in each case the developers were entirely unaware\nof the concept of autopoiesis and the interest surrounding it in system\ntheory circles.\n.pp\n.ul\nWorm programs\nwere an experiment in distributed computation (Shoch & Hupp, 1982).\n.[\nShoch Hupp 1982\n.]\nThe problem they addressed was to utilize idle time on a network of\ninterconnected personal computers without any impact on normal use.\nIt was necessary to be able to redeploy or unplug any machine at any time\nwithout warning.\nMoreover, in order to make the system robust to any kind of failure,\npower-down or ``I am dying'' messages were not employed in the protocol.\nA ``worm'' comprises multiple ``segments'', each running on a different\nmachine.\nSegments of the worm have the ability to replicate themselves in idle\nmachines.\nAll segments remain in communication with each other, thus preserving the\nworm's identity and distinguishing it from a collection of independent\nprocesses; however, all segments are peers and none is in overall control.\nTo prevent uncontrolled reproduction, a certain number of segments is\npre-specified as the target size of the worm.\nWhen a segment is corrupted or killed, its peers notice the fact because it\nfails to make its periodical ``I am alive'' report.\nThey then proceed to search for an idle machine and occupy it with another\nsegment.\nCare is taken to coordinate this activity so that only one new segment is\ncreated.\n.pp\nThere are two logical components to a worm.\nThe first is the underlying worm maintenance mechanism, which is responsible\nfor maintaining the worm \\(em finding free machines when needed and\nreplicating the program for each additional segment.\nThe second is the application part, and several applications have been\ninvestigated (Shoch & Hupp, 1982), such as\n.LB\n.NP\n.ul\nexistential\nworm that merely announces its presence on each computer it inhabits;\n.NP\n.ul\nbillboard\nworm that posts a graphic message on each screen;\n.NP\n.ul\nalarm clock\nworm that implements a highly reliable alarm clock that is not based on any\nparticular machine;\n.NP\n.ul\nanimation\nworm for undertaking lengthy computer graphics computations.\n.LE\n.pp\nCan worms shed any light on the controversies outlined above which surround\nthe concept of autopoiesis?\nFirstly, although they are not living and do not create their own material in\nany chemical sense, they are certainly autonomous, autopoietic systems.\nShoch & Hupp relate how\n.sp\n.BQ\na small worm was left running one night, just exercising the worm control\nmechanism and using a small number of machines.\nWhen we returned the next morning, we found dozens of machines dead,\napparently crashed.\nIf one restarted the regular memory diagnostic, it would run very briefly,\nthen be seized by the worm.\nThe worm would quickly load its program into this new segment; the program\nwould start to run and promptly crash, leaving the worm incomplete \\(em and\nstill hungrily looking for new segments.\n.FQ\n.sp\nJohn Brunner's science fiction story \\fIThe shockwave rider\\fR presaged just\nsuch an uncontrollable worm.\nOf course, extermination is always possible in principle by switching off or\nsimultaneously rebooting every machine on the network, although this may not\nbe an option in practice.\nSecondly, in the light of our earlier discussion of teleology and autopoiesis,\nit is interesting to find the clear separation of the maintenance mechanism\n\\(em the autopoietic part \\(em from the the application code \\(em the\n``purposive'' part \\(em of the worm.\nIt can be viewed quite separately as an autopoietic or an application\n(teleological?) system.\n.pp\n.ul\nSelf-replicating Trojan horses.\nIn his Turing Award lecture, Thompson (1984) raised the specter of\nineradicable programs residing within a computer system \\(em ineradicable in\nthe sense that although they are absent from all source code, they can survive\nrecompilation and reinstallation of the entire system!\n.[\nThompson 1984 reflections trust\n.]\nMost people's reaction is ``impossible! \\(em it must be a simple trick'',\nbut Thompson showed a trick that is extremely subtle and sophisticated, and\neffectively impossible to detect or counter.\nThe natural application of such a device is to compromise a system's security,\nand Thompson's conclusion was that there can be no technical substitute for\nnatural trust.\nFrom a system-theoretic viewpoint, however, this is an interesting example\nof how a parasite can survive despite all attempts by its host to eliminate\nit.\n.pp\nTo understand what is involved in creating such an organism, consider first\nself-replicating programs.\nWhen compiled and executed, these print out themselves (say in source code\nform); no more and no less.\nAlthough at first sight they seem to violate some fundamental intuitive\nprinciple of information \\(em that to print oneself one needs\n\\fIboth\\fR ``oneself'' \\fIand, in addition\\fR, something to print it out,\nthis is not so.\nProgrammers have long amused themselves with self-replicating programs, often\nsetting the challenge of discovering the shortest such program in any given\ncomputer language.\nMoreover, it is easy to construct a self-replicating program that includes\nany given piece of text.\nSuch a program divides naturally into the self-replicating part and the\npart that is to be reproduced, in much the same way that a worm program\nseparates the worm maintenance mechanism from the application part.\n.pp\nView self-replication as a source program ``hiding'' in executable binary\ncode.\nNormally when coaxed out of hiding it prints itself.\nBut imagine one embedded in a language compiler, which when activated\ninterpolates itself into the input stream for the compiler, causing itself\nto be compiled and inserted into the binary program being produced.\nNow it has transferred itself from the executable version of the compiler\nto the executable version of the program being compiled \\(em without ever\nappearing in source form.\nNow imagine that the program being compiled is itself the compiler \\(em a\nvirgin version, uncorrupted in any way.\nThen the self-replicating code transfers itself from the old version of\nthe compiler to the new version, without appearing in source form.\nIt remains only for the code to detect when it is the compiler that is being\nrecompiled, and not to interfere with other programs.\nThis is well known as a standard Trojan Horse technique.\nThe result is a bug that lives only in the compiled version and replicates\nitself whenever the compiler is recompiled.\n.pp\nIf autopoiesis is the ability of a system to develop and maintain its own\norganization, the self-replicating Trojan horse seems to be a remarkable\nexample of it.\nIt is an organism that it extremely difficult to destroy, even when one\nhas detected its presence.\nHowever, it cannot be autonomous, but rather survives as a parasite on a\nlanguage compiler.\nIt does not have to be a compiler:  any program that handles other programs\n(including itself) will do\\u4\\d.\n.FN\n4.\\ \\ As Thompson (1984) remarks, a well-installed microcode bug will be\nalmost impossible to detect.\n.EF\nAlthough presented as a pathological example of computer use, it is possible\nto imagine non-destructive applications \\(em such as permanently identifying\nauthorship or ownership of installed software even though the source code is\nprovided.\nIn the natural world, parasites can have symbiotic relationships with their\nhosts.\nIt would be interesting to find analogous circumstances for self-replicating\nTrojan horses, but I do not know of any \\(em these examples of benevolent\nuse do not seem to benefit the host program directly, but rather its author or\nowner.\n.pp\n.ul\nViruses\nare perhaps less subtle but more pervasive kinds of bugs.\nThey spread infection in a computer system by attaching themselves to\nfiles containing executable programs.\nThe virus itself is a small piece of code which gains control whenever the\nhost is executed, performs its viral function, and then passes control to\nthe host.\nGenerally the user is unaware that anything unusual is happening:  as far as\nhe is concerned, the host program executes exactly as normal\\u5\\d.\n.FN\n5.\\ \\ The only difference is a small startup delay which probably goes\nunnoticed.\n.EF\nAs part of its function, a virus spreads itself.\nWhen it has control, it may attach itself to one or several other files\ncontaining executable programs, turning them into viruses too.\nUnder most computer protection schemes, it has the unusual advantage of\nrunning with the privileges of the person who invoked the host, not with\nthe privileges of the host program itself.\nThus it has a unique opportunity to infect other files belonging to that\nperson.\nIn an environment where people sometimes use each others programs, this allows\nit to spread rapidly throughout the system\\u6\\d.\n.FN\n6.\\ \\ More details of the construction of both viruses and self-replicating\nTrojan horses are given by Witten (1987).\n.[\nWitten 1987 infiltrating open systems\n.]\n.EF\n.pp\nUnlike self-replicating Trojan horses, a virus can be killed by recompiling\nthe host.\n(Of course, there is no reason why a virus should not be dispatched to install\na self-replicating Trojan horse in the compiler.)  \\c\nIf all programs are recompiled ``simultaneously'' (ie without executing any of\nthem between compilations), the virus will be eradicated.\nHowever, in a multi-user system it is extremely hard to arrange for everyone\nto arrange a massive recompilation \\(em in the same way as it is difficult to\nreboot every machine on a network simultaneously to stamp out a worm.\n.pp\nViruses do not generally remain in touch with each other and therefore,\nunlike worms, are not really autopoietic.\nBut there is no intrinsic reason why they should not be.\nThey provide a basic and effective means of reproduction which could be\nutilized for higher-level communicating systems.\nAs with the other devices reviewed above, when one hears about viruses one\ncannot help thinking of pathological uses.\nHowever, there are benevolent applications.\nThey could assist in system maintenance by recording how often programs were\nused and arranging optimization accordingly, perhaps migrating little-used\nones to slower memory devices or arranging optimization of frequently-used\nprograms.\nSuch reorganizations could take place without users being aware of it, quietly\nmaking the overall system more efficient.\n.sh \"Conclusions\"\n.pp\nWe have examined two rather different directions in which autonomy can be\npursued in computer systems.\nThe first concerns representation and manipulation of goals.\nExamination of some current AI systems shows that they do not escape the\nold criticism that their goals and aspirations are merely planted there\nby the programmer.\nIndeed, it is not easy to see how it could be different, unless goals were\ngenerated randomly in some sense.\nRandom exploration is also being investigated in current AI systems, and these\nshow that syntactic mutation can be an extremely powerful technique when\ncombined with semantically dense representations.\n.pp\nBut according to modern biological thinking, the lower-level goals of people\nand animals are also implanted in their brains in a remarkably literal sense.\nHigher-level goals are not so easy to pin down.\nAccording to one school of psychological thought they stem from a\nsingle ``super-goal'' called self-actualization.\nThis is remarkably in tune with the architecture of some prominent discovery\nprograms in AI which strive to maximize the ``interestingness'' of the\nconcepts being developed.\nWhile one may be reluctant to equate self-actualization with interestingness,\nthe resemblance is nevertheless striking.\n.pp\nThe second direction concerns organizational independence in a sense of\nwholeness which is distinct from goal-seeking.\nThe concept of autopoiesis formalizes this notion.\nOrganizational independence can be identified in certain computer systems\nlike worm programs, self-replicating Trojan horses, and viruses.\nIt is remarkable that such applications have been constructed because\nthey offer practical advantages and not in pursuit of any theoretical\ninvestigation of autonomy;\nin this way they are quite different from contrived games.\nIn some sense self-replicating programs do have a goal, namely \\fIsurvival\\fR.\nA damaged worm exhibits this by repairing itself.\nBut this is a weak form of goal-seeking compared with living organisms, which\nactively sense danger and take measures to prevent their own demise.\n.pp\nThe architecture of these systems is striking in that the mechanism which\nmaintains the artificial organism (be it the worm maintenance code,\nthe self-replicating part of a Trojan horse, or the viral infection-spreader)\nis quite separate from the application part of the organism.\nMost people think of such programs as somehow pathological, and the\napplication as a harmful or subversive one, but this need not be so:  there\nare benign examples of each.\nIn any case, separation of the organism's maintenance from its purpose is\ninteresting because the concept of autopoiesis has sparked a debate in\nsystem-theoretic circles as to whether teleological descriptions are even\nlegitimate, let alone necessary.\nIn both domains a clear separation seems to arise naturally between the\nautopoietic and teleological view of organisms.\n.pp\nThere have been no attempts to build computer programs which combine these two\ndirections.\nThe AI community which developed techniques of goal-seeking has historically\nbeen somewhat separate from the system software community which has created\nrobust self-replicating programs like worms and viruses.\nWhat will spring from the inevitable combination and synthesis of the two\ntechnologies of autonomy?\n.sh \"Acknowledgements\"\n.pp\nFirst and foremost I would like to thank Brian Gaines for suggesting and\nencouraging this line of research.\nI am grateful to Saul Greenberg and Roy Masrani for many insights into topics\ndiscussed here, and to Bruce MacDonald for making some valuable suggestions.\nThis research is supported by the Natural Sciences and Engineering Research\nCouncil of Canada.\n.sh \"References\"\n.ls1\n.sp\n.in+4n\n.[\n$LIST$\n.]\n.in0\n", "encoding": "ascii"}