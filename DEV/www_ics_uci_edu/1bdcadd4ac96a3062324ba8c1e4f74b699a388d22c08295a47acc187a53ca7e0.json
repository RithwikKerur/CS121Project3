{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/aa1.txt", "content": "\tAnalysis of Algorithms (Complexity Classes and Big-O Notation)\r\n\r\n\r\nAnalysis of Algorithms is a mathematical area of Computer Science in which we\r\nanalyze the resources (mostly time, but sometimes space) used by algorithms to\r\nsolve problems. An algorithm is a precise procedure for solving a problem,\r\nwritten in any notation that humans understand (so that we can carry-out the\r\nalgorithm): if we write an algorithm as code in some programming language, then\r\na computer can execute it too. The analysis we do should be independent of any\r\ntechnology: e.g., language, compiler, or processor.\r\n\r\nThe main tool that we use to analyze algorithms is big-O notation: it means that\r\nthe resource is \"bounded above by growth on the order of\". We use big-O notation\r\nto bound from above the performance of an algorithm by placing it in a\r\ncomplexity class (most often based on its WORST-CASE behavior -but sometimes on\r\nits BEST or AVERAGE-CASE behavior) when solving a problem of size N: we will\r\nlearn how to characterize the size of a problem, which is most often as simple\r\nas \"N is the number of values in\" an array, file, vector, linked list, etc.\r\n\r\nOnce we know the complexity class of an algorithm, we have a good handle on\r\nunderstanding its actual performance on real machines (within certain limits).\r\nThus, in AA we don't necessarily compute the exact resources needed, but\r\ntypically an approximate upper bound on the resources, based on just the\r\nproblem size (not the details of the data: but if we know those, sometimes we\r\ncan compute a more accurate bound).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nGetting to Big-O Notation: Throwing away Irrelevant Details\r\n\r\nHere is a simple C++ function for computing the maximum value in an array (or\r\nthrowing and exception if there are no values in the array).\r\n\r\nint maximum(int a[], int N) { //N is the length of the array\r\n    if (N == 0)\r\n      throw \"empty array\";\r\n    int answer = a[0];\r\n    for (int i=1; i<N; ++i)\r\n      if (a[i] > answer)\r\n        answer = a[i];\r\n    return answer;\r\n}\r\n\r\nOften, the problem size is the number of values processed: e.g., the number of\r\nvalues in an array, file, linked list, tree, etc. But we can use other metrics\r\nas well: e.g., it can be the count of number of digits in an integer value, when\r\nlooking at the complexity of multiplication based on the size of the numbers.\r\nThus, there is no single measure of size that fits all problems: instead, for\r\neach problem we try to choose a measure that makes sense and is natural for\r\nthat problem.\r\n\r\nC++ compiles functions like maximum into a sequence of machine language\r\ninstructions that the computer executes. To solve a problem, the computer\r\nalways executes an integer number of instructions. For simplicity, we will\r\nassume that all instructions take the same amount of time to execute. So to\r\ncompute the amount of time it takes to solve a problem is equivalent to\r\ncomputing how many instructions the computer must execute to solve it (which\r\ndepends on the compiler technology we are using), which we can divide by the\r\nnumber of instructions/second the machine executes to compute the time taken\r\n(which also depends on technology, and changes). \r\n\r\nAgain, we typically look at the worst case behavior of algorithms. For maximum,\r\nthe worst case occurs if the array value are stored in increasing order. In\r\nthis case, each new value examined in the array will be bigger than the\r\nprevious answer, so the if statement's condition will always be true, which\r\nalways requires executing the code that updates the answer. If any value was\r\nlower, it wouldn't have to update the answer and thus would execute fewer\r\ninstructions and take less time to execute.\r\n\r\nIt turns out that for an array of N values, on a certain C++ compiler for a\r\ncertain processor, the computer executes 14N + 9 instructions in the worst case\r\nfor this function. You may need to know more CS than you do at this time to\r\ndetermine this formula, but you will get there by ICS 51; and if you have taken\r\nICS-51, you should understand the machine instructions that such a program\r\nmight be translated into.\r\n\r\nA simple way to think about this formula is that there are 14 computer\r\ninstructions that are executed each time the body of the for loop is executed,\r\nand 9 instructions that are executed only once: they deal with starting\r\nand terminating the loop and the entire function. We can say I(N) = 14N + 9 for\r\nthe worst case of the maximum function, where I(N) is the number of instructions\r\nthe computer executes when solving a problem on an array with N values. Or to be\r\nmore specific we would write Imaximum(N) = 14N + 9, for the maximum function.\r\n\r\nI would like to argue now that if simplify this function to just Imaximum(N)\r\n= 14N, we get a simpler function; although we lose some information, we don't\r\nlose much. Specifically, as N gets bigger (i.e., we are dealing with very big\r\nproblems - the kinds computers are used to solve), 14N and 14N+9 are relatively\r\nclose: as N gets bigger the loop accounts for most of the instructions executed)\r\nLet's look at the result of this function vs. the original as N gets for values\r\nof N increasing by a factor of 10.\r\n\r\n    N     |   14N + 9  |    14N     | error: (14N+9 - 14N)/(14N+9) as a % of N\r\n ---------+------------+------------+---------------------------\r\n        1 |         23 |         14 |   61%         or 39%       accurate\r\n       10 |        149 |        140 |    6%            94%       accurate\r\n      100 |       1409 |       1400 |     .6%          99.4%     accurate\r\n     1000 |      14009 |      14000 |     .06%         99.94%    accurate\r\n     ...\r\n1,000,000 | 14,000,009 | 14,000,000 |     .00006%      99.99994% accurate\r\n     ...\r\n\r\nEach line shows the % error of computing 14N when the true answer is 14N + 9.\r\nSo by the time we are processing an array of 1,000 values, using the formula\r\n14N instead of 14N+9 is 99.94% accurate. For computers solving real problems,\r\nan array of 1,000 values is tiny: an array of millions is more normal. For\r\n1,000,000 values 14N is off by just 9 parts in 14 million. So the 9 doesn't\r\naffect the formula much. Almost all the time is spent in the loop.\r\n\r\nAnalysis of Algorithms really should be referred to as ASYMPTOTIC Analysis of\r\nAlgorithms, as it is mostly concerned with the performance of algorithms as\r\nthe problem size gets very big (N -> infinity). We see here that as N->infinity\r\n14N is a better and better approximation to 14N+9: dropping the extra 9 becomes\r\nless and less important, when divided by the actual number of instructions\r\nexecuted.\r\n\r\nA simple function for sorting is the following. This is much simpler than the\r\nreal sort method in C++ (and the simplicity results in the function taking much\r\nmore time for large N, but it is a good starting point for understanding sorting\r\nnow). If you are interested in how this function accomplishes sorting, hand\r\nsimulate it working on an array of 5-10 values (try arrays with increasing\r\nvalues, decreasing values, and values in random order): basically each execution\r\nof the outer loops mutates the array so that the next value in the array is in\r\nthe correct position. We will spend a few lectures studying sorting more\r\nformally towards the end of the quarter.\r\n\r\nvoid sort(int a[], int N) { //N is the length of the array\r\n  for (int base=0; base<N; ++base)\r\n    for (int check=base+1; check<N; ++check)\r\n       if (a[base] > a[check])\r\n         std::swap(a[base], a[check]);\r\n}\r\n\r\nIt turns out that for an array of N values, the computer executes 8N^2 + 12N + 6\r\ninstructions in the worst case for this function. The outer loop executes N\r\ntimes and inner loop on average executes N/2 times (when base is 0 it executes\r\nN-1 times; when base is N-1 it executes 0 times), so the if statement in the\r\ninner loop is executed a quadratic number of times. So Isort(N) = 8N^2+12N+6\r\nfor the worst case of the sort function, where Isort(N) is again the number of\r\ninstructions that the computer executes. Or to be more specific we would write\r\nIsort(N) = 8N^2 + 12N  + 6.\r\n\r\nI would like to argue in the same way that if we simplify this function to just\r\nIsort(N) = 8N^2, we get a much simpler function (dropping 2 of the 3 additive\r\nterms) but we have not lost much information. Let's look at the result of this\r\nthis function vs. the original as N gets bigger and bigger\r\n\r\n    N     | 8N^2+12N+6 |      8N^2   | error: (12N+6)/(8N^2+12N+6) as a % of N\r\n ---------+------------+-------------+---------------------------\r\n        1 |         26 |           8 |   70%      or 30%    accurate\r\n       10 |        926 |         800 |   14%         86%    accruate\r\n      100 |     81,206 |      80,000 |    1.5%       98.5%  accurate\r\n     1000 |  8,012,006 |   8,000,000 |     .15%      99.85% accurate\r\n\r\nSo by the time we are processing an array of 1,000 values, using the formula\r\n8N^2 instead of 8N^2 + 12N + 6 is 99.85% accurate. For 1,000,000 values\r\n(10^6), 8N^2 is 8*10^12 so 8N^2 + 12N + 6 is 8*10^12 + 12*10^6 + 6; the\r\nsimpler formula is 99.999985% accurate. Note the difference in the powers of\r\n10 for the first and second terms in the formula: 10^12 and 10^6, a factor of a\r\nmillion difference.\r\n\r\nCONCLUSION (though not proven): If the real formula I(N) is a sum of a bunch of\r\nterms, we can drop any term that doesn't grow as quickly as the most quickly\r\ngrowing term. As N->infinity, we can neglect the effects from all these lower\r\norder terms without losing much accuracy. In computing the maximum, the linear\r\nterm 14N grows more quickly than the next term, the constant 9, which doesn't\r\ngrow at all (as N grows) so we drop the 9 term. In sorting, the quadratic term\r\n8N^2 grows more quickly than the next two terms, the linear term 12N and the\r\nconstant 6, so we drop the 12N and 6 terms.\r\n\r\n    In fact note that we can prove that the Limit as N->infinity of 12N/8N^2 = \r\n    3/(2N) -> 0, which means we can discard the 12N term as growing more slowly\r\n    than the 8N^2 term. Similarly, the Limit as N->infinite of 6/8N^2 -> 0. This\r\n    ratio tells us what terms we can discard.\r\n\r\nThe result is a simple function (a constant times some function of N) that is\r\nstill an accurate approximation of the number of computer instructions executed\r\nfor arrays of various large sizes.\r\n\r\n    But also note, if the formula were something like N^2 + 1,000,000N (with the\r\n    coefficient of N one million times greater than the coefficient of N^2), at\r\n    N = 1,000,000 the N^2 term is the same size as 1,000,000N term, so the\r\n    percent error would be 50%. Of course if we increase N to a billion or\r\n    trillion, N^2 becomes much bigger. In most algorithms, the coefficients\r\n    are of the same magnitude, so throwing away lower-order terms is still\r\n    accurate for even moderate-sized N. We will soon discuss where these\r\n    constants come from, so you can better undertand them.\r\n\r\nWe now will explain the rationale for dropping the constant in front of N (in\r\nImaximum) and N^2 (in Isort), and classifying these algorithms as just O(N)\r\nbounded by linear growth, or O(N^2) bounded by quadratic growth. Again O means\r\n\"bounded by growth on the order of\", so O(N) means bounded by growth on the\r\norder of N and O(N^2) means bounded by growth on the order of N^2.\r\n\r\n1) If we assume that every instruction in the computer takes the same amount\r\n   of time to execute. Then the time taken for maximum is about 14N/speed and\r\n   the time for sort is about 8N^2/speed. We should really think about these\r\n   formulas as (14/speed)N and (8/speed)N^2. We know the 14 and 8 came from\r\n   the number of instructions inside loops that C++ needed to execute: but a\r\n   different C++ compiler (or a different language) might generate a different\r\n   number of instructions and therefore a different constant. Thus, this number\r\n   is based on technology, and we want our analysis to be independent of\r\n   technology. Of course, \"speed\" changes based on technology too, so we lump\r\n   that with the compiler constant.\r\n\r\nSince we are trying to come up with a \"science\" of algorithms, we don't want\r\nour results to depend on technology, so we are also going to drop the constant\r\nin front of the biggest term as well. For the reason explained above (relating\r\nto instructions generated by C++ and the speed of the machine), this number\r\nis based solely on technology.\r\n\r\nHere is a second justification for not being concerned with the constant in\r\nfront of the biggest term.\r\n\r\n2) The fundamental question that we want answered about any algorithm is, \"how\r\n   much MORE resources does it need when solving a problem TWICE AS BIG\". In\r\n   maximum, when N is big (so we can drop the +9 without losing much accuracy)\r\n   the ratio of time to solve solve a problem of size 2N to the time to solve a\r\n   problem of size N is easily computed:\r\n\r\n    Imaximum(2N)     14(2N)\r\n   -------------- ~ -------- ~ 2\r\n    Imaximum(N)      14 N\r\n\r\n   The ratio is a simple number (no matter how many instructions are in the\r\n   loop, since the constant 14 appears as a multiplicative factor in both the\r\n   numerator and denominator, and cancels itself out).  So, we know for this\r\n   code, if we double the size of the array, we double the number of\r\n   instructions that maximum executes to solve the problem, and thus double the\r\n   amount of time (for whatever the speed of the computer is).\r\n\r\n   Thus, the constant 14 is irrelevant when asking this \"doubling\" question. \r\n\r\n   Likewise, for sorting we can write\r\n\r\n    Isort(2N)     8(2N)^2\r\n   ----------- ~ ---------- ~ 4\r\n    Isort(N)      8 N^2\r\n\r\n   Again, the ratio is a simple number, with the constant (no matter what it\r\n   is), disappearing.  So, we know for this code that if we double the size of\r\n   the array, we increase by a factor of 4 the number of instructions that are\r\n   executed, and thus increase by a factor of 4 the amount of time (for\r\n   whatever the speed of the computer is).\r\n\r\n   Thus, the constant 8 is irrelevant when asking this \"doubling\" question. \r\n\r\n   Note if we didn't simplify, we'd have\r\n\r\n    Isort(2N)     8(2N)^2 + 12(2N) + 6       32N^2 + 24N + 6\r\n   ----------- = ---------------------- = -------------------\r\n    Isort(N)      8N^2 + 12N + 6              8N^2 + 12N + 6\r\n\r\n   which doesn't simplify easiy; although, as N->infinty, this ratio gets\r\n   closer and closer to 4 (and is very close even for relatively small-sized\r\n   problems, because the coefficients are the same magnitude).\r\n\r\nAs with air-resistance and friction in physics, typically ignoring the\r\ncontribution of these negligible factors (for big, slow-moving objects) allows\r\nus to quickly and easily solve an approximately correct problem.\r\n\r\nUsing big-O notation, we say that the complexity class of the code to find the\r\nmaximum is O(N). The big-O means \"bounded by growth on the order of\" N, which\r\nmeans at most a linear growth (double the input size, double the time). For the\r\nsorting code, its complexity class is O(N^2), which means \"bounded by growth on\r\nthe order of N^2\", which means at most a quadratic growth rate (double the\r\ninput size, quadruple the time).\r\n\r\n----------\r\nIMPORTANT: A Quick way to compute the complexity class of an algorithm\r\n\r\nTo analyze a C++ function's code and compute its complexity class, we\r\napproximate the number of times the most frequently executed statement is\r\nexecuted, dropping all the lower (more slowly growing) terms and dropping the\r\nconstant in front of the most frequently executed statement (the fastest\r\ngrowing term). We will show how to do this much more rigorously in the next\r\nlecture.\r\n\r\nHere is how we apply this simple rule to the maximum code: it executes the if\r\nstatement N times, so it is O(N). The sorting code executes the if statement\r\nN(N-1)/2 times (we will justify this number below), which is N^2/2 - N/2, so\r\ndropping the lower term and the constant 1/2, yields a complexity class of\r\nO(N^2).\r\n\r\nIn the next lecture we will generalize this method for quickly calculating the\r\ncomplexity class of C++ code by examining source code.\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComparing Algorithms by their complexity classes\r\n\r\nPrimarily from this definition we know that if two algorithms, a and b, both\r\nsolve the same problem, and a is in a lower complexity class than b, then for\r\nall BIG ENOUGH N, Ta(N) < Tb(N): here Ta(N) means the Time it takes for\r\nalgorithm a to solve the problem of size N. NOTE THAT NOTHING HERE IS SAID ABOUT\r\nSMALL N; which algorithms uses fewer resources on small problems depends on the\r\nactual constants that we dropped (and even on the additive terms that we\r\ndropped), so COMPLEXITY CLASSES HAVE LITTLE TO SAY FOR SMALL PROBLEM SIZES.\r\n\r\nFor example, if algorithm a is O(N) with a constant of 100, and algorithm b\r\nis O(N^2) with a constant of 1, then for values of N in the range [1,100],\r\n\r\n   Tb(N) = 1N^2 <= 100N = Ta(N)\r\n\r\nbut for all values bigger than 100,\r\n\r\n   Ta(N) = 100N <= 1N^2 = Tb(N)\r\n\r\nSo for big-sized problem, the algorithm in the lower complexity class is faster.\r\n\r\nAs a second example, if algorithm a is O(N) with a constant of 1, and algorithm\r\nb is O(N^2) with a constant of 10, then for all values of N\r\n\r\n   Ta(N) = 1N <= 10N^2 = Tb(N)\r\n\r\nSo in some cases, a lower complexity class can be worse (but only for small N),\r\nand in some cases it can be better for all N. We need determine which is faster\r\nfor small N by going beyond our use of complexity classes.\r\n\r\nBut it is guaranteed that FOR ALL SIZES BEYOND SOME VALUE OF N, the algorithm\r\nin the lower complexity class will run faster.\r\n\r\nAgain, we use the term \"asymptotic\" analysis of algoritms to indicate that we\r\nare mostly concerned with the time taken by the code when N gets very large\r\n(going towards infinity). In both cases, because of their complexity classes,\r\nalgorithm a will be better.\r\n\r\nWhat about the constants? Are they likely to be very different in practice? It\r\nis often the case that the constants of different algorithms are close. (They\r\nare often just the number of instructions in the main loop of the code, so code\r\nwill small loops will have similar constants). So the complexity classes are a\r\ngood indication of faster vs slower algorithms for all but the smallest problem\r\nsizes.\r\n\r\nAlthough all possible mathematical functions might represent complexity classes\r\n(and many strange ones do), we will mostly restrict our attention to the\r\nfollowing complexity classes. Note that complexity classes can interchangably\r\nrepresent computing time, the # of machine operations executed, and more\r\nnebulous terms such as as \"effort\" or \"work\" or \"resources\".\r\n\r\nAs we saw before, a fundamental question about any algorithm is, \"What is the\r\ntime needed to solve a problem twice as big\". We will call this the DOUBLING\r\nSIGNATURE of the complexity class (knowing this value empirically -which we can\r\ndetermine by running the program on various problem sizes- often allows us to\r\ndeduce the complexity class as well).\r\n\r\nClass   |  Algorithm Example\t\t\t\t| Doubling Signature\r\n--------+-----------------------------------------------+----------------------\r\nO(1)\t| pass argument->parameters/copying a pointer   | T(2N) = T(N)\r\nO(LogN) | binary searching of a sorted array\t\t| T(2N) = c + T(N)\r\nO(N)\t| linear searching an array    \t\t\t| T(2N) = 2T(N)\r\nO(NLogN)| Fast sorting\t\t\t\t\t| T(2N) = cN + 2T(N)\r\n\r\n  Fast algorithms come before here; NLogN grows a bit more slowly than linearly\r\n  (because logarithms grow so slowly compared to N) and nowhere near as fast as\r\n  O(N^2).\r\n\r\nO(N^2) | Slow sorting; scanning N times array size N    | T(2N) = 4T(N)\r\nO(N^3) | Slow matrix multiplication\t\t        | T(2N) = 8T(N)\r\nO(N^m) | Graph algorithms, for some fixed m: 4, 5, ...\t| T(2N) = 2^mT(N)\r\n\r\n  Tractable algorithms come before here; their work is polynomial in N (and\r\n  typically a small power).  In the complexity class below, N is in an\r\n  exponent.\r\n\r\n  For example, for an O(N^2) algorithm, doubling the size of the problem\r\n  quadruples the time required: T(2N) ~ c(2N)^2 = c4N^2 = 4cN^2 = 4T(N).\r\n\r\nO(2^N) | Finding boolean values that satisfy a formula  | T(2N)=2^N T(N)\r\n\r\n  Note T(2N) = 2^(2N) = (2^N)^2 = 2^N T(N)\r\n\r\nNote that in Computer Science, logarithms are mostly taken to base 2. (Remember\r\nthat algorithms and logarithms are very different terms). So when we write\r\nlogarithms, they are implicitly to base 2 (e.g., Log N = Log2 N). You should\r\nmemorize and be able to use the following facts to approximate logarithms\r\nwithout a calculator.\r\n\r\nLog 1000 ~ 10\r\n  Actually, 2^10 = 1,024, 2^10 is approximatley 1,000 with < a 3% error.\r\n\r\nLog a^b = b Log a, or more usefully, Log 1000^N = N Log 1000; so ...\r\n  Log 1,000,000     = 20 : 1,000,000     = 1,000^2; Log 1000^2 = 2*Log 1000\r\n  Log 1,000,000,000 = 30 : 1,000,000,000 = 1,000^3; Log 1000^3 = 3*Log 1000\r\n\r\nSo note that Log is a very slowly growing function. When we increase from\r\nLog 1,000 to Log 1,000,000,000 (the argument grows by a factor of 1 million)\r\nthe result only grows by from 10 to 30 (by a factor or 3). That represents a\r\nvery flat curve, with its slope decreasing assympotically toward 0 (the\r\nconstant function).\r\n\r\nWe can compute logarithms base 2 on any calculator that computes Log in any\r\nbase, using a \"base conversion function\". For example,\r\n\r\nLog (base b) X = Log (base a) X / Log (base a) b\r\n\r\nSo, Log (base b) X is just a constant (1/Log (base a) b) times Log (base a) X,\r\nso logarithms to any base really all specify the SAME COMPLEXITY CLASS\r\n(regardless of the base) because they differ only by a multiplicative constant\r\n(which we ignore when writing complexity classes). For example,\r\n\r\nLog(base 10) X = Log(base  2) X  /  Log(base 2) 10 ~ .3  Log(base  2) X\r\nLog(base  2) X = Log(base 10) X  /  Log(bas 10)  2 ~ 3.3 Log(base 10) X\r\n\r\n\r\n----------\r\nIMPORTANT:\r\n\r\nDetermining the Complexity Class Empirically from a Doubling Signature\r\n\r\nIf we can demonstrate that doubling the size of the input approximately\r\nquadruples the time of an algorithm, then the algorithm is O(N^2). We can use\r\nthe doubling signatures shown above for other complexity classes as well. Thus,\r\neven if we cannot mathematically analyze the complexity class of an algorithm\r\nbased on inspecting its code (something we will highlight in the next lecture),\r\nif we can measure it running on various sized problems (doubling the size over\r\nand over again), we can use the doubling signature information to approximate\r\nits complexity class. In this approach we don't have to understand (or even\r\nlook at) the code.\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComputing Running Times from Complexity Classes\r\n\r\nWe can use knowledge of the complexity class of an algorithm to predict its\r\nactually running time on a computer as a function of N easily. For example, if\r\nwe know the complexity class of algorithm a is O(N^2), then we know that\r\nTa(N) ~ cN^2 when N is large, some constant c. The constant c represents the\r\n\"technology\" used: the language, compiler, machine speed, etc.; the N^2 (from\r\nO(N^2)) represents the \"science/math\" part: the complexity class. Now, given\r\nthis information, we can time the algorithm for some large value N. Let's say\r\nfor N = 10,000 (which is actually a pretty small N these days) we find that\r\nTa(10,000) is 4 seconds.\r\n\r\nFirst, if I asked you to estimate Ta(20,000) you'd  immediately know it is\r\nabout 16 seconds (doubling the input of an O(N^2) algorithm approximately\r\nincreases the running time by a factor of 4). Second, if we solved the equation\r\nfor Ta(N) above for c, we have\r\n\r\n  Ta(N) ~ cN^2, substituting 10,000 for N and 4 for Ta(N) (by actually running\r\n  the code and timing it) we have 4 ~ c 10,000^2 (from the formula); so solving\r\n  for the technology constant we have c ~ 4x10^-8.\r\n\r\nSo, by measuring the run-time of this code once, we can calculate the constant\r\n\"c\", which involves all the technology (language, compiler, computer speed,\r\netc). Roughly, we can think of c as being the amount of time it takes to do one\r\nloop (# of instructions per loop/speed of executing instructions) where the\r\nalgorithm requires N^2 iterations through the loop to do all its work.\r\n\r\nTherefore, Ta(N) ~ 4x10^(-8) x N^2. So, if asked to estimate the time to\r\nprocess 1,000,000 (10^6) values (100 times more than 10,000), we'd have\r\n\r\n  Ta(10^6) ~ 4x10^(-8) x (10^6)^2\r\n  Ta(10^6) ~ 4x10^(-8) x 10^12\r\n  Ta(10^6) ~ 4x10^4, or about 40,000 seconds (about 1/2 a day)\r\n\r\nNotice that solving a problem 100 times as big takes 10,000 (which is 100^2)\r\ntimes as long, which is based on the signature for an O(N^2) algorithm when\r\nwe increase the problem size by a factor of 100. If we go back to our sorting\r\nexample,\r\n\r\n    Isort(100N)    8(100N)^2\r\n   ----------- ~ ------------- ~ 10,000\r\n    Isort(N)       8 N^2\r\n\r\nIn fact, while we often analyze code to determine its complexity class, if we\r\ndon't have the code (or find it too complicated to analyze) we can run the\r\ncode, doubling the input sizes a few times, plot the result, and then check\r\nwhether we can \"fit the resulting times\" to any of the standard doubling\r\nsignatures to estimate the complexity class of the algorithm. We should do this\r\nfor some N that is as large as reasonble (taking some number of seconds to\r\nsolve on the computer). Yes, we just discussed this method above.\r\n\r\nNote for an O(2^N) algorithms, if we double the size of the problem from 100\r\nto 200 values the amount of time needed goes up by a factor of 2^100, which is\r\n~ 1.3x10^30. Notice that each time we add one more value to process, the time it\r\ntakes doubles: this \"exponential\" time is the inverse function of logarithmic\r\ntime, in terms of its growth rate: exponentials grow incredibly quickly while\r\nlogarithms grow incredible slowly. The function 2^N eventually is bigger than\r\nany polynomial in N: e.g., 2^N is eventually bigger than N^1000.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nMathematical Definition of big-O Notation\r\n\r\nMathematically, we say that f(n) is O(g(n)) -meaning the function f has an\r\norder of growth that is BOUNDED by  function g- if we can find values n0 and c\r\nsuch that for all n > n0, f(n) <= c g(n): read this as, \"for all n greater than\r\nn0, f(n) is less than or equal to a constant c times g(n).\" We don't care about\r\nsmall values of n, when n <= n0.\r\n\r\nThis definition allows us to do precisely what we did above more intuitively:\r\nfind the upper-bound on the complexity class of an algorithm by computing how\r\noften the most frequently executed statement is executed (the f(n)), and\r\nthrowing away lower order terms and constant coefficients: the result is a\r\nsimpler function g(n) that characterizes the complexity class.\r\n\r\nLet's use this defintion to prove that certain functions are in certain\r\ncomplexity classes: recall a complexity class is a single term involving n\r\n(no lower order terms and no constant multiplier).\r\n\r\nFinally, recall that n is typically an integer (often the size of some data\r\nstructure: e.g., an array or linked list). So talking about n = .5 or n = -10\r\nmakes no sense.\r\n\r\nFirst, note that\r\n\r\n  (1) 1 <= n, 1 <= n^2  whenever n>0\r\n  (2) log n <= n        whenever n>0 (log is undefined if n<=0)\r\n  (3) n <= n^2          whenever n>=0\r\n  (4) n log n <= n^2    whenever n>0 (by multiplying each side in (2) by n)\r\n\r\nSo, let's formally prove thta f(n) = 5n^2 + 3nlogn + 2n + 5 is O(n^2). That\r\nmeans we must find an n0 and c such that\r\n\r\n  5n^2 + 3nlogn + 2n + 5 <= c n^2 for all n>n0\r\n\r\nNote from (4) 3n log n <= 3n^2  for n > 0,\r\n          (3) 2n       <= 2n^2  for n > 0, and\r\n          (1) 5        <= 5n^2  for n > 0\r\n\r\n  and trivially 5n^2 <= 5n^2 for all n.\r\n\r\nso we can add each term on the left and add each term on the right (with each\r\nterm on the left being <= each term on the right for n > 0) with the result\r\n5n^2 + 3nlogn + 2n + 5 <= 5n^2 + 3n^2 + 2n^2 + 5n^2 for all n>0 (because\r\neach term on the left is less than its corresponding term on the right for\r\nn>0). We can simplify all the terms on the right (they are all constants times\r\nn^2) to be (5 + 3 + 2 + 5)n^2 or just 15n^2.\r\n\r\nSo, we have proven f(n) = 5n^2 + 3nlogn + 2n + 5 <= 15n^2 for all n>0,\r\ntherefore 5n^2 + 3nlogn + 2n + 5 is O(n^2) (with c = 15 and n0 = 0).\r\n\r\n\r\nLikewise, we can see that for f(n) = 2n + 100log n, f(n) is O(n) because\r\n\r\n 100log n <= 100n (for n>0) -multiply each side of (2) by 100, and\r\n 2n + 100logn <= 2n + 100n = 102n for all n>0 - add 2n to each side.\r\n\r\n\r\nLikewise, we can see that for f(n) = 2n^2 - 100n + 50, f(n) is O(n^2) because\r\n\r\n  f(n) <= f(n)+100n (for all n>0) = 2n^2+50 <= 2n^2+50n^2 = 52n^2 (for all n > 0)\r\n\r\nBy a similar \"addition trick\" we can can drop/ignore all negative terms that\r\nare in a formula f when computing its complexity class.\r\n\r\n \r\nFinally, we can see that for f(n) = 2n + 10, f(n) is O(n) because\r\n\r\n  10 <= 10n (for n>0)  -multiply each side of (1) by 100, so\r\n  2n + 10 <= 12n for n>0\r\n\r\n-------\r\nThere is more than one way to compute the c and n0 (which are not independent:\r\nthey depend on each other, in interesting ways). For example, there are many\r\nother ways to show that for f(n) = 2n + 10, f(n) is O(N). Originally we chose\r\nc = 12, and found f(n) < 12n, for n>0. But, we can also choose c = 4, and use\r\nthat value to find for what n0, f(n) = 2n + 10 <= 4n.\r\n\r\n  2n + 10 <= 4n\r\n  10      <= 2n\t\tsubtract 2n from each side\r\n   5      <= n\t\tdivide each side by 2\r\n\r\nand see that this inequality holds when n>=5. So, we have c=4 and n0 = 5 as\r\nanother demonstration that f(n) is O(n).\r\n\r\nIf we choose c = 2.001, we can solve for when\r\n\r\n  2n + 10 <= 2.001n\r\n  10      <= .001n\tsubtract 2n from each side\r\n  10,000  <= n\t\tdivide each side by .001\r\n\r\nand see that this inequality holds when n>=10,000. So, we have c = 2.001 and\r\nn0 = 10,000 as another demonstration that f(n) is O(n).\r\n\r\nSo, there are MULTPLE PROOFS (multiple cs and n0s) that meet the requirement\r\nstated at the beginning. If can choose a large c, we can choose a small n0; if\r\nwe choose a smaller c, we will need to choose a larger n0.\r\n\r\nNote that we must choose a c bigger than 2.0 (even a tad bigger: .001 as was\r\nshown above, or even .0000001). If we try to use 2, the inequality\r\n\r\n  2n + 10 <= 2n\r\n\r\nhas no solution that makes sense for non-negative problem sizes n0; the\r\nsolution to the inequality\r\n\r\n  2n + 10 <= 1.5n\r\n  10      <= -.5n\tsubtract 2n from each side\r\n  -20     >= n\t\tdivide each side by -.5 (inequality changes <= to >=)\r\n\r\nis meaningless: it isn't in the form, n >= something. It concerns problem\r\nsizes less than -20, and -20 is not a legal problem size!\r\n-------\r\n\r\nFinally, notice that if f(n) is O(n^2) then f(n) -by the definition- is also\r\nO(n^3). Here is the proof. If f(n) is O(n^2) then we know\r\n\r\n  for all n > n0, f(n) <= c n^2\r\n\r\nbut we also know n^2 <= n^3 for n>=0 (and n is an integer and always >= 0)\r\n\r\nso for all n > n0, f(n) <= cn^3 (the same c as above), so f(n) is O(n^3).\r\n\r\nIMPORTANT: Saying f(n) is O(g(n)) is telling us just that g(n) is an UPPER\r\nBOUND on how fast the function f(n) grows. In practice, we always try to get a\r\n\"tight\" upper bound (the smallest growing function g(n) for which f(n) is\r\nO(g(n))), but big-O notation says NOTHING ABOUT \"TIGHTNESS\", just inequality.\r\n\r\nWhen we talk about the big Theta and big Omega notation in a later lecture, we\r\nwill see how to bound f(n) from below as well as from above. If the bounding\r\ncomplexity classes (both big O and big Omega) are both g(n), then we will have\r\na tight bound on f(n) and pronounce it big Theta(g(n)). More on this in the\r\nnext lecture.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nOdds and Ends:\r\n\r\nIt is important to be able to analyze the following code. Notice that the upper\r\nbound of the inner loop (i, in j<i) is defined/incremented in the outer loop\r\n(i=0;;++i). Here body is just some sequence of statements inside the loop.\r\n\r\nfor (int i=0; i<N; ++i)\r\n  for (int j=0; j<i; ++j)   // maximum j depends on current i in outer loop\r\n    body\r\n\r\nHow many times does the \"body\" of the loop get executed? When the outer loop\r\nindex i is 0, \"body\" gets executed 0 times; when the outer loop index i is 1,\r\n\"body\" gets executed 1 time; when the outer loop index i is 2, \"body\" gets\r\nexecuted 2 times; .... when the outer loop index i is N-1 (as big as i gets),\r\n\"body\" gets executed N-1 times. So, in totality, \"body\" gets executed\r\n0 + 1 + 2 + 3 + ... + N-1 times, or dropping the 0, just 1 + 2 + 3 + ... + N-1\r\ntimes. Is there a simpler way to write this sum?\r\n\r\nThere is a simple, general closed form solution of adding up consecutive\r\nintegers. Here is the proof that 0 + 1 + 2 + 3 + ... + N =N*(N+1)/2. This is a\r\ndirect proof, but this relationship can also be proved by induction.\r\n\r\nLet\r\n\r\nS = 0 + 1 + 2 + 3 + ... + N-1 + N.\r\n\r\nSince the order in which we add the numbers makes no difference in the sum, we\r\ncan also write this sum reversed, as\r\n\r\nS = N + N-1 + ... + 3 + 2 + 1 + 0.\r\n\r\nIf we add the left and right side (column by column) we have\r\n\r\nS   =   0  +    1  +   ...  +   N-1  +  N\r\nS   =   N  +   N-1 +   ...  +    1   +  0\r\n---------------------------------------------\r\n2S  =   N  +    N  +   ...  +   N    +  N\r\n\r\nThat is, each pair in a column sums to N, and there are N+1 pairs to sum.\r\nSince there are N+1 pairs, each summing to N, the right hand side can be\r\nsimplified to just N+1 terms, each with values N: (N+1)*N. So\r\n\r\n2S = (N+1)*N, therefore S = (N+1)*N/2 = N^2/2 + N/2\r\n\r\nThus, S is O(N^2): with a constant of 1/2 and an additive term of N/2 that is\r\ndropped (because its order, N, is lower than that N^2). Note that either N or\r\nN+1 must be an even number, so dividing their product by 2 is always an integer,\r\nas it must be, for the sum of integers: 0+1+2+3+4+5+6 = 6*7/2 = 21.\r\n\r\nLooking back at the example of the code above, the total number of times the\r\nbody gets executed is 0 + 1 + 2 + ... + N-1, so plugging N-1 in for N we have\r\n(N-1+1)*(N-1)/2 = \r\nN^2/2 - N/2 which is smaller, but still O(N^2), using the same reasoning.\r\n\r\nWe can easily apply this formula to the task of reading N values from a file\r\n(one at a time) and putting them in a linked list IN THE SAME ORDER. This is\r\nthe same as putting N values at the end of a linked list that is initially\r\nempty (and has no cache pointer to the last node). To put in the 1st value\r\nrequires skipping 0 nodes; to put in the 2nd value requires skipping 1 node; to\r\nput in the 3rd value requires skipping 2 nodes; ... to put in the Nth value\r\nrequires skipping N-1 nodes. So the number of nodes skipped is\r\n0 + 1 + ... + N-1, which by the formula is (N-1)N/2: so building a linked list\r\nin this way is in the O(N^2) complexity class.\r\n\r\nThere are two other ways to accomplish this same task. We can use a cache\r\npointer, which is O(N) since we insert N values at the end of the list and\r\neach insertion requires O(1) work (changing a fixed number of pointers). We \r\ncan also build a linked list in the \"reverse\" order by adding each value to\r\nthe front of the list; for a reason similar to caching, that requires O(N) work.\r\nWe can then reverse the linked list using the algorithm we hand-simlated in\r\nclass, which visits every node once, doing a constant amount of work for each\r\nnode, so the entire process is O(N) + O(N) = O(2N) = O(N); again, we always\r\nignore multiplicative constants.\r\n\r\n\r\nFast Searching and Sorting:\r\n\r\nSimple algorithms for searching an array are in the complexity class O(N) and \r\nsorting an array are in the complexity class O(N^2). But, there are surprisingly\r\nfaster algorithms for these tasks: searching an array can be O(Log N) if the\r\narray is sorted (by the \"binary search\" algorithm); and sorting values in an \r\narray can be in O(N Log N) by various algorithms that we will study.\r\n\r\nIn lecture, I will briefly discuss the binary searching algorithm, for searching\r\na sorted array: its complexity class is O(Log N). In fact, if we are measuring\r\nthe number of comparisons made, the constant is 1. That means that when\r\nsearching an array of 1,000,000 values, we must access the array at most about\r\n20 times to either (a) find the index of the value in the array or\r\n(b) determine the value is not in the array. This is potentially 50,000\r\ntimes faster than a simple for loop checking one index after another (which we\r\nmust do if the array is not sorted, and we must do in a linked list)! On large\r\nproblems, algorithms in a lower complexity class typically execute much faster.\r\nAlso note that when increasing the size of the array by 1,000 (to 1 billion\r\nvalues) the maximum number of comparisons for binary searching goes from 20 only\r\nto 30.\r\n\r\nYou should know that C++'s standard sorting method on arrays is O(N Log N). We\r\nwill discuss the algorithm (and many other sorting algorithms) later in the\r\nquarter. But whenever you are asked to sort, assume an O(N Log N) algorithm\r\nunless told differently.\r\n\r\nNote that we CANNOT perform binary searching efficiently on linked lists,\r\nbecause we cannot quickly find the middle of a linked list (for arrays we just\r\ncompute the middle index and access the array there, which is O(1); for linked\r\nlists we would have to scan over N/2 values, which is O(N)). In fact, we will\r\nsoon see another self-referential data structure, trees, that can be used to\r\nperform efficient searches.\r\n\r\nSorting is one of the most common tasks performed on computers. There are\r\nhundreds of different sorting algorithms that have been invented and studied. \r\nMany small and easy to write sorting algorithms are in the O(N^2) complexity\r\nclass (see the sorting code above, for example). Complicated but efficient \r\nalgorithms are often in the O(N Log N) complexity class. We will study sorting\r\nin more detail in ICS-46, including a few of these efficient algorithms. We\r\nwill also discuss the concept of \"stable sorting\" at that time.\r\n\r\nFor now, memorize that fast (binary) searching is in the O(Log N) complexity\r\nclass and fast sorting algorithms are in the O(N Log N) complexity class. If\r\nyou are ever asked to analyze the complexity class of a task that requires\r\nsorting data as part of the task, assume the sorting function/method for that\r\npart is in the O(N Log N) complexity class.\r\n\r\n\r\nClosing:\r\n\r\nTo close for now, finding an algorithm to solve a problem in a lower complexity\r\nclass is a big accomplishment; a more minor accomplishment might be decreasing\r\nthe constant for an algorithm in the same complexity class (certainly useful,\r\nbut often based more on technology than science). By knowing the complexity\r\nclass of an algorithm, we know a lot about the performance of the algorithm as\r\nthe problem size gets big. We can measure the time it takes to solve certain\r\n(big) sized problems, and using this information we can accurately predict the\r\ntime to solve other (big) sized problems. Finally, we can also invert the\r\nprocess, and use a few measurements, doubling the size of the (big) problem\r\neach time, to approximate the complexity class of an algorithm.\r\n", "encoding": "ascii"}