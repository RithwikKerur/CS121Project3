{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/hashing.txt", "content": "\t\t\tHashing and Hash Tables\r\n\r\n\r\nIntroduction:\r\n\r\nHash tables are a data structure for storing and retrieving unordered\r\ninformation, whose primary operations are in complexity class O(1) -\r\nindependent of the number of items stored in the hash table. We saw that\r\ndigital trees had this same property, but only for special keys (that were\r\ndigital: meaning we could decompose them into a first part of the key, a second\r\npart of the key, etc. as we can with digits in a number and characters in a\r\nString). Hash tables work with any kind of key. The most commonly used\r\nimplementations of sets and maps (which are unordered) are implemented by hash\r\ntables. We will also implement a map via a hash table (including iterators) in\r\nProgram #4.\r\n\r\nHere are some terms that we need to become familiar with to understand (and\r\ntalk about) hash tables: hash codes, compression function, bins/buckets, \r\noverflow-chaining, probing, load factor, and open-addressing. We will discuss\r\neach below.\r\n\r\n\r\nOur Approach:\r\n\r\nWe will start by discussing linear searching (using a linked list) of a\r\ncollection of names. If we instead used an array of 26 indexes and put in\r\nindex 0 a linked list of all names starting with \"a\", and in index 1 a linked\r\nlist of all names starting with \"b\", ... and in index 25 a linked list of all\r\nnames starting with \"z\", we could search for a name about 26 times faster by\r\nlooking just in the correct index for any name (according to its first letter).\r\nThis speed increase assumes each letter is equally likely to start a last name,\r\nwhich is not a realistic assumption. Even if the assumption is true, the\r\ncomplexity class for searching is still O(N), but with a constant 1/26th less\r\nthan the original constant. Meaning, however long it takes to search among N\r\nnames, it would take 1,000 times as long to search among 1,000N names: each of\r\nthe linked lists's length would grow by a factor of 1,000.\r\n\r\nIn fact, if we used an array of 26x26 (676) indexes, storing in index 0 a\r\nlinked list of all names starting with \"aa\", and in index 1 a linked list of\r\nall names starting with \"ab\", ... and in index 676 a linked list of all names\r\nstarting with \"zz\", we could search for a name 676 times faster by looking\r\njust in the correct index for any name (according to its first two letters).\r\nThis speed increase assumes each letter pair is equally likely to start a last\r\nname, which is an even less realistic assumption. Even if the assumption is\r\ntrue, the complexity class for searching is still O(N), but with a constant of\r\n1/676th less than the original constant.\r\n\r\nOf course, this speedup isn't achieved unless we have many names, and each box\r\nis equally likely to have a name (which isn't true: few names start with\r\ncombinations like \"bb\", etc). And what about looking-up information that\r\naren't strings: for example, we might want a set storing queues or a map whose\r\nkeys are priority queues. So while this approach seems promising, we need to\r\nmodify it to be truly useful.\r\n\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHash Codes:\r\n\r\nHashing is that modification. We declare an array with any number of \"bins or\r\nbuckets\" and use a \"hash code function\" to compute an integer fingerprint for\r\nany piece of data that can go into the hash table - indicating in which\r\nbin/bucket it belongs. A hash code function must always compute the same hash\r\ncode for the same value, so it cannot use random numbers. We should design such\r\na hash code function to generate the widest variety of numbers (over the range\r\nof all integers), with as small a probability as possible of two different\r\nvalues hashing to the same integer.\r\n\r\nOf course, in the case of using strings as values, there are more strings than\r\ninteger values. For 32-bit ints, there are only about 4 billion different\r\nvalues -actually, exactly 4,294,967,296- but an infinite number of strings,\r\nwhich can be of any length, meaning any number of characters: even if we\r\nconsider only strings with lower-case letters, there are 26^N different strings\r\nwith N chacters; 26^7 is 8,031,810,176 so there are already more 7-letter\r\nstrings than 32-bit ints.\r\n\r\nOnce we have a hash code function, we use a \"compression function\" to convert\r\nthe hash code into a legal index in our hash table. One simple compression\r\nfunction computes the absolute value of the hash code (hash codes should cover\r\nboth negative and positive values but array indexes are always non-negative)\r\nand then computes the remainder (using the % operator) using the hash table\r\nsize/length as the 2nd operand, producing a number between 0 and length-1 of the\r\nhash table. Other compression functions use-bitwise operations to compute a bit\r\npattern in the correct range (doing so is faster than computing a remainder,\r\nbut it typically works only for hash table sizes/lengths that are a perfect\r\npower of 2: of course we can arrange hash table to start at length/size 1 and\r\nalways double their size to meet this constraint).\r\n\r\nHere is one example of a hash code function for strings.\r\n\r\n  int hash(const std:string& s) {\r\n    int hash_code = 0;\r\n    for (int i = 0; i < s.size(); ++i)\r\n      hash_code = 31*hash_code + s[i];//promotion: char -> int: its ASCII value\r\n    return hash_code;\r\n  }   \r\n\r\nHere, hash(\"a\") returns 97 ('a' has an ASCII value of 97) and hash(\"aa\")\r\nreturns 3104 (31*97 + 97). Generally, if s.size() is n (the chars array\r\ncontains n values), then its hash value is given by the formula\r\n\r\n   chars[0]*31^(n-1) + chars[1]*31^(n-2) + ... + chars[n-2]*31^1 + chars[n-1]\r\n\r\nSo, hash(\"ICS46\") returns 69,494,459, and hash(\"Richard Pattis\")\r\nreturns -125,886,044! Yes, because of arithmetic overflow and the standard\r\nproperties of binary numbers, the result might be negative (and overflow of\r\nnegative numbers can go positive again). Recall that C++ does not throw any\r\nexceptions when arithmetic operators produce values outside of the range of\r\nint: hashing is one of the few places where this behavior produces results\r\nthat are still useful. \r\n\r\nGenerally the hash function for all the numeric types is an int value with\r\nthe same bit pattern (just interpret those bits as an int). Characters hash to\r\ntheir ASCII values. We can build hash functions for other types in C++ from\r\nfrom these: for examples strings, are an ORDERED sequence of chars and we can\r\ncompute the hash code of the string by looking at every char in it. Note\r\nthat generally hash(\"ab\") != hash(\"ba\"). When hashing strings, the order of the\r\nletters is important. In fact, a good hash code function for any ordered data\r\ntype (say a queue or a stack) will produce different values for different\r\norders of the same values.\r\n\r\nIn C++ we can use the following to compute the hash codes. Here\r\nstd::hash<std::string> is a class whose default constructor initializes\r\nstr_hash, which overloads call -the () operator- to compute a hash value for\r\nstd::string.\r\n\r\n  std::hash<std::string> str_hash;\r\n  std::cout << str_hash(\"ICS46\") << std::endl;\r\n\r\nInterestingly, str_hash(\"a\") returns 2,167,009,006. So it is using a hash code\r\nfunction different from the one shown above. I have not been able to find much\r\non the web about how C++ actually computes the hash code for strings, but that\r\ninformation is irrelevant for using it.\r\n\r\nJava uses a simpler approach: every class in Java overloads the hash_code\r\nmethod: it is parameterless, and returns an object's hash code. If we used this\r\nstyle in C++, here is an example of how we could compute the hash code for a\r\nqueue: it uses an iterator to compute the hash code for every value in the\r\nqueue. Computing the hash code for a sequence of values in a queue is similar\r\nto computing the hash code for a sequence of characters in a string, because\r\nthe order is important. \r\n\r\nWe can define this same hash_code function in every class implementing a Queue,\r\nbecause it is the same for every queue implementation.\r\n\r\n  int hash_code() {\r\n    int hash = 0;\r\n    for (auto e : *this)\r\n      hash = 31*hash + e.hash_code();\r\n    return hash;\r\n  }\r\n\r\nIt is critical that the operator== and hash_code methods for a class are\r\ncompatible. The key property is: if  a == b then a.hash_code() == b.hash_code().\r\nOf course the opposite is NOT true, because many different strings have equal\r\nhash codes, because there are more strings than 32-bit ints: of course it is\r\nunlikely that many different strings actually used in some problem will have\r\nthe same hash code (although it is likely that some will; we will look at\r\nempirical results to get a better understanding of the actual numbers).\r\n\r\nThis compatibility requirement is very important for UNORDERED collections like\r\nsets. Typically we iterate through the values of a set to compute the hash code.\r\nBut, values in the set can be stored in (and iterated through in) any order.\r\nRegardless of the order these values are processed, they must compute the\r\nsame hash code each time hash_code is called (because set implementations that\r\nhappen to store their values in different order are still ==).\r\n\r\nSo, hash codes are different, but only slightly, for unordered collections\r\n(like a set). Since no matter what order the values are stored, the hash codes\r\nshould be the same, we cannot use the hash code method above, but instead must\r\nuse something that accumulates the hash code values of its elements without\r\nregard to their order. Here we just add together (without the weighting of 31*)\r\nall the values.\r\n\r\n  public int hash_code() {\r\n    int hash = 0;\r\n    for (auto e : *this)\r\n       hash += e.hash_code();   //or we could use hash *= e.hash_code()\r\n     return hash;\r\n   }\r\n\r\nThus, if we add or multiply together all the hash codes, it doesn't make a\r\ndifference what order we do the addition or multiplication: a+b+c, b+a+c,\r\nc+b+a, etc. all compute the same value (as does a*b*c, b*a*c, c*b*a, etc.)\r\n\r\nFinally, here is something that I found interesting, when I came across it when\r\nI was reading the Java String class. The real hashCode method in string looks\r\nlike the following, with cachedHash being an instance variable for all objects\r\nin the String class, which is initially set to 0. The first time hashCode is\r\ncalled, cachedHash is 0 so it computes the hash value and stores it in\r\ncachedHash before returning it. For every other time it is called, it\r\nimmediately returns cachedHash, doing no further computation. Note that in\r\nJava, Strings are immutable, so once they are constructed their contents do not\r\nchange, so once the hashCode is computed for a String object, that String\r\nobject will always return the same result for its hashCode, so we don't have to\r\nrecompute it. Here is how this method looks in Java.\r\n\r\n  public int hashCode() {\r\n    if (cachedHash != 0)\r\n      return cachedHash;\r\n\r\n    int hash = 0;\r\n    for (int i = 0; i < chars.length; i++) {\r\n      hash = 31*hash + chars[i];   //promotion of char -> int\r\n    return cachedHash = hash;\r\n  }   \r\n\r\nIf a String's computed hashCode is 0, even after its hashCode is computed and\r\ncached, it will be recomputed (because with the != 0 test, Java cannot tell\r\nthe difference between a hash code that has not been computed and a hash code\r\nthat has been computed with value 0). Typically, the only String whose hash\r\ncode is 0 is \"\"; most other Strings will have a non-0 hashCode. Recomputing the\r\nhash code of \"\" is very quick, because it stores no values (chars.length is 0,\r\nso the loop immediately exits). We could include an extra boolean instance\r\nvariable named hashIsCached, initialized to false and set it to true after\r\ncaching. So we would have\r\n\r\n  public int hashCode() {\r\n    if (hashIsCached)\r\n      return cachedHash;\r\n\r\n    int hash = 0;\r\n    for (int i = 0; i < chars.length; i++) {\r\n      hash = 31*hash + chars[i];   //promotion of char -> int\r\n\r\n    hashIsCached = true;\r\n    return cachedHash = hash;\r\n  }   \r\n\r\n...but that extra instance variables is a bit overkill. \r\n\r\nOf course, we also could compute the hashCode of every String WHEN IT IS\r\nCONSTRUCTED, storing it in cachedHash, and never checking this value, always\r\nreturning the cached value. The upside is that the hashCode function would\r\nalways just return this cached value; the downside is that we would have to\r\ncompute the hash code for every String when it was created, even if we were\r\nnever are going to call hashCode on it. The approach above, actually used in\r\nJava, caches a hash code only if asked to compute it at least once.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHash Tables with Overflow-Chaining\r\n\r\nHash Tables are used frequently in practice, so there has been a lot of\r\nstudies, both theoretical and empirical, of hash code methods, which are\r\nat the heart of Hash Tables working efficiently. The best methods are quick\r\nto compute and return results scattered all over the range of int. Given such\r\na hash code, the rest of the code to implement a Hash Table (see below) is\r\nstraightforward.\r\n\r\nAccompanying this lecture is a program and an example of the results produced\r\nby running the program, which empirically examines the hash code function shown\r\nearly in this lecture for strings (and you can write your own hash code to test\r\ntoo). The program itself was originally written in Java and I converted it to\r\nC++. It allows students to posit various hash code functions and test them\r\nby generating random strings (which is an easy way to test a hash function, but\r\nmay not accruately model the strings used in a real application), putting them\r\nin the hash table, and counting the collisions: when two different values\r\nHash/Compress to the same bin. We will look at this data briefly.\r\n\r\nNow let's look at how to insert a value into a hash table. We will assume that\r\nas in sets and map keys, the values are distinct. Recall the basic structure\r\nused in a hash table is an array of bins/buckets.Here, each refers to a linked\r\nlist of values that hash/compress to that index. The basic picture looks like\r\nthe following (here with 5 bins/buckets, 0-4, containing 6 values, v1-v6).\r\n\r\n   Bin/Bucket     Collisions (handled through chaining, using a linked list)\r\n\r\n     +---+\r\n     |   |    +----+---+    +----+---+\r\n  0  | ------>| v1 | --+--->| v2 | / |\r\n     |   |    +--------+    +--------+\r\n     +---+\r\n     |   |\r\n  1  | / |\r\n     |   |\r\n     +---+\r\n     |   |    +----+---+\r\n  2  | ------>| v3 | / |\r\n     |   |    +--------+\r\n     +---+\r\n     |   |    +----+---+    +----+---+\r\n  3  | ------>| v4 | --+--->| v5 | / |\r\n     |   |    +--------+    +--------+\r\n     +---+\r\n     |   |    +----+---+\r\n  4  | ------>| v6 | / |\r\n     |   |    +--------+\r\n     +---+\r\n\r\nGenerally, bins can have zero, one, or many values. We say that values v1 and\r\nv2 COLLIDED in bin 0, because these two different values both hashed/compressed\r\nto the same bin in the hash table. And we have used \"overflow chaining\" (using a\r\nlinked list) to keep track of all the \"collisions\"/\"overflows. With good hash\r\nand compression functions, the values stored in a hash tabled should be\r\napproximately equally distributed throughout the bins. Of course, there will\r\ntypically be some bins with fewer values (many may store none) and some bins\r\nwith more because hash codes aren't perfect. If a hash code always returned 0\r\n(terrible!) then we would end up with all values colliding and being stored in\r\nbin/bucket 0.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHash Table Algorithms:\r\n\r\n3 Important Algorithms manipulating Hash Tables\r\n\r\n1) insert(for Set)/put or setting with [] and = (for Map):\r\n   Use hash_code/compression to compute a bin index (for the value/key).\r\n   Search all the collisions to see if the information is there\r\n      (sets have unique values/maps have unique keys).\r\n    If it is there: for a Sets don't change anything; for Maps change the value\r\n       associated with the key in the ics::pair.\r\n    If it is not there: add the information anywhere convenient in that\r\n      bin: in a list node at the front, rear, wherever: there is no required\r\n      ordering of information in the bins for sets and maps.\r\n\r\n2) contains(for Set)/has_key or lookup with [] (for Map):\r\n   Use hash_code/compression to compute a bin index.\r\n   Search through the linked list (all the collisions) to see if the\r\n     information is there.\r\n\r\n3) erase(for Set/Map)\r\n   Use hash_code/compression to compute a bin index.\r\n   Search through the linked list (all the collisions) to see if the\r\n     information is there, and if it is, remove it from the linked list\r\n\r\nAs with the original array implementations of all the data structures, we could\r\nalso decrease the array size when it becomes sparse. Recall the rule was when\r\nthe array was 1/4 filled, its size would be reduced by 1/2 (leaving  the result\r\nhalf filled -or half empty).\r\n\r\n------------------------------\r\n\r\n2 more Important Algorithms\r\n\r\nThe \"Load Factor\" of a hash table is the ratio of values it contains divided\r\nby the number of bins in the hash table. It computes the expected number of the\r\nvalues that will hash/compress to each bin. Generally classes using hash tables\r\ntry to keep the load factor below 1 (more bins than values, so each bin\r\ncontains zero or very few values -hopefully just 1, but this is typically not\r\nachieved: see empirical analysis).\r\n\r\nWhen the insert/put method is called for a Set/Map, if a new value is to be put\r\nin the hash table, the load factor is checked; if adding the new value will make\r\nthe hash table exceed the load factor, we increase the size/length of the hash\r\ntable to ensure that it is always below the specified threshold. Typically we\r\ndouble the size of the hash table, when we must increase its size.\r\n\r\n4) Double Size/Length of Hash table\r\n   Remember the old hash table array and allocate a new/empty one 2 times as big\r\n   Traverse the old hash table (you can do it directly or use the Iterator\r\n     if one is available, but that will be slower), adding each value to the\r\n     new hash table, but NOT NECESSARILY IN THE SAME BIN! Instead, add it by\r\n     applying the same hashing/compression again (compression will be DIFFERENT,\r\n     because the size of the new hash table is doubled, so we may compute the\r\n     same hash value, but compute the remainder using the DIFFERENT TABLE SIZE).\r\n   By being clever, we can re-use the entire LN (list node), so we don't have\r\n     to allocate any new objects when doubling the hash table size; but this\r\n     makes the code harder to write.\r\n\r\n5) Iterator: Uses a combination of index and cursor instance variables.\r\n   (a) Constructor:\r\n     Loop to find the first bin with a list of values (not nullptr):\r\n       Succeed: set index to the bin number, cursor to the first LN in the bin\r\n       Fail   : set cursor to nullptr\r\n\r\n   (b) ++:\r\n     Advance cursor = cursor.next; if it becomes nullptr, loop to advance the\r\n       index to later bins, stopping at the first non-nullptr bin\r\n         Succeed: set index to bin number, cursor to the first LN in the bin\r\n         Fail   : set cursor to nullptr\r\n\r\n   (c) Erase in Iterator\r\n     Store previous cursor (in extra instance variable) to help do removal.\r\n       or\r\n     Store no extra information but use a trailer node in every bin (which\r\n       makes removal easier; we will do this in Programming Assignment #4)\r\n\r\nNote that iterators for data types implemented by hash tables return their\r\nvalues in a \"strange\" order (based on their hash values and collisions). In\r\nfact, adding one value to a hash table can cause it to exceed its load factor,\r\nand thus it will double the number of bins (doubling the size/length of the hash\r\ntable) which causes rehashing. Now, the iterator order might be completely\r\ndifferent. So, we should never assume any special ordering for these iterators,\r\nsince data types might be implemented by hash table data structures. If we want\r\nto ensure they are processed in a specific order, we should put the values\r\nproduced by iterators into a PriorityQueue and iterate through it.\r\n\r\n----------\r\nNote that in the STL there are maps and unordered maps. Regular maps are\r\ntypically implemented by BST (or self-balancing trees) whereas unordred maps\r\nare typically implemented by hash tables. Generally hash table operations are\r\nfaster than trees: O(1) vs O(Log N). But if we must iterate over the data in\r\na specific key order frequently, using regular maps might be better; of course\r\nwe can always iterate over an unordered map in an ordred way by putting all its\r\nvalues in a priority queue and then iterate over the priority queue in any\r\nspecified (by a function) order, not just the order of the keys in a BST.\r\n---------\r\n\r\nImplementing Overflows: Linked Lists and Beyond\r\n\r\nNote that we can store in each bin an array of values, a linked list of values,\r\na binary search tree of values, etc.(in fact, we can use anything that\r\nimplements a Set data type). The reason that linked lists, and not more exotic\r\ndata structures, are used, is that with a load factor <= 1 we expect to find\r\nfew values in each bin, so more exotic data structures just make the coding\r\nmore difficult with little gain in speed for searching a small number of values.\r\nBut if we are going to use a load factor >> 1, using more complicated data\r\nstructures to handle overflows might be a good idea.\r\n\r\nWe can even use another Hash Table (with a different hashing function) to\r\nstore the overflows.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nWhy are hash tables O(1) with good hash functions:\r\n\r\nIn the worst case, a hash-code will produce exactly the same value for\r\neverything it hashes (not likely, but possible) so no matter how big the hash\r\ntable is, we would go to the same bin and put/search for all N values there.\r\nSuch a process would be O(N). But let's assume that we are using a hash_code\r\nthat does a pretty good job (as most do: review the empirical data).\r\n\r\nFor such a good hash code, if the table size/length is M, we would expect to\r\nhave to search for N/M values in each bin. Thus, for any given M the method is\r\nO(N/M), but that is just O(N) because M is \"a constant\" so we remove it from our\r\nbig-O notation.\r\n\r\nBut, we are doing something a bit more subtle. By keeping the load factor <= 1,\r\nfor example, we ensure that M >= N (say M is always at least N, and sometimes\r\nas much as 2*N, right when the load factor exceeds 1 and we double the\r\nsize/length of the hash table). So, M IS NOT A CONSTANT, but it grows linearly\r\nwith N. In fact, for a load factor <= 1, we know that M >= N. In the \"worst\r\ncase\" M = N. That is, since M >= N\r\n\r\n  N/M < N/N = 1\r\n\r\nTherefore the complexity class of O(N/M) is reallyO(N/N) or O(1). In fact, for\r\nany constant load factor the complexity class is O(1). For a load factor of\r\n10, the complexity is O(N / N/10) or O(10) which is the same as O(1). Certainly\r\nwith this bigger load factor, we'd expect to spend 10 times longer searching\r\nbut on average we'd still examine some fixed number of values in each bin, no\r\nmatter how big N grows.\r\n\r\nSo the magic here is that while O(N/M) for a constant M is O(N), when M is \r\nlinearly related to N, growing as N grows such that N/M is <= some constant,\r\nO(N/M) is O(1).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nSecurity via Hashing as a 1-way function: A Digression\r\n\r\nGiven a string, it is very easy to compute the hash code of it; but it is\r\ntypically not very easy, given a hash code, to determine what string(s) will\r\nhash to that value. In mathematics, such functions are called 1-way (or\r\nnon-invertible) functions. There are many kinds of 1-way functions beyond\r\nhashing functions. We can use any 1-way function to provide security: let's\r\nlook at one example, supplying password security, using hashing.\r\n\r\nHave you ever wondered how a computer system stores your password? If the\r\ncomputer stored everyone's password in a file (as a list of user-ids and their\r\npasswords), then anyone who could steal/read that file could compromise all the\r\naccounts. Here is another way to store this information: instead store a list of\r\nuser-ids and the hash code of their password. When a user tries to log in, the\r\nsystem would hash the password they type in, and see if it matched the hashed\r\nentry in the password table.\r\n\r\nAssume the hashing method is public (if it wasn't, someone could steal it\r\nanyway), but it would be a 1-way function. So, even knowing the algorithm\r\nwouldn't allow you to easily compute a password from its hash code (but would\r\nallow you to easily compute a hash code from a password).\r\n\r\nNow, if someone could read the password file, they would see only the hash code\r\nof the passwords, but not the passwords themselves. So they wouldn't know what\r\npassword to use. Of course, they could write a program that generated all\r\npossible strings, hash each, and look for one that had the same hash code. Then\r\nthey could log into the system with the userid and that password (which would\r\nhash to the one stored in the password file, even if it wasn't the same\r\npassword: getting the right hash code is all that is important).\r\n\r\nThat is why you are encouraged to have passwords that are long, and have upper\r\nAND lower case letters (and maybe even symbols in them). It increases the size\r\nof the alphabet, so makes it harder to search for passwords that hash to the\r\nright value, when searching over all symbols in that alphabet harder.\r\n\r\n(although, hackers have lots of information about the passwords structures\r\nthat are used, and can search more efficiently than generating all possible\r\npasswords)\r\n\r\nAssume that a computer could compute 10^9 (a billion) hash codes per second.\r\nThere are 1.4 x 10^17 different strings of length 10 (52^10, using upper- and\r\nlower-case letters). If we tried to generate all these strings, and hash each,\r\nand compare it to the one we are looking for, it would take  about 1.4 x 10^8\r\nseconds, or about 4.5 years. This is another reason to change your password\r\nfrequently (say, every 4.5 years!). Of course, such a search doesn't really need\r\nto find your password: it just needs to find a password that has the same hash\r\ncode as your password. Often such hash codes generate integers that are 2^62\r\nbits, allowing for ~ (2^10)^6 ~ 10^18 different hash values. So with an optimal\r\nhash code, every 10 letter string would have its own different hash code (not\r\nreally an easy thing to do).\r\n\r\nActual password systems are more complicated these days, and use advanced\r\ncryptographic methods. But these methods themselves are typically based on the\r\ngeneral theory of 1-way/non-invertible functions: the functions are just much\r\nmore interesting than the ones we have seen for computing the hash codes of\r\nstrings.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nThe Complexity of Doubling Arrays:\r\n\r\nWe have seen that array data structures are used to store all kinds of data\r\ntypes: I have supplied array implementations of all the templated classes, and\r\nsome advanced implementations that we will write, like HeapPriorityQueue and\r\nHashMap, also are based on arrays. In all these implementations, when adding\r\nvalues we must determine whether to double the size/length of the array. Most\r\ntimes we add a value, we don't double the size/length of the array: in all the\r\ncode we've seen before, we just put the value in the next unused index in the\r\narray (or in a hash table, chain it in a linked list in its bin). But, as the\r\nsize/length grows, eventually we double it, which means we must copy all the\r\nvalues currently in the array to a new array. So most additions are O(1) but\r\nsome adds are O(N). \r\n\r\nSo, if we are talking about upper bounds, we might say at worst each addition is\r\nO(N) and we do N adds, so the process of doubling and copying to get N values\r\ninto an array is O(N^2). But, we can derive a better (smaller) upper bound.\r\nWe will talk about \"amortized complexity\" to analyze this case.\r\n\r\nAt worst, we allocate a collection to have 1 array cell. Adding the 1st value\r\nstores it in the array and requires no copying. Adding a 2nd value doubles the\r\nsize of the array, copying the 1st value (so 1 copy). Adding a 3rd value\r\ndoubles the size of the array, copying the 1st-2nd values (so 2 more copies)\r\nAdding a 4th value stores it in the array and requires no copying. Adding a\r\n5th value doubles the size of the array, copying the 1st-4th  values (so 4 more\r\ncopies). Adding the 6th-8th value stores them in the array and requires no\r\ncopying. Adding a 9th value doubles the size of the array, copying the 1st-8th\r\n values (so 8 more copies). Adding the 10th-16th value stores them in the array\r\nand requires no copying. Etc.\r\n\r\nEach time we double the length, we copy twice as many values as before, but we\r\ncan add twice as many values before having to double again. If we end up with\r\nN values in the array, what is the total number of copies that we have to make?\r\nWe will see below it is O(N) -actually bounded by 2N.\r\n\r\nWhen we double the array size from 1 to 2, we have copied 1 value in total.\r\nWhen we double the array size from 2 to 4, and copy 2 more values for a total\r\nof 1+2=3 copies. When we double the array size from 4 to 8, and copy 4 more\r\nvalues for a total of 1+2+4=7 copies.\r\n\r\nNotice that the sum 2^0 + 2^1 + 2^2 + ... + 2^N = 2^(N+1) - 1. We used this\r\nformula before to compute the maximum number of nodes in a binary tree of\r\nheight h: 2^(h+1) - 1 (which has 1 node at depth 0, 2 nodes at depth 1, 4\r\nnodes at depth 2, etc).\r\n\r\nHere is a table. On the left, N is the number of values in the array, and\r\non the right is the total number of values we need to copy.\r\n\r\n  N    Movements/Copying\r\n--------------------------\r\n  1      0\r\n  2      1\r\n 3- 4    3 = 1 (1->2) + 2 (2->4)\r\n 5- 8    7 = 1 (1->2) + 2 (2->4) + 4 (4->8)\r\n 9-16   15 = 1 (1->2) + 2 (2->4) + 4 (4->8) + 8 (8->16)\r\n17-32   31 = 1 (1->2) + 2 (2->4) + 4 (4->8) + 8 (8->16) + 16 (16->32)\r\n33-64   63 = ....\r\n\r\nNotice that for N a perfect power of 2, there are N-1 copies. When N is 1 bigger\r\nthan a power of two, there are at most 2*N-3 copies. So, the number of times\r\nthat we must copy a piece of data as an array grows linearly and is O(N).\r\n \r\nRather than thinking about \"adding\" sometimes doing very little work and\r\nsometime doing lots, we can think about \"adding\" doing a bit of extra work (a\r\nconstant amount) every time that we call it: really, most of the time we don't\r\ndo the work, but every so often we have to do a lot of work, not just for the\r\nnew value, but for all the ones before it. This is called amortized complexity.\r\nStill, the total work done for adding N values is just O(N) -it cannot be less\r\nbecause N*O(1) is O(N). You will study this more in ICS-161.\r\n\r\nAnother way to think of this is just how much extra work is there to double\r\nthe number of values in an array. If the original length is N (say it is a\r\npower of 2), then when we add N more values, we will have to first copy\r\neach of the N values originally in the array into the new array, and then copy\r\neach of the new N values into the array. Thus, overall, adding N more values\r\ninto the array requires copying N values and then adding N values without\r\ncopying, for a total of 2N operations. The total complexity is still just O(N),\r\nsince every add required the actual add and a total of N adds also required\r\ncopying a total of N values originally in the array. Think about every add as\r\ncounting for itself and one of the copying operations.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nMutating a Value in A Hash Table (or BST):\r\n\r\nIf we are using a BST or hash table to store an object in a Set or a key to a\r\nMap, we should not mutate that object inside the Set/Map. This is because the\r\nplace it is stored depends on the value/key: when we put a value into a tree,\r\nwe use comparisons to determine in which subtree(s) it belongs in; when we put a\r\nvalue into a hash table, we compute its hash code to determine which array\r\nindex it belongs in.\r\n\r\nSo, if we mutate a key in a tree or hash table, it probably will not belong\r\nwhere it currently is: it would be in a different location in a tree or\r\ndifferent bin in a hash table. That is, the code to locate and store a value in\r\nSet/key in a Map is based on using its state (for comparison or hashing). If we\r\nstore a value in  a Set/key in a Map, and then mutate it (change its state)\r\nthen we may never be able to locate that value/key again.\r\n\r\nSo, it is a good idea to use immutable classes for keys, or at least be extra\r\ncareful not to mutate a value INSIDE a tree/key INSIDE a Map. Instead, we can\r\nremove it, change the key, and then add it back. For a hash table, both removal\r\nand addition are O(1) operations, so although awkward, changing a value in this\r\nway (remove it, change it, add it) only requires a constant amount of work to\r\nupdate it in the hash table.\r\n\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHashing with Open Addressing Instead of Chaining:\r\n\r\nThe final big topic on hashing is collision/overflow resolution without overflow\r\nchaining. By far the most useful way to handle different values that hash to\r\nthe same bin is to store all these results in a linear linked list that the\r\nbin points to. Hash table operations will do a linear search of such a list,\r\nwhich, by having a good hash code and low load factor, will generally not be\r\nlong. But this approaches leaves some bins empty (about 1/3 from our empirical\r\nanalysis) and linked lists require extra space that must be dynamically created\r\nand deallocated.\r\n\r\nThere is an alternative way to deal with collisions. While it takes up no extra\r\nspace (compared to chaining, which constructs new LN objects) it can cause an\r\nincrease in the time needed to search for a value in a hash table, unless the\r\nload factor is kept low (< 70%). A load factor > 1 is not even possible, because\r\nwe use a different bin for each value, so we must have as many bins ans values.\r\n\r\nThe method is called Probing via Open Addressing. We will discuss 3 different\r\nforms of probing: linear, quadratic, and double hashing.\r\n\r\nIn linear probing, we compute the bin for storing a value; if the table already\r\ncontains a value at that bin (we must be able to distinguish indexes with and\r\nwithout values: we can use a parallel array of information, or an array of\r\npointers to objects and use nullptr to detect the absence of data), we\r\nincrement the index by 1 circularly (incrementing the last array index brings\r\nus back to index 0) and keep probing bins until we find an empty bin, and then\r\nput the value there.\r\n\r\nSo, the find method hashes to a bin and checks if the value is there; if not,\r\nit continues from the original bin, linearly looking through other bins,\r\nuntil we (a) find the value we are looking for, or (b) reach an empty bin\r\n(meaning the value is not in the hash table; if it were there we would have\r\nreached it before an empty bin). Note that many values can be searched,\r\nincluding many that have different hash codes that happen to be a bit bigger\r\nthan the hash code of the value we are looking for. For example, let's use\r\nlinear probing via open addressing for the following hash table.\r\n\r\n   0     1     2     3     4    5      6     7     8     9\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n|     |     |     |     |     |     |     |     |     |     |\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n\r\nThus, if \"a\" hashes to bin 4, we put it there because it is empty.\r\n\r\n   0     1     2     3     4    5      6     7     8     9\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n|     |     |     |     | \"a\" |     |     |     |     |     |\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n\r\nLikewise, if \"b\" hashes to bin 5 we put it there because it is empty.\r\n\r\n   0     1     2     3     4     5     6     7     8     9\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n|     |     |     |     | \"a\" | \"b\" |     |     |     |     |\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n\r\nBut now, if \"c\" hashes to bin 4, we have to probe bin 4 and 5 until we find\r\nthat bin 6 is the first empty one after 4, and put \"c\" there.\r\n\r\n   0     1     2     3     4     5     6     7     8     9\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n|     |     |     |     | \"a\" | \"b\" | \"c\" |     |     |     |\r\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\r\n\r\nIf we are looking to see whether \"d\" is in the table (say it hashes to\r\nbin 4) we start probing at bin 4 for d, then check bin 5, bin 6, and finally\r\nbin 7: it is empty, so we know \"d\" is not in the hash table (if it were, we\r\nwould find it before the empty value). With chaining we would never examine\r\n\"b\" because it would be in a different bin/bucket from \"a\", \"c\", and \"d\".\r\n\r\nAnother problem with probing via open addressing involves removing values. If\r\nwe want to remove \"b\", we first try to find it by hashing to bin 5 and find it\r\nthere. But if we actually removed it (making it empty), if we were looking\r\nfor \"c\" the following problem would occur: we hash \"c\" (see above) and get bin\r\n4, then we look at the next bin (which is now empty, because we removed \"b\")\r\nand we would think that \"c\" is not in the hash table because we reached an\r\nempty bin first. But at the time \"c\" was added, there was a value in bin 5,\r\nwhich is why \"c\" had to go to bin 6 and not bin 5.\r\n\r\nSo, in order to know we are done when we reach an empty bin, when we remove a\r\nvalue, we find it, and mark its bin as \"available\". Bins marked \"available\"\r\nhad previously stored values and can store new values (if we reach them in the\r\ninsertion algorithm), but unlike empty bins they cannot stop the probing:\r\nprobing must continue until it reaches the value to locate or has passed\r\nthrough all occupied and \"available\" bins and reached a bin that has always\r\nbeen empty. We can use a parallel array that stores special values to represent\r\nbin that is \"empty\", \"used\", and \"available\" (= was \"used\", but not now).\r\n\r\nIteration over such a structure is easy: it is just a linear traversal of the \r\narray, skipping unoccupied (empty or available) positions. Likewise we can\r\neasily create a hash table twice as big, iterate through the original hash\r\ntable and put (by hashing with a new compression function) each value into the\r\nnew hash table.\r\n\r\nInstead of linear probling (where the bin number increases by 1 every time),\r\nwe can do quadratic probing, where the bin number increases by ai+bi^2 (for\r\ni=0 the first time, i=1 the second time, etc. once we specify the values for\r\na and b: if both are 1/2, e.g., 1/2(i+i^2) we probe hash, hash+1, hash+3,\r\nhash+6, hash+10, etc). Of course the compression function handles indexes\r\nthat get bigger than the table size/length.\r\n\r\nLikewise in double hashing, we use a second hash method h2 (hash, hash+h2(1),\r\nhash+h2(2), hash+h2(3), ... ) to compute a value that is continually added to\r\nthe bin index (with the compression function) until the value is located or an\r\nempty bin is found.\r\n\r\nUnlike linear probing, by using quadratic probing or double hashing, the probe\r\nsequence \"after a bin\" depends on how many probes it takes to reach that bin in\r\nthe first place. That is, in quadratic probing if we hash to a bin, the next\r\nprobe is at hash+1; but if we reach that bin on the third probe (getting there\r\nas hash+6) the next probe is hash+10. This typically improves performance by\r\nspreading out (avoiding clustering) values in the hash table.\r\n\r\nUnless space is critical, it is typically better to use overflow chaining than\r\nany kind of probing in open-addressing discussed above. Often the extra\r\noverhead of the LN is small compared to its data, although doing chaining\r\nrequires using storage allocation/deallocation.\r\n\r\nHash tables using probing via open addressing get clogged up with values in a\r\nnon-linear way: as the load factor approaches 1, the searching time approaches\r\nO(N). So, if this method of probing is used, we need to keep the load factor\r\nlower, say at .7. You can simulate such a hash table and measure the\r\nperformance degradation by counting the average number of probes at various\r\nload factors.\r\n\r\nIf you look on the Programs link on the course web site (or the link for this\r\nlecture), you will see a download that allows you to test various hashing\r\nfunctions statistically. If you want, you can write your own hashing function\r\nand test it compared to the one built-in to C++. There are two drivers there,\r\none testing chaining and one testing open addressing. It would also be useful\r\nto test hash functions for the amount of time they take to compute their result.\r\n\r\n\r\n------------------------------------------------------------------------------\r\n\r\nSome Mathematics and Hashing: The Birthday Problem\r\n\r\nSuppose we had a hash table of size N and a perfectly random hashing function.\r\nHow many values would we have to add before the probability of a collision is\r\n50%? The answer might surprise you because it is low. In mathematics, this is\r\nrelated to \"The Birthday Problem\": How many people (k people) must be in a room\r\nbefore there is at least a 50% chance that two have the same birthday?\" Here we\r\nassume that each person is equally likely to be born on every day in the year\r\n(which is not true, but is close to true) The answer is nowhere near 365, or\r\neven 180: it is just 23.\r\n\r\nWhy are these problems similar? The birthday problem is like having a hash\r\ntable of 365 dates (lets' ignore February 29th) and hashing each person (with\r\na good hash function) to the date they were bornd: a value between 1 and 365.\r\nWe want to know what is the probability of a collision (two people hashing to\r\nthe same date).\r\n\r\nThink about the problem this way. We will compute the probability of k people\r\nhaving DIFFERENT birthdays: then 1 minus that number is the probability of\r\nat least two people sharing a birthday. For everyone to have a different\r\nbirthday, the second person must have a birthday different than the first. The\r\nthird person must have a birthday different than the first two. The fourth\r\nperson must have a birthday different than the first three...\r\n\r\nWe can compute the probability of k people have different birthdays exactly as\r\n\r\n   365-0     365-1      365-2      365-3      365-4             365-(k-1)\r\n  ------- x ------- x  ------- x  ------- x  ------- x  ... x  -----------\r\n    365       365        365        365        365                365\r\n\r\nLet us generalize 365 to N, so we can compute the probability of k people having\r\ndifferent birthdays given N=365 days a year, or k values different bin numbers\r\nin a hash table of any size N. This product becomes\r\n\r\n         N!\r\nP =  -------------\r\n      (N-k)! N**k\r\n\r\nIf we choose N=365 and we compute this value for different values of k (say in\r\nExcel), we have following data. Note that the probability of two people having\r\nthe same birthday (PS) is 1 - PD.\r\n\r\n k   |     PD (Probability of all Different birthdays)\r\n-----+---------------------------\r\n 1   |  100.00%\r\n 2   |   99.73%\r\n 3   |   99.18%\r\n 4   |   98.36%\r\n 5   |   97.29%\r\n 6   |   95.95%\r\n 7   |   94.38%\r\n 8   |   92.57%\r\n 9   |   90.54%\r\n10   |   88.31%\r\n11   |   85.89%\r\n12   |   83.30%\r\n13   |   80.56%\r\n14   |   77.69%\r\n15   |   74.71%\r\n16   |   71.64%\r\n17   |   68.50%\r\n18   |   65.31%\r\n19   |   62.09%\r\n20   |   58.86%\r\n21   |   55.63%\r\n22   |   52.43%\r\n23   |   49.27%  Answer: PS = 1-49.27% is now >= 50%\r\n24   |   46.17%\r\n25   |   43.13%\r\n26   |   40.18%\r\n...\r\n\r\nThus, if we had a hash table of 365 values, storing 23 or more values into it\r\nis likely (>= 50% of the time) to lead to at least one collision: 2 values being\r\nhashed to the same bin. Using the same methodology of constructing a table, if\r\nif we had a hash table of 1,000 values, hashing 38 or more values into it is\r\nlikely to lead to at least one collision. Finally, for a hash table with\r\n1,000,000 bins, hashing 1,178 is likely to lead to at least one collision.\r\n\r\nWe will prove that this number grows O(sqrt(N)) and even compute the actual\r\ncoefficient (which we will find as sqrt(ln(4)) = 1.17741....\r\n\r\nStirling's approximation for N! is N**N x e**-N x sqrt(2pixN).\r\n\r\nIf we substitute it in the formula above we get\r\n\r\n              N   -N\r\n            N   e    sqrt(2piN)\r\nP = -------------------------------------\r\n          N-k   -(N-k)                k\r\n    (N-k)      e      sqrt(2pi(N-k)) N\r\n\r\n\r\n\t\t\t+-\t -+  (k-N-.5)\r\n                     -k |      k  |\r\nwhich simplifies to e   |  1 - -  |\r\n                        |      N  |\r\n                        +-       -+\r\n\r\n\r\nSo, ln(P) = -k + (k-N-.5)ln(1-k/N)\r\n\r\nNow, ln(1+x) = x - x**2/2 + x**3/3 - x**5/5 + ... (alternating sign)\r\n\r\nso ln(1-k/N) = -(k/N + k**2/(2N**2) + k**3/(3N**3) + ...)\r\n\r\nso ln(P) = -k + (N-k+.5)[k/N + k**2/(2N**2) + k**3/(3N**3) + ...] (see +/- here)\r\n\r\nNext, assume k << N, so when we multiply these two sums, we ignore any terms\r\nlike k/N, k/N**2, etc., in which k's power is less than or equal to N's power;\r\nbut we keep terms like k**2/N (where k's power is more than N's power).\r\n\r\nso ln(P) ~ -k + k - k**2/N + k**2/(2N) (all other terms are dropped)\r\n\r\n   ln(P) ~ -k**2/(2N)\r\n\r\nand\r\n         -k**2/(2N)\r\n   P ~ e\r\n\r\nIf we wanted to determine for what k the probability of unique values was P\r\n(the same as the probability of colisions = 1-P) we would solve k**2 ~ -2Nln(P)\r\nso, k ~ sqrt(-2Nln(P))\r\n\r\nIf we want to know for what k the probablity is about .5 (P=.5), we have\r\n\r\nk ~ sqrt(N) x sqrt(-2ln(.5)) and sqrt(-2ln(.5)) ~ 1.177410...\r\n\r\nSo k ~ 1.177410 sqrt(N)\r\n\r\nNote, for N = 365, 1,000 and 1,000,000 we get\r\n\r\nk ~ 22.5, k ~ 37.2, and k ~ 1,177 which all agree very well with the values\r\ncomputed in Excel.\r\n\r\nIf we wanted to determine for what k the probability of unique hashes was 10%\r\n(the probability of a collision was 90%), the formula is k ~ 2.145966 sqrt(N),\r\nabout twice as big as for a 50% chance.\r\n", "encoding": "ascii"}