{"url": "https://www.ics.uci.edu/~eppstein/265/exponential.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Exponential Algorithms</title>\n</head>\n<body>\n<h1>Graph algorithms: Lecture notes on exponential algorithms</h1>\n\n<p>Throughout algorithms classes we learn that polynomial time\nbounds are good, exponential bad. This attitude has led to\nsystematic avoidance of studying exponential time algorithms in\ntheoretical CS, so it's an area where there may be many low-hanging\nfruit. This is also evidenced by the big gap between known\nworst-case bounds and experimentally measured behavior.</p>\n\n<h2>Why is it reasonable to study exponential algorithms?</h2>\n\n<p>If they're impractical, isn't it as useful as counting angels on\npinheads?</p>\n\n<ul>\n<li>Some exponentials are more impractical than others.</li>\n\n<li>The better ones lead to solutions of moderate-sized real\nproblems.</li>\n\n<li>Improvements mean big differences in solvable problem sizes\n(typically multiplying problem size by some factor) while faster\ntechnology doesn't help so much (typically adding only a small\nconstant to the problem size).</li>\n\n<li>Polynomial time algorithmics has been criticized on the basis\nthat if everything's fast, you don't care exactly how fast it is.\nThis may not always be true (e.g. if server must handle many\nrequests, speed matters) but exponential problems more likely to\nlead to visible runtimes where improvements can be perceived or\neven make the difference between solving and not solving a\nproblem.</li>\n\n<li>The alternative approach of approximation algorithms (more\nstandard in theoretical CS) is not always suitable (approximations\ncan be bad, e.g. for graph coloring problems; the cost of computing\na true optimal answer may be made up for by the value of having\nthat answer; or it may be a problem where approximation does not\nmake sense).</li>\n\n<li>One can sometimes make the exponential part of the time bound\ndepend on a parameter other than problem size, which could be much\nsmaller (\"fixed parameter tractability\").</li>\n</ul>\n\n<h2>Growth rates</h2>\n\n<p>(This is a typical freshman exercise, but let's go through it\nagain.)</p>\n\n<p>What is a typical \"reasonable\" problem size we can solve for a\ngiven exponential time bound?</p>\n\n<p>Modern computers perform roughly 2<sup>30</sup> operations/second. So, if\nsome algorithm takes time f(n), how big can n be to solve the\nproblem in the given time?</p>\n\n<table>\n<tr>\n<td width=100>Operations:</td>\n<td width=100>2<sup>30</sup></td>\n<td width=100>2<sup>36</sup></td>\n<td width=100>2<sup>42</sup></td>\n<td width=100>2<sup>48</sup></td>\n<td width=100>2<sup>54</sup> </td>\n</tr>\n\n<tr>\n<td>Time:</td>\n<td>1 sec</td>\n<td>1 minute</td>\n<td>1 hour</td>\n<td>3 days</td>\n<td>&gt; 6 months</td>\n</tr>\n\n<tr>\n<td colspan=\"6\">\n<hr>\n</td>\n</tr>\n\n<tr>\n<td>f(n):</td>\n<td colspan=\"5\">max n for given time:</td>\n</tr>\n\n<tr>\n<td colspan=\"6\">\n<hr>\n</td>\n</tr>\n\n<tr>\n<td>1.05<sup>n</sup></td>\n<td>426</td>\n<td>511</td>\n<td>596</td>\n<td>681</td>\n<td>767</td>\n</tr>\n\n<tr>\n<td>1.1<sup>n</sup></td>\n<td>218</td>\n<td>262</td>\n<td>305</td>\n<td>349</td>\n<td>392</td>\n</tr>\n\n<tr>\n<td>1.2<sup>n</sup></td>\n<td>114</td>\n<td>136</td>\n<td>159</td>\n<td>182</td>\n<td>205</td>\n</tr>\n\n<tr>\n<td>1.3<sup>n</sup></td>\n<td>79</td>\n<td>95</td>\n<td>111</td>\n<td>127</td>\n<td>142</td>\n</tr>\n\n<tr>\n<td>1.4<sup>n</sup></td>\n<td>62</td>\n<td>74</td>\n<td>86</td>\n<td>99</td>\n<td>111</td>\n</tr>\n\n<tr>\n<td>1.5<sup>n</sup></td>\n<td>51</td>\n<td>61</td>\n<td>72</td>\n<td>82</td>\n<td>92</td>\n</tr>\n\n<tr>\n<td>2<sup>n</sup></td>\n<td>30</td>\n<td>36</td>\n<td>42</td>\n<td>48</td>\n<td>54</td>\n</tr>\n\n<tr>\n<td>3<sup>n</sup></td>\n<td>19</td>\n<td>23</td>\n<td>26</td>\n<td>30</td>\n<td>34</td>\n</tr>\n\n<tr>\n<td>n!</td>\n<td>12</td>\n<td>14</td>\n<td>15</td>\n<td>17</td>\n<td>18</td>\n</tr>\n\n<tr>\n<td>n<sup>n</sup></td>\n<td>9</td>\n<td>10</td>\n<td>11</td>\n<td>13</td>\n<td>14</td>\n</tr>\n\n<tr>\n<td>2<sup>n<sup>2</sup></sup></td>\n<td>5</td>\n<td>6</td>\n<td>6</td>\n<td>7</td>\n<td>7</td>\n</tr>\n</table>\n\n<p>Typical naive algorithms take times in the range 2<sup>n</sup>\nup, and can only solve smaller problems. Typical fast worst case\nbounds are in the range 1.2<sup>n</sup> - 1.5<sup>n</sup>. typical\nempirically measured time bounds are in the range 1.05<sup>n</sup>\n- 1.1<sup>n</sup>. So for instance we can solve certain kinds of\nconstraint satisfaction problems exactly up to 500 variables even\nfor the hardest examples (and examples coming from applications are\noften not hardest): <b>Exponential algorithms can be\npractical</b>.</p>\n\n<h2>Backtracking search (branch and bound)</h2>\n\n<p>A simple example: 3-coloring. Start with a recursive\ngenerate-and-test 3-coloring algorithm:</p>\n\n<pre>\ncolor(G,i) {\n   if (i==n) {\n    test if valid coloring\n        if so, return success\n        else return failure\n   }\n   for (each possible color c) {\n       try giving v[i] color c\n       color(G,i+1)\n   }\n   uncolor v[i]\n   return failure\n}\n</pre>\n\n<p>There are n levels of recursion, and the recursion branches 3\nways each level, so the time is 3<sup>n</sup>. One obvious\nimprovement: interleave validity testing into recursion, rather\nthan waiting until the graph is all colored before discovering some\nearly mistake.</p>\n\n<pre>\ncolor(G,i) {\n   if (i==n) return success\n   for (each color c not already used by a neighbor of v[i]) {\n       try giving v[i] color c\n       color(G)\n   }\n   uncolor v[i]\n   return failure\n}\n</pre>\n\n<p>Unfortunately while this will make a practical improvement, the\nworst case is still 3<sup>n</sup>. The problem is that we need to\nchoose ordering of vertices in a way that allows early termination\nto happen often One possibility: spanning tree preorder (e.g. depth\nfirst search numbering) then, each vertex is colored *after* its\nparent (except for the tree root, which has no parent, but we only\nneed to try one of the three colors there) so we can reduce the\nnumber of branches to 2<sup>n-1</sup>.</p>\n\n<h2>Changing the solution space</h2>\n\n<p>Instead of looking for a 3-coloring, let's look for the subset\nof red vertices of a 3-coloring. We can then test whether the\nremaining vertices can be 2-colored (i.e., whether they form a\nbipartite graph) in linear time. This immediately gives us a\n2<sup>n</sup> algorithm (that's how many different subsets of\nvertices there are) without as much effort as we took above. With\nmore thought, we can do even better: if we choose a coloring with\nas many red vertices as possible, the red vertices will form a\n<i>maximal independent set</i>: a set of vertices, with no edge\nconnecting any two vertices in the set, such that every remaining\nvertex in the graph shares an edge with a vertex in the set. So, we\ncan solve 3-coloring by listing all maximal independent sets and\ntesting for each whether the complementary set is bipartite.</p>\n\n<p>The algorithm below solves a slightly more general problem:\ngiven graph G, and sets Y and N, list the maximal independent sets\ncontaining everything in Y and nothing in N.</p>\n\n<pre>\nlistMIS(G,Y,N)\n{\n    if (G = Y u N) output Y and return\n\n    choose a vertex v\n\n    if (v not adjacent to anything in Y)\n    listMIS(G, Y u {v}, N)\n\n    if (v isn't the final neighbor of a vertex in N)\n        listMIS(G, Y, N u {v})\n}\n</pre>\n\n<p>How to analyze? Obviously the time at most 2<sup>n</sup>, and\none can come up with examples like in 3-coloring where a careless\nvertex ordering leads to at least this much time.</p>\n\n<p>We'd like to do better than this. Here's an idea: each iteration\nreduces size of G-Y-N by only one vertex, and we want to reduce the\nset of undecided vertices more quickly. If we add v to Y, we can\nquickly remove all its neighbors (the more neighbors the better).\nDefine degree(v)=number of neighbors in G-N-Y. Then when we add v\nto Y, the size of G-Y-N is reduced by 1+degree(v). If we add v to\nN, we don't get as big a reduction in subproblem size, but we MUST\nchoose at least one neighbor in Y and reduce it that way (so we\nwant all v's neighbors to have high degree)</p>\n\n<pre>\nlistMIS(G,Y,N)\n{\n    if (G = Y u N) output Y and return\n    if (some vertex in N has all neighbors in N) return without output\n\n    choose vertex v s.t. all neighbors have degree(neighbor) &gt;= degree(v)\n    (e.g. let v = minimum degree vertex in G-Y-N)\n\n    listMIS(G, Y u {v}, N u nbrs(v))\n\n    let A = N\n    for each neighbor w in G-Y-N {\n    listMIS(G, Y u {w}, A u nbrs(w))\n    A = A u {w}\n    }\n}\n</pre>\n\n<p>Analysis: suppose v has degree d then we get d+1 recursive calls\neach to a problem with at least d+1 fewer vertices (degree of\nneighbors could all equal d, addition of w to A could be redundant\nif w is also in later neighborhoods</p>\n\n<p>Oversimplified analysis: suppose d is the same in all calls (it\nwon't be) then n/(d+1) levels of recursion so time =\n(d+1)<sup>n/(d+1)</sup> = ((d+1)<sup>1/(d+1)</sup>)<sup>n</sup>.\nThe worst case is d=3, time (3<sup>1/3</sup>)<sup>n</sup>.</p>\n\n<p>Now do the analysis more carefully: analyse by counting the\nnumber of leaves of the recursion tree (total time is polynomial in\nthis). We prove by induction that L(n) &lt;= 3<sup>n/3</sup> In a\ncall with degree(v)=d, then we get</p>\n\n<pre>\nL(n) &lt;= (d+1) L(n - (d+1))\n     &lt;= (d+1) 3^(n - (d+1))/3  [induction hyp.]\n     = (d+1)/3^(d+1)/3 3^(n/3)\n     &lt;= 3^(n/3)\n</pre>\n\n<p>Corollary: any graph has at most 3<sup>n/3</sup> maximal\nindependent sets An example showing the analysis is tight: disjoint\nunion of n/3 triangles</p>\n</body>\n</html>\n\n", "encoding": "ascii"}