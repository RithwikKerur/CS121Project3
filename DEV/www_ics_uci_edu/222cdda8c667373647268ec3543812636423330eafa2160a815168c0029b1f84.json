{"url": "https://www.ics.uci.edu/~pazzani/RTF/AAAI.html", "content": "<html><head><!-- This document was created from RTF source by rtftohtml version\n2.7.5 --><title>Syskill &amp; Webert: Identifying interesting web sites</title></head><body><b></b><p>\n<b>Syskill &amp; Webert: Identifying interesting web sites </b><p>\n<p>\nMichael Pazzani, Jack Muramatsu  &amp; Daniel Billsus<br>\nDepartment of Information and Computer Science<br>\nUniversity of California, Irvine<br>\nIrvine, CA 92717<br>\npazzani@ics.uci.edu<br>\nphone: (714) 824-5888<br>\n fax (714) 824-4056<br>\nhttp://www.ics.uci.edu/~pazzani/<p>\n<p>\n<b>Abstract</b><p>\n<i>We describe Syskill &amp; Webert, a</i><b> </b><i>software agent that learns\nto rate pages on the World Wide Web (WWW), deciding what pages might interest a\nuser.   The user rates explored pages on a three point scale, and  Syskill\n&amp; Webert learns a user profile by analyzing the information on each page.\nThe user profile can be used in two ways.  First, it can be used to suggest\nwhich links a user would be interested in exploring.  Second, it can be used to\nconstruct a LYCOS query to find pages that would interest a user.   We compare\nsix different algorithms from machine learning and information retrieval on\nthis task. We find that the naive Bayesian classifier offers several advantages\nover other learning algorithms on this task. Furthermore, we find that an\ninitial portion of a web page is sufficient for making predictions on its\ninterestingness substantially reducing the amount of network transmission\nrequired to make predictions. </i><p>\n<i></i><p>\n\n<b>1 Introduction</b><p>\n<b></b>There is a vast amount of information on the World Wide Web (WWW) and\nmore is becoming available daily.  How can a user locate information that might\nbe useful to that user?  In this paper, we discuss Syskill &amp; Webert, a\nsoftware agent that learns a profile of a user's interest, and uses this\nprofile to identify interesting web pages in two ways.  First,  by having the\nuser rate some of the links from a manually collected \"index page\" Syskill\n&amp; Webert can suggest which other links might interest the user. Syskill\n&amp; Webert can annotate any HTML page with information on whether the user\nwould be interested in visiting each page linked from that page. Second,\nSyskill &amp; Webert can construct a LYCOS (Maudlin &amp; Leavitt, 1994) query\nand retrieve pages that might match a user's interest, and then annotate this\nresult of the LYCOS search.  Figure 1 shows a Web page\n(http://ai.iit.nrc.ca/subjects/ai_subjects.html) that has been annotated by\nSyskill and Webert.  This web page is a subject listing of AI topics that\nserves as an example of one index page.  In this case, the user has indicated\nstrong interest in \"Machine Learning\" and \"Reinforcement Learning\" (indicated\nby two thumbs up), a mild interest in \"Agents\" (indicated by one thumb up and\none thumb down) and no interest in  \"Business, Finance and AI\" and \"Philosophy\nof AI' (indicated by two thumbs down).  The other annotations are the\npredictions made by Syskill &amp; Webert about whether the user would be\ninterested in each unexplored page. A smiley face indicates that the user\nhasn't visited the page and Syskill &amp; Webert recommends the page to the\nuser.   For this topic, these pages are \"Neural Networks,\" \"Evolutionary\nAlgorithms,\" \"Bayesian Inference,\" \"General AI,\" and \"Case-Based Reasoning.\"\nThe international symbol for \"no\" is used to indicate a page hasn't been\nvisited and the learned user profile indicates the page should be avoided.\nFollowing any prediction is a number between 0 and 1 indicating the probability\nthe user would like the page.  <p>\n<IMG SRC=\"AAAI1.GIF\"><p>\n<b>Figure 1</b>. An example of a page annotated by Syskill &amp; Webert.<p>\n<p>\nIn this paper, we first describe how the Syskill &amp; Webert interface is used\nand the functionality that it provides.  Next, we describe the underlying\ntechnology for learning a user profile and how we addressed the issues involved\nin applying machine learning algorithms to classify HTML texts rather than\nclassified attribute-value vectors. We describe experiments that compare the\naccuracy of several algorithms at learning user profiles.   Finally, we relate\nSyskill &amp; Webert  to other agents for learning on the Web.<p>\n<p>\n<b>2 Syskill &amp; Webert</b> <p>\n<p>\nSyskill &amp; Webert learns a separate profile for each topic of each user.  We\ndecided to learn a profile for user topics rather than users for two reasons.\nFirst, we believe that many users have multiple interests and it will be\npossible to learn a more accurate profile for each topic separately since the\nfactors that make one topic interesting are unlikely to make another\ninteresting. Second, associated with each topic is a URL that we call an\n<i>index </i>page. The index page is a manually constructed page that typically\ncontains a hundred or more links to other information providers. For example,\nthe Web page at http://golgi.harvard.edu/biopages/all.html contains links to\nover 400 sites on the topic of Biosciences. Syskill &amp; Webert allows a user\nto explore the Web using the index page as a starting point.  In one mode of\nusing Syskill &amp; Webert,  it learns a profile from the user's ratings of\npages and uses this profile to suggest other pages accessible from the index\npage. To collect ratings, the HTML source of users' pages is intercepted, and\nan additional functionality is added to each page (see Figure 2).  This\nfunctionality allows the user to rate a page as either hot (two thumbs up),\nlukewarm (one thumb up and one thumb down), or cold (two thumbs down).  The\nuser can return to the index page or switch topics.  Furthermore, the user can\ninstruct Syskill &amp; Webert to learn  a user-profile for the current topic,\nmake suggestions or consult LYCOS to search the Web.<p>\n<b></b><p>\n<b></b>\n<IMG SRC=\"AAAI2.GIF\"><b></b><p>\n<b>Figure 2.</b> Syskill &amp; Webert interface for rating pages.<p>\n<b></b><p>\n<b></b>When a user rates a page, the HTML source of the page is copied to a\nlocal file<a href=\"AAAI_fn.html#fn0\">[1]</a> and a summary of the rating is\nmade.  The summary contains the classification (hot, cold, or lukewarm), the\nURL and local file, the date the file was copied (to allow for the bookkeeping\nthat would occur when a file changes), and the page's title (to allow for the\nproduction of a summary of the ratings).<p>\n Syskill &amp; Webert  adds functionality<a href=\"AAAI_fn.html#fn0\">[2]</a> to\nthe page (see Figure 2) for learning a user profile, using this user profile to\nsuggest which links to explore from the index page, and forming LYCOS queries.\nThe user profile is learned by analyzing all of the previous classifications of\npages by the user on this topic.  If a profile exists, a new profile is created\nby reanalyzing all previous pages together with any newly classified pages.<a\nhref=\"AAAI_fn.html#fn0\">[3]</a><p>\nOnce the user profile has been learned, it can be used to determine whether the\nuser would be interested in another page.  However, this decision is made by\nanalyzing the HTML source of a page, and it requires the page to be retrieved\nfirst.  To get around network delays, we allow the user to prefetch all pages\naccessible from the index page and store them locally.  Once this has been\ndone, Syskill &amp; Webert can learn a new profile and make suggestions about\npages to visit quickly.  Section 5 discusses one means of avoiding a\nsignificant amount of network transmission overhead.  Once the HTML has been\nanalyzed, Syskill &amp; Webert annotates each link on the page with an icon\nindicating the user's rating or its prediction of the user's rating together\nwith the estimated probability that a user would like the page.  Following any\nprediction is a number between 0 and 1 indicating the probability the user\nwould like the page.  The default version of Syskill &amp; Webert uses a simple\nBayesian classifier (Duda &amp; Hart, 1973) to determine this probability.\nNote that these ratings and predictions are specific to one user and do not\nreflect on how other users might rate the pages.<p>\nAs described above, Syskill &amp; Webert is limited to making suggestions about\nwhich link to follow from a single page. This is useful if someone has\ncollected a nearly comprehensive set of links about a topic.  Syskill &amp;\nWebert contains another feature that is useful in finding pages that might\ninterest a user anywhere on the Web (provided the pages have been indexed by\nLYCOS).  The user profile contains information on two types of words that occur\nin pages that have been rated.  First, it contains words that occur in the most\nnumber of pages that have been rated \"hot.\"  For these words, we do not\nconsider whether they have also occurred in pages that have other ratings.\nHowever, we ignore common English words and all HTML commands.  The second set\nof words we use are those whose presence in an HTML file helps discriminate\npages that are rated hot from other pages.  As described in Section 3, we use\nmutual information to identify discriminating words. Since LYCOS cannot accept\nvery long queries, we use the 7 most discriminating words that are found in a\nhigher proportion of hot pages than all pages and the 7 most commonly occurring\nwords as a query.  Experimentally, we have found that longer queries\noccasionally exhaust the resources of LYCOS.  The discriminating words are\nuseful in distinguishing pages of a given topic but do not describe the topic.\nFor example (see Figure 3) the discriminating words for one user about the\nBiosciences are \"grants,\" \"control,\" \"WUSTL,\" \"data,\" \"genome,\" \"CDC,\"  and\n\"infectious.\" The common words are useful for defining a topic. In the example\nin Figure 3 these are \"university,\" \"research,\" \"pharmacy,\" \"health,\"\n\"journal,\" \"biology,\"  and  \"medical.\" <p>\n<b></b><p>\n<b></b>\n<IMG SRC=\"AAAI3.GIF\"><b></b><p>\n<b>Figure 4. </b> Syskill &amp; Webert constructs a LYCOS query from a user\nprofile.<p>\n<b></b><p>\n<b></b>A strength of LYCOS is that it indexes a large percentage of the Web and\ncan quickly identify URLs whose pages contain certain keywords.  However, it\nrequires a user to filter the results.  Syskill &amp; Webert can be used to\nfilter the results of LYCOS (provided the pages are fetched).  For example,\nFigure 4 shows part of a LYCOS result that has been augmented by Syskill &amp;\nWebert to contain a recommendation against visiting one page and for visiting\nothers.<b></b><p>\n<b></b><p>\n<b>3. Learning a user profile. </b><p>\nLearning algorithms require a set of positive examples of some concepts (such\nas web pages one is interested in) and negative examples (such as web pages one\nis not interested in). In this paper, we learn a concept that distinguishes\npages rated as hot by the user from other pages (combining the two classes\nlukewarm and cold, since few pages are rated lukewarm, and we are primarily\ninterested in finding pages a user would consider hot).  Most learning programs\nrequire that the examples be represented as a set of feature vectors.\nTherefore, we have constructed a method of converting the HTML source of a web\npage into a Boolean feature vector. Each feature has a Boolean value that\n&#29;indicates whether a particular \"word\" is present (at least once) or absent\nin a particular web page.  For the purposes of this paper, a word is a sequence\nof letters, delimited by nonletters.  For example, the URL &lt;A HREF=\nhttp://golgi.harvard.edu/biopages/all.html&gt; contains nine \"words\"  a, href,\nhttp, golgi, harvard, edu, biopages, all, and html.  All words are converted to\nupper case.  <p>\nNot all words that appear in an HTML document are used as features.  We use an\ninformation-based approach, similar to that used by an early version of the\nNewsWeeder program (Lang, 1995) to determine which words to use as features.\nIntuitively, one would like words that occur frequently in pages on the\nhotlist, but infrequently on pages on the coldlist (or vice versa).  This is\naccomplished by finding the expected information gain (<i>E(W,S)</i>) (e.g.,\nQuinlan, 1984) that the presence or absence of a word (<i>W</i>) gives toward\nthe classification of elements of a set of pages (<i>S</i>):<p>\n<IMG SRC=\"AAAI4.GIF\"><p>\nwhere <p>\n<IMG SRC=\"AAAI5.GIF\"><p>\nand <i>P(W=present) </i>is the probability that <i>W</i> is present on a page,\nand \n<IMG SRC=\"AAAI6.GIF\">is\nthe set of pages that contain at least one occurrence of <i>W</i>  and  \n<IMG SRC=\"AAAI7.GIF\">\nare the pages that belong to the class <i>c</i>.<p>\nUsing this approach, we find the set of <i>k</i> most informative words. In the\nexperiment discussed in Section 4, we use the 128 most informative words.\nSection 6 discusses the impact of selecting other values for <i>k</i>.  Table 1\nshows some of the most informative words obtained from a collection of 140 HTML\ndocuments on independent rock bands.  <p>\n<p>\n<b>Table 1. <i> </i></b>Some of the words used as features. <p>\nnirvana \tsuite \tlo \tfi \tsnailmail \thim <p>\npop\trecords\trockin\tlittle\tsingles\trecruited<p>\njuly\tjams\tsongwriting\tcollege\trr\this<p>\nfollowing\ttoday\twrite\thandling\tdrums\tvocals<p>\nisland\ttribute\tprevious\tsmashing\thaunting\tbass<p>\nfavorite\tairplay\tnoise\tcause\tfabulous\tbecomes<p>\n<p>\n<p>\nOnce the HTML source for a given topic has been converted to positive and\nnegative examples represented as  feature vectors, it's possible to run many\nlearning algorithms on the data.  We have investigated a variety of machine\nlearning algorithms.  In addition, we compare our results to TF-IDF, an\nweighting strategy from information retrieval adapted to perform the task of\nclassification.<p>\n<b>3.1 Bayesian classifier</b><p>\n<b></b>The Bayesian classifier (Duda &amp; Hart, 1973) is a probabilistic\nmethod for classification. It can be used to determine the probability that an\nexample <i>j</i>  belongs to class <i>Ci </i> given values of attributes of the\nexample: <p>\n<IMG SRC=\"AAAI8.GIF\"><p>\nIf the attribute values are independent, this probability is proportional to:<p>\n<IMG SRC=\"AAAI9.GIF\"><p>\nBoth \n<IMG SRC=\"AAAI10.GIF\">\nand \n<IMG SRC=\"AAAI11.GIF\">\n may be estimated from training data. To determine the most likely class of an\nexample, the probability of each class is computed. An example is assigned to\nthe class with the highest probability.<p>\n<p>\n<b>3.2 Nearest Neighbor</b><p>\n<b></b>The nearest neighbor algorithm operates by storing all examples in the\ntraining set. To classify an unseen instance, it assigns it to the class of the\nmost similar example. Since all of the features we use are binary features, the\nmost similar example is the one that has the most feature values in common with\na test example.  <p>\n<b></b><p>\n<b>3.3 PEBLS</b><p>\n<b></b>PEBLS (Cost &amp; Salzberg, 1993) is a nearest neighbor algorithm that\nmakes use of a modification of the value difference metric, MVDM, (Stanfill\n&amp; Waltz, 1986) for computing the distance between two examples.  This\ndistance between two examples is the sum of the value differences of all\nattributes of the examples.  The value difference between two values \n<IMG SRC=\"AAAI12.GIF\">\nand \n<IMG SRC=\"AAAI13.GIF\">\nof attribute Aj is given by:<p>\n<IMG SRC=\"AAAI14.GIF\"><p>\n<p>\n<b>3.4 Decision Trees</b><p>\n<b></b>Decision tree learners such as ID3 build a decision tree by recursively\npartitioning examples into subgroups until those subgroups contain examples of\na single class.  A partition is formed by a test on some attribute (e.g., is\nthe feature database equal to 0). ID3 selects the test that provides the\nhighest gain in information content.<p>\n<p>\n<b>3.5 TF-IDF</b><p>\n<b></b>TF-IDF is one of the most successful and well-tested techniques in\nInformation Retrieval (IR). A document is represented as a vector of weighted\nterms. The computation of the weights reflects empirical observations regarding\ntext. Terms that appear frequently in one document (TF = term-frequency), but\nrarely on the outside (IDF = inverse-document-frequency), are more likely to be\nrelevant to the topic of the document. Therefore, the TF-IDF weight of a term\nin one document is the product of its term-frequency (TF) and the inverse of\nits document frequency (IDF). In addition, to prevent longer documents from\nhaving a better chance of retrieval, the weighted term vectors are normalized\nto unit length.<p>\nIn Syskill &amp; Webert, we use the average of the TF-IDF vectors of all\nexamples of one class in order to get a prototype-vector for the class (Lang,\n1995). To determine the most likely class of an example we convert it to a\nTF-IDF vector and then apply the cosine similarity measure to the example\nvector and each class prototype. An example is assigned to the class that has\nthe smallest angle between the TF-IDF vector of the example and the class\nprototype.<p>\n<p>\n<b>3.6 Neural Nets</b><p>\n<b></b>We used two approaches to learning with neural nets.  In the perceptron\napproach, there are no hidden units and the single output unit is trained with\nthe delta rule (Widrow &amp; Hoff, 1960). The perceptron is limited to learning\nlinearly separable functions (Minsky &amp; Papert, 1969). We also use\nmulti-layer networks trained with error backpropagation (Rummelhart, Hinton\n&amp; Williams, 1986). We used 12 hidden units in our experiments.<p>\n<p>\n<b>4 Experimental Evaluation</b><p>\n<b></b>To determine whether it is possible to learn user preferences\naccurately, we have had four users use the Syskill &amp; Webert interface to\nrate pages. A total of six different user profiles were collected (since one\nuser rated pages on three different topics).  The topics are summarized in\nTable 2 together with the total number of pages that have been rated by the\nuser.  Two users rated the pages on independent recording artists. One (A)\nlistened to an excerpt of  songs, and indicated whether the song was liked.  Of\ncourse, the machine learning algorithms only analyze at the HTML source\ndescribing the bands and do not analyze associated sounds or pictures.  Another\nuser (B) read about the bands (due to the lack of sound output on the computer)\nand indicated whether he'd be interested in the band.<p>\n<b>Table 2. </b>Topics used in our experiments.\n\n<pre>\nUser  Topic            URL of topic's index page                                   Pages    \nA     Biomedical       http://golgi.harvard.edu/biopages/medicine.html            127       \nA     Lycos            not applicable                                              54       \nA     Bands            http://www.iuma.com/IUMA-2.0/olas/location/USA.html4        57       \n      (listening)                                                                           \nB     Bands (reading)  http://www.iuma.com/IUMA-2.0/olas/location/USA.html        154       \nC     Movies           http://rte66.com/Movies/blurb.html                          48       \nD     Protein          http://golgi.harvard.edu/sequences.html                     26       \n\n</pre>\n<p>\nSyskill &amp; Webert is intended to be used to find unseen pages the user would\nlike.  In order to evaluate the effectiveness of the learning algorithms, it is\nnecessary to run experiments to see if Syskill &amp; Webert's prediction agrees\nwith the users preferences. Therefore we use a subset of the rated pages for\ntraining the algorithm and evaluate the effectiveness on the remaining rated\npages.   For an individual trial of an experiment,  we randomly selected\n<i>k</i>  pages to use as a training set, and reserved the remainder of the\ndata as a test set.  From the training set, we found the 128 most informative\nfeatures, and then recoded the training set as feature vectors to be used by\nthe learning algorithm. We tried six learning algorithms on each training set.\nThe learning algorithm created a representation for the user preferences. Next,\nthe test data was converted to feature vectors using the features found\ninformative on the training set. Finally, the learned user preferences were\nused to determine whether pages in the test set would interest the user.  We\nalso tested TF-IDF using a similar scheme, except that TF-IDF operated directly\non all of the words in HTML pages and did not require converting the pages to a\nset of 128 informative features<a href=\"AAAI_fn.html#fn0\">[5]</a>. For each\ntrial, we recorded the accuracy of the learned preferences (i.e., the percent\nof test examples for which the learned preferences agreed with the user's\ninterest).  We ran 24  paired trials of each algorithm. Figures 5 and 6 show\nthe average accuracy of each algorithm as a function of the number of training\nexamples.  <p>\n<p>\n<IMG SRC=\"AAAI15.GIF\"><IMG SRC=\"AAAI16.GIF\"><p>\n<b>Figure 5. </b> The average accuracy of each learning algorithm at predicting\na user's preferences.<p>\n<p>\n<IMG SRC=\"AAAI17.GIF\"><IMG SRC=\"AAAI18.GIF\"><p>\n<p>\n<IMG SRC=\"AAAI19.GIF\"><IMG SRC=\"AAAI20.GIF\"><p>\n<b>Figure 6. </b> The average accuracy of each learning algorithm at predicting\na user's preferences.<p>\n<p>\nThe results are promising in that on most of the problems the predictions are\nsubstantially better than simply guessing that the user would not be interested\nin a page (which is the most frequent prediction on all topics).  However, no\none algorithm is clearly superior on this set.  To get a more detailed idea of\nwhich algorithms perform well, we analyzed the data to find the algorithm(s)\nthat were most accurate with 20 training examples in each domain and the\nalgorithm(s) that were least accurate. In each case, we used a paired, two\ntailed t-test to find other algorithms that were not significantly different\nfrom the best and the worst.  On the biomedical domain, the naive Bayesian\nclassifier was most accurate and ID3 was least. On the LYCOS search domain,\nTF-IDF was most accurate and ID3 was least.  On the bands (listening) problem,\nnearest neighbor, PEBLS and  backpropagation were most accurate and TF-IDF was\nleast.  On the bands (reading) problem nearest neighbor, PEBLS ,\nbackpropagation and  were most accurate and TF-IDF was least.  On the movies\ndomain  the naive Bayesian classifier was most accurate and nearest neighbor,\nID3 and PEBLS were least. On the protein problem, PEBLS, nearest neighbor, ID3\nand the naive Bayesian classifier were most accurate and backprop and the\nperceptron were least accurate.  In summary, it appears that ID3 is not\nparticularly suited to this problem, as one might imagine since it learns\nsimple necessary and sufficient descriptions about category membership. The\nTF-IDF algorithm does not appear to have an advantage over the machine learning\nalgorithms. In one domain it is significantly better than others, but in\nseveral others it was significantly worse.  Although one must be careful not to\nread too much into averaging accuracies across domains, the naive Bayesian\nclassifier has the highest average accuracy with 20 training examples: 77.1\n(standard deviation 4.4). In contrast, PEBLS is  75.2 (4.7), backprop is 75.0\n(3.9),  nearest neighbor is 75.0 (5.5), ID3 is 70.6 (3.6) and TF-IDF is 68.5\n(12.0).  These results do illustrate that ID3 is not suited to this task and\nthat TF-IDF has a large amount of variation in its performance.  We have also\nexperimented with a more advanced decision tree learner C4.5 with similar\nresults.<p>\nWe have decided to use the naive Bayesian classifier as the default algorithm\nin Syskill &amp; Webert for a variety of reasons.  It is very fast for both\nlearning and predicting.  Its learning time is linear in the number of examples\nand its prediction time is independent of the number of examples.  It is\ntrivial to create an incremental version of the naive Bayesian classifier. It\nprovides relatively fine-grained probability estimates that may be used to\norder the exploration of new pages in addition to rating them.  For example, on\nthe biomedical domain, we used a leave-one-out testing methodology to predict\nthe probability that a user would be interested in a page.  The ten pages with\nthe highest probability were all correctly classified as interesting and the 10\npages with the lowest probability were all correctly classified as\nuninteresting.  There were 21 pages whose probability of being interesting was\nabove  0.9 and 19 of these were rated as interesting by the user.  There were\n64 pages whose probability of being interesting was below 0.1 and only 1 was\nrated as interesting by the user.<p>\nIn the remainder of this paper, we'll concentrate our experimentation on the\nnaive Bayesian classifier with 20 training examples.  We choose a small number\nof examples since most users will want to get results after ranking such a\nsmall number of pages.  We investigate two issues:<p>\n* \tIs it possible to get similar accuracy with less work by looking at an\ninitial portion of the page during learning and prediction?<p>\n* \tCan better results be achieved by selecting more (or less) that 128\ninformative words to use as features?<p>\n<p>\n<b>4. 1 Processing an initial portion of the page.</b><p>\n<b></b>As described so far, it is necessary to store the complete HTML source\nof every page rated by the user and to evaluate an unseen page it is necessary\nto retrieve the entire HTML to convert the page to a feature vector. Here, we\ninvestigate an alternate approach.  Instead of analyzing the entire HTML to\nfind informative words, only the words contained in the initial <i>c</i>\ncharacters are used during learning when creating a profile and only those\nwords in the initial <i>c</i> characters are used  to predict whether a user\nwould like the page.  Of course, we still select the 128 most informative words\nin the initial portions of the pages.  Note that we look at an initial sequence\nof characters, rather than words, since the protocol for transmission of\nsegments of the file is based on characters.<p>\nWe ran an experiment with the naive Bayesian classifier on the six domains with\n20 training examples where we varied the value of <i>c</i>.  The values used\nwere 256, 512, 1024, 2048, 3072, 4096 and infinity (i.e., using the entire\ndocument). The results of this experiment, averaged over 24 trials, are shown\nin Figure 7.  We plot each domain separately and also plot an average over the\nsix domains. <p>\n<b></b><p>\n<b></b>\n<IMG SRC=\"AAAI21.GIF\"><b></b><p>\n<b>Figure 7. </b> The effect of only analyzing the initial  <i>c</i> characters\nof a file.<p>\n<p>\nWhile in some domains (protein, bands and LYCOS), there is an advantage in not\nanalyzing the entire document, on average there is a small decrease in\naccuracy.  For example, only looking at the first 2048 characters yields an\naverage accuracy of 74.2 while looking at all characters yields an accuracy of\n76.3<a href=\"AAAI_fn.html#fn0\">[6]</a>.  We have run experiments with all of\nthe other learning algorithms and the general pattern is the same. On average,\nit is best to analyze the entire file, but the first 1024 -2048 characters is\nsufficient to achieve nearly the same accuracy.  Usually, less than 512\ncharacters results in a significant decrease in accuracy.<p>\nAlthough there is a small decrease in accuracy when looking at the initial\nsegment of the file, in the next version of Syskill &amp; Webert we will store\nand process only an initial segment to reduce the transmission overhead\nassociated with fetching pages to rank.  We also note that many users of\nSyskill &amp; Webert rate a page as interesting without looking at the entire\npage and that many information retrieval systems index abstracts rather than\nthe full text of a document.  <p>\nAnecdotally, we have observed that some errors in Syskill &amp; Webert are\ncaused by analyzing too much of a page.  For example, Syskill &amp; Webert\nsometimes rates a page as interesting to a user when it is not. Sometimes this\noccurs because the page itself is uninteresting, while at the bottom of the\npage, there are pointers and short descriptions of related interesting sites<a\nhref=\"AAAI_fn.html#fn0\">[7]</a>.  This may explain why in some domains the\naccuracy of Syskill and Webert is improved when only analyzing an initial\nsegment.  <p>\n<p>\n   <b>4. 2 Varying the number of informative features</b><p>\n<b></b>The decision to use the 128 most informative words as features was made\nby looking at one initial domain (biomedical) when only 50 examples were\ncollected.  In this section, we explore the impact of selecting other numbers\nof features.  Once again, we show results only for the naive Bayesian\nclassifier with 20 examples. We experimented with selecting  16, 32, 64, 96,\n128, 200, 256, and 400. of the most informative words to use as features. The\nresults, averaged over 24 trials, are shown in Figure 8. We plot each domain\nseparately and also plot an average over the six domains.<p>\n<IMG SRC=\"AAAI22.GIF\"><b></b><p>\n<b>Figure 8. </b>  The effect of varying the number of informative features.<p>\n<b></b><p>\n<b></b>Figure 8 shows that having too few features can cause problems because\nimportant discriminating features are ignored.  On the other hand, having too\nmany features can also cause problems if many words that aren't very relevant\nare used as features.  On average for the values we tested, 96 performed best.\nOne might consider using information-gain to select a large group of\ninformative features, and then using existing approaches for feature subset\nselection (e.g.,  Kittler, 1986; John, Kohavi, Pfleger, 1994) to select some of\nthese features using a criteria other than informativeness.  However, such\nalgorithms increase the complexity of the Bayesian classifier, making it\nimpractical to learn a profile interactively. <p>\n<p>\n<b>5. Related work</b><p>\n<b></b>The methods developed for our learning agent are related to work in\ninformation retrieval and relevance feedback (e.g., Salton &amp; Buckey, 1990;\nCroft &amp; Harper, 1979).  However, rather than learning to adapt user\nqueries, we are developing a user profile that may be used for classification\ntasks such as filtering new information as it becomes available.<p>\nThere are several other agents designed to perform tasks similar to ours. The\nWebWatcher (Armstrong, Freitag, Joachims,  and Mitchell, 1995) system is\ndesigned to help a user retrieve information from Web sites.  When given a\ndescription of a goal (such as retrieving a paper by a particular author), it\nsuggests which links to follow to get from a starting location to a goal\nlocation.  It learns by watching a user traverse the WWW and it helps the user\nwhen similar goals occur in the future.  The WebWatcher and the work described\nhere serve different goals. In particular, the user preference profile may be\nused to suggest new information sources related to ones the user is interested\nin.<p>\nLike our work, WebHound (Lashkari, 1995) is designed to suggest new Web pages\nthat may interest a user.  WebHound uses a collaborative approach to filtering.\nIn this approach, a user submits a list of pages together with ratings of these\npages. The agent finds other users with similar ratings and suggests unread\npages that are liked by others with similar interests. One drawback of the\ncollaborative filtering approach is that when new information becomes\navailable,  others must first read and rate this information before it may be\nrecommended. In contrast, by learning a user profile, our approach can\ndetermine whether a user is likely to be interested in new information without\nrelying on the opinions of other users.  Furthermore, the profile learned also\ncontains information that can be used to create queries of Web search engines\nsuch as LYCOS.   However, one advantage of the collaborative approaches is that\nthey do not require transmission and analysis of the HTML source of Web\npages.<p>\nBalabanovic, Shoham, and Yun (1995) have developed an agent that searches links\nfrom existing pages for pages that might interest a user, using the TF-IDF\nweights as part of a user profile.  In our experiments,  TF-IDF did not perform\nas well at classification as algorithms designed for classification.  Croft\n(1995, personal communications) has argued that the task of classifying pages\ndiffers from the tasks  for which TF-IDF and relevance feedback were designed\nand that algorithms designed for classification may provide a more appropriate\nbias for the task.  <p>\n<b>5 Future Work</b><p>\nWe are planning two types of enhancements to Syskill &amp; Webert.  First, we\nwill investigate improvements to the underlying classification technology:  <p>\n*\tSelecting an appropriate numbers of  features.  The performance of the\nlearning algorithms is sensitive to the number of relevant features. We\ncurrently use a fixed number of features in a user profile. Rather than fixing\nthe number, we plan on investigating having a threshold on the minimum\ninformativeness of a feature.  This may allow Syskill &amp; Webert to adapt the\nnumber of features to the given problem and the number of examples without\nincreasing the complexity of converting HTML to Boolean features.<p>\n*\tExplore the use of ordinal rather than Boolean features. Boolean features\nindicate whether a word is present or absent in a document. The ordinal\nfeatures could indicate the number of times a word is present.  Note that words\ninclude items such as \"jpg\" and \"html\" so these may be used to make decisions\nbased on the number of pictures or links if they are informative.  Like\nextensions to TF-IDF, these may be normalized to the length of the document. <p>\n*\tUsing linguistic and hierarchical knowledge in the forming of features.\nLinguistic routines such as stemming (i.e., finding the root forms of words)\nmay improve the accuracy of  Syskill &amp; Webert by having a smaller number of\nmore informative features.  Currently, words in the pages are used as features\nwithout any knowledge of the relationship between words such as \"protein\"  and\n\"proteins.\"  Semantic knowledge such as the relationship between \"pigs\" and\n\"swine\" may also prove useful (e.g., Greffenstette, 1992).  Similarly, knowing\nthat \"gif,\" \"jpg\" and \"jpeg\" are all extensions of graphic files would\nfacilitate Syskill &amp; Webert learning that a user has a preference for (or\nagainst) pages with in-line graphics.<p>\n*\tWe will continue experimentation with various learning algorithms.  Nearest\nneighbor algorithms perform about as well as Bayesian classifiers and have many\nof the same desirable properties. Nearest neighbor algorithms have a few\npotential advantages such as not making independence assumptions and having the\nability to learn nonlinearly separable functions.  One possibility that we are\nnow implementing is using the TF-IDF weights as part of similarity metric for\nnearest neighbor.  Syskill &amp; Webert is implemented in a fashion that allows\nus to easily change the representation of user profiles.<p>\n<p>\nAnother set of enhancements to Syskill &amp; Webert involve the redesign of the\nuser interface to make it more interactive.   We are currently reimplementing\nmany of its capabilities as JAVA applets.  One important advantage of this\nreimplementation is that the user profile can then be stored on the client\nrather than on the Syskill &amp; Webert server.  We are also exploring several\nother enhancements to the interface that will make it easier to use (and as a\nconsequence allow us to collect more data for our experiments). <p>\n*\tImplementing routines that interactively annotate the index page with Syskill\n&amp; Webert's predictions as the initial 2k of each link is processed.\nCurrently, no annotations are added until all links have been retrieved and\nrated.  We allow the user to prefetch links so that the rating can occur\nrapidly, but this does require patience the first time Syskill &amp; Webert is\nused and disk space to store local copies of files. <p>\n*\tCurrently, Syskill &amp; Webert displays its raking as annotations on the\ncurrent page that is displayed.  We are planning on an option that will create\na new page with Syskill &amp; Webert's rankings sorted by the probability\nestimate that the page is hot. <p>\n*\tCurrently, Syskill &amp; Webert retrieves the original source of a page to\ndetermine its interestingness.  Several of the Web search engines such as LYCOS\nstore a summary of the page (e.g., See Figure 4).  We are implementing routines\nto use this summary for rating the interestingness of a page.  Combined with\nthe previous option, this will reorder the suggestion made by LYCOS based on\nthe user's profile. This may be particularly useful with \"CyberSearch\" which is\na copy of much of the LYCOS database on CD-ROM eliminating the network\nconnection overhead as well as the network transmission overhead.<p>\n* \tWe plan on monitoring the topic page of each topic of a user, and notifying\nthe user when a new link is added to the page that is rated as interesting.<p>\n<p>\n<b>7 Conclusions</b> <p>\nWe have introduced an agent that collects user evaluations of the\ninterestingness of pages on the World Wide Web.  We have shown that a user\nprofile may be learned from this information and that this user profile can be\nused to determine what other pages might interest the user.   Such pages can be\nfound immediately accessible from a user-defined index page for a given topic\nor by using a Web search engine.   Experiments on six topics with four users\nshowed that the Bayesian classifier performs well at this classification task,\nboth in terms of accuracy and efficiency.  Other learning algorithms that make\nclassifications based on combining evidence from a large number of features\nalso performed well. ID3 was not very accurate perhaps since it tries to\nminimize the number of features it tests to make a classification and accurate\nclassifications cannot be based on the presence or absence of a few words.\nFurther experimentation showed that nearly the same classification accuracy\ncould be achieved by looking only at the initial portion of a page, suggesting\nan enhancement to the interface that reduces storage space and network\ntransmission overhead.<p>\n<p>\n<p>\n<b>References</b><p>\nArmstrong, R.  Freitag, D., Joachims, T., and Mitchell,  T. (1995). WebWatcher:\nA learning apprentice for the World Wide Web. <p>\nBalabanovic, Shoham, and Yun (1995).  An adaptive agent for automated web\nbrowsing,  Journal of Visual Communication and Image  Representation 6(4).<p>\nCost, S. &amp; Salzberg, S. (1993). A weighted nearest neighbor algorithm for\nlearning with symbolic features  <i>Machine Learning, 10,</i> 57-78. <p>\nCroft, W.B. &amp; Harper, D. (1979). Using probabilistic models of document\nretrieval without relevance. <i>Journal of Documentation, 35,</i> 285-295.<p>\nDuda, R. &amp; Hart, P. (1973). <i>Pattern classification and scene\nanalysis.</i> New York: John Wiley &amp; Sons.<p>\nJohn, G. Kohavi, R., &amp; Pfleger, K. (1994). Irrelevant Features and the\nsubset selection problem  <i>Proceedings of the Eleventh International\nConference on Machine Learning.</i> New Brunswick, NJ.<p>\nKittler, J. (1986). Feature selection and extraction. In Young &amp; Fu,\n(eds.),  <i>Handbook of pattern recognition and image processing.</i> New York:\nAcademic Press. <p>\nKononenko, I. (1990). Comparison of inductive and naive Bayesian learning\napproaches to automatic knowledge acquisition. In B. Wielinga (Eds..),\n<i>Current trends in knowledge acquisition.</i> Amsterdam: IOS Press.<p>\nLang, K. (1995).  NewsWeeder: Learning to filter news. <i>Proceedings of the\nTwelfth International Conference on Machine Learning.</i> Lake Tahoe, CA.<p>\nLashkari, Y. (1995). The WebHound Personalized Document  Filtering System.\nhttp://rg.media.mit.edu/projects/webhound/<p>\nMaudlin, M &amp; Leavitt, J.  (1994). Web Agent Related Research at the Center\nfor Machine Translation  <i>Proceedings of the ACM Special Interest Group on\nNetworked Information Discovery and Retrieval</i><p>\nMinsky, M., &amp; Papert, S.  (1969). <i>Perceptrons</i>.  Cambridge, MA: MIT\nPress. <p>\nQuinlan, J.R. (1986).  Induction of decision trees.  Machine Learning, 1,\n81-106.<p>\nRachlin, Kasif, Salzberg &amp; Aha, (1994). Towards a better understanding of\nmemory-based reasoning systems. Proceedings of the Eleventh International\nConference on Machine Learning. New Brunswick, NJ.<p>\nRumelhart, D., Hinton, G., &amp; Williams, R.  (1986).  Learning internal\nrepresentations by error propagation.  In D. Rumelhart and J. McClelland\n(Eds.), <i>Parallel Distributed Processing: Explorations in the Microstructure\nof Cognition.  Volume 1: Foundations,</i> (pp 318-362).  Cambridge, MA: MIT\nPress.<p>\nSalton, G. &amp; Buckley, C. (1990). Improving retrieval performance by\nrelevance feedback.<i> Journal of the American Society for Information Science,\n41, </i>288-297.<p>\nSkalak, D. (1994) Prootype and feature selection by sampling and random\nmutation hill climbing algorithms. Proceedings of the Eleventh International\nConference on Machine Learning. New Brunswick, NJ.<p>\nStanfill, C. &amp; Waltz, D. (1986). Towards memory-based reasoning.\nCommunications of the ACM, 29, 1213-1228.<p>\nWidrow, G., &amp; Hoff, M.  (1960). Adaptive switching circuits.    Institute\nof Radio  Engineers,  Western  Electronic  Show  and  Convention,  Convention\nRecord, Part 4.  <p>\n<p>\n<p>\n<p>\n<p>\n<p>\n<!--GH_SEARCH-->\n<hr>\n<center>\n<table border=5><tr border=0>\n<td align=center valign=middle>\n<a href=http://glimpse.cs.arizona.edu/webglimpse>\n<img src=http://www.ics.uci.edu/~mlearn/.glimpse-eye.jpg alt=\"WG\" align=middle width=50><br>\n<font size=-3>WebGlimpse</font></a></td>\n<td> <FORM method=get ACTION=/cgi-bin/webglimpse?/i1/dy/ua/mlearn/public_html>\n<INPUT NAME=query size=20>\n<INPUT TYPE=submit VALUE=\"Search\">\n<INPUT name=file type=hidden value=\"/i1/dy/ua/pazzani/public_html/RTF/AAAI.html\">\n<a href=/cgi-bin/webglimpse-fullsearch/i1/dy/ua/mlearn/public_html?file=/i1/dy/ua/pazzani/public_html/RTF/AAAI.html>\nSearch Options</a></td></tr>\n<tr><td colspan=2>\nSearch:\t\n<INPUT TYPE=radio NAME=scope VALUE=full CHECKED>The full archive\n</td></tr></form></table></center><hr>\n<!--GH_END-->\n</body>\n</html>\n", "encoding": "ascii"}