{"url": "https://www.ics.uci.edu/~eppstein/280/tree.html", "content": "<HTML><HEAD>\n<TITLE>Computational Statistics: Hierarchical Clustering</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Hierarchical Clustering</H2>\n\n<I>Hierarchical clustering</I> refers to the formation of a recursive\nclustering of the data points:\na partition into two clusters, each of which is itself hierarchically\nclustered.\n\n<P>One way to draw this is some kind of system of nested subsets,\nmaximal in the sense that one can't identify any additional subsets\nwithout violating the nesting:\n\n<PRE><B>\n                     __________________________\n                    |   _________              |\n                    |  |  ______  |    _____   |\n                    |  | |      | |   |     |  |\n                    |  | | x  y | |   |  u  |  |\n                    |  | |______| |   |     |  |\n                    |  |          |   |  v  |  |\n                    |  |    z     |   |_____|  |\n                    |  |__________|            |\n                    |__________________________|\n</B></PRE>\n\n<P>Alternatively, one can draw a \"dendrogram\", that is,\na binary tree with a distinguished root, that has all the data items at\nits leaves:\n\n<PRE><B>\n                            /\\\n                           /  \\\n                          /    \\                 ________\n                         /      \\               |        |\n                        /\\       \\     or     __|__      |\n                       /  \\       \\          |     |    _|_\n                      /\\   \\      /\\        _|_    |   |   |\n                     /  \\   \\    /  \\      |   |   |   |   |\n                    x    y   z  u    v     x   y   z   u   v\n</B></PRE>\n\n<P>Conventionally, all the leaves are shown at the same level of the drawing.\nThe ordering of the leaves is arbitrary, as is their horizontal position.\nThe heights of the internal nodes may be arbitrary, or may be related to\nthe metric information used to form the clustering.\n\n<H3><A NAME=\"models\">Data Models</A></H3>\n\nThe data for a clustering problem may consist of points in a Euclidean\nvector space,\nor more structured objects such as DNA sequences\n(in which case the hierarchical clustering problem is essentially\nequivalent to reconstructing evolutionary trees and is also known as\n\"phylogeny\").\nHowever many clustering algorithms assume simply that the input is given\nas a distance matrix.\nThe distances may or may not define a metric; one popular data model is that\nthe data form an \"ultrametric\" or \"Archimedean metric\", a special type\nof metric in which the distances satisfy the \"ultrametric triangle inequality\"\n<DIV ALIGN=CENTER>\ndist(a,c) <U>&lt;</U> max(dist(a,b), dist(b,c))\n</DIV>\n\nThis inequality is satisfied, for instance, if the data points are\nleaves of a dendrogram drawing, with the distance defined to be the height of\ntheir least common ancestor; in fact this is just an equivalent way of\ndefining an ultrametric. The inequality is also satisfied for\nvertices in any graph,\nwith the length of a path defined to be its maximum weight edge.\n\n<P>The ultrametric requirement may be too strong -- e.g.\nin evolutionary trees, if one measures distance by mutation rate,\nit would imply the biologically unrealistic condition that\nall species evolve at the same rate.\nA weaker condition is that the distances are formed by path lengths in a\ntree, without the ultrametric requirement that each root-leaf path have\nequal length.  A distance function meeting this weaker requirement is\nalso known as an additive metric.\n\n<P>If the data model is that the data points form an ultrametric,\nand that the input to the clustering algorithm is a distance matrix,\na typical noise model would be that the values in this matrix are\nindependently perturbed by some random distribution.\nAdditional data observations may consist of new points in this ultrametric\n(e.g. in the large-population genetic studies used to test the\nhypothesis that the human species evolved in Africa before migrating to\nother continents [cite?]),\nor may be used to reduce the perturbation in existing distance measurements\n(e.g. by comparing larger amounts of DNA sequence information).\n\n<P>Another common data and noise model for DNA sequence input is the\nCavender-Farris model\n[<A HREF=\"bib.html#C78\">C78</A>],\nin which the data parameters consist of an ultrametric (dendrogram with\nedge lengths)\ntogether with\na prototype sequence stored at the root of the dendrogram.\nThe observed data in this model represent the contents of a single\nposition of the DNA sequence for each species, and are formed by\nstarting with a symbol at the root of the dendrogram and then\npropagating that value downwards, with mutation rates proportional to\nthe dendrogram edge lengths.\n\n<H3><A NAME=\"dynprog\">Dynamic Programming Methods</A></H3>\n\nDynamic programming may be used to find the \"most parsimonious\" tree for\na given set of sequences; that is, a dendrogram, with each internal\nvertex labeled by a sequence constructed by the algorithm\n(the leaves are labeled by the input sequences), such that the total\namount of mutation\nbetween sequences adjacent to each other on the tree is minimized.\nHowever, the computational complexity of this is prohibitive:\ntypically, it is something like O(n<sup>k</sup>) where n is the sequence\nlength and k is the number of sequences.\nThis method may not always converge to the correct tree (e.g. in the\nCavender-Ferris model), but\nas Rice and Warnow show\n[<A HREF=\"bib.html#RW97\">RW97</A>],\nit performs very well in practice\n\n<P>Another dynamic program arises from the problem of fitting an optimal\ndendrogram to a distance matrix.  If one has a particular dendrogram in mind\n(as an abstract tree), the problem of assigning heights to its vertices\nto minimize the maximum difference between the dendrogram distance and\nthe given distance matrix is simply a linear program, where each height\nis linearly constrained to be above its children and within D of each\ndistance represented by it in the matrix.  Since this linear program has\nonly two variables per inequality, it can be solved efficiently [cite?].\nHowever, the difficult part of this procedure is in choosing which\ndendrogram to use.  The number of possible different dendrograms\nwith k leaves is (2k-1)!!=1*3*5*7*...(2k-1)\nsince there are 2k-1 ways of adding a kth leaf to a (k-1)-leaf dendrogram,\nso unless k is very small one can't hope to try them all.\nOne could possibly find the optimal solution for somewhat larger (but\nstill not very large) k by a dynamic programming method in which\none finds the optimal dendrogram for each subset of the data points,\nby trying all ways of partitioning that subset into two smaller subsets.\n\n<H3>Local Improvement Methods</H3>\n\nIf one can't compute a global optimum, one can at least achieve a local\noptimum: start with any tree, and then repeatedly use dynamic\nprogramming to re-optimize small subtrees.  This idea can be used to get\na (1+epsilon)-factor approximation to parsimony [cite?].\n\n<H3><A NAME=\"bu\">Bottom Up Methods</A></H3>\n\nThere is a large family of clustering algorithms that all work as\nfollows: Choose some measure of \"affinity\" for clusters.  Start with n\nclusters, each consisting of a single point.  Then repeatedly merge the\ntwo clusters with the highest affinity into a single supercluster.  The\ndifferences between these algorithms involve the definition of affinity,\nand the implementation details.  Note that even for points in a metric\nspace, the \"affinity\" need not satisfy the triangle inequality or other\nnice properties.\n\n<P>If the affinity of two clusters is defined to be the distance between\nthe closest pair of points, the problem is known as \"single-linkage\"\nclustering.  Essentially, it is solved by the minimum spanning tree\n(the top level clustering is formed by removing the heaviest edge from\nthe MST, and the remaining levels are formed in the same manner recursively).\nSince minimum spanning trees can often be found efficiently, so can this\nclustering, and it does produce the correct results when the input is an\nultrametric distance matrix (without errors).  However, for points in\nEuclidean spaces, it tends to produce unsatisfactory long stringy clusters.\n\n<P>Another common affinity function is the average distance between\npoints in a cluster (complete linkage clustering).  For distance matrix\ninput, this can be found by replacing two rows of the matrix by an\nappropriate weighted average.  Alternatively, if one has a good notion\nof single point estimation for a cluster (e.g. the centroid), one can\ndefine affinity in terms of the distance between cluster centers.\n\n<P>\"Neighbor-joining\"\n[<A HREF=\"bib.html#SN87\">SN87</A>],\nis a more complicated affinity:\nIt assumes that one has a tree in which each point is connected\nto a common center, and measures the amount by which the total edge\nlength of the tree would be reduced by placing a \"Steiner point\"\nbetween two points, and connecting that Steiner point to the center.\nFormally, the affinity between x and y is\n<DIV ALIGN=CENTER>\ndistance(x,center) + distance(y,center)\n- distance(x,SP) - distance(y,SP) - distance(SP,center)\n</DIV>\nwhere SP is the Steiner point.\nFor distance matrix input, the center is found by averaging the rows of\nthe whole matrix, so the distance from x to the center is just the average\ndistance of x from any point.  Similarly, the Steiner point can be found\nby averaging the rows for x and y (in which case the two terms\ndistance(x,SP)+distance(y,SP) add to distance(x,y)).\nAtteson\n[<A HREF=\"bib.html#A96\">A96</A>]\nshowed that this method converges to the correct tree\nfor distance matrices that are sufficiently close\n(in terms of L<sub>infinity</sub> distance) to a tree metric.\n\n<P>Many of these methods have efficient algorithms, both\nfor distance matrix input\n[<A HREF=\"bib.html#E98\">E98</A>],\nand for points in low dimensional Euclidean spaces\n[<A HREF=\"bib.html#KL95\">KL95</A>].\nHowever, Neighbor-Joining seems more difficult,\nwith the best known time bound being O(n<sup>3</sup>)\n(and some commonly available implementations\ntaking even more than that).\nHowever even the faster of these methods are too slow for data consisting\nof millions of points in moderate to high dimensional Euclidean spaces.\n\n<H3>Top Down Methods</H3>\n\nOne can also form hierarchical clusterings top down, following the definition\nabove: use your favorite <A HREF=\"cluster.html\">nonhierarchical clustering</A>\nalgorithm to find a partition of the input into two clusters,\nand then continue recursively on those two clusters.\n\n<P>The advantage of these methods is speed: they can scale to problem sizes\nthat are beyond the bottom up methods.  However, the quality of the results\nmay be poorer, because the most important decisions are being made early\nin the algorithm before it has accumulated enough information to make\nthem well.  Also, because of the emphasis on speed, the nonhierarchical\nclustering methods used tend to be primitive (e.g. split the points by\nan axis-parallel hyperplane).\n\n<H3>Incremental Methods</H3>\n\nIncremental hierarchical clustering methods can be even faster\nthan the top down approach.\nThese methods\nbuild the hierarchy one point at a time, without changing the existing\nhierarchy.  To add a new point, simply trace down from the root,\nat each step choosing the child cluster that best contains the given point.\nOnce you reach a cluster containing a single point, split it into two\nsmaller clusters, one for that point and one for the new point.\n\n<P>This is extremely fast, and good for applications such as nearest\nneighbor classification, but not very good for accurately reconstructing\nthe clusters present in the original data model.\n\n<H3><A NAME=\"nt\">Numerical Taxonomy</A></H3>\n\nCavalli-Sforza and Edwards\n[<A HREF=\"bib.html#CE67\">CE67</A>]\nintroduced the problem of finding taxonomy by finding the\n\"nearest\" tree metric or ultrametric to a given distance matrix.\nOf course one has to define \"nearest\"; the natural choices\nseem to be L<sub>1</sub>, L<sub>2</sub>, or L<sub>infinity</sub>\nmetrics on the distance matrix coordinates.\n\n<P>The L<sub>1</sub> and  L<sub>2</sub> problems are NP-complete\nfor both the tree metric and ultrametric\nvariants&nbsp;[<A HREF=\"bib.html#D87\">D87</A>].\nHowever, the L<sub>infinity</sub>-nearest ultrametric can\nbe found in time linear in the distance matrix\nsize&nbsp;[<A HREF=\"bib.html#FKW93\">FKW95</A>].\n\n<P>Agarwala et al.\n[<A HREF=\"bib.html#ABFNPT98\">ABFNPT98</A>]\nshow that it is NP-complete to get within a factor of 9/8\nof the L<sub>infinity</sub>-nearest\ntree metric; however they describe a fast approximation algorithm\nthat achieves a factor of three.\n\n<P>There has also been some work within the theoretical computer science\ncommunity (motivated by approximation algorithms for various network\ndesign problems) on approximating a distance matrix by a tree metric\nwith minimum \"dilation\" or \"stretch\" (maximum ratio of original distance\nto tree distance).  The main result in this area is that one can define\na random distribution on trees such that the expected dilation for a\ngiven pair of vertices is O(log&nbsp;n&nbsp;log&nbsp;log&nbsp;n)\n[<A HREF=\"bib.html#CCGGP98\">CCGGP98</A>]\nThis doesn't seem to mean much about finding an individual tree\nwith optimal or nearly-optimal dilation, but\nit does imply that one can find a tree with average dilation\nO(log&nbsp;n&nbsp;log&nbsp;log&nbsp;n).\n\n<P>Gu&eacute;noche\n[<A HREF=\"bib.html#G96\">G96</A>]\nhas proposed a way for measuring the nearness of a given distance\nmatrix to a tree metric, that depends only on the ordering among\ndistances and not so much on the exact distance values;\nin that sense it is \"distance-free\" similarly to the centerpoint and\nregression depth methods for single point estimation and linear regression.\nHe defines the \"order distance\" Delta(x,y)\nto be the number of pairs (u,v) for which x and y\ndisagree on the ordering of their distances:\ndist(x,u)&nbsp;&lt;&nbsp;dist(x,v)\nbut\ndist(y,u)&nbsp;&gt;&nbsp;dist(y,v)\n(with a pair counted only half in case of an equality).\nHe then suggests applying an incremental method using Delta\ninstead of the original distance.\n\n<H2><A HREF=\"bib.html\">NEXT: References</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"tree.html\" --></SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}