{"url": "https://www.ics.uci.edu/~eppstein/261/s13-hw7-answers.txt", "content": "CS 261, Spring 2013, Homework 7, Due Thursday, May 30\n\n\n1. The standard deviation of a collection of n data values x_i may be\ncalculated by the formula\n\n          n sum(x_i^2) - (sum x_i)^2\ns = sqrt( -------------------------- ).\n                  n (n-1)\n\n(a) Give an example that shows that the standard deviation is not\ndecomposable.\n\n    There are many examples. Perhaps the simplest is to choose n=2\n    and to have two data values that are not the same as each other.\n    Then the value of s for the whole set is a nonzero number\n    but for each subset of just one element it is either undefined\n    (because of the division by zero) or zero (if you define\n    s to be zero when the top and bottom of the fraction in its\n    definition are both themselves zero), neither of which helps\n    to find the value of the whole formula.\n\n    By the way, there are many different variations in the definition of the\n    standard deviation or variance, depending on whether you include the\n    square root operation or not and on whether you divide by n or n-1.\n    The details of the definitions don't make a difference for this problem.\n\n(b) Describe a decomposable range querying problem such that the\nstandard deviation of the data values within a query range can be\ncomputed in constant time from the answer to your query.\n(We went through a similar example for the average in the lecture.)\n\n    Return the triple (n, sum x_i, sum x_i^2).\n    If a set S is decomposed into two disjoint subsets A and B,\n    the value for S is the vector sum of the values for A and B,\n    so this is decomposable. And once this triple is known,\n    the value of s can be computed using the formula above.\n\n\n2. In class we saw how to compute the average of any contiguous subarray\nof a one-dimensional matrix by looking up two sums of prefixes of the\nmatrix. Suppose that we have a two-dimensional matrix A (such as the\npixels in a grayscale image), and we store another matrix B of the same\ndimensions, such that the value in B[i,j] is the sum of all entries\nA[x,y] with 0 <= x <= i and 0 <= y <= j. Give a formula that uses the\nvalues in B to compute the average of any rectangular subarray of A in\nconstant time.\n\n    Suppose we want to compute the average of the subarray\n    l <= x <= r and b <= y <= t.\n    The sum of the elements in this subarray is\n    B[r,t] - B[l-1,t] - B[r,t-1] + B[l-1,t-1]:\n    each element in the subarray is included in the B[r,t] term,\n    the elements with smaller x-coordinates (but with y-coordinates\n    in the same range as the subarray) are included in both\n    that term and the B[l-1,t] term, which cancel,\n    the elements with smaller y-coordinates are similarly cancelled\n    by the combination of the B[r,t] and B[r,t-1] terms,\n    and the elements whose x and y coordinates are both smaller\n    are included in all four terms, twice positively and twice\n    negatively, so they also cancel.\n\n    Once we know the sum, the average is sum/((t-b+1)(r-l+1)).\n\n\n3. Recall that in the dynamic prefix sum problem, we are maintaining an\narray of data values, subject to operations that update a single cell in\nthe array or that query the sum of a prefix of the array. There is a\nsolution that takes time O(log n) per operation and there is a lower\nbound showing that the problem cannot be solved more quickly than O(log\nn) per operation (in the cell probe model measuring the amount of\ncommunication between main memory and CPU needed to solve the problem)\n\nNow suppose that we are given a sequence of n operations in a batch,\nrather than individual operations one at a time. Does the lower bound\nimply that solving this batched problem requires Omega(n log n) time, in\nthe same cell probe model? Why or why not?\n\n    So the question is about, if you're given in advance the\n    whole sequence of updates to be performed, whether there's some\n    smarter way of doing them in a different order so that you get\n    the identical sequence of results more quickly. I don't know of\n    any algorithm that solves this faster than O(n log n),\n    and I don't know of a lower bound that proves that no fast\n    algorithm is possible. However, what the question actually asked\n    was whether the lower bound given in class applies to this\n    version of the problem. The answer is no.\n\n    The adversary used in the lower bound almost gives you a batch\n    of the updates it's going to do: you know in advance exactly which\n    positions it's going to query and update, and what order it\n    will query and update them. However, it does not tell you in\n    advance what values will be stored in the updates, so it's\n    not completely batched, and the lower bound does not apply.\n\n    Another way of explaining why the lower bound doesn't apply is\n    to look at what it's assuming about how the data structure\n    operations are solved. It assumes that each query is answered\n    by looking at memory that has only been modified by previous\n    update and query operations, and what is counted in the lower bound\n    is the number of times these memory lookups access memory\n    that was modified in specific subsets of previous operations.\n    But in the batched model, it is possible that some of the queries\n    are answered by looking at memory that has been modified while\n    processing other operations out of order, something that\n    does not fit the assumptions of the lower bound.\n", "encoding": "ascii"}