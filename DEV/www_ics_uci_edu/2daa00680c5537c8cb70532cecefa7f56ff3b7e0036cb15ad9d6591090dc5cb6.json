{"url": "https://www.ics.uci.edu/~pattis/ICS-33/lectures/aa.txt", "content": "\tAnalysis of Algorithms (Complexity Classes and Big-O Notation)\r\n\r\n\r\nAnalysis of Algorithms (AA) is a mathematical area of Computer Science in which\r\nwe analyze the resources (mostly time; but sometimes space; and at the chip\r\nlevel, even electrical power) used by algorithms to solve problems. An\r\nalgorithm is a precise procedure for solving a problem, written in any notation\r\nthat humans understand (and thus can carry-out the algorithm): if we write an\r\nalgorithm as code, in some programming language, then a computer can execute it\r\ntoo. We are primarily interested in using computers to solve very large-sized\r\nproblems, and that is where Analysis of Algorithms works best.\r\n\r\nThe main tool that we use to analyze algorithms is big-O notation: it means\r\n\"growth (in resources, based on the problem size) on the Order of\". We use\r\nbig-O notation to characterize the performance of an algorithm by placing it in\r\na complexity class (most often based on its WORST-CASE behavior -but sometimes\r\non its AVERAGE-CASE behavior ore even BEST-CASE behavior) when solving a problem\r\nof size N: we will learn how to characterize the SIZE of problem, which is most\r\noften as simple as N is the number of values in a list/set/dictionary/file being\r\nprocessed. Thus, in AA we don't necessarily compute the exact resources needed,\r\nbut typically an approximate upper bound on the resources, based on the problem\r\nsize. Once we know the complexity class of an algorithm, we can easily get\r\na good handle on understanding its actual performance (within certain limits):\r\nthe time it will take on a specific computer to solve a specific-sized problem. \r\n\r\n------------------------------------------------------------------------------\r\n\r\nGetting to Big-O Notation: Throwing away Irrelevant Details\r\n\r\nHere is one simple Python function for computing the maximum of a list (or\r\nreturning None if there are no values, for an empty list). \r\n\r\ndef maximum(alist):\r\n    answer = None if alist == [] else alist[0]\r\n    for i in range(1,len(alist)):\r\n        if alist[i] > answer:\r\n            answer = alist[i]\r\n    return answer\r\n\r\nOften, the problem size is the number of values processed: e.g., the number of\r\nvalues in a list or lines in a file. But we can use other metrics as well: it\r\ncan be the count of number of digits in an integer value, when looking at the\r\ncomplexity of multiplication based on the size of the numbers. Thus, there is\r\nno single measure of size that fits all problems: instead, for each problem we\r\ntry to choose a measure that makes sense and is natural for that problem.\r\n\r\nPython translates functions like maximum into a sequence of instructions that\r\nthe computer executes (the subject of ICS-51 and one of ICS-33's last week's\r\nlectures). To solve a problem, the computer always executes an integral number\r\nof instructions. For simplicity, we will assume that all instructions take the\r\nsame amount of time to execute. So to compute the amount of time it takes to\r\nsolve a problem is equivalent to computing how many instructions the computer\r\nmust execute to solve it, which we can then divide by the number of instructions\r\nper second a machine executes, to compute the actual amount of time taken to\r\nsolve the problem on that machine.\r\n\r\nAgain, we typically look at the worst case behavior of algorithms. For maximum\r\nthe worst case occurs if the list is in increasing order. In this case, each\r\nnew value examined in the list will be bigger than the previous maximum, so the\r\nif statement's condition will always be True, which always requires updating\r\nthe answer local variable. If any value was lower, it wouldn't have to update\r\nanswer and thus take fewer instructions/less time to execute.\r\n\r\nIt turns out that for a list of N values, we find that the computer executes\r\n14N + 9 instructions in the worst case for this function. You need to know more\r\nCS than you do at this time to determine this formula, but you will get there by\r\nICS 51 (and I expect to cover the basics during the last week of the quarter,\r\nwhen I cover the Python Virtual Machine: its equivalent to machine code).\r\n\r\nA simple way to think about this formula is that there are 14 computer\r\ninstructions that are executed each time Python executes the body of the for\r\nloop and 9 instructions that are executed only once: they deal with starting\r\nand terminating the loop and the other parts of the function not in the loop.\r\nWe can write I(N) = 14N + 9 for the worst case of the maximum function, where\r\nI(N) is the largest number of instructions the computer executes when solving a\r\nproblem on a list with N values. Or to be more specific we could include the\r\nname of the function in this formula and write Imaximum(N) = 14N  + 9.\r\n\r\nI would like to argue now that if we simplify this function to just I(N) = 14N\r\nwe have lost some information, but not much. Specifically, as N gets bigger\r\n(i.e., when we are dealing with very big problems - the kinds computers are\r\nused to solve), 14N and 14N+9 are relatively close. Let's look at the result of\r\nthis function vs. the original as N gets for values of N increasing by a factor\r\nof 10.\r\n\r\n    N     |   14N + 9  |    14N     | error: (14N+9 - 14N)/(14N+9) as a % of N\r\n ---------+------------+------------+---------------------------\r\n        1 |         23 |         14 |   61%         or 39%       accurate\r\n       10 |        149 |        140 |    6%            94%       accurate\r\n      100 |       1409 |       1400 |     .6%          99.4%     accurate\r\n     1000 |      14009 |      14000 |     .06%         99.94%    accurate\r\n     ...\r\n1,000,000 | 14,000,009 | 14,000,000 |     .00006%      99.99994% accurate\r\n     ...\r\n\r\nEach line shows the % error of computing 14N when the true answer is 14N + 9.\r\nSo by the time we are processing a list of 1,000 values, using the formula\r\n14N instead of 14N+9 is 99.94% accurate. For computers solving real problems,\r\na list of 1,000 values is small: a list of millions is more normal. For\r\n1,000,000 values 14N is off by just 9 parts in 14 million. So the 9 doesn't\r\naffect the value of the formula much.\r\n\r\nAnalysis of Algorithms really should be referred to as ASYMPTOTIC Analysis of\r\nAlgorithms, as it is mostly concerned with the performance of algorithms as\r\nthe problem size gets very big (N -> infinity). We see here that as N->Infinity\r\n14N is a better and better approximation to 14N+9: dropping the extra 9 becomes\r\nless and less important.\r\n\r\nHere is another simple Python function, for sorting a list of values. This is\r\nmuch simpler than the actual sort method in Python (and the simplicity of code\r\nresults in this function taking much more time, but it is a good starting point\r\nfor understanding sorting now; ICS-46 spends a week studying sorting). If you\r\nare interested in how this function accomplishes sorting, hand simulate it\r\nworking on a list of 5-10 values (try increasing values, decreasing values, and\r\nvalues in a random order): basically, each execution of the outer loops mutates\r\nthe list so that the next value in the list is in the correct position.\r\n\r\ndef sort(alist):\r\n    for base in range(len(alist)):\r\n        for check in range(base+1,len(alist)):\r\n            if alist[base] > alist[check]:\r\n                alist[base], alist[check] = alist[check],alist[base]\r\n    return None  # list is mutated\r\n\r\n\r\nIt turns out that for a list of N values, the computer executes 8N**2 + 12N + 6\r\ninstructions in the worst case for this function. The outer loop executes N\r\ntimes (N is len(alist)) and inner loop on average executes N/2 times, so the\r\nif statement in the inner loop is executed a quadratic number of times. We can\r\nsay I(N) = 8N**2 + 12N  + 6 for the worst case of the sort function, where I(N)\r\nis again the number of instructions the computer executes. Or to be more\r\nspecific we would write the formula as Isort(N) = 8N**2 + 12N  + 6.\r\n\r\nI would like to argue in the same way that if simplify this function to just\r\nthe dominant term, I(N) = 8N**2, we have not lost much information. Let's look\r\nat the result of this this function vs. the original as N gets bigger.\r\n\r\n    N     | 8N**2+12N+6|      8N**2  | error: (12N+6)/(8N**2+12N+6) as a % of N\r\n ---------+------------+-------------+---------------------------\r\n        1 |         26 |           8 |   70%      or 30%    accurate\r\n       10 |        926 |         800 |   14%         86%    accruate\r\n      100 |     81,206 |      80,000 |    1.5%       98.5%  accurate\r\n     1000 |  8,012,006 |   8,000,000 |     .15%      99.85% accurate\r\n\r\nSo by the time we are processing a list of 1,000 values, using the formula\r\n8N**2 instead of 8N**2 + 12N + 6 is 99.85% accurate. For 1,000,000 values\r\n(10**6), 8N**2 is 8*10^12 while 8N**2 + 12N + 6 is 8*10^12 +12*10^6 + 6; the\r\nsimpler formula is is 99.999985% accurate. \r\n\r\nCONCLUSION (though not proven): If the real formula I(N) is a sum of a bunch of\r\nterms, we can drop any term that doesn't grow as quickly as the most quickly\r\ngrowing term. In teh maximum function, the linear term 14N grows more quickly\r\nthan the next term, the constant 9, which doesn't grow at all (as N grows) so\r\nwe drop the 9 term. In the sorting function, the quadratic term 8N**2 grows\r\nmore quickly than the next two terms, the linear term 12N and the constant 6,\r\nso we drop both the 12N and 6 terms.\r\n\r\n    In fact, we can prove that the Limit as N->infinity of 12N/8N**2 = 3/(2N)\r\n    -> 0, which means we can discard the 12N term because it grows more slowly\r\n    than the 8N**2 term, and will eventually be dominated by this faster growing\r\n    term. Of course the Limit as N->infinity of 6/8N**2 = 3/4N**2 -> 0.\r\n\r\nThe result is a simple function that is still an accurate approximation of the\r\nnumber of computer instructions executed for lists of various LARGE sizes for\r\nthe maximum and sort functions. Each consists of just a constant multiplied by\r\nsome simple function of N (here N and N**2, but many other functions are\r\npossible too; we will see others later in this lecture).\r\n\r\nWe now will explain the rationale for dropping the constant in front of N and\r\nN**2 terms, and classifying these algorithms as O(N) = growing at a linear rate\r\nand O(N**2) = growing at a quadratic rate. Again O means \"grows on the order of\"\r\nso O(N) means execution time grows on the order of N and O(N**2) means execution\r\ntime grows on the order of N**2. Technically, it means \"execution time grows no\r\nmore quickly than the the order of\".\r\n\r\n1) If we assume that every instruction in the computer takes the same amount\r\n   of time to execute, then the time taken for maximum is about 14N/speed and\r\n   the time for sort is about 8N**2/speed. We should really think about these\r\n   formulas as (14/speed)N and (8/speed)N**2. We know the 14 and 8 came from\r\n   the number of instructions inside loops that Python executes: but a different\r\n   Python interpreter (or a different language) might generate a different\r\n   number of instructions in these loops and therefore would result in a\r\n   different constant. Thus, this number is based purely on TECHNOLOGY, and we\r\n   want our analysis to be independent of technology. And, of course, \"speed\"\r\n   changes based on technology too, and that is also part of that constant.\r\n\r\nSince we are trying to come up with a \"science\" of algorithms, we don't want\r\nour results to depend on technology, so we are also going to drop the constant\r\nin front of the biggest term as well. For the reason explained above (relating\r\nto instructions generated by Python and the speed of the machine), the resulting\r\nnumber is based solely on technology. \r\n\r\nHere is another example from technology. Computers can be categorized at a very\r\nhigh level as either CISC (Complex Instruction Set), RISC (Reduced Instruction\r\nSet), or VLIW (Very Long Instruction Word). ICS-51 will cover the differences.\r\nLet's just look at just the first two. A program translated to a RISC processor\r\nwill have more instructions than a CISC processor: each instruction does\r\nsomething simpler, so there are more of them. But each RISC instruction\r\n(because it is simpler) executes more quickly.\r\n\r\nGenerally, a program on a RISC machine translates to 2 times as many\r\ninstructions, but each instruction runs 3 times as fast. So, based on the\r\nconstants computing the speed, the numerator doubles and the denominator\r\ntriples. Therefore the code will run faster by a factor of about 1.5. But NONE\r\nof this has anything to do with the actual program/algorithm; it relates only\r\nto the technology, so should not be part of a science of algorithms.\r\n\r\nHere is another justification for not being concerned with the constant in\r\nfront of the biggest term.\r\n\r\n2) A fundamental question that we want answered about any algorithm is, \"how\r\n   much MORE resources does it need when solving a problem TWICE AS BIG\". In\r\n   maximum, when N is big (so we can drop the +9 without losing much accuracy)\r\n   the ratio of time to solve solve a problem of size 2N to the time to solve a\r\n   problem of size N is easily computed by the following approximation\r\n\r\n    Imaximum(2N)     14(2N)\r\n   -------------- ~ -------- ~ 2\r\n    Imaximum(N)      14 N\r\n\r\n   The ratio is always a simple number (here 2), no matter how many instructions\r\n   are executed in the loop, since the constant 14 appears as a multiplicative\r\n   factor in both the numerator and denominator: so when divided it cancels out\r\n   itself.  So, we know for this code, if we double the size of the list, we\r\n   approximately double the number of instructions that maximum executes to\r\n   solve the problem (approximately because we did drop the 9 term), and thus\r\n   approximately double the amount of time (for whatever the speed of the\r\n   computer is).\r\n\r\n   Thus, the constant 14 is irrelevant when asking this \"doubling\" question. \r\n\r\n   Likewise, for sorting we can write\r\n\r\n    Isort(2N)     8(2N)**2\r\n   ----------- ~ ---------- ~ 4\r\n    Isort(N)      8 N**2\r\n\r\n   Again, the ratio is is a simple number (4), with the constant (no matter\r\n   what it is, disappearing).  So, we know for this code that if we double the\r\n   size of the list, we increase by approximately a factor of 4 the number of\r\n   instructions that are executed to sort it (approximately because did drop\r\n   the 12N + 6 term), and thus increase by approximately a factor of 4 the\r\n   amount of time (for  whatever the speed of the computer is).\r\n\r\n   Thus, the constant 8 is irrelevant when asking this \"doubling\" question. \r\n\r\n   Note if we didn't simplify, we'd have\r\n\r\n    I(2N)     8(2N)**2 + 12(2N) + 6     32N**2 + 24N + 6\r\n   ------- = ----------------------- = -----------------------\r\n    I(N)      8N**2 + 12N + 6            8N**2 + 12N + 6\r\n\r\n   which doesn't simplify easily; although, as N->inifinty, this ratio gets\r\n   closer and closer to 4 (and is close even for pretty small-sized problems).\r\n\r\nAs with air-resistance and friction in physics, typically ignoring the\r\ncontribution of these negligible factors (for big, slow-moving objects) allows\r\nus to quickly and simply solve an approximately correct problem, which gives us\r\nuseful information.\r\n\r\nUsing big-O notation, we say that the complexity class of the code to find the\r\nmaximum is O(N). The big-O means \"grows on the order of\" N, which means a\r\nlinear growth rate (double the input size, double the time). For the sorting\r\ncode, its complexity class is O(N**2), which means grows on the order of N**2,\r\nwhich means a quadratic growth rate (double the input size, quadruple the time).\r\n\r\n----------\r\nIMPORTANT: A Quick way to compute the complexity class of an algorithm\r\n\r\nTo analyze a Python function's code and compute its complexity class, we\r\napproximate THE NUMBER OF TIMES THE MOST FREQUENTLY EXECUTED STATEMENT IS\r\nEXECUTED, dropping all the lower (more slowly growing) terms and dropping the\r\nconstant in front of the most frequently executed statement (the fastest\r\ngrowing term). We will show how to do this much more rigorously in the next\r\nlecture.\r\n\r\nThe maximum code executes the if statement N times, so the code is O(N). The\r\nsorting code executes the if statement N(N-1)/2 times (we will justify this\r\nnumber later in this lecture), which is N**2/2 - N/2, so dropping the lower\r\nterm and the constant 1/2, yields a complexity class of O(N**2) for this code.\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComparing Algorithms by their complexity classes: When we can/When we cannot\r\n\r\nPrimarily from this definition we know that if two algorithms, a and b, both\r\nsolve some problem, and a is in a lower complexity class than b, then for all\r\nBIG ENOUGH N, Ta(N) < Tb(N): here Ta(N) means the Time it takes for algorithm\r\na to solve the problem. Note that nothing here is said about small N; which\r\nalgorithms uses fewer resources on small problems depends on the actual\r\nconstants and lower-order terms that we drop. So COMPLEXITY CLASSES HAVE MUCH TO\r\nSAY ABOUT BIG PROBLEM SIZES BUT  LITTLE TO SAY FOR SMALL PROBLEM SIZES.\r\n\r\nFor example, if algorithm a is O(N) with a constant of 100, and algorithm b\r\nis O(N**2) with a constant of 1, then for values of N (problem sizes) in the\r\nrange [0,100],\r\n\r\n   Tb(N) = 1N**2 <= 100N = Ta(N)\r\n\r\nbut for all values of N (problem sizes) >= 100,\r\n\r\n   Ta(N) = 100N <= 1N**2 = Tb(N)\r\n\r\nSo, for all N >= 100, Ta(N) <= Tb(N).\r\n\r\nConstrast this with a second example. If algorithm a is O(N) with a constant of\r\n1, and algorithm b is O(N**2) with a constant of 10, then for all values of N\r\n(all problem sizes)\r\n\r\n   Ta(N) = 1N <= 10N**2 = Tb(N)\r\n\r\nIn this case, for all N >= 0, Ta(N) <= Tb(N).\r\n\r\nThese two examples illustrate (but don't prove) that regardless of the\r\nconstants, there is always a problem size P such that for all N >= P,\r\nTa(N) <= Tb(N). This means that as problems get big enough, there will always\r\nbe a size for which an O(N) algorithm eventually executes in less time than an\r\nO(N**2) algorithm, and it executes in less time for all problem sizes bigger\r\nthan that one.\r\n\r\nBut, in some cases a lower complexity class can execute in less time for small\r\nN (2nd example), and in some cases it can execute in more time (1st example).\r\nWe need information beyond their complexity classes to find out: constants and\r\nlower-oreder terms. But it is guaranteed that FOR ALL SIZES N BEYOND SOME\r\nPROBLEM SIZE P, the algorithm in the lower complexity class will run faster.\r\n\r\nAgain, we use the term \"asymptotic\" analysis of algoritms to indicate that we\r\nare mostly concerned with the time taken by the code when N gets very large\r\n(going towards infinity). In both cases, because of their complexity classes,\r\nalgorithm a will eventually take less time to execute than algorithm b.\r\n\r\nWhat about the constants? Are they likely to be very different in practice? It\r\nis often the case that the constants of different algorithms are close (say\r\nwithin a factor of 10 of each other): they are often just the number of\r\ninstructions in the innermost loop of the code. So the complexity classes are a\r\ngood indication of faster vs slower algorithms for all but the smallest problem\r\nsizes.\r\n\r\nAlthough all possible mathematical functions might represent complexity classes\r\n(and many strange ones do), we will mostly restrict our attention to the\r\nfollowing complexity classes. Note that complexity classes can interchangably\r\nrepresent computing time, the # of machine operations executed, and such more\r\nnebulous terms as \"effort\" or \"work\" or \"resources\".\r\n\r\nAs we saw before, a fundamental question about any algorithm is, \"What is the\r\ntime needed to solve a problem twice as big\". We will call this the DOUBLING\r\nSIGNATURE of the complexity class (measuring this value empirically, by timing\r\nthe code on problem sizes that increase by a factor of 2, allows us to know\r\n-or at least approximate- the complexity class as well). \r\n\r\nClass   |  Algorithm Example\t\t\t\t| Doubling Signature\r\n--------+-----------------------------------------------+----------------------\r\nO(1)\t| pass argument->parameters: copying a reference| T(2N) = T(N)\r\nO(LogN) | binary searching of a sorted list\t\t| T(2N) = c + T(N)\r\nO(N)\t| linear searching a list (the in operator)\t| T(2N) = 2T(N)\r\nO(NLogN)| Fast sorting\t\t\t\t\t| T(2N) = cN + 2T(N)\r\n\r\n  Fast algorithms come before here; NLogN grows a bit more quickly than linearly\r\n  (because logarithms grow so slowly compared to N) and nowhere near as quickly\r\n  as O(N**2). 1000 Log 1000 (for logarithms base 2) is about 10,000;\r\n  1,000,000 Log 1,000,000 (ditto) is about 20,000,000. So increasing N by a\r\n  factor of 1,000 increase the function by 2,000: the linear term grows by a\r\n  factor of 1,000, the logarithmic term grows by just a factor of 2.\r\n\r\nO(N**2) | Slow sorting; scanning N times list of size N | T(2N) = 4T(N)\r\nO(N**3) | Slow matrix multiplication\t\t        | T(2N) = 8T(N)\r\nO(N**m) | for some fixed m: 4, 5, ... graph algorithms\t| T(2N) = 2**mT(N)\r\n\r\n  Tractable algorithms come before here; their work is polynomial in N for\r\n  some fixed power. In the complexity class below, N is an exponent.\r\n\r\nO(2**N) | Finding boolean values that satisfy a formula | T(2N)=2**N T(N)\r\n\r\nFor example, for an O(N**2) algorithm, doubling the size of the problem\r\nquadruples the time required: If T(N) ~ cN^2 then T(2N) ~ c(2N)**2 = c4N**2\r\n= 4cN**2 = 4T(N). So the ratio T(2N)/T(N) ~ c4N**2/cN**2 = 4.\r\n\r\nNote that in Computer Science, logarithms are mostly taken to base 2. (Remember\r\nthat algorithms and logarithms are very different terms). All logarithms are\r\nimplicitly to base 2 (e.g., Log N = Log2 N). You should memorize and be able to\r\nuse the following facts to approximate logarithms without a calculator.\r\n\r\nLog 1000 ~ 10\r\n  Actually, 2**10 = 1,024, 2**10 is approximatley 1,000 with < a 3% error.\r\n\r\nLog a**b = b Log a, or more usefully, Log 1000**N = N Log 1000; so ...\r\n  Log 1,000,000     = 20 : 1,000,000     = 1,000**2; Log 1000**2 = 2*Log 1000\r\n  Log 1,000,000,000 = 30 : 1,000,000,000 = 1,000**3; Log 1000**3 = 3*Log 1000\r\n\r\nSo note that Log is a very slowly growing function. When we increase from\r\nLog 1,000 to Log 1,000,000,000 (the argument grows by a factor of 1 million)\r\nthe Log only grows by from 10 to 30 (by a factor or 3).\r\n\r\nIn fact, we can compute these logarithms on any calculator that computes Log\r\nin any base. The following shows how to compute Log (base b) in terms of\r\nLog (base a).\r\n\r\nLog (base b) X = Log (base a) X / Log (base a) b\r\n\r\nSo, Log (base b) X is just a constant\r\n  (1/Log (base a) b)\r\ntimes\r\n   Log (base a) X,\r\nso logarithms to any base are really ALL THE SAME COMPLEXITY CLASS (regardless\r\nof the base) because they differ only by a multiplicative constant (and we\r\nignore such multiplicative constants when writing complexity classes). For\r\nexample,\r\n\r\nLog(base 10) X = Log(base 2) X  /  Log(base 2) 10    ~    .3 Log(base 2) X\r\n\r\nSo O(Log(base 10) X) = O(.3 Log(base 2) X) which simplifies to O(Log(base 2) X)\r\nby removing the constant.\r\n\r\n----------\r\nA great example (from the physical world) involving O(1) complexity, which\r\n  students often struggle with.\r\n\r\nWhen I came to interview at UCI (late in my career) I went to see Alex Thornton\r\nteach one of his classes and he showed a great example that I have been using\r\nevery since. Here is my version of what Alex taught...\r\n\r\nSuppose that you want to get from point A to point B, and you have two ways to\r\ntravel: by walking and by teleporting. If you walk, you walk at a constant\r\nrate from A to B; if you teleport, you must wait 1 minute for the teleporter to\r\nwarm up, and then you will be instantaneously transported from A to B. Here N\r\n(the size of the problem to solve) is the distance between A and B.\r\n\r\nNow, walking is represented by the linear complexity class, O(N): if you double\r\nthe distance you need to walk, you double the time you take to get there. Next,\r\nteleportation is represented by the constant complexity class, O(1): if you\r\ndouble the distance you need to teleport, the amount of time does not change to\r\nget there: it is always 1 minute.\r\n\r\nFrom the discussion above, because the complexity class O(1) is a lower\r\ncomplexity class than O(N), we know that for a big enough distance teleportation\r\nwill be faster than walking. What is that distance? It is the distance we can\r\nwalk in 1 minute (the amount of time it takes the teleporter to warm up). We\r\ncan draw these relationships as follows, showing where the Walk and Teleport\r\nlines cross.\r\n\r\n                 Walk\r\n     TIME |       /\r\n          |      /\r\n   60 secs+-----+--------------------------- Teleport\r\n          |    /\r\n          |   /\r\n          |  /\r\n          | /\r\n          |/\r\n          +--------------------------------\r\n                  PROBLEM SIZE N = DISTANCE\r\n\r\nThe sloping line represents the function Twalk(N) = (1/speed)*N; when we remove\r\nthe constant (1/speed), this function is just O(N). The horizontal line\r\nrepresent the function Tteleport(N) = 60; the time is always 60 seconds. We\r\ntreat that as 60*1 (so we remove the 60 as a constant multiplier), which leaves\r\nthis function as O(1). The meaning of O(1) is not 1 second, but is any constant\r\ntime.\r\n\r\nThe slope of the Walk line represents the speed: slower speeds have a bigger\r\nslope; higher speeds have a smaller slope. But no matter the slope, the Walk\r\nline will eventually intersect the Teleport line: where both modes of\r\ntransportation take the same amount of time. For all larger distances,\r\nteleporting will take less time than walking.\r\n\r\nWhat if we needed 30 seconds to put on our shoes. The graphs would change to be\r\n\r\n              Walk\r\n     TIME |    /\r\n          |   /\r\n   60 secs+--+----------------------------- Teleport\r\n          | /\r\n          |/\r\n   30 secs+   Putting on shoes in 30 seconds\r\n          |\r\n          |\r\n          +--------------------------------\r\n                  PROBLEM SIZE N = DISTANCE\r\n\r\nHere Twalk(N) = (1/speed)*N + 30\r\n\r\nThe lines still cross in the same way (and still at 60 seconds, although now at\r\na smaller distance on the X axis): the distance we can walk in 30 seconds,\r\nbecause that is how much time we can walk between putting on our shoes and\r\nstarting to teleport.\r\n\r\nFor these specific lines we once again see that walking is better (takes less\r\ntime) than teleporting for small distances, but that is because we have written\r\nout the full equations here. Generally, when we know only that one process is\r\nO(N) and another one is O(1) we don't know which is better for small N.\r\n\r\nFor example, if it took us 90 seconds to put on our shoes, it would always be\r\nfaster to teleport. \r\n\r\n            Walk\r\n     TIME | /\r\n          |/\r\n   90 secs+   Putting on shoes in 90 seconds\r\n          |\r\n          |\r\n   60 secs+-------------------------------- Teleport\r\n          |\r\n          |\r\n   30 secs|\r\n          |\r\n          |\r\n          +--------------------------------\r\n                  PROBLEM SIZE N = DISTANCE\r\n\r\nWhether Twalk(N) = (1/speed)*N or (1/speed)*N + 30 or Twalk(N) = (1/speed)+ 90,\r\nwe still say walking is O(N). So, asking whether O(1) or O(N) is faster for\r\nsmall sized problems is unknown: it depends on the speed and the dropped term\r\n(either 0, 30, or 90 above). But regardless of constants and dropped term, we\r\nknow there is some distance, at which (and for all larger distances)\r\nteleporting, which is O(1), is faster than walking, which is O(N).\r\n\r\nAgain: We cannot tell whether an O(1) algorithm is faster than an O(N) one for\r\nsmall problem sizes, but for large ones we know O(1) is faster.\r\n\r\n----------\r\n\r\n----------\r\nIMPORTANT: Determining the Complexity Class Empirically\r\n  - from a Doubling Signature\r\n\r\nIf we can demonstrate that doubling the size of the input approximately\r\nquadruples the time of an algorithm, then the algorithm is likely O(N**2). We\r\ncan use the doubling signatures shown above for other complexity classes as\r\nwell. Thus, even if we cannot mathematically analyze the complexity class of an\r\nalgorithm based on inspecting its code (something we will highlight in the next\r\nlecture), if we can measure it running on various sized problems (doubling the\r\nsize over and over again), we can use the signature information to approximate\r\nits complexity class. In this approach we don't have to understand (or even look\r\nat) the code.\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComputing Running Times from Complexity Classes\r\n\r\nWe can use knowledge of the complexity class of an algorithm to predict its\r\nactually running time on a computer as a function of N easily. For example, if\r\nwe know the complexity class of algorithm a is O(N**2), then we know that\r\nTa(N) ~ cN**2 for some constant c. The constant c represents the \"technology\"\r\nused: the language, interpreter, machine speed, etc.; the N**2 (from O(N**2))\r\nrepresents the \"science/math\" part: the complexity class. Now, given this\r\ninformation, we can time the algorithm for some large value N. Let's say for\r\nN = 10,000 (which is actually a pretty small N these days) we find that\r\nTa(10,000) is 4 seconds.\r\n\r\nWe can solve for c using the first measurement, we have\r\n\r\n  Given Ta(N) ~ cN**2, substituting 10,000 for N and 4 for Ta(N) (from timing \r\n  algorithm a running on a computer, as described above) we have\r\n  Ta(10,000) = 4 ~ c 10,000**2 (from the formula), so solving for the the\r\n  technology constant, we have c ~ 4x10**-8.\r\n\r\nIf I asked you to estimate Ta(20,000) you could plug into this equation and\r\ncompute 4x10**-8 x (20,000)**2 = 16. But, you could also shortcut this process\r\nbecause you know that doubling the input of an O(N**2) algorithm approximately\r\nincreases its running time by a factor of 4, so the answer woudl be 16 seconds.\r\n\r\nSo, by measuring the run-time of this code, we can calculate the constant \"c\",\r\nwhich involves all the technology (language, interpreter, computer speed, etc).\r\nRoughly, we can think of c as being the amount of time it takes to do one loop\r\n(# of instructions per loop/speed of executing instructions) where the\r\nalgorithm requires N**2 iterations through the loops to do all its work. That\r\nis why the constant is so small: modern computers execute billions of operations\r\nper second.\r\n\r\nTherefore, Ta(N) ~ 4x10**(-8) x N**2. So, if asked to estimate the time to\r\nprocess 1,000,000 (10**6) values (100 times more than 10,000), we'd have\r\n\r\n  Ta(10**6) ~ 4x10**(-8) x (10**6)**2\r\n  Ta(10**6) ~ 4x10**(-8) x 10**12\r\n  Ta(10**6) ~ 4x10**4, or about 40,000 seconds (~1/2 a day: day = 86,400 secs)\r\n\r\nNotice that solving a problem 100 times as big takes 10,000 (which is 100**2)\r\ntimes as long, which is based on the signature for an O(N**2) algorithm when\r\nwe increase the problem size by a factor of 100. If we go back to our sorting\r\nexample,\r\n\r\n    I(100N)    8(100N)**2\r\n   ------- ~ ------------- ~ 10,000\r\n    I(N)       8 N**2\r\n\r\nIn fact, while we often anaylze code to determine its complexity class, if we\r\ndon't have the code (or find it too complicated to analyze) we can double the\r\ninput sizes a few times and see whether we can \"fit the resulting times\" to any\r\nof the standard signatures to estimate the complexity class of the algorithm.\r\nWe should do this for some N that is as large as reasonble (taking some number\r\nof seconds to solve on the computer).\r\n\r\nNote for an O(2**N) algorithms, if we double the size of the problem from 100\r\nto 200 values the amount of time needed goes up by a factor of 2**100, which is\r\n~ 1.3x10**30. The time for such algorithms \"explodes\" as the size of the problem\r\nincreases. Notice that even adding just one more value to process doubles the\r\ntime: this \"exponential\" time is the inverse function of logarithmic time, in\r\nterms of its growth rate: it grows incredibly quickly while logarithms grow\r\nincredible slowly. An exponential algorithm does littel good as problem sizes\r\nget bigger.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nOdds and Ends:\r\n\r\nMany algorithms (including sorting) exhibit the following behavior. Notice that\r\nthe upper bound of the inner loop (i) is changed by the outer loop.  It is\r\nimportant to be able to analyze this code to compute its complexity class. \r\n\r\nfor i in range(N):\r\n  for j in range(i):\r\n     body\r\n\r\nHow many times does the \"body\" of the loop get executed? When the outer loop\r\nindex i is 0, \"body\" gets executed 0 times; when the outer loop index i is 1,\r\n\"body\" gets executed 1 time; when the outer loop index i is 2, \"body\" gets\r\nexecuted 2 times; .... when the outer loop index i is N-1 (as big as i gets),\r\n\"body\" gets executed N-1 times. So, in totality, \"body\" gets executed\r\n0 + 1 + 2 + 3 + ... + N-1 times, or dropping the 0, just 1 + 2 + 3 + ... + N-1\r\ntimes. Is there a simpler way to write this sum?\r\n\r\nThere is a simple, general closed form solution of adding up consecutive\r\nintegers. Here is the proof that 1 + 2 + 3 + ... + N =N*(N+1)/2. This is a\r\ndirect proof, but this relationship can also be proved by induction.\r\n\r\nLet\r\n\r\nS = 1 + 2 + 3 + ... + N-1 + N.\r\n\r\nSince the order of the numbers makes no difference in the sum, we also have\r\n\r\nS = N + N-1 + ... + 3 + 2 + 1.\r\n\r\nIf we add the left and right side (column by column) we have\r\n\r\nS   =   1  +    2  +   ...  +   N-1  +  N\r\nS   =   N  +   N-1 +   ...  +   2    +  1\r\n-------------------------------------\r\n2S  = (N+1) + (N+1) +  ... +  (N+1) + (N+1)\r\n\r\nThat is, each pair in the column sums to N+1, and there are N pairs to sum.\r\nSince there are N pairs, each summing to N+1, the right hand side can be\r\nsimplified to just N*(N+1). so\r\n\r\n2S = N*(N+1), therefore S = N(N+1)/2 = N**2/2 + N/2\r\n\r\nThus, S is O(N**2): with a constant of 1/2 dropped and a lower order term of N/2\r\ndropped (all terms whose \"order\" is lower than N**2 are dropped). Note that\r\neither N or N+1 is an even number, so dividing their product by 2 is always a\r\ninteger as it must be for the sum of integers: 6*7/2 = 21.\r\n\r\nSo, looking back at the example of the code above, the total number of times\r\nthe body gets executed is 0 + 1 + 2 + ... + N-1 which is the same as\r\n1 + 2 + ... + N-1 so plugging N-1 in for N we have (N-1)(N-1+1)/2 = \r\nN**2/2 - N/2 which is still O(N**2) for the same reason.\r\n\r\nWe can apply this formula for putting N values at the end of a linked list\r\nthat is initially empty (and has no cache reference to the last node). It takes\r\na certain amount of time to start at the front and skip all nodes to reach the\r\nback. To put in the 1st value requires skipping 0 nodes; to put in the 2nd value\r\nrequires skipping 1 nodes; to put in the 3rd value requires skipping 2 nodes;\r\n... to put in the Nth value requires skipping N-1 nodes. So the number of nodes\r\nskipped is 0 + 1 + ... + N-1, which by the formula is (N-1)N/2: so building a\r\nlinked list in this way is in the O(N**2) complexity class.\r\n\r\n\r\nFast Searching and Sorting:\r\n\r\nThere are obvious algorithms for searching a list in complexity O(N) and \r\nsorting a list in complexity O(N**2). But, there are surprisingly faster\r\nalgorithms for these tasks: searching a list can be O(Log N) IF THE LIST IS\r\nSORTED (the algorithm is called binary search); and sorting values in a list\r\ncan be in O(N Log N).\r\n\r\nIn lecture, I will briefly discuss the binary searching algorithm, for searching\r\na sorted list: its complexity class is O(Log N). In fact if we ask in the worst\r\ncase, how many times do we need to compare the \"value being searched for\"\r\nagainst \"a value in the list\" the answer is exactly Log N. That means that when\r\nsearching a list of 1,000,000 values, we must access the list at most about 20\r\ntimes to either (a) find the index of the value in the list or (b) determine\r\nthe value is not in the list. This is potentially 50,000 times faster than a\r\nloop checking one index after another (which we must do if the list is not\r\nsorted)! On large problems, algorithms in a lower complexity class typically\r\nexecute much faster. Also note that when increasing the size of the list by\r\n1,000 (to 1 billion values) the maximum number of accesses for binary search\r\ngoes from 20 to 30.\r\n\r\nYou should know that Python's sorting method (called TimSort, named after Tim\r\nPeters, who invented it; it is a variant of simpler and more well-known\r\nmergesort) on lists is O(N Log N), but we will not discuss the algorithm. As\r\nwith binary searching, we will discuss the details in ICS-46.\r\n\r\nNote that we CANNOT perform binary searching efficiently on linked lists,\r\nbecause we cannot quickly find the middle of a linked list (for lists we just\r\ncompute the middle index and access the list there). In fact, we have seen\r\nanother self-referential data structure, binary search trees, that we can use\r\nto perform efficient searches (assuming the tree is bushy).\r\n\r\nSorting is one of the most common tasks performed on computers. There are\r\nhundreds of different sorting algorithms that have been invented and studied. \r\nMany small and easy to write sorting algorithms are in the O(N**2) complexity\r\nclass (see the sorting code above, for example). Complicated but efficient \r\nalgorithms are often in the O(N Log N) complexity class. We will study sorting\r\nin more detail in ICS-46, including a few of these efficient algorithms.\r\n\r\nFor now, memorize that fast (binary) searching is in the O(Log N) complexity\r\nclass (on sorted data) and fast sorting algorithms are in the O(N Log N)\r\ncomplexity class. If you are ever asked to analyze the complexity class of a\r\ntask that requires sorting data as part of the task, assume the sorting\r\nfunction/method is in the O(N Log N) complexity class.\r\n\r\n\r\nClosing:\r\n\r\nTo close for now, finding a new algorithm to solve a problem in a lower\r\ncomplexity class is a big accomplishment; a more minor (but still useful)\r\naccomplishment is decreasing the constant for an algorithm in the same\r\ncomplexity class (certainly useful, but often based more on technology than\r\nscience). By knowing the complexity class of an algorithm we know a lot about\r\nthe performance of the algorithm when processing a large amount of data\r\n(especially if we measure the time it takes to solve certain sized problems:\r\nusing this information we can accurately predict the time to solve other size\r\nproblems). Finally, We can also reverse the process, and use a few measurements,\r\ndoubling the size of the problem each time, to approximate the complexity class\r\nof an algorithm.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComing up: In the next two lectures we will first learn the complexity classes\r\nof many of the operations in Python, so we can look at the code of a Python\r\nfunction and use these operations to directly determine its complexity class.\r\nThe second lecture includes code for easily timing Python functions, including\r\ndicussion of hashing that will reveal why dicts and sets behave the way they do\r\n(very efficiently) in Python.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nProblems:\r\n\r\n1) A PC in the 1980s executed 10**6 operations per second; a PC today executes\r\n1,000 times faster 10**9 operations per second. Assume a slow sorting method\r\ntakes exactly N**2 operations to sort a list of length N; assume a fast sorting\r\nmethod takes exactly 5 N Log N operations to sort a list of length N. When\r\nsorting a small amount of data, the fast machine runinng the slow algorithm\r\n(call this configuration 1) is better; when sorting a large amount of data the\r\nslow machine running the fast algorithm is better (call this configuration 2).\r\nFor approxmiately what size array does the transition occur: for smaller arrays\r\nconfiguration 1 is faster, but for larger arrays configuration 2 is better.\r\n\r\n2) Suppose an algorithm is O(N Log N) and it takes .003 seconds to run on a\r\ncomputer for a problem of size 1000. Determine the function Ta(N) and compute\r\nhow long it would take to run on a problem of size 1 million. Hint: us the\r\nsimple approximation shown in this lecture for logarithms.\r\n\r\n3) Suppose that we time a function f by doubling the size of its input and we\r\nget the following data\r\n\r\n   N   |  Time to compute f(n)\r\n-------+----------------------\r\n1,000  |     .32\r\n2,000  |    1.31\r\n4,000  |    5.12\r\n8,000  |   20.5\r\n\r\nWhat complexity class has a signature that matches this data?\r\n", "encoding": "ascii"}