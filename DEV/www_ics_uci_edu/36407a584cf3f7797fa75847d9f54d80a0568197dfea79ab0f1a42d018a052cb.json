{"url": "https://www.ics.uci.edu/~eppstein/180a/990202b.html", "content": "<HTML>\n<HEAD>\n<TITLE>ICS 180, February 2, 1999</TITLE>\n<META name=\"Owner\" value=\"eppstein\">\n<META name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</HEAD><BODY>\n<IMG SRC=\"icslogo2.gif\" WIDTH=472 HEIGHT=72 ALT=\"\"><P>\n<A HREF=\"index.html\">\n<H1>ICS 180, Winter 1999:<BR>\nStrategy and board game programming</H1></A>\n\n<H2>Lecture notes for February 2, 1999<BR>\nVariants of Alpha-Beta Search</H2>\n\nAlthough the basic alpha-beta search discussed already is simple and works \nwell, there have been several attempts to search game trees even more \nefficiently.  The basic idea behind most of these is that, if we consider \nthe scores in the range from alpha to beta as being \"interesting\" and all \nother scores to be \"uninteresting\", then alpha-beta gets its \nefficiency by cutting off the search quickly at nodes with uninteresting \nscores.  If we narrow the gap between alpha and beta, fewer scores will be \ninteresting and more cutoffs will happen.\n\n<P>First, let's quickly review the original alpha-beta search,\nomitting details like hashing or\n<A HREF=\"990202a.html\">adjust winning scores for the current ply</A>.\n\n<PRE>\n    // basic alpha-beta search\n    int alphabeta(int depth, int alpha, int beta)\n    {\n        move bestmove;\n        if (game over or depth <= 0) return winning score or eval();\n        for (each possible move m) {\n            make move m;\n            score = -alphabeta(depth - 1, -beta, -alpha)\n            if (score >= alpha) { alpha = score; bestmove = m; }\n            unmake move m;\n            if (alpha >= beta) break;\n        }\n        return alpha;\n    }\n</PRE>\n\n<H3>Fail-Soft Alpha-Beta</H3>\n\nThe code above always returns alpha, beta, or a number between alpha and \nbeta.  In other words, when a score is uninteresting, no extra information \nis returned about that score.  The reason for this is that the current \nscore is kept in the variable alpha, which starts at the bottom of the \nwindow of interesting scores, and always increases from there, so it is not \npossible to return a score less than alpha.\nOne of the simplest improvements to \nalpha-beta is to keep the current score and alpha in separate variables.\nThe following pseudocode uses the constant \"WIN\" to denote the maximum \nscore that can be returned by any call to alpha-beta search.\n\n<PRE>\n    // fail-soft alpha-beta search\n    int alphabeta(int depth, int alpha, int beta)\n    {\n        move bestmove;\n        int current = -WIN;\n        if (game over or depth <= 0) return winning score or eval();\n        for (each possible move m) {\n            make move m;\n            score = -alphabeta(depth - 1, -beta, -alpha)\n            unmake move m;\n            if (score >= current) {\n                current = score;\n                bestmove = m;\n                if (score >= alpha) alpha = score;\n                if (score >= beta) break;\n            }\n        }\n        return current;\n    }\n</PRE>\n\nWith this change, one can determine a little more information than before \nabout a position.  If the returned value x is less than or equal to alpha, \nthen we still don't know the true value of the position (because we may \nhave pruned away some important lines of the search), but we do know that \nthe true value is at most x.  Similarly, if x is greater than or equal to \nbeta, we know that the true search value is at least x.\nThese slightly tighter upper and lower bounds don't improve the search \nitself, but they could lead to a greater number of successful hash probes.\nThe use of fail-soft alpha-beta is also essential in the MTD(f) algorithm \ndescribed below.\n\n<H3>Aspiration Search</H3>\n\nThis is not a replacement for alpha-beta, it is just a change to the way \nthe outermost call to the search is made.\nNormally, when using alpha-beta to choose the best move, one calls \n<PRE>\n    alphabeta(depth, -WIN, WIN)\n</PRE>\nwhere the huge range between -WIN and \nWIN indicates that we don't know what the true search value will be, so\nall possible scores should be considered interesting.\nThen, the move one makes should be the one set in the variable bestmove at \nthe outer level of the search.\n\n<P>Instead, it is often helpful to call alpha-beta with an artificially \nnarrow window centered around the previous search value.  If the result is \na score within that window, you've saved time and found the correct search \nvalue.  But if the search fails, you must widen the window and search \nagain:\n<PRE>\n    // aspiration search\n    int alpha = previous - WINDOW;\n    int beta = previous + WINDOW;\n    for (;;) {\n        score = alphabeta(depth, alpha, beta)\n        if (score <= alpha) alpha = -WIN;\n        else if (score >= beta) beta = WIN;\n        else break;\n    }\n</PRE>\n\nThe constant WINDOW should be set in a way that balances the time savings \nfrom a narrower search with the time lost from repeating an unsuccessful \nsearch.  A typical value in chess might be around half a pawn.\nVariants of aspiration search include widening the window more gradually \nin the event of an unsuccessful search, or using an initial search window \nthat is not necessarily centered around the previous search result.\n\n<H3>MTD(f)</H3>\n\nThis technique, like aspiration search, is just a modification to the \ninitial call to alpha-beta.  If a narrower search window leads to faster \nsearches, the idea here is to make the search window as narrow as possible:\nit always calls alpha-beta with beta=alpha+1.  The effect of such a \n\"zero-width\" search is to compare the true score with alpha: if the search \nreturns a value at most alpha, then the true score is itself at most alpha,\nand otherwise the true score is greater than alpha.\n\n<P>One could use this idea to perform a binary search for the true score:\n<PRE>\n    int alpha = -WIN;\n    int beta = +WIN;\n    while (beta > alpha+1) {\n        int test = (alpha+beta)/2;\n        if (alphabeta(depth, test, test+1) <= test) beta = test;\n        else alpha = test+1;\n    }\n</PRE>\n\n<P>However, this will lead to a large number of searches\n(the logarithm of the difference between WIN and -WIN).\nThe MTD(f) idea is to instead use fail-soft alpha-beta to \ncontrol the search: each call to fail-soft alpha-beta returns a search \nvalue which is closer to the final score, so if we use that search value as \nthe start of the next test, we should eventually converge.\n<PRE>\n    // MTD(f)\n    int test = 0;\n    for (;;) {\n        score = alphabeta(depth, test,test+1);\n        if (test == score) break;\n        test = score;\n    }\n</PRE>\n\nUnfortunately, complicated interactions with the hash table can cause this \nroutine to get into an infinite loop, so one needs additional code to halt \nthe search if too many iterations have been made without any convergence.\n\n<P>One big advantage of MTD(f) is that we can simplify the code to the \nalpha-beta search, since it only really has two parameters (depth and alpha)\nrather than three.\n\n<H3>PVS</H3>\n\nProbably the best of the alpha-beta variants, this goes by several names: \nNegascout, Principal Variation Search, or PVS for short.\nThe idea is that alpha-beta search works best if\nthe first recursive search is likely to be \nthe one with the best score.  Techniques such as sorting the move list or \nusing a best move stored in the hash table make it especially likely that \nthe first move is best.  If it is, we can search the other moves more \nquickly by using the assumption that they are not likely to be as good.\n\n<P>So PVS performs that first search with a normal \nwindow, but on subsequent searches uses a zero-width window to test each \nsuccessive move against the first move.  Only if the zero-width search \nfails does it do a normal search.\n\n<PRE>\n    // principal variation search (fail-soft version)\n    int alphabeta(int depth, int alpha, int beta)\n    {\n        move bestmove, current;\n        if (game over or depth <= 0) return winning score or eval();\n        move m = first move;\n        make move m;\n        current = -alphabeta(depth - 1, -beta, -alpha)\n        unmake move m;\n        for (each remaining move m) {\n            make move m;\n            score = -alphabeta(depth - 1, -alpha-1, -alpha)\n            if (score > alpha && score < beta)\n                score = -alphabeta(depth - 1, -beta, -alpha)\n            unmake move m;\n            if (score >= current) {\n                current = score;\n                bestmove = m;\n                if (score >= alpha) alpha = score;\n                if (score >= beta) break;\n            }\n        }\n        return current;\n    }\n</PRE>\n\nThis shares the advantage with MTD(f) that most nodes in the search tree \nhave zero-width windows, and can use a simpler two-parameter form of \nalpha-beta.  Since there are very few calls with beta > alpha+1,\none can do extra work in those calls (such as saving the best move for \nlater use) without worrying much about the extra time it takes.\n\n<H3>Recommendations</H3>\n\nMy own program uses a combination of aspiration search (for the outermost \ncall to the search routines) and PVS (for the inner calls).\nHowever, different games behave differently.  These searches are not so \nhard to implement; as with most aspects of game programming, the best way \nto choose between them, and to tune their parameters, is to implement them \nall and to try some experiments.  All of them should return the same \nsearch value (or nearly the same, if influenced by the hash table), but \nwith different numbers of nodes searched.  Pick the one that leads to the \nsmallest search trees in typical positions from your game.\n\n<P><HR>\n<A HREF=\"/~eppstein/\">David Eppstein,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.\n</BODY></HTML>\n", "encoding": "ascii"}