{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/avl.txt", "content": "\t\t\t\tAVL Trees\r\n\r\nIn this lecture we will cover AVL (Adelson-Velskii and Landis) trees. An AVL\r\ntree is a special kind of BST, with order AND structure properites. As with\r\nBSTs, the order property is the same: it ensures that we can search for any\r\nvalue in O(Height); the structure property ensures that the height of an AVL\r\ntree is always O(Log N). So, all AVL trees are always reasonably well balanced:\r\nthere are no pathological AVL trees. AVL trees, invented in 1962, were the\r\nfirst searchable binary trees whose height was guaranteed not to exceed O(Log N)\r\n(for trees with N nodes).\r\n\r\nYou can google other kinds of trees with a guaranteed or just expected (on\r\naverage) O(Log N) worst case for searching, adding, and removing nodes: Splay\r\nTrees, (2,4) Trees, and Red-Black trees. These tend to be more complicated to\r\nunderstand, but are faster (or use less space). We will study only AVL trees in\r\ndetail in this course.\r\n\r\n\r\nAVL Trees:\r\n  Order   : Same as a binary search tree. So they are easily/quickly searchable\r\n            using exactly the same algorithm as for BSTs\r\n\r\n Structure: For every node in the tree, the difference in heights between\r\n            its children cannot exceed 1. That is, the left child can have a\r\n            height one higher than the right, the right child can have\r\n            a height one higher than the left, or both children can have the\r\n            same height. This property keeps these trees reasonably well\r\n            balanced (a small constant times the the optimal height) and nowhere\r\n            near pathological. Processing time -which is always O(Height)- will\r\n\t    be O(Log N), but the constant can be bigger, because of now having\r\n            to satisfy a structure property.\r\n\r\nNote that to make everything work right, we will continue to assign an\r\nempty tree a height of -1. Thus the following tree (showing the height of\r\nevery node)\r\n\r\n   A\r\n   2 \\\r\n      B\r\n      1\\\r\n        C\r\n        0\r\n\r\ndoes not satisfy the AVL property (it certainly looks unbalanced) because A's\r\nleft child (an empty subtree) has height -1 (by the definition above) and its\r\nright child (B) has height 1, for a difference of 2, which violates the\r\nstructure property.\r\n\r\nRegular BSTs have no structure property. They are structured solely by the\r\norder in which the values are added to the tree (which allows for building\r\npathological trees). The structural property of AVL trees ensures that all are\r\nreasonably well balanced, regardless of the order that the nodes are added and\r\nremoved, and therefore guaranteed to be searched quickly, with no pathologies.\r\nSo some BSTs are not AVL trees (see above) because an unsatisfied structure\r\nproperty.\r\n\r\nRecall the height of an N node BST can be N-1 in the worst case. We've seen\r\nthat the best height we can get (from a perfectly balanced binary tree) is\r\nabout (Log2 N) - 1, which is O(Log2 N); for AVL trees, we can achieve this same\r\nheight in the best case, but in the worst case the height is 2 Log2 N, meaning\r\nat worst it is about twice as deep as optimal, but we can ALWAYS search for a\r\nnode in O(Log N) -throwing out the constant 2. We will prove this result at the\r\nend of the AVL section in these notes. There actually is a tighter bound that\r\nwe will prove, at worst the height is 1.44 Log2 N for an AVL tree. Recall that\r\nfor random BSTs that we constructed, the heights were mostly 2-4 times the\r\nminimum, so on average the height of a BST is still only a few times the height\r\nof an AVL tree. When asking about constants, the question is, \"Is it cost\r\neffective to restore the AVL property, or not do so and instead just use BSTs?\"\r\nThis  question must be answered empirically, because it depends on technology \r\nthe implementation) and the actual distribution of data.\r\n\r\nLet us now see how to use the order and structure properties to add and remove\r\nnodes in O(Log N) as well, ensuring these properties are restored after the\r\naddition and removal.\r\n\r\nThere are many ways to represent an AVL tree. One straightforward way is to\r\nstore an int representing the height in each node (caching it rather than\r\nhaving to recompute it). Of course, the height of any parent is 1 + the height\r\nof its highest child. This information will be used and updated in the\r\nalgorithms below.\r\n\r\nAdding and removing nodes follows the pattern we saw in heaps (but reversed:\r\nwe first satisfy the order property, and then work on satisfying the structure\r\nproperty while retaining the order property). We add/remove in an AVL tree just\r\nas in a BST, possibly violating the structure property, and then restore the\r\nstructural property \"cheaply\" -that is, having to look at O(Log N) nodes, and\r\ndoing at most O(1) work for each node.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nAdding a Value to an AVL tree:\r\n\r\nTo add a value, we start by adding it as in any BST. Then we traverse the tree\r\nbackwards -from that node towards the root- adjusting the heights as we go\r\nupward to account for the new node. IF we reach a node that violates the\r\n\"heights of the children are no different than 1\"  structure property, we\r\nperform an adjustment: one \"rotation\" on that node and two underneath it: the\r\ntwo visited right before the \"bad\" one, on the way up toward the root from\r\nthe added node. Each rotation ensures the order property is still satisified\r\nwhile helping to restore the structure property.\r\n\r\nThere are 4 possible patterns of these 3 nodes (actually 2 possible patterns,\r\nand their mirror-image versions), each with its own transformation/rotation that\r\nre-establishes the structure property. In all these cases, the 4 subtrees\r\nunder A, B, and C appear left to right, labelled T1, T2, T3, and T4\r\n\r\nNotice here, the nodes are labelled such that A < B < C; and everything\r\nin T1 < everything in T2 < ... T3  < ... T4. Finally, notice ALL the trees\r\non the right are the SAME in terms of their ABC and T1 T2 T3 T4 pattern. Note\r\nthat T1, T2, T3, and/or T4 can be empty.\r\n\r\n      C\t\t\t\t    B\r\n     / \\\t                 /     \\\r\n    B  T4                    A            C\r\n   / \\\t\t=>          / \\          / \\   \r\n  A  T3\t\t\t   T1  T2       T3  T4\r\n / \\\r\nT1 T2\r\n\r\n\r\n   A\t\t\t\t    B\r\n  / \\\t\t                 /     \\\r\nT1   B                       A            C\r\n    / \\         =>          / \\          / \\   \r\n   T2  C\t\t   T1  T2       T3  T4\r\n     / \\\r\n    T3 T4\r\n\r\n\r\n      C\t\t\t\t    B\r\n     / \\\t                 /     \\\r\n    A  T4                    A            C\r\n   / \\\t\t=>          / \\          / \\   \r\n  T1 B\t\t\t   T1 T2        T3  T4\r\n    / \\\r\n   T2 T3\r\n\r\n\r\n   A\t\t\t\t    B\r\n  / \\\t\t                 /     \\\r\nT1   C                       A            C\r\n    / \\         =>          / \\          / \\   \r\n   B  T4\t\t   T1 T2        T3  T4\r\n  / \\\r\n T2 T3\r\n\r\n\r\nDoing a rotation is O(1): it changes a fixed number of pointers in the tree.\r\n\r\nRecall that the heights of AVL trees are O(Log N). Threfore, both going down\r\nthe tree (to add the value where it belongs) and back up the tree (looking for\r\na violation of the AVL property and restoring it via a rotation) requires\r\nO(Log N) operations: so the combined complexity class for adding a node and\r\nrestoring the structure property is O(Log N). Any rotation is a constant\r\namount of work, O(1). Rotations are not trivial to do -lots of pointer changes-\r\nbut not dependent on N, the size of the tree. A rotation is something done\r\nlocally at any unbalanced node and its child and grandchild: only those nodes\r\nare affected.\r\n\r\nHere is an example. Given the following AVL tree (it would be useful for you to\r\nfill in the heights of every node, and verify that it satsified the order and\r\nstructure properties of AVL trees)\r\n\r\n       44\r\n     /     \\\r\n   17        78\r\n    \\        /  \\\r\n     32     50   88\r\n            / \\\r\n          48  62\r\n\r\nIf we added 54, according to the order property we would get the tree\r\n\r\n        44\r\n     /      \\\r\n   17         78\r\n    \\        /  \\\r\n     32     50   88\r\n            / \\\r\n          48  62\r\n              /\r\n             54\r\n\r\nWe move toward the root from 54 to 62, to 50, and finally to 78. At 78 we see\r\nthe first violation of the structure property: its left child (50) is of height\r\n2 and its right child (88) is of height 0 - a difference of more than 1. So,\r\nwhich rotation do we apply? We use 78 (where we noticed the problem) as the\r\nroot of the rotation; we use its child that has the larger height (it always\r\nhas one whose height is larger if we need to do the rotation there; in fact, it\r\nwill always be the child whose subtree we added the new node in), and we use\r\nits grandchild with the larger height, or IF THE HEIGHTS ARE TIED, we use the\r\ngrandchild related to the child in the same way that the child was related to\r\nthe root of the rotation: if we go left/right from root to child, we also go\r\nleft/right from child to grandchild, if both grandchildren have the same height.\r\nIn the picture above, the grandchild 62 has a height one bigger than the\r\ngrandchild 48.\r\n\r\nSo, we apply the rotation to 78, 50, and 62\r\n\r\n              78\r\n             /   \\\r\n            50   T4  \r\n           /  \\\r\n          T1   62\r\n              /  \\\r\n             T2  T3\r\n\r\nusing the 3rd of the four patterns above, which produces the following result\r\npattern\r\n\r\n                 62\r\n              /      \\\r\n            50       78\r\n            / \\     /  \\\r\n          T1  T2   T3  T4\r\n\r\nwhich when T1-T4 are filled in (some are empty) produces the result\r\n\r\n        44\r\n     /      \\\r\n   17         62\r\n    \\        /  \\\r\n     32     50   78\r\n            / \\    \\\r\n          48  54   88\r\n\r\nThe result (still looking a bit unbalanced) satisfies the structure property.\r\n\r\nAfter one rotation, we do not need to keep going back towards the root: we\r\ndon't need to update the stored heights nor look for any more violations of the\r\nstructure property. After one rotation, every node above the one rotated will\r\nhave the same height as before the new noded was added, and will satisfy the\r\norder and structure properties.\r\n\r\nSo when ADDING a node to the tree, at most ONE rotation is required to fix the\r\nstructure property: once we perform any rotation at a node, its ancestors will\r\nnot violate the structure property. Also, we can stop \"going toward the root\r\nrecomputing heights\" whenever the height of a node remains unchanged (its\r\nancestor nodes will all then keep their same heights).\r\n\r\nWhat justifies the comments above? When we add a node, the tree becomes\r\nunbalanced because the height of some node becomes one too big; after one\r\nrotation, the difference of the heights of its subtrees are made within 1, by\r\nreducing that height, thus restoring to its old height the node above it. So\r\nthe heights of ancestor nodes remain unchanged (so the AVL structure property\r\ndoesn't need further correction).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nRemoving a Value from an AVL tree:\r\n\r\nFor removing a value the process is different but similar. After removing the\r\nnode (as in a regular BST: remember the leaf node/one child/two children rules)\r\nwe again continue up from the parent of the removed node towards the root. If\r\nwe find a node whose children violate the structure property, we use that node,\r\nits child whose height is larger, and its grandchild with the larger height (or\r\nif the heights are tied, we use the grandchild related to the child in the same\r\nway that the child was related to the root of the rotation). Then we apply a\r\nrotation (as we did above for adding a node to an AVL tree).\r\n\r\nSo, in the above tree (after adding 54)  if we now remove 32 (a leaf so trivial\r\nto remove), we have the tree\r\n\r\n        44\r\n     /      \\\r\n   17         62\r\n             /  \\\r\n            50   78\r\n            / \\    \\\r\n          48  54  88\r\n\r\nWe move toward the root from 17, to 44. At 44 we see a violation of the\r\nstructure property: its left child (17) is of height 0 and its right child\r\n(62) is of height 2 -a difference of  more than one. So, we apply the rotation\r\nto 44, 62 (its bigger height child; always the other side from where the actual\r\nnode was removed) and 78 (since both grandchildren have the same height, and\r\nthe child is the right child of the root of the rotation) which is the second\r\nof the four patterns above) and get the following tree as a result.\r\n\r\n      62\r\n      / \\\r\n    44   78\r\n    / \\   \\\r\n  17  50  88 \r\n     /  \\\r\n   48   54\r\n\r\nNotice that the tree that used to be heavy on the right is now actually heavy\r\non the left; but not so much. So the structure property now holds.\r\n\r\nIn this case we are done, because we made it all the way back to the root.\r\nBut, in general, for REMOVE (unlike ADD), if we were not at the root, we'd\r\nhave to continue up towards the root, updating the stored heights AND looking\r\nfor more nodes not satisfying the structure property, and maybe doing more\r\nrotations (as many as necessary).\r\n\r\nWhat justifies the comments above? When we remove a node, the tree becomes\r\nunbalanced because the height of some node becomes one too small; after one\r\nrotation, the difference of the heights of its subtrees are made within 1, by\r\nreducing one height, and it is POSSIBLE that the height of the node above it\r\nalso has its height reduced by the rotation and now is too small when compared\r\nto its sibling, so this process might have to continue checking/rebalancing all\r\nthe way back to the root. Later in this lecture I actually show an example where\r\nremoval requires two rotations.\r\n\r\nBoth going down the tree (to remove the value) and back up the tree (looking\r\nfor one or more violation of the AVL property and restoring any via \r\nrotations) require O(Log N) operations, so the combined complexity class for\r\nremoving a node and restoring the structure property is O(Log N). Each\r\nrotation is a constant amount of work, O(1). Rotations are not trivial to do\r\n-lots of pointer changes- but not dependent on N, the size of the tree: A\r\nrotation is something done locally at any unbalanced node and its child and\r\ngrandchild.\r\n\r\n----------\r\nBottom-Line Expectations:\r\n\r\nI expect you to be able to draw pictures of AVL trees and update them\r\naccording to these algorithms; I DO NOT expect you to write the code for\r\nthem in C++. I also expect you to be able to reproduce the transformations\r\nfrom memory. This isn't pure memorization: we talked in class about all the\r\nrequirements and symmetries that make these rules easy to memorize/generate.\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nMetrics of AVL-Trees: Size vs Height\r\n\r\nLet us look at the problem of computing the minimum number of nodes that are\r\nneeded to create an AVL tree of height h. Call this number m(h). We want to \r\nfind some relationship between h and m(h). First, let's look at this problem\r\nfor plain BSTs.\r\n\r\nIn a BST, there is no structure property. We know that m(0) = 1, since we need\r\none node to create a BST of height 0. Also, we can always add a parent to a\r\ntree of height h with the minimum number of nodes to create a tree of height\r\nh+1 with the minimal number of nodes. so m(h) = 1 + m(h-1), where m(0) = 1.\r\nIterating evaluation of this function we find\r\n\r\n  m(h) = 1 + m(h-1)\r\n       = 2 + m(h-2)\r\n       = 3 + m(h-3)\r\n       ...\r\n       = i + m(h-i)\r\n\r\nwe know m(0) = 1, so we solve for when h-i = 0,; it is i = h. Now we have\r\nm(h) = h + m(0) = h + 1. Therefore the minimum number of nodes that are needed\r\nto create a BST with height h must be h+1 nodes (a pathological tree). Also \r\nnote that from our first lecture on BSTs, a BST with height h has at most\r\n2^(h+1) - 1 nodes. So all BSTs of height h have between h+1 and 2^(h+1)- 1\r\nnodes. So for BSTs, we can have both pathological and well-balanced trees.\r\n\r\nNow let's look at AVL trees using the same kind of argument; AVL trees have a\r\nmore stringent structure property (ruling out pathological trees) so the\r\nminimum number of nodes needed for a tree of height h will be more than h+1.\r\n\r\nFirst, let's examine two base cases: for h = 0, the tree must have at least 1\r\nnode as with a BST; likewise, for h = 1, the tree must have at least 2 nodes.\r\n\r\nh = 0                   h = 1\r\n\r\n  A            A                B\r\n                \\       or     /\r\n                 B            A \r\n\r\nAll these trees trivially satisfy the AVL structure property.\r\n\r\nNow for h = 2, each of the following four trees satisfies the AVL structure\r\nproperty, but no trees with just 3 nodes have h = 2 and satisfy the structure\r\nproperty, and no any pathological trees with 4 nodes do either. So m(2) = 4.\r\n   \r\n        B            B          C            C \r\n       / \\          /  \\       /  \\         /  \\\r\n      A   C        A    D     A    D       B    D\r\n           \\           /       \\          /\r\n            D         C         B        A\r\n\r\nNotice that in all these cases for an h = 2 AVL tree, we have a root, with a\r\nminimal sized tree of h = 1 as one subtree (so the height of the entire tree is\r\n2), and a minimual sized tree of h = 0 as the other subtree. We want as few\r\nnodes as possible here, and the AVL structure property allows a difference of\r\n1 in heights in these two minimal subtrees.\r\n\r\nIn fact, we can write an exact equation for m(h): m(h) = 1 + m(h-1) + m(h-2),\r\nwhere m(0) = 1 and m(1) = 2. The smallest number of nodes in a tree of height h\r\nis 1 (it root) + the smallest number of nodes in a tree of height h-1 (so the\r\nheight of the entire tree is h) plus the smallest number of nodes in a tree of\r\nheight h-2  (we want as few nodes as possible here, and the AVL structure\r\nproperty allows a minimum height AVL tree of h-2 here).\r\n\r\nHere is a short table of values for h and m(h)\r\n\r\n  h  |  m(h)\r\n-----+-------\r\n  0  |    1\r\n  1  |    2\r\n  2  |    4\r\n  3  |    7\r\n  4  |   12\r\n  5  |   20\r\n  6  |   33\r\n  7  |   54\r\n  8  |   88\r\n  9  |  143\r\n 10  |  232\r\n\r\nWe know that m is a strictly increasing function, so m(h-1) > m(h-2).\r\nTherefore, 1+m(h-1) > m(h-2), therefore 1+m(h-1)+m(h-2) > 2m(h-2), so using\r\nthe definition of m(h) we have  m(h) > 2m(h-2). Iterating evaluations of this\r\nfunction.\r\n\r\n  m(h) > 2 x m(h-2)\r\n          > 4 x m(h-4)\r\n          > 8 x m(h-6)\r\n          ...\r\n          > 2^i x m(h-2i)\r\n\r\nwe know m(0) = 1, so we solve for when h-2i = 0,; it is i = h/2. Now we have\r\nm(h) >= 2^(h/2) x m(0) = 2^(h/2). So the minimum number of nodes in an AVL tree\r\nof height h grows at least as fast than 2^(h/2): it is Omega(2^(h/2)). Now that\r\nwe have a relationship between h and m(h), but let us rewrite it as follows\r\n\r\n  m(h)      >= 2^(h/2)     just proved\r\n  Log2 m(h) >= h/2         taking logs (base 2) of each side we have\r\n  h/2       <= Log2 m(h)   reverse the  inequality\r\n  h         <= 2 Log2 m(h) flip the sides and multiply each side by two\r\n\r\nThis says the height of any AVL tree grows no faster than 2 times the Log of the\r\nminimum number of nodes it contains: h is O(Log2 m(h)).\r\n\r\nIf n(h) is the number of nodes in some actual AVL tree of height h. We know\r\nthat m(h) <= n(h) (from the definition of m(h): the minimum number of nodes in\r\nan AVL tree of height h), so h <= 2 Log2 m(h) <= 2 Log2 n(h). Since the number\r\nof operations needed to search, add, and remove in an AVL tree is proportional\r\nto its height h, the number of operations is <= 2 Log2 N, where N is the number\r\nof nodes in some actual AVL tree. \r\n\r\nIn fact, the equation m(h) = 1 + m(h-1) + m(h-2) (with m(0) = 1 and m(1) = 2)\r\nis a slight modification of the fibonacci sequence: f(i) = f(i-1) + f(i-2),\r\nwith f(0) = 0 and f(1) = 1. Using that knowledge, with appropriate mathematics\r\nwe can can approximate m(h) very accurately as  m(h) = phi^(h+3)/sqrt(5) - 1,\r\nwhere phi is (1+sqrt(5))/2 or ~1.618034, and is also known as the golden\r\nratio.  Here is a short table of these values\r\n\r\n h  |  m(h)   |  phi^(h+3)/sqrt(5) - 1\r\n----+---------+-------------------------\r\n 0  | \t 1    |\t    0.9\r\n 1  |\t 2    |\t    2.1\r\n 2  |\t 4    |\t    4.0\r\n 3  |\t 7    |\t    7.0\r\n 4  |\t12    |\t   12.0\r\n 5  |\t20    |\t   20.0\r\n 6  |\t33    |\t   33.0\r\n 7  |\t54    |\t   54.0\r\n 8  |\t88    |\t   88.0\r\n 9  |  143    |\t  143.0\r\n10  |  232    |\t  232.0\r\n\r\nDropping the - 1 term (it is small compared to phi^(h+2)/sqrt(5) as h gets big)\r\nand taking logs (base phi) of each side we have\r\n\r\n Log(base phi) m(h) ~ h+3 - Log(base phi) sqrt(5)\r\n\r\nrecall Loga x = Log2 x/log2 a, so Log(base phi) x = Log2 x /Log2 phi, so we\r\nhave\r\n\r\n Log2 m(h)/Log2 phi ~ h+3 - Log(base phi) sqrt(5)\r\n\r\n1/Log2 phi ~1.44, and Log(base phi) sqrt(5) = 1.672, so we have\r\n\r\n  1.44 Log2 m(h) ~ h + 3 - 1.662\r\n\r\n  h ~ 1.44 Log2 m(h) - 1.338, and by dropping this constant term we have\r\n\r\n  h ~ 1.44 Log2 m(h)\r\n\r\nAgain, if n(h) is the number of nodes in some actual AVL tree of height h. We\r\nknow that m(h) <= n(h) (from the definition of m(h): the minimum number of\r\nnodes in an AVL tree of height h), so\r\n\r\n  h ~ 1.44Log2 m(h) - 1.338 <= 1.44Log2 n(h) - 1.338 < 1.44Log2 N\r\n\r\nSince the number of operations needed to search, add, and remove in an AVL tree\r\nis proportional to its height h, the number of operations is < 1.44 Log2 N,\r\nwhere N is the number of nodes in the actual AVL tree. \r\n\r\nSo, in the worst case for AVL trees, the search, add, and remove operations are\r\nall O(Log2 N) and the constant is actually closer to 1.44 than to 2. Again,\r\ncompare this to the empirical numbers we saw for random BSTs: but here it is\r\na guarantee for all AVL trees.\r\n\r\nAnother interesting fact about AVL trees is that if the leaf at the minimum\r\ndepth is at depth d, then the leaf at the maximum depth is at most at depth 2d\r\n(this also represents the height of the tree). Observe for any AVL tree, if we\r\ngo upward from the node at the minimum depth its parent can have a descendant\r\nthat is at most 1 depth deeper; its grandparent can have a node that at most is\r\n2 depths deeper; .... For each ancestor, it can have a descendant that is at\r\nmost one depth deeper. It has d ancestors (is at depth d), the deepest node in\r\nthe entire tree is d+d = 2d : each of the d ancestors can have a descedant one\r\nmore deeper than d.\r\n\r\nFor example, in the minimal AVL tree below\r\n\r\n                44\r\n             /      \\\r\n           /          \\\r\n         17           70\r\n        /  \\         /   \\\r\n       8   30      62     78\r\n      /    / \\            /\r\n     5   20  40         75\r\n             /\r\n            35\r\n\r\nNode 62 is at the minimum depth of 2. It parent has a descendant (75) of depth\r\n3; its  grandparent (the root) has a descendant (35) of depth 4 (2*2).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nA Removal Requiring Two Rotations:\r\n\r\nHere is an example of removal in an AVL tree that requires two rotations. This\r\ntree can be constructed using the procedure/formula above. It has the minimum\r\nnumber of nodes for a height 4 tree. Every internal node (but  not the leaves)\r\nhas children that have a a height difference of 1: no childrend heights are the\r\nsame.\r\n\r\n                44\r\n             /      \\\r\n           /          \\\r\n         17           70\r\n        /  \\         /   \\\r\n       8   30      62     78\r\n      /    / \\            /\r\n     5   20  40         75\r\n             /\r\n            35\r\n\r\nVerify that this is an AVL tree (order property and structure property: fill\r\nin all the heights and compare the heights for a difference of more than 1)\r\n\r\nNow, remove the value 62 (the leaf at the minimum depth).\r\n\r\n                44\r\n             /      \\\r\n           /          \\\r\n         17           70\r\n        /  \\             \\\r\n       8   30             78\r\n      /    / \\            /\r\n     5   20  40         75\r\n             /\r\n            35\r\n\r\nAt this point, the value 70 has two children in its right subtree (height 1)\r\nand no childtren in its left subtree (height -1), so the height difference is\r\n> 1. So we do a rotation using the nodes containing 70, 78, and 75 (all the\r\nT subtrees here are empty!). The tree becomes.\r\n\r\n                44\r\n             /      \\\r\n           /          \\\r\n         17           75\r\n        /  \\         /   \\\r\n       8   30      70     78\r\n      /    / \\\r\n     5   20  40\r\n             /\r\n            35\r\n\r\nRecompute the heights for all the changed nodes.\r\n\r\nNow notice that this operations reduces by one the height of the subtree that\r\nused to store 70 as its root (it now stores 75): from 2 to 1. When we started\r\nthe height of this right subtree was 1 less than the height of the left\r\nsubtree, and now the height of the right subtree has been reduced by 1 because\r\nof the rotation. So, the rotation causes the root of the tree (44) to now have\r\na left child (storing value 17) with a height of 3 and a right child (storing\r\nvalue 75) with a height of 1; the difference between these two heights is > 1.\r\n\r\nSo we do another rotation, this time using 44, 17, and 30. (we use 30, because\r\nthis grandchild of the root of rotation has a bigger height than its sibling,\r\n8). The tree becomes\r\n\r\n                30\r\n             /      \\\r\n           /          \\\r\n         17           44\r\n        /  \\         /   \\\r\n       8   20      40     75\r\n      /           /      /  \\\r\n     5           35     70  78\r\n\r\nFinally, notice that the root of this tree changed its value from 44 to 30 and\r\nits height from 4 down to 3. So if the root were a left subtree of a node whose\r\nright subtree was height 5 (which is OK when the height of this subtree was 4),\r\nthe height of the right subtree will now be > 1 larger then the height of this\r\nleft subtree, causing it to do another rotation.... That is why when removing\r\nnodes, we might have to do more than one rotation.\r\n\r\n\r\nOther Balanced Trees:\r\n\r\nRed-Black trees and Splay trees are more \"modern\" balanced trees, also\r\nguaranteed to have their heights or average heights O(Log N). Their actual\r\nheight bounds are a bit worse than AVL trees, but they do less work to \"restore\r\nthe structure property of the tree\". Thus, their search algorithms runs more a\r\nbit more slowly, but their algorithms for insertion/deletion run more quickly.\r\nTheir main advantage is that they store no extra information (recall that AVL\r\ntrees need to store/cache the height of every node, stored at that node).\r\n\r\n\r\n------------------------------\r\n\r\nOdds & Ends:\r\n\r\nIn this section we will discuss augmenting tree nodes (however they are defined)\r\nwith an extra pointer that allows a child to point directly to its parent. This\r\nis not needed for many simple trees and tree operations, but in all the data\r\nstructures that we define, we are at liberty to add any pointers that we\r\nmight find useful. \r\n\r\nNotice that locating the parent, given its child, was critical for algorithms\r\nthat process heaps. But in heaps, we didn't have to store parent pointers,\r\nbecause heap nodes were store in an array, and knowing the index of a node\r\nallowed us to calculate the index of its parent (and children). And because of\r\nthe structure property, we needed an array whose size was exactly the number\r\nof nodes in the heap.\r\n\r\nRecall in our study of doubly-linked lists, although the extra links allow us\r\nto do new operations (like go backwards in the list), we often have to write\r\nextra code (for old operations, like insert, remove etc.) to maintain/update\r\nthese extra pointers. So, having such extra pointers can be both useful and\r\ncumbersome.\r\n\r\nWe could augment our typical TN to be\r\n\r\ntemplate<class T>\r\nclass TN<T> {\r\n  public:\r\n    TN ()                     : left(nullptr), right(nullptr), parent(nullptr){}\r\n    TN (Entry v, TN<T>* l = nullptr,\r\n                 TN<T>* r = nullptr,\r\n                 TN<T>* p = nullptr) : value(v), left(l), right(r), parent(p){}\r\n\r\n    T        value;\r\n    TN<T>*   parent;\r\n    TN<T>*   left;\r\n    TN<T>*   right;\r\n};\r\n\r\n\r\nNote that the root of the tree would be the only node whose parent was nullptr.\r\nSo, given a pointer to any node t, we could determine whether t points to the\r\nroot of th tree by checking whether t.parent == nullptr.\r\n\r\nGiven a pointer to any non-root node, here is the code to make its parent\r\npoint to its left child (instead of to it) and the left child to point to its\r\nnew parent. This is how we would remove a node with one (left) child from a BST\r\nand AVL tree. This is a bit tricky and we need write an if first to determine\r\nwhether the node is a left or right child of its parent, to know which pointer\r\nof its parent to change.\r\n\r\ntemplate<class T>\r\nvoid makeParentReferToLeftChild(TN<T>* t) {//Assumes t not root/has a parent\r\n  if (t->parent->left == t)\t\t   //Make parent's left/or right\r\n    t->parent->left  = t->left;            //  point to t's left\r\n  else\r\n    t->parent->right = t->left;\r\n  t->left->parent = t->parent;\t\t   //make t->left refer to\r\n}\t\t\t\t\t   //  its new parent\r\n\r\nHand simulate this code on an example to see which two pointers are changed. \r\nWe might want to delete the node (t) in this code as well.\r\n\r\nWe can also use parent pointers to to compute the depth of a node (how many\r\nancestors it has) easily.\r\n\r\ntemplate<class T>\r\nint depth(TN<T>* t) {\r\n  if (t == nullptr)\r\n    return -1;\r\n  else\r\n    return 1 + depth(t->parent);\r\n  }\r\n\r\nNote that the depth of the root should be 0, as it is in this calculation.\r\n", "encoding": "ascii"}