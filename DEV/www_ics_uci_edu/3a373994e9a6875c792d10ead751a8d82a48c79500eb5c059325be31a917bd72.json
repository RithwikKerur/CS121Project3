{"url": "https://www.ics.uci.edu/~dan/pubs/DC-Sec678.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 6 </TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_6\">\n<H2> 6.  EMPIRICAL RESULTS</H2> </a>\n\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<a href=\"#Sec_7\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\nEmpirical tests of the efficiencies of the algorithms presented here\nare reported in [Bentley et al. 1986; \nKnuth 1985; Schwartz and Kallick 1964; Vitter 1987; Welch 1984].\nThese experiments compare the number of bits per word\nrequired and processing time is not reported.  While theoretical\nconsiderations bound the performance of the various algorithms,\nexperimental data is invaluable in providing additional insight.\nIt is clear that the performance of each of these methods is \ndependent upon the characteristics of the source ensemble.\n<P>\nSchwartz and Kallick test an implementation of\nstatic Huffman coding in which bottom merging is used to determine\ncodeword lengths and all codewords of a given length are sequential\nbinary numbers [Schwartz and Kallick 1964].  The source alphabet in \nthe experiment consists\nof 5,114 frequently-used English words, 27 geographical names,\n10 numerals, 14 symbols, and 43 suffixes.  The entropy of the \ndocument is 8.884 binary digits per message and the average codeword \nconstructed has length 8.920.  The same document is also coded \none character at a time.  In this case, the entropy of the source is \n4.03 and the coded ensemble contains an average of 4.09 bits per\nletter.  The redundancy is low in both cases.  However, the\n<EM>relative redundancy</EM> (i.e., redundancy/entropy) is lower when\nthe document is encoded by words.\n<P>\nKnuth describes algorithm FGK's performance on three types of data:\na file containing the text of Grimm's first ten Fairy Tales, \ntext of a technical book,\nand a file of graphical data [Knuth 1985].  For the first two files, the\nsource messages are individual characters and the alphabet size is 128.\nThe same data is coded using pairs of characters, so that the alphabet\nsize is 1968.  For the graphical data, the number of source messages \nis 343. In the case of the Fairy Tales the performance\nof FGK is very close to optimum, although performance degrades with \nincreasing file size.\nPerformance on the technical book is not as good, but is still \nrespectable.  The graphical data proves harder yet to\ncompress, but again FGK performs reasonably well.  In the latter\ntwo cases, the trend of performance degradation with file size continues.\nDefining source messages to consist of character pairs results in\nslightly better compression, but the difference would not appear to justify\nthe increased memory requirement imposed by the larger alphabet.\n\n<PRE>\n n      k    Static    Alg. V    Alg. FGK\n\n100    96     83.0      71.1       82.4\n500    96     83.0      80.8       83.5\n961    97     83.5      82.3       83.7\n\nFigure 6.1 -- Simulation results for a small text file [Vitter 1987];\n              <VAR>n</VAR> = file size in 8-bit bytes, \n              <VAR>k</VAR> = number of distinct messages.\n</PRE>\n\nVitter tests the performance of algorithms V and FGK against that of \nstatic Huffman coding.\nEach method is run on data which includes Pascal source code, \nthe TeX source of the author's\nthesis, and electronic mail files [Vitter 1987].  Figure 6.1 summarizes\nthe results of the experiment for a small file of text.  The performance\nof each algorithm is measured by the number of bits in the coded ensemble and\noverhead costs are not included.  \nCompression achieved by each algorithm is represented by the size of the\nfile it creates, given as a percentage of the original file size.\nFigure 6.2 presents data for Pascal source code.  For the TeX source, the\nalphabet consists of 128 individual characters; for the other two file\ntypes, no more than 97 characters appear.  For each experiment, when \nthe overhead\ncosts are taken into account, algorithm V outperforms static Huffman \ncoding as long as the size of the message\nensemble (number of characters) is no more than 10^4.  Algorithm FGK \ndisplays slightly\nhigher costs, but never more than 100.4% of the static algorithm.\n\n<PRE>\n   n     k    Static    Alg. V    Alg. FGK\n\n  100   32     57.4      56.2       58.9\n  500   49     61.5      62.2       63.0\n 1000   57     61.3      61.8       62.4\n10000   73     59.8      59.9       60.0\n12067   78     59.6      59.8       59.9\n\nFigure 6.2 -- Simulation results for Pascal source code [Vitter 1987];\n              <VAR>n</VAR> = file size in bytes,\n              <VAR>k</VAR> = number of distinct messages.\n</PRE>\n\nWitten et al. compare adaptive arithmetic coding with adaptive Huffman\ncoding [Witten et al. 1987].  The version of arithmetic coding tested\nemploys single-character adaptive frequencies and is a mildly optimized\nC implementation.  Witten et al. compare the results provided by this\nversion of arithmetic coding with the results achieved by the UNIX\n<EM>compact</EM> program (<EM>compact</EM> is based on algorithm FGK).\nOn three large files which typify data compression\napplications, compression achieved by arithmetic coding is better than\nthat provided by <EM>compact</EM>, but only slightly better (average file\nsize is 98% of the <EM>compact</EM>ed size).  A file over a three-character\nalphabet, with very skewed\nsymbol probabilities, is encoded by arithmetic coding in less than one\nbit per character; the resulting file size is 74% of the size of the \nfile generated by <EM>compact</EM>.  Witten et al. also report encoding\nand decoding times.  The encoding time of arithmetic coding is generally \nhalf of the time required by the adaptive Huffman coding method.\nDecode time averages 65% of the time required by <EM>compact</EM>.\nOnly in the case of the skewed file are the time statistics quite\ndifferent.  Arithmetic coding again achieves faster encoding, 67% of\nthe time required by <EM>compact</EM>.  However, <EM>compact</EM> decodes\nmore quickly, using only 78% of the time of the arithmetic method. \n<P>\nBentley et al. use C and Pascal source files, TROFF source files, and a\nterminal session transcript of several hours for experiments which\ncompare the performance of algorithm BSTW to static Huffman coding.\nHere the defined words consist of two disjoint classes, \nsequences of alphanumeric characters and sequences of\nnonalphanumeric characters.\nThe performance of algorithm BSTW is very close to that of static\nHuffman coding in all cases.  The experiments reported by Bentley et al.\nare of particular interest in that they incorporate\nanother dimension, the possibility that in the move-to-front\nscheme one might want to limit the size of the data structure containing\nthe codes to include only the <VAR>m</VAR> most recent words, for some <VAR>m</VAR> [Bentley et al. 1986].  The\ntests consider cache sizes of 8, 16, 32, 64, 128 and 256.  Although\nperformance tends to increase with cache size, the increase is erratic,\nwith some documents exhibiting nonmonotonicity (performance\nwhich increases with cache size to a point and then decreases when cache\nsize is further increased).\n<P>\nWelch reports simulation results for Lempel-Ziv codes\nin terms of compression ratios [Welch 1984].\nHis definition of compression ratio is the one given in\n<a href=\"DC-Sec1.html#Sec_1.3\">Section 1.3</a>,\n<VAR>C</VAR> = (average message length)/(average codeword length).  The ratios\nreported are:  1.8 for English text, 2 to 6 for Cobol data files, 1.0 for\nfloating point arrays, 2.1 for formatted scientific data, 2.6 for system\nlog data, 2.3 for source code, and 1.5 for object code.  The tests involving\nEnglish text files showed that long\nindividual documents did not compress better than groups of\nshort documents. \nThis observation is somewhat surprising, in that it seems to refute the\nintuition that redundancy is due at least in part to correlation in content.\nFor purposes of comparison, Welch cites results of Pechura and Rubin. \nPechura achieved a 1.5 compression ratio using static Huffman coding on files of\nEnglish text [Pechura 1982].\nRubin reports a 2.4 ratio for English text when employing a complex technique\nfor choosing the source messages to which Huffman coding is applied\n[Rubin 1976].\nThese results provide only a very weak basis for comparison, since the\ncharacteristics of the files used by the three authors are unknown.\nIt is very likely that a single algorithm may produce compression ratios\nranging from 1.5 to 2.4, depending upon the source to which it is\napplied.\n\n<a name=\"Sec_7\">\n<H2> 7.  SUSCEPTIBILITY TO ERROR</H2> </a>\n\n<a href=\"#Sec_6\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<a href=\"#Sec_8\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\nThe discrete noiseless channel is, unfortunately,\nnot a very realistic model of a communication system.  Actual data \ntransmission systems are prone\nto two types of error: <EM>phase error</EM>, in which a code symbol\nis lost or gained; and <EM>amplitude error</EM>, in which a code\nsymbol is corrupted [Neumann 1962].\nThe degree to which channel errors degrade transmission is\nan important parameter in the choice of a data compression\nmethod.  The susceptibility to error of a coding algorithm\ndepends heavily on whether the method is static or adaptive.\n\n<a name=\"Sec_7.1\">\n<H3> 7.1  Static Codes</H3> </a>\n\nIt is generally known that Huffman codes tend to be self-correcting\n[Standish 1980].  That is, a transmission error tends not to propagate \ntoo far.  The codeword in which the error occurs is incorrectly\nreceived and it is likely that several subsequent codewords are misinterpreted\nbut, before too long, the receiver is back in synchronization with the sender.\nIn a static code, synchronization means simply that both sender and\nreceiver identify the beginnings of the codewords in the\nsame way.  In Figure 7.1, an example is used\nto illustrate the ability of a Huffman code to recover from\nphase errors. The message ensemble \"<kbd>BCDAEB</kbd>\" is encoded using\nthe Huffman code of Figure 3.4 where the \nsource letters <VAR>a</VAR>(1) ... <VAR>a</VAR>(5) represent <VAR>A ... E</VAR>\nrespectively, yielding the coded ensemble \"<kbd>0110100011000011</kbd>\".\nFigure 7.1 demonstrates the impact of loss of the first bit, the second\nbit, or the fourth bit.  \nThe dots show the way in which each line is parsed\ninto codewords. The loss of the first bit results in re-synchronization\nafter the third bit so that only the first source message (<VAR>B</VAR>) is lost\n(replaced by <VAR>AA</VAR>).\nWhen the second bit is lost, the first eight bits of the coded ensemble\nare misinterpreted and synchronization is regained by bit 9.  \nDropping the fourth bit causes the same\ndegree of disturbance as dropping the second.  \n\n<PRE>\n<kbd>011.010.001.1.000.011.  </kbd> coded ensemble <VAR>BCDAEB</VAR>\n<kbd>1.1.010.001.1.000.011.  </kbd> bit 1 is lost, interpreted as <VAR>AACDAEB</VAR>\n<kbd>010.1.000.1.1.000.011.  </kbd> bit 2 is lost, interpreted as <VAR>CAEAAEB</VAR>\n<kbd>011.1.000.1.1.000.011.  </kbd> bit 4 is lost, interpreted as <VAR>BAEAAEB</VAR>\n\nFigure 7.1 -- Recovery from phase errors\n</PRE>\n\n<PRE>\n<kbd>0 1 1.0 1 0.00 1.1.000.011  </kbd> coded ensemble (<VAR>BCDAEB</VAR>)\n<kbd>1.1.1.0 1 0.00 1.1.000.011  </kbd> bit 1 is inverted, interpreted as <VAR>DCDAEB</VAR>\n<kbd>0 0 1.0 1 0.00 1.1.000.011  </kbd> bit 2 is inverted, interpreted as <VAR>AAACDAEB</VAR>\n<kbd>0 1 1.1.1.0 00.1.1.000.011  </kbd> bit 4 is inverted, interpreted as <VAR>BAAEAAEB</VAR>\n\nFigure 7.2 -- Recovery from amplitude errors\n</PRE>\n\nThe effect of amplitude errors is demonstrated in Figure 7.2.\nThe format of the illustration is the same as that in Figure 7.1.  This time\nbits 1, 2, and 4 are inverted rather than lost.  Again\nsynchronization is regained almost immediately.  When bit 1 or bit 2 is changed,\nonly the first three bits (the first character of the ensemble) are disturbed.\nInversion of bit four causes loss of synchronization through the ninth bit.\nA very simple explanation of the self-synchronization present in these\nexample can be given.  Since many of the codewords end in the same sequence\nof digits, the decoder is likely to reach a leaf of the Huffman code tree\nat one of the codeword boundaries of the original coded ensemble.  When\nthis happens, the decoder is back in synchronization with the encoder.\nSo that self-synchronization may be discussed more carefully,\nthe following definitions are presented.  (It should be noted that\nthese definitions hold for arbitrary prefix codes, so that the discussion\nincludes all of the codes described in\n<a href=\"DC-Sec3.html#Sec_3\">Section 3</a>.)  If <VAR>s</VAR> is a suffix of some \ncodeword and there exist sequences of codewords <VAR>Gamma</VAR> and <VAR>Delta</VAR> \nsuch that <VAR>s Gamma</VAR> = <VAR>Delta</VAR>, then <VAR>Gamma</VAR> is said to be a \n<VAR>synchronizing sequence</VAR> for <VAR>s</VAR>.  For example, in the Huffman\ncode used above, 1 is a synchronizing sequence for the suffix 01 while\nboth 000001 and 011 are synchronizing sequences for the suffix 10.\nIf every suffix (of every codeword) has a synchronizing sequence,\nthen the code is <EM>completely self-synchronizing</EM>.  If some\nor none of the proper suffixes have synchronizing sequences, then the\ncode is, respectively, <EM>partially-</EM> or <EM>never-self-synchronizing</EM>.\nFinally, if there exists a sequence <VAR>Gamma</VAR> which is a synchronizing\nsequence for every suffix, <VAR>Gamma</VAR> is defined to be a <EM>universal\nsynchronizing sequence</EM>.  The code used in the examples above is\ncompletely self-synchronizing, and has universal synchronizing\nsequence 00000011000.  Gilbert and Moore prove that the \nexistence of a universal synchronizing sequence is a necessary as well\nas a sufficient condition for a code to be completely self-synchronizing\n[Gilbert and Moore 1959].   They also state that any prefix code\nwhich is completely self-synchronizing will synchronize itself with\nprobability 1 if the source ensemble consists of successive messages\nindependently chosen with any given set of probabilities. \nThis is true since the probability of occurrence of\nthe universal synchronizing sequence at any given time is positive.\n<P>\nIt is important to realize that the fact that a completely self-synchronizing\ncode will re-synchronize with probability 1 does not guarantee recovery\nfrom error with bounded delay.  In fact, for every completely self-synchronizing\nprefix code with more than two codewords, there are errors within one\ncodeword which cause unbounded error propagation [Neumann 1962]. \nIn addition, prefix codes are not always completely self-synchronizing.\nBobrow and Hakimi state a necessary condition for\na prefix code with codeword lengths <VAR>l</VAR>(1) ... <VAR>l</VAR>(<VAR>r</VAR>) to be completely\nself-synchronizing:  the greatest common divisor of the <VAR>l</VAR>(<VAR>i</VAR>) must\nbe equal to one [Bobrow and Hakimi 1969].  The Huffman code\n{ 00, 01, 10, 1100, 1101, 1110, 1111 } is not completely\nself-synchronizing, but is partially self-synchronizing since\nsuffixes 00, 01 and 10 are synchronized by any codeword.  \nThe Huffman code { 000, 0010, 0011, 01, 100, 1010, 1011, 100, 111 }\nis never-self-synchronizing.  Examples of never-self-synchronizing\nHuffman codes are difficult to construct, and the example above is \nthe only one with fewer than 16 source messages.  Stiffler\nproves that a code is never-self-synchronizing if and only if\nnone of the proper suffixes of the codewords are themselves codewords\n[Stiffler 1971].\n<P>\nThe conclusions which may be drawn from the above discussion are:\nwhile it is common for Huffman codes to self-synchronize, \nthis is not guaranteed; and when self-synchronization is assured,\nthere is no bound on the propagation of the error.  An additional\ndifficulty is that self-synchronization provides no indication\nthat an error has occurred.  \n<P>\nThe problem of error detection and correction in connection with\nHuffman codes has not received a great deal of attention.  Several\nideas on the subject are reported here.  \nRudner states that synchronizing sequences should be as short as\npossible to minimize re-synchronization delay.  In addition,\nif a synchronizing sequence is used as the codeword for a \nhigh probability message, then re-synchronization will be\nmore frequent.  A method\nfor constructing a minimum-redundancy code having the shortest\npossible synchronizing sequence is described by Rudner [Rudner 1971].  Neumann\nsuggests purposely adding some redundancy to Huffman codes\nin order to permit detection of certain types of errors \n[Neumann 1962].  Clearly this has to be done carefully, so as\nnot to negate the redundancy reduction provided by Huffman coding.\nMcIntyre and Pechura cite data integrity as an advantage of the codebook\napproach discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a> [McIntyre and Pechura 1985].\nWhen the code is stored separately from the coded data, the code may\nbe backed up to protect it from perturbation.  However, when the code is\nstored or transmitted with the data, it is susceptible to errors.\nAn error in the code representation constitutes a drastic loss and therefore\nextreme measures for protecting this part of the transmission\nare justified.\n<P>\nThe Elias codes of <a href=\"DC-Sec3.html#Sec_3.3\">Section 3.3</a>\nare not at all robust.  Each of the codes\n<VAR>gamma</VAR> and <VAR>delta</VAR> can be thought of as generating codewords which\nconsist of a number of substrings such that each substring encodes the\nlength of the subsequent substring.  For code <VAR>gamma</VAR> we may think of\neach codeword <VAR>gamma</VAR>(<VAR>x</VAR>) as the concatenation of <VAR>z</VAR>, a string of <VAR>n</VAR> \nzeros, and <VAR>b</VAR>, a string of length <VAR>n</VAR>+1 (<VAR>n</VAR> = floor[ lg <VAR>x</VAR> ] ).\nIf one of the zeros in substring <VAR>z</VAR> is lost, synchronization will be\nlost as the last symbol of <VAR>b</VAR> will be pushed into the next codeword.\n<P>\nSince the 1 at the front of substring <VAR>b</VAR> delimits the end of <VAR>z</VAR>, if a \nzero in <VAR>z</VAR> is changed to a 1, synchronization will be lost as symbols \nfrom  <VAR>b</VAR> are pushed into the following codeword.  Similarly, if ones\nat the front of <VAR>b</VAR> are inverted to zeros, synchronization will be\nlost as the codeword <VAR>gamma</VAR>(<VAR>x</VAR>) consumes symbols from the \nfollowing codeword.  Once synchronization is lost, it cannot \nnormally be recovered.\n<P>\nIn Figure 7.3, codewords <VAR>gamma</VAR>(6), <VAR>gamma</VAR>(4), <VAR>gamma</VAR>(8) are used\nto illustrate the above ideas.  In each case, synchronization is lost\nand never recovered.\n\n<PRE>\n<kbd>001 1 0.00 1 00.0 00100 0.  </kbd> coded integers 6, 4, 8\n<kbd>0 1 1.0 00 1 00 0.00100.0   </kbd> bit 2 is lost, interpreted as 3, 8, 2, etc.\n<kbd>011.1.0 00 1 00 0.00100.0   </kbd> bit 2 is inverted, interpreted as 3, 1, 8, 4, etc.\n<kbd>000 1 0 00.1.00 0 00100 0   </kbd> bit 3 is inverted, interpreted as 8, 1, etc.\n\nFigure 7.3 -- Effects of errors in Elias Codes.\n</PRE>\n\nThe Elias code <VAR>delta</VAR> may be thought of as a three-part ramp where \n<VAR>delta</VAR>(<VAR>x</VAR>) = <VAR>zmb</VAR> with <VAR>z</VAR> a string of <VAR>n</VAR> zeros, <VAR>m</VAR> a string of\nlength <VAR>n</VAR>+1 with binary value <VAR>v</VAR>, and <VAR>b</VAR> a string of length <VAR>v</VAR>-1.\nFor example, in <VAR>delta</VAR>(16)=00.101.0000, <VAR>n</VAR>=2, <VAR>v</VAR>=5, and the\nfinal substring is the binary value of 16 with the leading 1 removed\nso that it has length <VAR>v</VAR>-1=4.  Again the fact that each substring\ndetermines the length of the subsequent substring means that an\nerror in one of the first two substrings is disastrous, changing the\nway in which the rest of the codeword is to be interpreted.  And,\nlike code <VAR>gamma</VAR>, code <VAR>delta</VAR> has no properties which aid in\nregaining synchronization once it has been lost.\n<P>\nThe Fibonacci codes of\n<a href=\"DC-Sec3.html#Sec_3.3\">Section 3.3</a>, on the other hand, are quite\nrobust.  This robustness is due to the fact that every codeword\nends in the substring 11 and that substring can appear nowhere else\nin a codeword.  If an error occurs anywhere other than in the 11\nsubstring, the error is contained within that one codeword.  It is\npossible that one codeword will become two (see the sixth line of \nFigure 7.4), but no other codewords will be disturbed.  If the\nlast symbol of a codeword is lost or changed, the current codeword\nwill be fused with its successor, so that two codewords are lost.\nWhen the penultimate bit is disturbed, up to three codewords can\nbe lost.  For example, the coded message 011.11.011 becomes\n0011.1011 if bit 2 is inverted.  The maximum disturbance resulting\nfrom either an amplitude error or a phase error is the disturbance of\nthree codewords.\n<P>\nIn Figure 7.4, some illustrations based on the Fibonacci coding\nof ensemble <VAR>EXAMPLE</VAR> as shown in Figure 3.9 are given.\nWhen bit 3 (which is not part of a 11 substring) is lost or changed,\nonly a single codeword is degraded.  When bit 6 (the final bit of the\nfirst codeword) is lost or changed, the first two codewords are\nincorrectly decoded.  When bit 20\nis changed, the first <VAR>b</VAR> is incorrectly decoded as <VAR>fg</VAR>. \n\n<PRE>\n<kbd>000011.000011.00011.010 11.01011.  </kbd> coded ensemble \"<kbd>aa bb</kbd>\".\n<kbd>00 011.000011.00011.010 11.01011.  </kbd> bit 3 is lost, interpreted as \"<kbd> a bb</kbd>\".\n<kbd>001011.000011.00011.010 11.01011.  </kbd> bit 3 is inverted, interpreted as \"<kbd>?a bb</kbd>\".\n<kbd>00001  000011.00011.010 11.01011.  </kbd> bit 6 is lost, interpreted as \"<kbd>? bb</kbd>\".\n<kbd>000010 000011.00011.010 11.01011.  </kbd> bit 6 is inverted, interpreted as \"<kbd>? bb</kbd>\".\n<kbd>000011.000011.00011.011.11.01011.  </kbd> bit 20 is inverted, interpreted as \"<kbd>aa fgb</kbd>\".\n\nFigure 7.4 -- Effects of errors in Fibonacci Codes.\n</PRE>\n\n<a name=\"Sec_7.2\">\n<H3> 7.2  Adaptive Codes</H3> </a>\n\nAdaptive codes are far more adversely affected by transmission errors\nthan are static codes.  For example, in the case of a adaptive\nHuffman code, even though the receiver may re-synchronize with\nthe sender in terms of correctly locating the beginning of a\ncodeword, the information lost represents more than a few bits or\na few characters of the source ensemble.  The fact that sender\nand receiver are dynamically redefining the code indicates\nthat by the time synchronization is regained, they may have\nradically different representations of the code.  Synchronization\nas defined in\n<a href=\"#Sec_7.1\">Section 7.1</a> refers to synchronization of the bit stream,\nwhich is not sufficient for adaptive methods.  What is needed here\nis <EM>code synchronization</EM>, that is, synchronization of both\nthe bit stream and the dynamic data structure representing the\ncurrent code mapping.  \n<P>\nThere is no evidence that adaptive methods\nare self-synchronizing.  \nBentley et al. note that, in algorithm BSTW, loss of synchronization\ncan be catastrophic, whereas this is not true with static Huffman\ncoding [Bentley et al. 1986].  \nZiv and Lempel recognize that the major\ndrawback of their algorithm is its susceptibility to error\npropagation [Ziv and Lempel 1977].  Welch also considers the\nproblem of error tolerance of Lempel-Ziv codes and suggests that\nthe entire ensemble be embedded in an error-detecting code [Welch 1984].\nNeither static nor adaptive arithmetic coding has the ability to tolerate\nerrors.\n\n<a name=\"Sec_8\">\n<H2> 8.  NEW DIRECTIONS</H2> </a>\n\n<a href=\"#Sec_7\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<a href=\"#Sec_9\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\nData compression is still very much an active research area.\nThis section suggests possibilities for further study.\n<P>\nThe discussion of\n<a href=\"#Sec_7\">Section 7</a> illustrates the susceptibility to error\nof the codes presented in this survey.  Strategies for increasing the\nreliability of these codes while incurring only a moderate loss of efficiency\nwould be of great value.  This area appears to be largely unexplored.  \nPossible approaches include embedding the entire ensemble\nin an error-correcting code or reserving one or more codewords to\nact as error flags.  For adaptive\nmethods it may be necessary for receiver and sender to verify\nthe current code mapping periodically.  \n<P>\nFor adaptive Huffman coding, Gallager suggests an \"aging\" scheme,\nwhereby recent occurrences of a character contribute more to its\nfrequency count than do earlier occurrences [Gallager 1978].\nThis strategy introduces the notion of locality into the adaptive\nHuffman scheme.  Cormack and Horspool describe an algorithm for\napproximating exponential aging [Cormack and Horspool 1984].  However,\nthe effectiveness of this algorithm has not been established.\n<P>\nBoth Knuth and Bentley et al. suggest the possibility of using\nthe \"cache\" concept to exploit locality and minimize the effect\nof anomalous source messages. \nPreliminary empirical results indicate that this may be helpful\n[Knuth 1985; Bentley et al. 1986].   A problem related to the use\nof a cache is overhead time required for deletion.  Strategies\nfor reducing the cost of a deletion could be considered.\nAnother possible extension to algorithm BSTW is to investigate other\nlocality heuristics.  Bentley et al. prove that intermittent-move-to-front\n(move-to-front after every <VAR>k</VAR> occurrences) is as effective as\nmove-to-front [Bentley et al. 1986].  It should be noted that\nthere are many other self-organizing methods yet to be considered.\nHorspool and Cormack describe experimental results which imply that\nthe transpose heuristic performs as well as move-to-front, and suggest\nthat it is also easier to implement [Horspool and Cormack 1987].  \n<P>\nSeveral aspects of free-parse methods merit further attention. \nLempel-Ziv codes appear to be promising, although the absence of a worst-case\nbound on the redundancy of an individual finite source ensemble\nis a drawback.  The variable-block type Lempel-Ziv codes have\nbeen implemented with some success [ARC 1986] and \nthe construction of a variable-variable Lempel-Ziv code has been \nsketched [Ziv and Lempel 1978].\nThe efficiency of the variable-variable model should be investigated.\nIn addition, an implementation of Lempel-Ziv coding which combines\nthe time efficiency of Rodeh et al. method with more efficient use of space\nis worthy of consideration.\n<P>\nAnother important research topic is the development of theoretical models\nfor data compression which address the problem of local redundancy.\nModels based on Markov chains may be exploited to take advantage of\ninteraction between groups of symbols.  Entropy tends to be overestimated\nwhen symbol interaction is not considered.  Models which exploit\nrelationships between source messages may achieve better compression \nthan predicted by an entropy calculation based only upon symbol probabilities.\nThe use of Markov modeling is considered by Llewellyn and by Langdon\nand Rissanen [Llewellyn 1987; Langdon and Rissanen 1983].\n\n<a name=\"Sec_9\">\n<H2> 9.  SUMMARY</H2> </a>\n\n<a href=\"#Sec_8\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<a href=\"DC-references.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\nData compression is a topic of much importance and many applications.\nMethods of data compression have been studied for almost four decades.\nThis paper has provided an overview of data compression methods of\ngeneral utility.  The algorithms have been evaluated in terms of\nthe amount of compression they provide, algorithm efficiency, \nand susceptibility to error.  While algorithm efficiency and susceptibility\nto error are relatively independent of the characteristics of the source\nensemble, the amount of compression achieved depends upon the\ncharacteristics of the source to a great extent.\n<P>\nSemantic dependent data compression techniques, as discussed in\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a>, are special-purpose methods designed to exploit\nlocal redundancy or context information.  A semantic dependent scheme\ncan usually be viewed as a special case of one or more general-purpose\nalgorithms.  It should also be noted that algorithm BSTW is a general-purpose\ntechnique which exploits locality of reference, a type of local redundancy.\n<P>\nSusceptibility to error is the main drawback of each of the algorithms\npresented here.  Although channel errors are more devastating to\nadaptive algorithms than to static ones, it is possible for an error\nto propagate without limit even in the static case.  Methods of\nlimiting the effect of an error on the effectiveness of a data\ncompression algorithm should be investigated.\n\n<P>\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<a href=\"DC-references.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}