{"url": "https://www.ics.uci.edu/~eppstein/261/f03-outline/02.hash", "content": "Hash tables (chapter 11)\n\nProblem: dictionary\n    set(key,value)\n    get(key)\n    [also avail in Python, easy: len(), keys(), values()]\n\ngeneral idea of hash table:\n    store key-value pairs in a table\n    use a hash function h(key) to tell where to look in the table\n\n    def set(key,value):\n        if table[h(key)] is None:\n            table[h(key)] = (key,value)\n        else...\n\n    def get(key):\n        if table[h(key)] is a pair (key,something)\n            return something\n        else...\n\n    important details omitted:\n        what is h?\n        what to do if table entry already occupied on set\n            (and how can get find keys that had this problem when set)\n\n        how big is the table?\n\t\tload factor = number of stored items / table size \n\t\tclose to 1 -> lots of collisions -> slower\n\t\tlarger -> faster but wastes more space\n\t\ttypically around 2\n\n\tto maintain constant load factor:\n\t\trehashing: when load gets too high, make larger table\n\t\t\twhen load gets too low, reduce table size\n\n\t\te.g. maintain alpha in range [1/3,2/3]\n\t\tdouble table size when alpha=2/3 (=>1/3)\n\t\thalve table size when alpha=1/3 (=>2/3)\n\t\tproblem: sequence of insertions/deletions can\n\t\thover right at boundary, need some hysteresis\n\n\t\tbetter: maintain alpha in range [1/5,4/5]\n\t\tdouble table size when alpha=1/5 (=> 2/5)\n\t\thalve table size when alpha=4/5 (=> 2/5)\n\n\t\tcharging scheme:\n\t\t\tcharge future time expenses against present operations\n\t\t\teach insertion or deletion => charge 1 unit\n\t\t\tresize of a size n table takes time O(n),\n\t\t\t\thappens after at least n/5 ops since\n\t\t\t\tprevious resize => pay for it with charges\n\t\t\ttotal time resizing = O(# operations charged)\n\t\t\tso average resize time per operation = O(1)\n\nhash functions\n\n    standard theoretical assumption: h is random\n\n        any possible key is hashed to a location that is\n        uniformly random among all possible table locations\n\tand independent of all other key locations\n\n\tmakes analysis (relatively) easy\n\timpossible in practice\n\t(would have to fill out table of possible keys\n\twith randomly generated numbers, if could do that then\n\tcould just use keys as addresses directly)\n\n    standard practical choice: do something simple\n\n        interpret keys as (large) integers\n\tuse some numerical function to map them to smaller integer range\n\ttest empirically whether it works ok but drop theoretical analysis\n\n\tbook has some discussion of this\n\ttypical recommendation: key mod p, where p is a large prime\n\tPython choice: key mod 2^k (book says this is bad)\n\n    recent theoretical work:\n\tstart with a small amount of randomness (e.g. choice of prime p)\n\tdevise easy-to-compute pseudorandom number generator\n\t\ts.t. can prove that hash still works well\n\ncollision strategies:\n\n\tassociative, k-way associative:\n\t\tkeep one or a constant number k of keys per location,\n\t\talways replace one when set collides\n\n\t\tso, not all key-value pairs are stored\n\n\t\tcan be useful anyway when missing keys can be recomputed\n\t\t(e.g. cache for memory hierarchy, AI search)\n\n\tperfect:\n\t\tchoose hash function in such a way that no two keys collide\n\t\tcan only be done when you know all keys in advance\n\n\t\te.g. if we have n numbers in range 0..N\n\t\tthen choosing h(x) = x mod p\n\t\tfor random p ~ max(2n, log N log log N)\n\n\t\tif two keys have same value mod many primes\n\t\tthen (chinese remainder) they have same value\n\t\tmod product of these primes,\n\t\tbut p chosen so product > N\n\t\t=> w/high probability, no collisions\n\n\t\tused for keywords in compilers\n\n\t\topen source implementation gperf\n\n\tchaining:\n\t\teach hash location stores list of key,value pairs\n\t\thash lookup -> sequential search of list\n\t\thash set -> search list for prev value\n\t\t\tor add new value to list\n\n\t\tthm: if hash function is random,\n\t\t\texpected time per op <= 1 + alpha.\n\t\t(expected contrib from others = p(collide) = alpha/n)\n\n\t\tnote works well even when load factor > 1...\n\n\t\tmore generally can work out formula\n\t\tfor probability of hitting a chain of k items\n\n\t\texpected worst case (longest chain)\n\t\tfor n items, constant load factor:\n\t\t\tO(log n / log log n)\n\n\topen addressing (used in python):\n\n\t\thash function maps each key to a probe sequence\n\t\t\tlocation0, location1, ...\n\n\t\ttypical assumption: whole sequence is random\n\n\t\tget(key):\n\t\t\tfollow probe sequence until\n\t\t\tfinding item or hitting empty location\n\n\t\tset(key):\n\t\t\tfollow probe sequence until hitting empty loc,\n\t\t\tput item there\n\n\t\tto delete items:\n\t\t\tplace a special delete marker\n\t\t\t(distinguishable from empty marker)\n\n\t\t\tset(key,value) can safely replace delete markers\n\t\t\tbut get(key) treats them as nonempty collisions\n\n\t\tthm:\n\t\t\texpected time for set <= 1/(1 - alpha)\n\t\t\t = 1 + alpha + alpha^2 + ...\n\n\t\tslightly worse than chaining but maybe avoiding extra list\n\t\tobjects makes up for it\n\n\t\texpected worst case O(log n)\n\t\t(last n/2 items have constant chance of colliding at\n\t\teach step, w/prob. 1/n have >= c.log n collisions)\n\n\tdouble chaining:\n\t\thash chaining, but hash each key to two locations\n\t\tand put new item in the shorter of two chains\n\t\texpected worst case O(log log n)\n\n\tdouble hashing:\n\t\thash chaining, but store each chain as a\n\t\t(very sparse) hash table\n\t\te.g. chain of k items => hash table of k^2 items\n\n\t\twith some care gives constant worst case time\n\t\tper search\n\n\trobin hood hashing:\n\t\topen addressing, but keep track of how many\n\t\tprobes were needed for each key\n\n\t\tif adding new key, have made x probes,\n\t\tand see a location where previous key has fewer probes,\n\t\tput new key there and move previous key further\n\t\talong its own probe sequence\n\n\t\texpected worst case O(log log n)\n\n\t\thttp://www.fing.edu.uy/inco/pedeciba/bibliote/reptec/TR0212.pdf\n\n\tblum filter:\n\t\tkeep track of a set of items (keys without values)\n\t\tallowing positive errors\n\t\t\t(erroneously think an item is in the set)\n\t\tbut not negative ones\n\n\t\tconverse (negative but not positive) can be handled\n\t\tby associative hash\n\n\t\tfor n keys, store table of 2kn single bits\n\t\tinitially, all bits zero\n\t\tfor each keys, turn on k bits\n\t\t\t(selected using random hash probe sequence)\n\n\t\tto test if item is in the set,\n\t\tcheck its probe sequence and make sure all bits are on\n\t\tif so, report that it's in the set (may be incorrect)\n\t\tif any bit is zero, report that it's not in (always correct)\n\n\t\t# set bits <= kn, so p[any bit is set] <= 1/2\n\t\tfor any key not in the set, we check k\n\t\tindependent random positions,\n\t\tso p[all checked bits are set] <= (1/2)^k\n\n\t\tby choosing large enough k can make p[mistake] arb. small.\n", "encoding": "ascii"}