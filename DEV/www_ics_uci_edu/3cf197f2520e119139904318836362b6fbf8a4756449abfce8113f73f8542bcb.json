{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/heaps.txt", "content": "\t\tHeaps for Priority Queues and O(N Log2 N) Sorting\r\n\r\n\r\nWe will next study heaps (this lecture) and AVL trees (the next lecture). Both\r\nof these are special kinds of binary trees that have interesting order/structure\r\nproperties that are different from binary search trees. They both use similar\r\nrules when adding/removing values from these trees, and then restoring their\r\nproperties.\r\n\r\nHeaps (this term has another important use in Computer Science: memory used\r\nfor allocating the storage for objects when the \"new\" operator is used) are\r\nalmost the perfect data structure for storing priority queues. By using heaps\r\nwe can add a value in O(Log2 N) and also remove the highest priority value in\r\nO(Log2 N). The reason for these complexity classes: heaps always stay perfectly\r\nbalanced, unlike BSTs whose height can vary from N to Log2 N. In the discussion\r\nbelow I will characterize Min-Heaps and the algorithms that process them. We\r\ncould use Max-Heaps, which are appropriate for storing priority queues: their\r\noperations follow almost identical (but flipped) rules.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nOrder and Structure Properties:\r\n\r\nRecall that BSTs had a special order property (FOR EVERY NODE, only smaller\r\nvalues are in its left subtree and only larger values are in its right subtree)\r\nand no structure property (BSTs can take on any shape, from well-balanced to\r\npathologically unbalanced, depending on the order the values are added into the\r\nBST).\r\n\r\nMin-Heaps have both a special order and structure property:\r\n\r\nOrder:     Every node must be less than or equal to all nodes in its left and\r\n           right subtrees. This is the same as saying something simpler: every\r\n           node must be less than or equal to its children (because each child\r\n           will in turn be then less that or equal to its two children, so each\r\n           node is less than or equal to its grandchildren, ...). We can also\r\n           say the same thing as \"every node must be greater than or equal to\r\n           its parent.\"\r\n\r\n           Note: unlike BSTs, it is reasonable to allow heaps to store multiple \r\n           copies of the same value. So that is why \"equal\" is allowed in the\r\n           order definition above.\r\n\r\nStructure: All depths must be filled, except possibly the deepest. If the\r\n           deepest is not filled, then all the nodes at that depth must appear\r\n           as far to the left as possible. This property ensures that every\r\n           heap with N values has the SAME STRUCTURE as every other heap with N\r\n           values: only the values inside the nodes are different. This is a\r\n           bit more like the structure of linked lists than BSTs: every N\r\n           element linked list has the same structure, only its values are\r\n           different.\r\n\r\nNote that like BSTs, there can be multiple legal heaps with the same values:\r\nbut, they all have the same structure (by the structure property); it is only\r\nthe values stored in the nodes that might be different (except the minimum node\r\nmust always be at the root). For example, the following 3 trees all represent\r\nmin-heaps with 4 different values.\r\n\r\n       1         1         1  \r\n      / \\       / \\       / \\ \r\n     2   3     3   2     2   4\r\n    /         /         /     \r\n   4         4         3      \r\n\r\nWhich values are in which nodes will depend on the order in which the values\r\nare added to the heap, just as the structure property of a BST depends on the\r\norder in which values are added to the BST. But, there are more constraints\r\ngiven the structure property of heaps, so not every binary tree structure can\r\nrepresent a heap: no pathological ones, and not even \"right-heavy\" trees.\r\n\r\nThe pictures accompany this lecture illustrate Min-heaps, including how values\r\nare added and how the minimum value is removed (two fast operations; for\r\nMax-Heaps, related to priority queues, removing the maximum value is fast). We\r\nwill examine the algorithms to add values and remove-the-minimum value from a\r\nheap (and how to restore the heap properties for both). Then we will change our\r\nperspective and learn how to store heaps compactly in arrays (doable because of\r\nthe structure property). Finally we will learn an \"offline\" algorithm to build\r\na heap with N values in O(N) time instead of O(N Log2 N): this bigger bound is\r\nachieve with an \"online\" algorithm that simply adds N values to the heap, each\r\ntaking at worst O(Log2 N) time.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nInsertion into a Heap:\r\n\r\nTo add a value into a min-heap, first we must place it in a new node in the tree\r\naccording to the heap STRUCTURE property, so we put it directly to the right of\r\nthe rightmost node at the deepest depth (or if that depth is filled, as the \r\nleftmost child of the deepest leftmost node: at one greater depth). By adding\r\nthis node in this way, we now have a tree (with one more node) that satisifes\r\nthe heap STRUCTURE property, but not necessarily the ORDER property - yet. \r\n\r\nThe added value may violate the order property: it may be smaller than its\r\nparent: in fact it may be very small, and belong near the top of the heap. To\r\nrestore the order property, we compare that node's value with its parent's\r\nvalue: we stop if these values are in the correct order (for a min-heap, if the\r\nparent's value <= child's value); if they are in the wrong order (parent's value\r\n> child's value) we swap the values in these nodes; note that putting a smaller\r\nvalue in the parent will not affect the order property in the other subtree of\r\nthe parent: the subtree values were all >= the original parent, and the old\r\nparent's value has now been replaced with a yet smaller new value, so all the\r\nvalues in the other subtree are all still >= than their parent's new value.\r\nNote that swapping does not affect the STRUCTURE of the heap at all, so the\r\nSTRUCTURE is unaffected, meaning the structure property is still satisfied.\r\n\r\nThen we compare the parent (which now has a new value) with its parent according\r\nto the same rules and again swap if necessary. We continue comparing and\r\nswapping until (a) the value settles into its correct place: we find a parent's\r\nvalue <= than the child's value, or (b) the added value is now at the root of\r\nthe tree, where there is no parent's value to compare/swap with. The tree will\r\nthen satisfy the heap ORDER property (and because swapping does not change its\r\nstructure, it also satisfies the STRUCTURE property). So both properties are\r\nrestored after the new value is added.\r\n\r\nWe call this the \"percolate up\" operation, because the value added at the bottom\r\nof the tree percolates up to its correct position (according to the ORDER\r\nproperty), while not changing the STRUCTURE property.\r\n\r\nBecause of the heap structure property, heaps are perfectly balanced trees, so\r\nthe height of a tree is always O(Log2 N) and therefore the add operation is\r\nO(Log2 N): comparing/swapping the leaf all the way to the root in the worst\r\ncase requires O(Log2 N) comparisons and swaps (each comparison and swap\r\noperation take O(1) time).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nDeletion (and Return) of the Mininum Value in a Heap:\r\n\r\nWe can also remove the smallest value in a Min-Heap efficiently, returning it\r\nas a value from the remove operation (e.g., dequeue from a priority queue in a\r\nMax-Heap). To do so, we save the value from the root (it is guaranteed to be the\r\nsmallest in the tree by the order property), which will be returned by the\r\n\"remove\" method when it finishes. Then we take the farthest right node at the\r\ndeepest depth and remove that node from the structure of the tree, but first\r\nplace its value into the root node (whose value we have already saved, for\r\nreturning from this method). By removing the node at the bottom-right of the\r\ntree, we now have a tree with one fewer node, and a tree that still satisifes\r\nthe heap STRUCTURE property.\r\n\r\nBut, it is very likely that the value now at the top doesn't belong there; we\r\ntook it from the bottom, which tends to have the biggest values, yet the\r\nsmallest value belongs at the root of a heap. To restore the order property, if\r\nthe root is > than either/both of its children, we swap its value with the\r\nvalue of its SMALLEST child. Again, if the left/right child is the smaller\r\nvalue, then as a parent it will still be <= all the nodes in the right/left\r\nsubtrees.\r\n\r\nNote that in doing this comparing/swapping, we might just compare the parent\r\nwith its left child (if it has only one child), or maybe compare it to both\r\nchildren (if it has two children). Because of the STRUCTURE property of heaps,\r\na node will NEVER have JUST A RIGHT CHILD.\r\n\r\nWe continue the process, swapping the value downward, until it settles into its\r\ncorrect place: we find it is <= its children, or it is in a leaf and there are\r\nno children's value to compare it to. Again, at most we do O(Log2 N) operations\r\n(although here each operations might be two comparisons and a swap), one\r\noperation per depth in the tree (each comparison and swap operation takes\r\nO(1) time). \r\n\r\nWe call this the \"peroclate down\" operation. Actually, because we are comparing\r\nits value against up to two children (for percolate up we always compare it to\r\njust its one parent), we might have to do two times the work when comparing\r\nvalues, so it might take longer to percolate a value down than it does to\r\npercolate a value up. But factor of two (a constant) disappears from our\r\ncomplexity class analysis, so we can ignore the difference and just count swaps,\r\nnot comparisons, in which both do at most O(Log2 N).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nCompactly Storing Heaps in Arrays:\r\n\r\nFinally, we will learn that we can store a heap compactly in an array, with\r\neach node storing no explicit pointers to its parent, or its left or right\r\nchildren. We start by by storing the root at position 0. If a parent is stored\r\nat index i, then its left child is stored at index 2i+1 and its right child is\r\nstored at index 2i+2. For any child is stored at index i, its parent is stored\r\nat index (i-1)/2 (remember integer division truncates: note that (2*i+1-1)/2\r\nand (2*i+2-1)/2 both result in the value i). Storing a heap of N values\r\nrequires an array of length N.\r\n\r\nThe picture below shows at which index (the number shown) each value in a heap\r\nof size 15 (0-14) is stored.\r\n\r\n                                     0\r\n                  /                                    \\\r\n                 1                                      2\r\n            /          \\                          /          \\\r\n           3            4                        5            6\r\n         /   \\        /   \\                    /   \\        /   \\\r\n        7     8      9    10                  11  12       13   14\r\n\r\nAlso look at the pictures accompanying this lecture to see how to store a heap\r\nin an array. Notice that the order of values in the array is a equivalent to a\r\nbreadth-first traversal of the tree. There will be no \"holes\" in the array: its\r\nvalues are  contiguous (all the values from indexes 0 to N-1 will be filled with\r\ndata) because of the heap STRUCTURE property.\r\n\r\nVerify the numeric computations of parent/children for a few nodes. It is useful\r\nto hide these computations in functions: writing the functions int left(int i),\r\nint right(int i), and int parent(int i) which each take the INDEX of any heap\r\nnode and computes the INDEX of it left child, its right-child, or its parent\r\nrespectively. A bool in_heap(int i) function would compute whether or not index\r\ni is in the heap: for percolate down we might ask whether in_heap(left(n)) or\r\nin_heap(right(n)) to determine whether n has a left or right child respectively.\r\n\r\nNote that if we have a N node heap it occupies an array of length N (in indexes\r\n0 to N-1). If we need to add another node, it must go at index N; if instead we\r\nneed to remove a node, the node at index N-1 is removed and its value put at\r\nindex 0 (which is then percolated down). So adding and removing values always\r\noccurs at the end of the array (whose size might need to be doubled, to add a\r\nanother value with array implementations of data types).\r\n\r\nMax-heaps are good for implementing priority queues, where we must be able to\r\nenqueue any value but dequeue only the highest priority value. As we have seen,\r\nwe can use priority queues to sort a list of N values by (1) adding each to the\r\npq/heap in O(Log2 N), which is O(N Log2 N) and then (2) removing N values from\r\nthe pq/heap (they come out biggest to smallest) each in O(Log2 N), which is\r\nalso O(N Log2 N). So doing N adds and N removes is O(N Log2 N) and we have\r\ndiscovered an O(N Log2 N) sorting algorithm, in a lower complexity class than\r\nany of the O(N^2) sorts, but the O(N^2) algorithms are still easier to write,\r\nwith simple nested \"for\" loops.\r\n\r\nI expect you to be able to draw pictures of heaps (both as trees and arrays)\r\nand update them according to the algorithms discussed above; you will also\r\nwrite the code for Max-Heaps in C++ in Programming Assignment #3, where you\r\nwill use this data structure to implement lower-complexity class priority queue.\r\n\r\nWhy not store all binary trees in arrays using this mapping? Well, some BSTs\r\nhave a very pathological structure, making their storage in arrays very\r\ninefficient. We can store heaps with N values in an array with N values: the\r\nvalues are contiguous with no holes. But a binary tree with N values can\r\nrequire an array with 2^N-1 values. For example, the folowing pathological tree\r\nrequires an array with 17 values: 1 is stored in index 0, 2 is stored in index\r\n2, 3 is stored in index 6, 4 is stored in index 14, and 5 is stored in index 30.\r\nThe array needs 31 indexes (0-30) but contains only 5 values.\r\n\r\n   1\r\n    \\\r\n     2\r\n      \\\r\n       3\r\n        \\\r\n         4\r\n          \\\r\n           5\r\n\r\nSo, this method of storage is poor, unless the binary trees are close to\r\nperfectly balanced. Heaps are always perfectly balanced because of their\r\nSTRUCTURE property, so this method is perfect for storing heaps.\r\n\r\n\r\n------------------------------------------------------------------------------\r\n\r\nBuilding Heaps Offline: Some Interesting Mathematics Related to Algorithms\r\n\r\n\"Online\" algorithms receive their inputs one at a time and have to completely\r\nupdate their data structure before the next value is received and processed.\r\nBuilding a heap by adding one value at a time is an example of an online\r\nalgorithm. We start with an emtpy heap, and after we add each value we get\r\na new heap with one more value in it, until we have the final heap with all\r\nthe required values.\r\n\r\n\"Offline\" algorithms receive ALL THEIR INPUTS before they are required to \r\nprocess any of them. In an offline heap algorithm, we can use our knowledge of\r\nall its values before even starting to make a heap from them.\r\n\r\nWe can write a suprisingly simple ane efficient (O(N) algorithm) to build a\r\nheap of N values if we can get all the values that will be added to a heap\r\nBEFORE we start building the heap. This is another interesting result of the\r\nheap order/structure properties.\r\n\r\nWe previously saw an offline algorithm for building a balanced BST from a\r\n(sorted) array of values in O(N). Here we will examine how to build a heap in\r\nO(N), if we know all the values that must be added to the heap before we start.\r\n\r\nWe will first use \"h\", the height of a heap with as many nodes as possible, as\r\na metric, not N, the size (number of nodes) in the heap. We will count the\r\nnumber of comparisons needed to build heaps of different heights using an\r\noffline algorithm.\r\n\r\nLet's start with the smallest height, h=0. A heap of height 0 has only one\r\nnode in it, so it takes no comparisons to build (that node is the root).\r\n\r\nWe can build a heap of height h=1 (3 nodes) by putting a (\"a\" stands for any\r\nvalue) on top of two h=0 heaps (\"b\" and \"c\", standing for any values) and then\r\ndo one pair of comparisons (b compared to c and a compared to the smaller) and\r\nat most one swap a with its smallest child (if a is not already less than both\r\nits children). Then we have a heap storing these three values and requires at\r\nmost 2 comparisons and a swap, which is just a constant number of operations.\r\nSuch a tree can look like\r\n\r\n         a             b             b             c             c\r\n       /   \\         /   \\         /   \\         /   \\         /   \\  \r\n      b     c       a     c       c     a       a     b       b     a \r\n\r\nIn fact if we have two heaps of height h, then we can efficiently build a new\r\nheap of height h+1 by putting the new value x on top of these two heaps\r\n\r\n         x\r\n       /   \\\r\n      / h+1 \\\r\n     +       +\r\n    / \\     / \\\r\n   / h \\   / h \\\r\n  +-----+ +-----+\r\n\r\nand then percolating the value x down into the heap where it belongs.\r\n\r\nLet Ph be a heap that is a perfect tree of height h; perfect means every depth\r\nis filled. Height(Ph) is h, and Size(Ph) is the number of nodes in this heap\r\nthat is a perfect tree, which we've computed as 2^(h+1) - 1. Let C(Ph) be the\r\nnumber of pairs of comparisons needed to build such a heap with the algorithm\r\noutlined above (it is easier to count pairs, and multiply by two: of course\r\nmultiplying by two doesn't change the complexity class, so we will never even\r\nbother with this \"correction\").\r\n\r\nAccording to the algorithm, we will build a heap Ph by first building two heaps\r\nPh-1 and then putting one value on the top and percolating it down at most\r\nh times. Recursively, we will will build each heap Ph-1 by first building two\r\nheaps Ph-2 and then putting one value on the top and percolating it down at\r\nmost h-1 times. Eventually we get to building heaps with 1 values, which as a\r\nbase case costs 0 comparisons.\r\n\r\nThus the number of comparison pairs is just two times the number needed to\r\nbuild the smaller (by one depth) heaps plus the maximum number of comparison\r\npairs (against each pair of children) needed to percolate the value down to its\r\ncorrect node. We can write this relationship with the following recurrence\r\nequations.\r\n\r\n  C(P0) = 0\r\n  C(Ph) = 2 * C(Ph-1) + Ph\r\n\r\nWe could write these equations as a trivial C++ function to compute C(Ph) as\r\n\r\nint C(int Ph) {\r\n  if (Ph == 0)\r\n    return 0;\r\n  else\r\n    return 2*C(Ph-1) + Ph\r\n\r\nWe can summarize this information as follows.\r\n\r\n  Ph  | Size(Ph)  |  C(Ph)\r\n------+-----------+-------------------------\r\n   0  |    1      |     0\r\n   1  |    3      |     1 = 2*  0  + 1\r\n   2  |    7      |     4 = 2*  1  + 2\r\n   3  |   15      |    11 = 2*  4  + 3\r\n   4  |   31      |    26 = 2* 11  + 4\r\n   5  |   63      |    57 = 2* 26  + 5\r\n   6  |  127      |   120 = 2* 57  + 6\r\n   7  |  255      |   247 = 2*120  + 7\r\n   8  |  511      |   502 = 2*257  + 8\r\n ...  |  ...      |   ...\r\n\r\nWe see, as Ph gets bigger, Size(Ph) is about the same as C(Ph). Size(Ph) and\r\nC(Ph) seem to grow at about the same rate (compute their ratio).\r\n\r\nTo a good approximation, increasing the height of Ph by 1 a bit more than\r\ndoubles the number of nodes in the tree and requires a bit more than doubling\r\nthe number of comparison pairs. In fact, the ratio C(Ph)/C(Ph-1) approaches 2\r\n(from above) as h goes to infinity (do the divisions above).\r\n\r\nWith higher mathematics, we can solve and write a solution to these recurrence\r\nequations exactly as\r\n\r\n  C(Ph) = Size(Ph) - (Ph+1)  =  2^(Ph+1) - 1 - Ph - 1  =  2^(Ph+1) - Ph - 2\r\n\r\n(recall Size(Ph) = 2^(Ph+1) - 1)\r\n\r\nTry computing a few values of this function and compare them against the values\r\nin the table, computed by the recurence equations. If we assume that this\r\nformula is true for C(Ph) we can show that it is true for C(Ph+1) as well.\r\n\r\n  C(Ph+1) = 2*C(Ph) + (Ph+1)\r\n          = 2*(2^(Ph+1) - Ph - 2) + (Ph+1)\r\n          = 2^(Ph+2) - 2Ph - 4 + (Ph+1)\r\n          = 2^(Ph+2) - 2(Ph+1) - 2 + (Ph+1)\r\n          = 2^(Ph+2) - (Ph+1) - 2\r\n          = 2^(Ph+2) - 1 - (Ph+1) - 1\r\n          = Size(Ph+1) - (Ph+2)  which is the formula above for C(Ph+1)\r\n\r\nFinally, since N (the number of nodes in Ph) = 2^(Ph+1) - 1. If we can discard\r\nthe -1 as insignificant for large N, to simplify things a bit and get\r\n\r\n      N ~ 2^(Ph+1)  (approximate because we discard the -1 term)\r\n Log2 N ~ Ph+1\r\n Log2 N -1 ~ Ph   therefore Log2 N ~ Ph+1\r\n\r\nWe can rewrite our solution substituting Long2 N for Ph+1, and Log2 N -1 for Ph\r\n\r\n   C(Ph) =  2^(Ph+1) - Ph - 2\r\nas C(N)  ~  2^(Log2 N) - (Log2 N - 1) - 2\r\n         ~  2^(Log2 N) - (Log2 N - 1) - 2\r\n         ~  N - Log2 N - 1\r\n\r\nwhich means C(N) is O(N), because we can drop the Log2 N and constant terms.\r\nThus, the time the algorithm requires is some constant times.\r\n\r\nThe algorithm to build small heaps and combine them into bigger and bigger\r\nheaps turns out to be trivial to write when the heaps are stored as an array.\r\nWe just percolate each value in the array down, while traversing the array from\r\nthe rear to the front. By the time we percolate a value down, it is on top of\r\ntwo heaps (its children, stored later in the array, are themselves already\r\nheaps).\r\n\r\nTo be concrete, suppose we are to build a Min-Heap from the following 7 random\r\nvalues: 4, 7, 3, 5, 2, 6, 1 (7 is the number of values needed for a height 2\r\nperfect tree). I will now arrange these values into a tree and its underlying\r\narray to illustrate how the algorithm works\r\n\r\nTo start, just put these values, in whatever order they are, into the array\r\nrepresenting the heap. It is NOT a heap yet, because even though it satisfies\r\nthe structure property, it doesn't satisfy the order property.\r\n\r\n      4\r\n    /   \\ \r\n   7     3\r\n  / \\   / \\\r\n 5   2 6   1\r\n  \r\n  0   1   2   3   4   5   6  \r\n+---+---+---+---+---+---+---+\r\n| 4 | 7 | 3 | 5 | 2 | 6 | 1 |\r\n+---+---+---+---+---+---+---+\r\n\r\nNow, for the values at the deepest depth: 1, 6, 2, and 5 (in that order:\r\nindexes, 6, 5, 4, and 3) percolate them down to become heaps. They are already\r\nleaves, so percolate down immediately stops before comparing their values to\r\nany of their children's values, because they have no children! Thus, the data\r\nstructure remains unchanged with a total of 0 comparison pairs.\r\n\r\n      4\r\n    /   \\ \r\n   7     3\r\n  / \\   / \\\r\n 5   2 6   1\r\n  \r\n  0   1   2   3   4   5   6  \r\n+---+---+---+---+---+---+---+\r\n| 4 | 7 | 3 | 5 | 2 | 6 | 1 |\r\n+---+---+---+---+---+---+---+\r\n\r\n\r\nNow, for the values at the next higher depth: 3 and 7 (in that order: indexes,\r\n2 and 1) percolate them down into their left or right subheaps to become heaps\r\nof height 1. Each requires one comparison pair to find the smaller childr and\r\none to decide whether to swap (1 and 3 are swapped; 2 and 7 are swapped). So we\r\nhave a total of 2 comparison pairs. Notice that 2 and 1 are both root of heaps.\r\n\r\n      4\r\n    /   \\ \r\n   2     1\r\n  / \\   / \\\r\n 5   7 6   3\r\n  \r\n  0   1   2   3   4   5   6  \r\n+---+---+---+---+---+---+---+\r\n| 4 | 2 | 1 | 5 | 7 | 6 | 3 |\r\n+---+---+---+---+---+---+---+\r\n\r\n\r\nNow, for the value at the next higher depth (the top): 4 (at the root of the\r\ntree in index 1) percolate it down into its left or right subheaps to become\r\none final heap. It requires two comparison pairs to twice find the smaller of\r\nits children and to decide whether to swap (4 and 1 and then 4 and 3 are\r\nswapped). So we have a total of 4 comparison pairs.\r\n\r\n      1\r\n    /   \\ \r\n   2     3\r\n  / \\   / \\\r\n 5   7 6   4\r\n  \r\n  0   1   2   3   4   5   6  \r\n+---+---+---+---+---+---+---+\r\n| 1 | 2 | 3 | 5 | 7 | 6 | 4 |\r\n+---+---+---+---+---+---+---+\r\n\r\n\r\nThe result is now one heap, satisfying both the order and structure properties.\r\nNotice that we percolated down the values stored at indexes 6, then 5, then 4,\r\nthen 3, then 2, then 1, then 0. We built heaps from the bottom to the top,\r\nright to left at each depth.\r\n\r\nSo, we can compactly state the offline algorithm to build a heap of N values:\r\npercolate down every index in the array starting and the last index (here 6)\r\nand going backwards until first index(always 0).\r\n\r\nSimple code and a beautiful result.\r\n\r\nI invite you to generate your own random arrays of 7- or 15-values and to\r\nconstruct heaps by hand simulating this algorithm to construct heaps\r\n\r\nAnother way to look at analyzing this algorithims is as follows. Imagine Ph is\r\na perfect heap with nodes at every depth from the root to the height. It has N\r\nnodes. For each node x, it takes C(x) comparison pairs to percolate that node\r\ndown to its correct location in Ph.\r\n\r\n     +\t\t\t+\r\n    / \\                / \\\r\n   /   \\\t      /   \\\r\n  +-----+\t     /     \\\r\n    Ph\t\t    +-------+ \r\n                       Ph+1\r\n\r\nIf we instead built Ph+1, the new N leaves (actually N+1) would require 0\r\ncomparisons to percolate down. Each of the N internal nodes would require at\r\nmost 1 more comparison pair to percolate down than it needed in Ph. So,\r\ndoubling the number of nodes in Ph (from N to 2N+1) requires doubling the\r\nnumber of comparison pairs (it was N, but is now 2N), which is the signature of\r\na linear process.\r\n\r\nFinally, using the algorithm above, note that about 1/2 the values in a perfect\r\ntree are leaves, so they need 0 comparison pairs; about 1/4 of the values in\r\nthe tree (the ones above each pair of leaves) need 1 comparison pairs; about\r\n1/8 of the values in the tree (the ones above those) need 2 comparison pairs.\r\nThus, at each depth, going upward, the nodes need 1 more comparison pair, but\r\nthere are 1/2 as many nodes at that depth as at the next one.\r\n\r\nFor the example above, 4 nodes require no comparison pairs, 2 nodes require one\r\ncomparison pair, and 1 node requires 2 comparison pairs. In a tree with about\r\n1,000,000 values, about 500,000 would require no comparison pairs, about\r\n250,000 would require 1 comparison pair,  about 125,000 would require 2\r\ncomparison pairs, ... , 2 would require about 18 comparison pairs, and 1 would\r\nrequire about 19 comparison pairs.\r\n\r\nAlthough the # of comparison pairs goes up by 1 each time, the number of nodes\r\nrequiring that many comparison pairs goes down by a factor of 1/2. The end\r\nresult is that very few nodes require lots of comparison pairs and very many\r\nnodes require few comparisons pairs, and the total number of comparison pairs\r\nin an N node heap is bounded by O(N).\r\n\r\nIn fact, we can prove this directly. The number of comparison pairs (grouped\r\ntogether by depth d) is given by the following formula, which captures the\r\nidea that as the depth increases, the number of comparisons (h-d) goes down\r\nbut the number of nodes requiring that many comparisons (2^d) goes up.\r\n\r\n  sum(from d=0, to d=h, of the expression (h-d)*2^d)\r\n\r\nFor example, for the height 2 tree (7 nodes) in the illustration above, we have\r\n  for d = 0, 2*1  (2 comparison pairs for 1 node  at depth 0)\r\n  for d = 1, 1*2  (1 comparison pair  for 2 nodes at depth 1)\r\n  for d = 2, 0*4  (0 comparison pairs for 4 nodes at depth 2)\r\n\r\nfor the height 5 tree (63 nodes), we would have\r\n  for d = 0, 5*1  (5 comparison pairs for 1  node  at depth 0)\r\n  for d = 1, 4*2  (4 comparison pairs for 2  nodes at depth 1)\r\n  for d = 2, 3*4  (3 comparison pairs for 4  nodes at depth 2)\r\n  for d = 3, 2*8  (2 comparison pairs for 8  nodes at depth 3)\r\n  for d = 4, 1*16 (1 comparison pairs for 16 nodes at depth 4)\r\n  for d = 5, 0*32 (0 comparison pairs for 32 nodes at depth 5)\r\n  \r\nNow let us solve the formula. We can rewrite it as follows:\r\n  multiplying by 1 = 2^h/2^h                                        (1)\r\n  factoring out 2^h, which does not depend on d                     (2)\r\n  changing * 2^(d-h) in the numerator to 2^(h-d) in the denominator (3)\r\n  changing the summation variable from d to i, where i = h-d        (4)\r\n   (so d going 0 to h is like i going h to 0, which  we reverse to 0 to h)\r\n\r\n  sum(from d=0, to d=h, of the expression 2^h*(h-d)*2^d/2^h)   =   (1)\r\n  2^h * sum(from d=0, to d=h, of the expression (h-d)*2^d/2^h) =   (2)\r\n  2^h * sum(from d=0, to d=h, of the expression (h-d)*2^(d-h)) = \r\n  2^h * sum(from d=0, to d=h, of the expression (h-d)/2^(h-d)) =   (3)\r\n  2^h * sum(from i=0, to i=h, of the expression i/2^i)             (4)\r\n\r\nThis is an interesting sum: its numerator increases slowly, an its denominator\r\nincreases quickly. Let us first note that \r\n\r\n  sum(from i=0, to i=h, of the expression i/2^i)\r\n\r\nis equal to (because when i = 0 the term is 0)\r\n\r\n  sum(from i=1 to i=h, of the expression i/2^i)\r\n\r\nwhich is < the following infinite sum (because it has even more positive terms)\r\n\r\n  sum(from i=1, to i=infinity, of the expression i/2^i)\r\n\r\nNow, we should know the following infinite sum (the top one; each successive one\r\nsubtracts the leading term (on the lect) from each side of the =\r\n\r\n  1/2 + 1/4 + 1/8 + 1/16 + 1/32 + 1/64 + .... = 1\r\n        1/4 + 1/8 + 1/16 + 1/32 + 1/64 + .... = 1/2\r\n              1/8 + 1/16 + 1/32 + 1/64 + .... = 1/4\r\n                  + 1/16 + 1/32 + 1/64 + .... = 1/8\r\n                           1/32 + 1/64 + .... = 1/16\r\n                                  1/64 + .... = 1/32\r\n                                  ....        = ....\r\n\r\nIf we add on the left and the right we have\r\n\r\n 1/2 + 2/4 + 3/8 + 4/16 + 5/32 + 6/64 + ... = 1 + 1/2 + 1/4 + 1/8 + ...\r\n 1/2 + 2/4 + 3/8 + 4/16 + 5/32 + 6/64 + ... = 2\r\n\r\nNotice the sum on the left side is\r\n\r\n  sum(from i=1, to i=infinity, of the expression i/2^i)\r\n\r\nSo, 2^h * sum(from i=0, to d=h, of the expression i/2^i) < 2^h * 2 = 2^(h+1)\r\nFinally, Ph has 2^(h+1) - 1 nodes, so the sum can also be written as N+1. So\r\nthe number of comparison pairs needed to build a Ph (an N node heap) O(N).\r\n\r\nSo, sorting with a heap is still O(N Log2 N), but building the heap should be\r\nfaster. The complexity to build a heap (offline) and remove all its values \r\nis now O(N) + O(N Log2 N) which is still O(N Log2 N), but the algorithm should\r\nrun faster.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nGeneralizing Heaps: d-heaps\r\n\r\nWe can generalize binary heaps to d-heaps, where d represents the maximum\r\nnumber of children of any node. So, the heaps representing the binary trees\r\nthat we have just studied can also be called 2-heaps.\r\n\r\nThe ordering property in d-heaps remains the same, and the structure property\r\ntoo, although we need to apply it with more children. A 3-heap with 9 nodes\r\nmust have the following structure.\r\n\r\n                   A\r\n                /  |     \\  \r\n           /       |          \\ \r\n    B              C              D\r\n  / |  \\         / |\r\nE   F   G      H   I\r\n\r\nNote that an array storing this heap would look as follows.\r\n\r\n  0   1   2   3   4   5   6   7   8\r\n+---+---+---+---+---+---+---+---+---+\r\n| A | B | C | D | E | F | G | H | I |\r\n+---+---+---+---+---+---+---+---+---+\r\n\r\nwith the access functions\r\n  1stChild(i) = 3i+1\r\n  2ndChild(i) = 3i+2\r\n  3rdChild(i) = 3i+3\r\n  parent  (i) = (i-1)/3, using truncating division\r\n\r\nThe storage scheme and access functions easily generalize to 4-heaps, 5-heaps,\r\netc. In fact, given a d-heap we can write just two functions that each have\r\nd as a parameter\r\n\r\n  nthChild(n,i,d) = d*i+n where 1<=n<=d: this is the nth child of i in a d-heap\r\n  parent  (i,d)   = (i-1)/d: parent of i in a d-heap using truncating division\r\n\r\nThe percolate up operation is the exactly the same: switching a child and its\r\nparent. Its complexity is O(Logd N), aka Log base d of N. The percolate down\r\noperation is a bit more complicated because in a Min-d-Heap each node must be\r\nswapped with the minimum of its children (if it is bigger than any child).\r\nThis requires d operations at each node (to find the minimum child), so at most\r\nO(d Logd N); but since d is a constant for any given heap, the complexity class\r\nis still logarithmic, with base d.\r\n\r\nHow much difference is there between logs of different bases? We can compute\r\nsome numbers easily for Log4 N because Log4 N = .5 Log2 N\r\nSo Log2 1,000 ~ 10 and Log4 1,000 ~ 5; Log2 10^6 ~ 20 and Log4 10^6 ~ 10.\r\nbut remember that each node percolated down would require 3 comparisons instead\r\nof 1 to determine which of its children is smallest. So, for percolating down\r\nthe time would probably be about the same, but percolate up would be twice as\r\nfast.\r\n\r\nThink of the case where d is huge: imagine d = 1,000,000. If our priority queue\r\nhad fewer than a million values, the result would be a tree that has height 1.\r\nDoing a percolate up would be trivial, requiring just one comparison; but doing\r\na percolate down would be very expensive, requiring up to a million\r\ncomparisions to find the smallest value at depth 1. Of course, with this d, as\r\nN->inf, the number of comparisons for each percolation down would stay at\r\n1,000,000.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nMerging and Changing Priority:\r\n\r\nFinally, there is one operation common to priority queues that heaps do not\r\ndo optimally: merging two priority queues into one. A simple way to do this\r\nfor any heap implementation is just add every value from one heap to another:\r\nthis would be O(N Log2 2N) = O(N + N Log2 N) = O(N Log2 N) operations (assuming\r\neach heap had N values). Of course O(N Log2 N) is good complexity class, but\r\nother more complicated priority queue data structures have a better complexity\r\nclass for this operation (and similar complexity classes for add and removemin).\r\n\r\nAnother way to merge is to put both heaps into an array and then use the\r\noffline technique above to build a heap from their contents. This is an O(2N)\r\noperations = O(N) operations: putting the two heaps in an array big enough to\r\nhold both is O(2N) = O(N) and then doing the offline heap construction\r\nalgorithm is also O(2N) = O(N), so the resulting complexity is O(N), an\r\nimprovement over enqueuing all the values from one priority queue into the\r\nother, if the sizes are approximately the same.\r\n\r\nBut (adding one more wrinkle) if the priority queues are vastly different\r\nsizes (one M, one N with M << N), it is probably better to enqueue all the\r\nvalues from the smaller queue into the bigger one, requiring O(M Log (N+M))\r\n- better than O(N+M) when M << N - and with M << N, O(M Log (N+M)) is about\r\nO(M Log N) and O(N+M) is about O(N). For M = 10 and N = 10^6 the difference is\r\nabout 200 compared to about 10^6.\r\n\r\nThere are more advanced/complicated data structures for implementating priority\r\nqueues that allows merge operations more quickly (while still quickly adding\r\nand removing the min): leftist, skew, binomial, fibonacci, etc. heaps. The key\r\nis to create order and structure properties that constrain the data enough (but\r\nnot too much) to allow all the operations to work, and work quickly. You can\r\nread about these data structures online or in data structures textbooks.\r\n\r\nAnother useful operation is changing/updating the priority of a value in a\r\npriority queue. In a heap we would have to percolate such a value up or down,\r\ndepending on how its priority changed. That is just an O(Log N) operations.\r\n\r\nThe hard part is finding the index of the value in the heap to percolate,\r\nbecause we would have to scan the heap (as an array) trying to find the value:\r\nthat is O(N), so it dominates the O(Log N) percolation. The order property of\r\na heap does not help us to \"find\" a values that is stored in the heap: if the\r\nvalue is not at the root, we might have to look at nodes in both its left and\r\nright subtrees (heaps are different than BSTs).\r\n\r\nIf we instead used a map (from values to indexes in the heap) to tell us where\r\neach value in the heap was stored, that could speed-up the process: if we\r\nstored the map as a BST, finding the index of a value is likely O(Log N)\r\n- assuming the tree is well balanced; later we will see that when storing a map\r\nin a hash table, finding the index of a value has an even better complexity\r\nclass: O(1). In both cases finding and percolating the value would be O(Log N).\r\n\r\nIn some quarter I have students modify their HeapPriorityQueue implementations\r\nto allow updating the value in a node and re-percolating it to the correct\r\nposition in the tree.\r\n\r\n", "encoding": "ascii"}