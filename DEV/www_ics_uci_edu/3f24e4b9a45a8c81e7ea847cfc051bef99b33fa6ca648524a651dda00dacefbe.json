{"url": "https://www.ics.uci.edu/~eppstein/280/intro.html", "content": "<HTML><HEAD>\n<TITLE>Computational Statistics</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Introduction</H2>\n\nTo begin with, I'm not a statistician, so take anything I say about\nstatistics (as opposed to statistical algorithms) with a big grain of salt.\nBut let's begin by talking about statistics in general.\n\n<H3>The Big Picture</H3>\n\nI view statistics as a sequence of transformations of numbers,\nstarting from parameters that define a model for the generation of data,\nthrough a model for noise (describing how the data may be corrupted by\nrandom events, transmission errors, or measurement errors),\nthrough an algorithm that makes estimates from the observed data.\nThe hope is that the estimates in some sense match the original parameters.\n\n<PRE>\n                          ______________\n                         |              |\n        parameters ----&gt; |  DATA MODEL  |\n                         |______________|\n\n                                |\n                                | data\n                                V\n                          _______________\n                         |               |\n                         |  NOISE MODEL  |\n                         |_______________|\n\n                                |\n                                | observations\n                                V\n                          _____________\n                         |             |\n                         |  ALGORITHM  | ----&gt; estimate\n                         |_____________|\n</PRE>\n\n<P>To some extent the division between the data model and noise model is\narbitrary (both are describing physical processes that are parts of the\nreal world) and comes from the division between which parameters we want\nto estimate and which other ones we want to treat as noise and ignore.\n\n<H3>Function Follows Form</H3>\n\nThe nature of the statistical algorithm to be used is determined\nby the data and noise models.\n\n<P>The data model sets the general\nflavor of problem being considered:\n<A HREF=\"point.html\">single point estimation</A>\n(data model: pass parameters unchanged as data coordinates),\nregression (data model: parameters are the coordinates of a linear\nfunction relating independent and dependent variables), clustering\n(data model: choose randomly among several different points or point\ndistributions), or hierarchical clustering\n(e.g. evolutionary tree reconstruction, in which the\nparameters define an evolutionary tree and the data model\nuses this tree to define mutated gene sequences for each leaf of the tree).\n\n<P>More complicated data models of interest to the AI students in the course\nare hidden Markov models for time series data, and belief propagation\nmethods for decoding error correcting codes (\"turbocodes\").\nThe problem of devising a good error correcting code could be seen\nas choosing a data model in such a way that whatever noise you\nhave doesn't destroy too much of your data.\n\n<P>The noise model determines which to choose among several different\nestimation algorithms returning the same sort of output.\nE.g., if one is estimating a single point value,\none might choose among least squares (Gaussian noise),\nthe centroid (for noise distributions with known zero expectation and unknown\nradial components), the circumcenter\n(for arbitrary bounded-radius noise distributions),\nor the minimum circumcenter of n/2 points (for robust outlier elimination).\n\n<H3>Desiderata</H3>\n\nAn estimation algorithm should have the following properties:\n\n<DL>\n<DT><B>Consistency.</B></DT>\n<DD>The estimates should converge (with enough data) to the original\nparameters: that is, there should exist a limit\n(as n, the number of observations, goes to infinity),\nand that limit should equal the parameters.\nIf this is impossible due to the noise model,\nwe would at least like the estimate to be near the parameters,\ne.g. by minimizing the worst case error or maximizing the likelihood.</DD>\n<DT><B>Invariance.</B></DT>\n<DD>The estimates should not be changed by irrelevant rescaling or other\nsuch transformations of the data.  In extreme cases (distance-free methods)\nthe estimates may depend only on the combinatorics of the data points\n(e.g. are they above/below the estimated regression line)\nand not on metric or distance information.</DD>\n<DT><B>Efficiency.</B></DT>\n<DD>The estimates should use only as much data as is required to solve\nthe problem accurately.  Another way of stating this is that the rate of\nconvergence of the estimates should be high.</DD>\n<DT><B>Robustness.</B></DT>\n<DD>The estimates should tolerate as broad a class of noise models as possible.\nA particularly important type of noise from the point of view of\nrobustness is <I>outliers</I>: a noise model in which some fraction\nof the data may be arbitrarily corrupted (and should be treated as\nbeing set by an adversary which will try to make the estimates as\ninaccurate as possible).  Robustness against outliers may be measured\nin terms of the number of outliers that can be tolerated before the\nestimate becomes arbitrarily inaccurate.</DD>\n<DT><B>Speed.</B></DT>\n<DD>How quickly will an implementation of the algorithm run?\nAlthough this is the most important characteristic from the theoretical\nCS point of view, it is probably the least important from the\nstatistical point of view, and possibly the \"greed for speed\"\nhas led to overuse of fast estimation methods such as least squares.\nOne of the contributions algorithmic research can make in this area\nis to convince users that methods other than least squares can\nbe efficient enough to be used when appropriate.\n<P>The simplest way of optimizing for speed\nwithout compromising statistical quality is to work on improved\nalgorithms for already known statistical methods: that is, to find fast\nalgorithms that produce exactly the same output as other slower\nalgorithms already in use.  One can also look for <I>approximation\nalgorithms</I> that quickly find a result \"almost as good as\"\nthe original slower algorithm, but one must be careful\nto preserve the other important statistical qualities of the original\nalgorithm -- generally it's safest to approximate the <I>estimated values</I>\nthemselves, rather than to approximate whatever objective function the\nestimation algorithm is optimizing.</DD>\n</DL>\n\n<H2><A HREF=\"point.html\">NEXT: Single point estimators</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"intro.html\" --></SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}