{"url": "https://www.ics.uci.edu/~eppstein/161/960125.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Selection and order statistics</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 25, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Selection and order statistics</h1>\n\nStatistics refers to methods for combining\na large amount of data (such as the scores of the whole class on a\nhomework) into a single number of small set of numbers that give an\noverall flavor of the data.\nThe phrase <i>order statistics</i> refers to statistical methods that\ndepend only on the\n<i>ordering</i> of the data and not on its numerical values.\nSo, for instance, the average of the data, while easy to compute and\nvery important as an\nestimate of a central value, is not an order statistic.\nThe <b>mode</b> (most commonly occurring value) also does\nnot depend on ordering, although the most efficient methods for\ncomputing it in a comparison-based model involve sorting algorithms.\nThe most commonly used order statistic is\nthe <b>median</b>, the value in the middle position in the sorted order of\nthe values.  Again we can get the median easily in O(n log\nn) time via sorting, but maybe it's possible to do better? We'll\nsee that the answer is yes, O(n).\n\n<p>We will solve the median recursively. Sometimes, especially in\nrelation to recursive algorithms, it is easier to solve a more\ngeneral problem than the one we started out with. (Making the\nproblem more difficult paradoxically makes it easier to solve.)\nThis is true because if the problem is more general it may be\neasier for us to find recursive subproblems that help lead us to a\nsolution.</p>\n\n<p><a name=\"select\">The more general problem we solve is <b>\nselection</b>: given a list of n items, and a number k between 1\nand n, find the item that would be kth if we sorted the list. The\nmedian is the special case of this for which k=n/2.</a></p>\n\n<p>We'll see two algorithms for this general problem, a randomized\none based on quicksort (\"quickselect\") and a deterministic one. The\nrandomized one is easier to understand &amp; better in practice so\nwe'll do it first.</p>\n\n<p>But before we get to it, let's warm up with some cases of\nselection that don't have much to do with medians (because k is\nvery far from n/2).</p>\n\n<h2>Second-best search</h2>\n\nIf k=1, the selection problem is trivial: just select the minimum\nelement. As usual we maintain a value x that is the minimum seen so\nfar, and compare it against each successive value, updating it when\nsomething smaller is seen. \n\n<pre>\n    min(L)\n    {\n    x = L[1]\n    for (i = 2; i &lt;= n; i++)\n        if (L[i] &lt; x) x = L[i]\n    return x\n    }\n</pre>\n\nWhat if you want to select the second best? \n\n<p>One possibility: Follow the same general strategy, but modify\nmin(L) to keep two values, the best and second best seen so far. We\nneed only compare each new value against the second best, to tell\nwhether it is in the top two, but then if we discover that a new\nvalue is one of the top two so far we need to tell whether it's\nbest or second best.</p>\n\n<pre>\n    second(L)\n    {\n    x = L[1]\n    y = L[2]\n    if (y &lt; x) switch x, y\n    for (i = 3; i &lt;= n; i++)\n        if (L[i] &lt; y) {\n        y = L[i]\n        if (y &lt; x) switch x, y\n        }\n    return y\n    }\n</pre>\n\nAlthough this algorithm is pretty easy to come up with, some\ninteresting behavior shows up when we try to analyze it. \n\n<ul>\n<li>In the <b>worst case</b>, the list may be sorted in decreasing\norder, so each of the n-2 iterations of the loop performs 2\ncomparisons. The total is then 2n-3 comparisons.</li>\n\n<li>In the <b>average case</b> (assuming any permutation of L is\nequally likely) the first comparison in each iteration still always\nhappens, but the second only happens when L[i] is one of the two\nsmallest values among the first i. Each of the first i values is\nequally likely to be one of these two, so this is true with\nprobability 2/i. The total expected number of times we make the\nsecond comparison is \n\n<pre>\n     n\n    sum 2/i  =  2 ln n + O(1)\n    i=3\n</pre>\n\n<a name=\"harmonic\">The ln is a natural logarithm. The sum (for i\nfrom 1 to n) of 1/i, known as the <i>harmonic series</i>, is ln n +\nO(1) (this can be proved using calculus, by comparing the sum to a\nsimilar integral).</a> Therefore the total expected number of\ncomparisons overall is n+O(log n).</li>\n</ul>\n\n<p>This small increase over the n-1 comparisons needed to find the\nminimum gives us hope that we can perform selection faster than\nsorting.</p>\n\n<h2>Heapselect</h2>\n\nWe saw a randomized algorithm with n + O(log n) comparison\nexpected. Can we get the same performance out of an unrandomized\nalgorithm? \n\n<p>Think about basketball tournaments, involving n teams. We form a\ncomplete binary tree with n leaves; each internal node represents\nan elimination game. So at the bottom level, there are n/2 games,\nand the n/2 winners go on to a game at the next level of the tree.\nAssuming the better team always wins its game, the best team always\nwins all its games, and can be found as the winner of the last\ngame.</p>\n\n<p>(This could all easily be expressed in pseudo code. So far, it's\njust a complicated algorithm for finding a minimum or maximum,\nwhich has some practical advantages, namely that it's <i>\nparallel</i> (many games can be played at once) and <i>fair</i> (in\ncontrast, if we used algorithm min above, the teams placed earlier\nin L would have to play many more games and be at a big\ndisadvantage).</p>\n\n<p>Now, where in the tree could the second best team be? This team\nwould always beat everyone except the eventual winner. But it must\nhave lost once (since only the overall winner never loses). So it\nmust have lost to the eventual winner. Therefore it's one of the\nlog n teams that played the eventual winner and we can run another\ntournament algorithm among these values.</p>\n\n<p>If we express this as an algorithm for finding the second best,\nit uses only n + ceil(log n) comparisons, even better than the\naverage case algorithm above.</p>\n\n<p>If you think about it, the elimination tournament described\nabove is similar in some ways to a <a href=\"960116.html#binheap\">\nbinary heap</a>. And the process of finding the second best (by\nrunning through the teams that played the winner) is similar to the\nprocess of removing the minimum from a heap. We can therefore use\nheaps to extend idea to other small values of k:</p>\n\n<pre>\n    heapselect(L,k)\n    {\n    heap H = heapify(L)\n    for (i = 1; i &lt; k; i++) remove min(H)\n    return min(H)\n    }\n</pre>\n\nThe time is obviously O(n + k log n), so if k = O(n/log n), the\nresult is O(n). Which is interesting, but still doesn't help for\nmedian finding. \n\n<h2>Quick select</h2>\n\nTo solve the median problem, let's go back to the idea of using a\nsorting algorithm then finding the middle element of the sorted\nlist. Specifically, look at <a href=\"960118.html#quicksort\">\nquicksort</a>: \n\n<pre>\n    quicksort(L)\n    {\n    pick x in L\n    partition L into L1&lt;x, L2=x, L3&gt;x\n    quicksort(L1)\n    quicksort(L3)\n    concatenate L1,L2,L3\n    }\n</pre>\n\nWe could have a selection algorithm that called quicksort\nexplicitly before looking at the middle element. Instead let's put\nthe \"look at the middle element\" line into the quicksort\npseudocode: \n\n<pre>\n    select(L)\n    {\n    pick x in L\n    partition L into L1&lt;x, L2=x, L3&gt;x\n    quicksort(L1)\n    quicksort(L3)\n    concatenate L1,L2,L3\n    return kth element in concatenation\n    }\n</pre>\n\nThis is not a recursive algorithm itself since it does not call\nitself (although it does call quicksort, which is recursive). Just\nlike quicksort, it takes time O(n log n). \n\n<p>But notice: if k is less than the length of L1, we will always\nreturn some object in L1. It doesn't matter whether we call\nquicksort(L3) or not, because the order of the elements in L3\ndoesn't make a difference. Similarly, if k is greater than the\ncombined lengths of L1 and L2, we will always return some object in\nL3, and it doesn't matter whether we call quicksort on L1. In\neither case, we can save some time by only making one of the two\nrecursive calls. If we find that the element to be returned is in\nL2, we can just immediately return x without making either\nrecursive call. We can also save a little more time (not very much)\nby not doing the concatenation, instead directly looking at the\nright place in L1, L2, or L3.</p>\n\n<pre>\n    select(L)\n    {\n    pick x in L\n    partition L into L1&lt;x, L2=x, L3&gt;x\n    if (k &lt;= length(L1)) {\n        quicksort(L1)\n        return kth element in L1\n    } else if (k &gt; length(L1)+length(L2)) {\n        quicksort(L3)\n        return (k-length(L1)-length(L2)) element in L3\n    } else return x\n    }\n</pre>\n\nSo far this is an improvement (it makes fewer calls to quicksort)\nbut it is still an O(n log n) algorithm. One final observation,\nthough, is that the code inside each if statement sorts some list\nand then returns some position in it. In other words, it solves\nexactly the same sort of selection problem we started with! And we\ncould make the same improvements (of only doing one out of two\nrecursive calls) to the two remaining calls to quicksort, simply by\nreplacing these pieces of code by a recursive call to the selection\nroutine. \n\n<pre>\n    quickselect(L,k)\n    {\n    pick x in L\n    partition L into L1&lt;x, L2=x, L3&gt;x\n    if (k &lt;= length(L1))\n        return quickselect(L1,k)\n    else if (k &gt; length(L1)+length(L2))\n        return quickselect(L3,k-length(L1)-length(L2))\n    else return x\n    }\n</pre>\n\n<h2>Analysis of quickselect</h2>\n\nQuickselect always makes a recursive call to one smaller problem.\nIf we pretended it was always a problem of half the size, we would\nget a recurrence T(n)=O(n)+T(n/2)=O(n), but of course it's not. In\nthe worst case, the recursive call can be to a problem with only\none fewer element (L3 could be empty, and L2 could only have one\nelement in it). This would instead give us a recurrence\nT(n)=O(n)+T(n-1)=O(n^2). So in the worst case this algorithm is\nvery bad. \n\n<p>Instead we want to analyze the average case, which is much\nbetter. To perform the average case analysis rigorously, we would\nform a randomized recurrence with two parameters n,k. The worst\ncase turns out to be when k=n/2, so to keep things simple, I'll\njust write a one-variable recurrence assuming that worst case.\n(This assumption only makes the analysis sloppier, but if it gives\nsome bound f(n) you know it will at least not be worse than f(n)\nbut it might actually be better.)</p>\n\n<p>Now we always eliminate |L2|+min(|L1|,|L3|) objects from the\nlist at each step. (except when we return x, in which case the\nalgorithm terminates). Roughly, this min is equally likely to be\nany number from 1 to n/2. So the recursive call to quickselect is\nequally likely to involve a list of any size from n/2 to n. (If k\nwere different from n/2, this would only make some smaller sizes\nmore likely and some larger sizes less likely, which can be used to\nprove that k=n/2 really is the worst case.)</p>\n\n<p>As usual with expected case analysis we get a recurrence\ninvolving a sum over possible choices of the probability of making\nthat choice, multiplied by the time it would take if we made the\nchoice. As before let's measure things in comparisons. The\nrecurrence is:</p>\n\n<pre>\n          n\n    T(n) = n-1 + sum  2/n T(i)\n        i=n/2\n</pre>\n\nHow do we analyze something like this? We know it should come out\nto O(n), but we don't know what the constant factor should be. \n\n<p>If we knew the correct constant factor c, we would be able to\nprove inductively that T(n) is at most cn. The method would be\nsimply to plug in ci for T(i) on the left side, grind through the\nsums, and verify that the result is at most cn. (One would also\nhave to prove a base case for the induction, of course.)</p>\n\n<p>Note that this sort of proof needs an explicit constant; it\nisn't enough to plug in O(i) on the left side and verify that the\nresult is O(n) on the right side. [Exercise: if T(n)=n+T(n-1), why\ndoesn't an inductive proof show that T(n)=n+O(n) (by induction\nhypothesis) which is O(n)? Hint: the constant factor hidden by the\nO-notation needs to truly be constant and not something that can\ngrow each time you induct.]</p>\n\n<p>If we knew c we could perform an explicit induction proof that\nT(n)=cn. Even though we don't know c, we can still work through the\nsums, and get something of the form T(n)=f(c) n. What this tells us\nis that the explicit induction proof works when f(c) is at most c.\nSo by working through the sums with an unknown value of c, we get\ninformation that helps us determine c.</p>\n\n<p>Let's try this strategy for quickselect.</p>\n\n<pre>\n          n\n    T(n) = n-1 + sum  2/n T(i)\n        i=n/2\n\n           n\n         &lt;= n-1 + sum  2/n ci\n         i=n/2\n\n          2c   n\n         = n-1 +  -   sum  i\n          n  i=n/2\n\n          2c     n      n/2-1\n         = n-1 +  -  (  sum i -  sum  i  )\n          n     i=1      i=1\n\n     = n-1 + 2c/n (n^2/2 - n^2/8 + O(n))\n\n     = (1+3c/4)n + O(1)\n</pre>\n\nThe induction works (with a large enough base case to swamp the\nO(1) term) whenever c is greater than 4. So quickselect uses\nroughly 4n comparisons in expectation. \n\n<h2>Quicker selection</h2>\n\n<a href=\"people.html#floyd\">Floyd</a> and <a href= \n\"people.html#rivest\">Rivest</a> noticed that by choosing the pivot\npoint x more carefully, one can get a much better algorithm. I'll\njust describe this with only sketchy analysis. It's not in the book\nand I won't test on it, but something like this is what you should\nbe using if you want medians for large data sets. If you want to\nread more about it, a good recent reference is \"Average Case\nSelection\", by Walter Cunto and Ian Munro, in the Journal of the\nACM (vol. 36, no. 2, April 1989, pages 270-279). \n\n<pre>\n    sampleselect(L,n,k)\n    {\n    given n,k choose parameters m,j \"appropriately\"\n    pick a random subset L' having m elements of L\n    x = sampleselect(L',m,j)\n\n    partition L into L1&lt;x, L2=x, L3&gt;x\n    if (k &lt;= length(L1))\n        return sampleselect(L1,k)\n    else if (k &gt; length(L1)+length(L2))\n        return sampleselect(L3,k-length(L1)-length(L2))\n    else return x\n    }\n</pre>\n\nThe basic idea is that the closer x is to the kth position, the\nmore items we'll eliminate in the final recursive call. By taking a\nmedian of a sample, instead of just choosing randomly, we're more\nlikely to get something closer to the kth position. \n\n<p>How to choose m? Typically it should be some small fraction of\nn, so that the O(m) time used in the first recursive call is small.\nWe'll pick a more exact value after doing the analysis.</p>\n\n<p>How to choose j appropriately? We want it to be very likely that\nthe j-median of L' is close to the k-median of L -- that way as\nmuch as possible gets removed from the recursive calls. More\nspecifically, if k is small we want the j-median of L' to be\nslightly greater than the k-median of L (so that the stuff that\ngets removed is likely to be on the larger side) and conversely if\nk is large we want j to be a little small. So</p>\n\n<pre>\n    j = k(m/n) + fudge\n</pre>\n\nwhere fudge is positive when k is small, negative when k is large.\nIn general the right value of fudge is O(sqrt(m) log n) for reasons\nthat would take some complicated probability theory to explain. \n\n<p>The result: after the top level call, we'll probably have\neliminated either most of the items larger than the kth position,\nor most of the items smaller than it. So in the recursive calls, k\nis very likely to be near 1 or near n (within O(n log n / sqrt m)\nof it) since adjacent samples in L' are likely to be separated by\nroughly n/m positions in L.</p>\n\n<p>After the second level of recursion, we'll probably have\neliminated most of the items on the other side of the kth position.\nThere are likely to be many fewer than n items left -- O(n log n\n/sqrt m) of them to be precise. So the bulk of the time happens in\nthose first two calls. In the first one we do at most n\ncomparisons, and in the second we are likely to only do at most\nmin(k,n-k).</p>\n\n<p>Putting this into a recurrence we get</p>\n\n<pre>\n    T(n)=n+min(k,n-k)+2T(m)+T(n log n / sqrt m)\n</pre>\n\nfor the total time (with high probability). We also can show that\nT(n)=O(n) (or just use quickselect in the recursive calls). So\ninstead of analyzing this recurrence as a recurrence (which would\nmean figuring out what \"with high probability\" starts meaning when\nyou iterate the recurrence several times) we just replace T(n) by\nO(n) in the formula above: \n\n<pre>\n    T(n)=n+min(k,n-k)+O(m)+O(n log n / sqrt m)\n</pre>\n\nWe still have a free parameter m. To finish the description of the\nalgorithm and its analysis, we need to choose m. If we make m too\nlarge, the O(m) term will dominate the formula. If we make m too\nsmall, the other term will dominate. To make m \"just right\", we\nmake the terms roughly equal to each other: \n\n<pre>\n    m = n log n / sqrt m\n\n    m^(3/2) = n log n\n\n    m = (n log n)^(2/3)\n</pre>\n\nThe result is an algorithm that uses n + min(k,n-k) + O((n log\nn)^(2/3)) comparisons (with high probability, or in expectation). \n\n<p>This turns out to be about as good as possible for a randomized\nalgorithm. But it remains an open problem exactly how many\ncomparisons are needed without randomization. (Of theoretical\nsignificance only since in practice randomization is good.) The\ncurrent best algorithms use roughly 2.95 n comparisons but are\nquite complicated. Next time we'll see a simpler (but still\ncomplicated) method with O(n) comparisons.</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960125.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}