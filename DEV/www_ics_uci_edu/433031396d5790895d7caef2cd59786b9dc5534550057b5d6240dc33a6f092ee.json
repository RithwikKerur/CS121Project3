{"url": "https://www.ics.uci.edu/~pazzani/Research.html", "content": "<HTML>\n<head>\n<title> Michael J. Pazzani: Research</title>\n</head>\n\n<body BGCOLOR=\"#FFFFFF\">\n<h1>  Michael J. Pazzani: Research </h1>\n<ul>\n<li>  <a href=http://www.ics.uci.edu/~pazzani/Publications/Publications.html>Publications</a> \n<li>  <a href=http://www.ics.uci.edu/~pazzani/CognitiveKDD.html> NSF Project:  Cognitive Knowledge Discovery in Databases</a> \n</ul>\n<H2> Research Overview: </H2>\n\nThe common theme behind my research is the investigation and analysis\nof learning methods that make use of prior knowledge to guide the\nlearning process.  Typically, these learning methods combine empirical\n(i.e., correlational or data-driven) and explanation-based (i.e.,\nanalytical or knowledge-intensive) learning techniques.  The goal is\nto create learning systems that accept as input background knowledge,\nalthough incomplete and incorrect, along with training examples, and\nlearn to make classifications that are more accurate than that made by\neither the background knowledge alone, or by the results of applying\nan induction algorithm on the training data.\n<P>\nMy early work on OCCAM [1] describes a learning system that has the\ncapability of acquiring knowledge empirically and later using this\nknowledge to facilitate knowledge-intensive learning.  This research\nwas inspired by psychological findings on the types of information\nthat people use during learning and how this information affects the\nrate of learning.  Part of this research also focused on the\nacquisition of causal relationships [2].  In this paper, it is argued\nthat in addition to specific knowledge of actions and effect, the\nprocess of learning causal relationships is also facilitated by\ngeneral knowledge of causality.  That is, causal relationships that\nconform to one of a number of common patterns of causal relationships\nare easier for human subjects to learn.  This paper also provides\nexperimental evidence collected from human subjects.  An experiment\nshows that human subjects learning a causal relationship that conforms\nto one particular causal pattern require fewer trials than subjects\nlearning a causal relationship that violates this pattern.  In more\nrecent research in this framework [3], I have addressed issues of\nlearning when the background knowledge is overly general.  In\naddition, in [4] I have addressed the issue of the acquisition of the\ncommon patterns of causal relationships used by OCCAM and show that\nthey can be formed by looking for commonalities among rules found by\nan empirical learner.\n<P>\n[1] Pazzani, M. (1990). Creating a memory of causal relationships-\nAn integration of empirical and explanation-based learning methods.\nHillsdale, NJ- Lawrence Erlbaum Associates.\n<P>\n[2]  Pazzani, M. (1991).  A computational theory of learning causal\nrelationships.  Cognitive Science, 15, 401-424.\n<P>\n[3] Pazzani, M. (1989).  Detecting and correcting errors of\nomission after explanation-based learning.  Proceedings of the\nEleventh International Joint Conference on Artificial Intelligence\n(pp. 713-718). Detroit, MI- Morgan Kaufmann.\n<P>\n[4]  Pazzani, M. (1992). <A HREF=\"ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Pazzani-MLJ93-Occam.ps.Z\">Learning causal patterns: Making a\n transition from data-driven to theory-driven learning.</A>   Machine\nLearning, 11, 173-194. <A HREF=\"http://www.ics.uci.edu/AI/ML/MLAbstracts.html#Pazzani-MLJ93-O\\\nccam\">Abstract</A>\n<P>\nInformation about getting a copy of <A HREF=\"http://www.ics.uci.edu/AI/ML/Occam.html\">Occam</A>.</P>\nMy more recent research has focused on more thoroughly investigating\nissues raised during the development of OCCAM.  In particular, I have\nexplored the use of prior knowledge in learning from mathematical,\npsychological, computational and applied points of view.\n\n<P>\n\n<h3> Mathematical modeling of learning algorithms </h3>\n<P>\nWe have developed an approach to average case analysis of\nlearning algorithms [5,6].  The average case model is based upon\ndetermining, for a given distribution of examples, the probability\nthat an algorithm will revise its hypothesis and the effect that\nrevising the hypothesis will have on the accuracy of the hypothesis.\nModels were created for  three different algorithms learning\nfrom a specific distribution (a product distribution).  It was\nshown that a particular algorithm combining empirical and\nexplanation-based learning [7] is more accurate than either its\nempirical or explanation-based component alone.  An average\ncase model was needed because the \"standard\" mathematical\nmodels of learning algorithms (i.e., Probably Approximately\nCorrect models) are worst-case models and the worst-case\nbehavior of all three algorithms is identical.  This paper also\nevaluates the assumptions from which the average-case model\nwas derived by experimentally demonstrating that it  accounts\nfor a large percentage of the variation in observed accuracy on a\nnaturally occurring data set and on several artificial data sets\ndeliberately constructed to violate assumptions of the model.\n<P>\n[5] Pazzani, M., & Sarrett, W. (1989). Average case analysis\nof conjunctive learning algorithms. Proceedings of the\nSeventh International Conference on Machine Learning\n (pp. 339-347).  Austin, TX- Morgan Kaufmann.\n<P>\n[6] Pazzani, M., & Sarrett, W.  (1992). \n<A HREF=\"ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Pazzani-MLJ92-Average.ps\">A framework for average\n case analysis of conjunctive learning algorithms.</A>  Machine Learning, 9, 342-372. <A HREF=\"http://www.ics.uci.edu/AI/ML/MLAbstracts.html#Pazzani-MLJ92-Average\">Abstract</A>\n<P>\n[7] Sarrett, W., & Pazzani, M. (1989).  One-sided algorithms\nfor integrating empirical and explanation-based learning.\n Proceedings of the Sixth International Workshop on\nMachine Learning  (pp. 26-28).  Ithaca, NY- Morgan\nKaufmann.\n<P>\n\n<h3>Investigation of human learning</h3>\n<P>\nMy interest in machine learning algorithms that combine correlational\nand analytical methods arises from the fact that much of human\nlearning of predictive relationships can be characterized as using a\ncombination of these methods.  Part of my research has been the\nconducting of psychological experiments to assess the impact of\nvarious types of prior knowledge on human learning rates.  The\npsychological research is performed to test hypotheses that occur\nduring the development of computational models of learning. Typically,\nthe dependent variable measured is the learning rate as determined by\nthe number of trials. We have described [7] a learning problem in\nwhich the effect of consistency with prior knowledge dominates the\neffect of concept complexity in human learners. Further work [8]\nexpands upon this finding.  In particular, in addition to the learning\nrate, this paper shows that prior knowledge influences the attributes\nthat subjects attend to and the types of hypotheses formed. In\naddition, this publication shows that the type of knowledge used by\nhuman subjects cannot easily be encoded into the domain theory used by\nexplanation-based learning.  A different encoding of knowledge, an\ninfluence theory, is proposed.  In such a theory, the influence of\nseveral factors is known, but a domain theory does not specify a\nsystematic means of combining the factors. In [9] we extend this work\nand evaluate a competing model based upon weighting of features\nrather than consistency with prior knowledge.  It is shown that\nfeature weighting models cannot account for certain complex concepts\nin which two individual features individually exert a positive\ninfluence on an outcome but collectively exert a negative influence.\n<P>\n[8] Pazzani, M., & Schulenburg, D. (1989). The influence of\nprior theories on the ease of concept acquisition.\nProceedings of the Eleventh Annual Conference of the Cognitive\nScience Society (pp. 812-819).  Ann Arbor, MI- Lawrence Erlbaum.\n<P>\n[9] Pazzani, M. (1991). \n<A HREF=\"ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Pazzani-JEPLMC91-Knowledge.ps\">The influence of prior knowledge on concept acquisition: Experimental and computational results.</A>  Journal of Experimental Psychology- Learning,\nMemory & Cognition, 17, 3,  416-432.\n<A HREF=\"http://www.ics.uci.edu/AI/ML/MLAbstracts.html#Pazzani-JEPLMC91-Knowledge\">Abstract</A>\n\n<P>\n[10] Pazzani, M., & Silverstein, G. (1990). Feature selection and\nhypothesis selection models of induction. Proceedings of the Twelfth\nAnnual Conference of the Cognitive Science Society (pp.  221-228).\nCambridge, MA- Lawrence Erlbaum.\n\n<p> A more recent paper <A href=\"CogSci95.html\">Learning Sets of\nRelated Concepts: A Shared Task Model</a> by Tim Hume and Michael J.\nPazzani from the 1995 Cognitive Science Conference is available in\nHTML format.\n\n<P>\n\n<P> <h3>The development of learning algorithms</h3> <P> I have also\nbeen involved with the creation of practical learning algorithms.\nRecently [11,12] several of my graduate students (Cliff Brunk, Kamal\nAli and Glenn Silverstein) and I have created a significant extension\nto Ross Quinlan's FOIL program.  FOIL is an empirical learning program\nthat uses an information-based evaluation function to learn Horn\nClause concepts.  I have constructed a compatible explanation-based\nlearning program that uses the same information-based metric to guide\nthe proof process.  The resulting combined system (FOCL) has been\nshown to take advantage of incomplete and incorrect domain theories.\nIn addition, the effect of other kinds of background knowledge, such\nas typing and commutative relationships, was considered both\nempirically and analytically. FOCL is novel in that the integration\nbetween empirical and explanation-based learning is tighter than that\nof previous systems.  Both the explanation-based and empirical\nlearning components serve the same purpose (adding literals to a\nclause under construction) and use the same evaluation function.\nFurthermore, the domain theory used by the explanation-based program\ndefines relations that can be used by the empirical program.  We have\nalso investigated learning in this framework when the training data is\nincorrectly classified [13].  <P> [11] Pazzani, M., & Kibler, D.\n(1992).  The role of prior knowledge in inductive learning. Machine\nLearning, 9, 54-97.  \n<P> [12] Pazzani, M., Brunk, C., & Silverstein,\nG. (1991).  A knowledge-intensive approach to learning relational\nconcepts.  Proceedings of the Eighth International Workshop on Machine\nLearning (pp. 432-436). Evanston, IL- Morgan Kaufmann.  \n<P> [13]\nBrunk, C., & Pazzani, M. (1991).  <A\nHREF=\"ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Brunk-MLW91-FOCL-Noise.ps\">An investigation of noise-tolerant relational\nconcept learning algorithms.</A> Proceedings of the Eighth\nInternational Workshop on Machine Learning (pp. 389-391). Evanston,\nIL- Morgan Kaufmann. <A HREF=\"http://www.ics.uci.edu/AI/ML/MLAbstracts.html#Brunk-MLW91-FOCL-Noise\">Abstract</A> <P>\n\n\nInformation about getting a copy of <A HREF=\"http://www.ics.uci.edu/AI/ML/FOCL.html\">FOCL</A>.\n\n<P>\n\n<h3>Applications of learning methods </h3>\nA portion of my research has focused on adapting existing\nlearning methods to problems in areas such as engineering and\npolitical science.   For example, [14] describes an application of\nexplanation-based learning to a problem  of identifying\nconditions that are indicative of a component failure in the\nattitude control system of the DSCS-III satellite.  This paper\ndescribes a method for learning diagnosis heuristics (i.e., rules\nthat encode associations between data values in a telemetry\nstream) from information contained in a qualitative model of the\nsatellite. The application of a learning method to data on foreign\ntrade negotiations is discussed in [15].\n<P>\n[14] Pazzani, M. (1989). Learning fault diagnosis heuristics from\ndevice descriptions. In Y. Kodratoff & R. Michalski (Eds.), Machine\nLearning- An artificial intelligence approach (Volume III). Los Altos,\nCA- Morgan Kaufmann.\n<P>\n[15] Cain, T., Pazzani, M., & Silverstein, G. (1991). Using domain\nknowledge to influence similarity judgments.  Case-Based Reasoning\nWorkshop. Washington, DC- Morgan Kaufmann.\n\n<h2>Research Talks</h2><ul>\n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/AAAI96/sld001.htm>Syskill & Webert: Identifying interesting web sites</a> AAAI96\n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/BSEJ/sld001.htm> Constructive Induction of Cartesian Product Attributes</a> ISIS96\n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/CogSci94/sld001.htm>The Role of Existing Knowledge in Generalization </a> CogSci94\n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/CogSci95/sld001.htm> Learning Sets of Related Concepts: A Shared Task Model </a> CogSci95\n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/Hill/sld001.htm>Learning as hill-climbing search </a> \n<li><a href=http://www.ics.uci.edu/~pazzani/Slides/MLC94/sld001.htm> Reducing Misclassification Costs</a> MLC94\n   </ul>\n\n<HR>\n\n<P>\n\n<ADDRESS>\n<A HREF=\"http://www.ics.uci.edu/~pazzani\">Michael J. Pazzani</A><br>\n<A HREF=\"http://www.ics.uci.edu/\">Department of Information and Computer Science,</A><br>\n<A HREF=\"http://www.uci.edu/\">University of California, Irvine</A><br>\nIrvine, CA 92697-3425 <br>\n<A href=\"mailto:pazzani@ics.uci.edu\">pazzani@ics.uci.edu </A>\n</ADDRESS>\n</BODY></HTML>\n", "encoding": "ascii"}