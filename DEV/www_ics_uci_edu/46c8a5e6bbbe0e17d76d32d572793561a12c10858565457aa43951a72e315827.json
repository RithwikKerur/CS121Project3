{"url": "https://www.ics.uci.edu/~eppstein/260/011023/", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>ICS 260: Lecture notes on optimal triangulation</title>\n</head>\n<body bgcolor=\"#ffffff\" text=\"#000000\">\n<h1><a href=\"/~eppstein/260/\">ICS 260, Fall 2001</a>: Lecture notes\nfor October 23<br>\nDynamic programming and optimal triangulation</h1>\n\n<h2>1. Three seemingly unrelated problems.</h2>\n\n<dl>\n<dt><b>Optimal scheduling of matrix multiplications</b><br>\n</dt>\n\n<dd>\n<p> Matrices (two-dimensional arrays) can be multiplied by the\nstandard formulas from linear algebra: if A*B=C, then C[i,k] = sum\nA[i,j]*B[j,k]. If A has dimensions x*y, and B has dimensions y*z,\nthen the time for performing these sums in the straightforward way\nis O(x*y*z). There are more efficient but more complicated matrix\nmultiplication algorithms, but they're not really relevant for\ntoday's lecture. Matrix multiplication has many applications\nincluding computer graphs and relevance ranking for information\nretrieval.</p>\n\n<p> Now, suppose that you have more than two matrices to multiply.\nMatrix multiplication obeys the associative law A*(B*C)=(A*B)*C, so\nthere's more than one order in which to do the multiplication.\nHowever, the amount of time to do it might be very different. If,\nsay, A is 10*1000, B is 1000*4, and C is 4*200, then multiplying\nA*(B*C) would take rougly 10*1000*200&nbsp;+&nbsp;1000*4*200\noperations, while multiplying the other way as (A*B)*C would give\nthe much smaller number 10*1000*4&nbsp;+&nbsp;10*4*200.</p>\n\n<p> More generally, you could be given a sequence of <i>n</i>\nmatrices M<sub>i</sub> to multiply, for i ranging from 1 to <i>\nn</i>, where the sizes of the matrices are given by a\none-dimensional array D: each M<sub>i</sub> has dimensions\nD[i-1]*D[i]. If you could spend a small amount of time planning the\noptimal order to multiply these matrices, this could lead to a big\nsavings in the time to actually perform these multiplications.\nHowever, the number of different multiplication orderings grows\nexponentially in <i>n</i>, so it would take too long to examine\neach one and determine its operation count. How can we find the\noptimal multiplication ordering in an amount of time measured by a\nslowly growing function of <i>n</i>?</p>\n</dd>\n\n<dt><b>Optimal binary search tree construction</b><br>\n</dt>\n\n<dd>\n<p> Suppose you have a situation where you will need to match input\ntext against a list of keywords. A standard technique for this is a\nbinary search tree: a tree in which each internal node represents\nsome string comparison, and each leaf represents one of the\nkeywords we are searching for. We perform a search by following\ndown a path in this tree, performing the indicated comparison at\neach node, until we reach a leaf. To keep things simple for this\nlecture, I'm going to assume that all comparisons are of type (a\n<u>&lt;</u> b) or (a &lt; b) rather than allowing equality tests or\nthree-way comparisons, and that we will only search for strings\nthat are already in our list of keywords. Here's a simple\nexample:</p>\n\n<br>\n<div align=\"center\"><img src=\"searchtree.png\" width=533 height=151\nalt=\"balanced search tree example\"></div>\n\n<p> The time to search for a given word is just the length of the\npath from that word to the root; in this example, all words have\nthe same height, and all require three comparisons per search.\nHowever, if we have some idea of the frequencies with which we'll\nencounter each word, we can do better. The average search time\nwould then be sum p(i)*h(i) where p(i) is the frequency or\nprobability of seeing word i and h(i) is its height in the tree. If\nsome words (like apple) have relatively high frequencies, and\nothers (like cherimoya) have low ones, we can reduce the average\ntime by moving the high-frequency words higher in the tree, at the\nexpense of increasing the path lengths for the low-frequency\nwords:</p>\n\n<br>\n<div align=\"center\"><img src=\"unbalst.png\" width=533 height=241\nalt=\"unbalanced search tree example\"></div>\n\n<p> So, if we are given a sequence of words, and the frequencies of\neach word, how can we come up with an optimal search tree, one that\nhas the minimum possible value of sum p(i)*h(i)?</p>\n</dd>\n\n<dt><b>Optimal triangulation of simple polygons</b><br>\n</dt>\n\n<dd>\n<p> The finite element method is a technique for solving numerical\nproblems such as stress or heat flow simulations, that involves\ndividing an input shape into simple elements such as triangles,\nsetting up a set of linear equations describing the relations\nbetween the simulated quantities in each element, and solving these\nequations. The solution time and accuracy both depend on the\nquality of the triangulation: generally, it is desired that the\ntriangles be as close to equilateral as possible.</p>\n\n<br>\n<div align=\"center\"><img src=\"triangulated.png\" width=364 height=221\nalt=\"triangulated polygon\"></div>\n\n<p> There has been a lot of work in the finite element mesh\ngeneration community on how to measure mesh quality. One commonly\nused measure for the quality of a single triangle is its area\ndivided by the sum of the squares of its three edge lengths. The\ngoal would then be to find a triangulation in which every triangle\nhas quality greater than some threshhold Q, and in which Q is as\nlarge as possible; that is, we are trying to maximize the minimum\nquality of any triangle.</p>\n\n<p> A closely related problem of more theoretical interest is to\nfind the triangulation maximizing the sum of the lengths of all the\nedges. One of the reasons this is interesting is that, when the\ninput is just a set of vertices and the triangulation must cover\ntheir convex hull, we do not know the problem's complexity: we\ndon't have a polynomial time algorithm, nor a proof that it is\nhard. However, for simple polygons (polygons without holes, in\nwhich the triangulation vertices are only allowed to lie on polygon\nvertices) both of these optimal triangulation problems can be\nsolved by dynamic programming.</p>\n</dd>\n</dl>\n\n<h2>2. Abstract formulation</h2>\n\n<blockquote>\n<p>By viewing these problems more abstractly, we can view these\nproblems in a common framework. We take the point of view of the\nthird problem, optimal triangulation, but now we restrict our\nattention to triangulations of regular polygons. We assume that\neach possible triangle T in a triangulation has a quality q(T), and\nthat we have a binary operation @ that lets us combine the quality\nof several triangles. We seek the triangulation of the regular <i>\nn</i>-gon that minimizes or maximizes\nq(T0)&nbsp;@&nbsp;q(T1)&nbsp;@&nbsp;q(T2)&nbsp;@&nbsp;... where T0,\nT1, T2, etc., denote the triangles in our triangulation.</p>\n\n<br>\n \n\n<div align=\"center\"><img src=\"regtri.png\" width=361 height=354\nalt=\"regular polygon triangulation\"></div>\n\n<p>In order for this to make sense independent of the indexing of\nthe triangles, the binary operation @ should be commutative and\nassociative. In all our examples, @ will be one of the sum,\nminimum, or maximum operations.</p>\n</blockquote>\n\n<h2>3. Relation of abstraction to initial problems</h2>\n\n<dl>\n<dt><b>Optimal triangulation of simple polygons</b><br>\n</dt>\n\n<dd>\n<p> It is most easy to see how this abstract problem matches the\noptimal simple polygon triangulation problem. Each vertex of the\nsimple polygon can correspond one-for-one with a vertex of a\nregular polygon, and this correspondence takes triangulations of\nthe simple polygon to triangulations of the regular polygon. For\nthe case of the max-min-quality triangulation, we can measure the\nquality of a regular polygon triangle by examining the quality of\nthe corresponding triangle in the input, however if a triangle\nwould be disallowed because it crosses the input polygon's\nboundary, we give the corresponding regular polygon triangle\nquality zero. We let @=min and seek to maximize\nq(T0)&nbsp;@&nbsp;q(T1)&nbsp;@&nbsp;q(t2)&nbsp;@&nbsp;... which is\njust the minimum quality of any triangle in triangulation of the\ninput, or zero when the regular polygon triangulation does not\ncorrespond to a triangulation of the input.</p>\n\n<p> Similarly, to represent the minimum weight triangulation\nproblem, we let q(T) be the perimeter (or +infinity if the regular\npolygon triangle corresponds to a triangle crossing the boundary of\nthe input polygon), let @=sum, and seek to minimize\nq(T0)&nbsp;@&nbsp;q(T1)&nbsp;@&nbsp;q(t2)&nbsp;@&nbsp;..., which is\njust the perimeter of the input polygon plus twice the weight of\nthe chosen diagonals.</p>\n</dd>\n\n<dt><b>Optimal binary search tree construction</b><br>\n</dt>\n\n<dd>\n<p> To see how this regular polygon triangulation problem\ncorresponds to construction of optimal binary search trees, draw\nthe search tree words in order around the polygon's sides, except\nfor the top side which will correspond to the tree root. Form a\ntree by placing a node in each triangle and connecting it across\nthe three sides of the triangle to neighboring nodes or to the\nwords outside the polygon.</p>\n\n<br>\n<div align=\"center\"><img src=\"tri2bst.png\" width=445 height=388\nalt=\"binary search tree from triangulation\"></div>\n\n<p> Conversely, from any search tree we could reverse this process\nand form a triangulation. The quality of a search tree is the sum\nof the frequencies of the words under each node: @=sum, q(T)=sum of\nfrequencies of sides below the top edge of T in the triangulation.\nTo find an optimal binary search tree, we should look for the\ntriangulation minimizing\nq(T0)&nbsp;@&nbsp;q(T1)&nbsp;@&nbsp;q(t2)&nbsp;@&nbsp;...</p>\n</dd>\n\n<dt><b>Optimal scheduling of matrix multiplications</b><br>\n</dt>\n\n<dd>\n<p> To view matrix multiplication ordering as an optimal\ntriangulation problem, label the sides of the regular polygon\n(except the top) with the matrix names, and the vertices of the\npolygon with the matrix dimensions. Label each diagonal of a\ntriangulation with the product of the two labels on the triangle\nbelow it (below meaning opposite from the top edge).</p>\n\n<br>\n<div align=\"center\"><img src=\"mmtri.png\" width=416 height=410\nalt=\"matrix multiplication ordering from triangulation\"></div>\n\n<p> The top edge will end up labeled with a formula for multiplying\nall the matrices together. Conversely, from any such formula we can\ndetermine a triangulation. To count the number of operations for\nmatrix multiplication, we should simply let @=sum and q(T) be the\nproduct of the three numbers written at the corners of each\ntriangle T.</p>\n</dd>\n</dl>\n\n<h2>4. Dynamic programming solution</h2>\n\n<blockquote>\n<p>Recall that in any dynamic programming problem we want to\nidentify a set of subproblems to solve with the following\nproperties:</p>\n\n<ul>\n<li>There is only a small number of distinct subproblems.</li>\n\n<li>The original problem, and each subproblem, can be solved\nquickly once we know the values of certain smaller\nsubproblems.</li>\n</ul>\n\n<p>The algorithm works by solving all the subproblems in a\nsystematic order, using an array to store all the subproblem\nvalues. For these optimal triangulation problems, we define a\nsubproblem to be the optimal triangulation of the smaller polygon\nlying below one of the diagonals of the original polygon. A\nsubproblem can be indexed by a pair of numbers: the positions of\nthe vertices on either end of the diagonal (these are the same\nnumbers we used to index array D in the matrix multiplication\nproblem). Each subproblem can be solved by trying all possibilities\nfor its top triangle, using the precomputed values of the smaller\nsubproblems on either side of the top triangle.</p>\n\n<p>In the pseudocode below, the subproblem values are stored in an\narray V[i,j]. The quality of a triangle is measured by a function\nq(i,j,k), where i, j, and k are the positions of the three triangle\ncorners. We assume we are trying to minimize\nq(T0)&nbsp;@&nbsp;q(T1)&nbsp;@&nbsp;q(t2)&nbsp;@&nbsp;... as in\nmost of the applications above; it is trivial to modify the code to\nmaximize it instead (as in the mesh generation application). We use\nPython syntax, with minor modifications such as the use of the @\noperation and a more intuitive syntax for loops over integers\n(\"for x < i < y\" means that i takes on all integer values that make the\ncomparisons true). The\nconstant \"id\" denotes the identify element for the @ operation:\nzero if @ is addition, -infinity if @ is maximization.</p>\n\n<pre>\nfor n > i >= 0:\n    for i < j <= n:\n        if i == j-1:\n            V[i,j] = id\n        else:\n            V[i,j] = min([q(i,k,j) @ V[i,j] @ V[j,k] for i < k < j])\n</pre>\n\n<p>The last line of the pseudocode constructs a list of j-i-1\nvalues and finds the minimum value in the list. Here it is again\nwith that line expanded out into a loop of constant-time\noperations:</p>\n\n<pre>\nfor n > i >= 0:\n    for i < j <= n:\n        if i == j-1:\n            V[i,j] = id\n        else:\n            V[i,j] = infinity\n            for i < k < j:\n                V[i,j] = min(V[i,j], q(i,k,j) @ V[i,k] @ V[k,j])\n</pre>\n\n<p>As a student pointed out in class, the outer loop needs to run\nbackwards so that all the smaller V[x,y] values will be available\nwhen needed. Since the algorithm consists of three nested loops, each of\nwhich iterates at most <i>n</i> times, the total time is\nO(<i>n</i><sup>3</sup>). As in the <a href= \n\"/~eppstein/161/960229.html\">longest common subsequence\nproblem</a>, this code merely computes the value of the optimal\nsolution; to find the solution itself involves a simple linear-time\nbacktracking procedure through the computed array values.</p>\n</blockquote>\n</body>\n</html>\n\n", "encoding": "ascii"}