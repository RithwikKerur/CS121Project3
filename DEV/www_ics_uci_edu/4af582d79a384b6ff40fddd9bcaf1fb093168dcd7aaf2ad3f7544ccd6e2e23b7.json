{"url": "https://www.ics.uci.edu/~theory/269/051202.html", "content": "<!DOCTYPE html PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html>\n<head>\n<title>Theory Seminar, December 2, 2005</title>\n</head>\n<body>\n<a href=\"/~theory/\"><img src=\"/~theory/logo/shortTheory.gif\" width=\" \n521\" height=\"82\" border=\"0\" alt=\"ICS Theory Group\"></a> \n\n<h2>ICS 269, Fall 2005: Theory Seminar</h2>\n\n<h1>Measure and Conquer: Domination -- A Case Study</h1>\n\n<h2>Jeremy Yu Meng</h2>\n\n<h3>December 2, 2005, in CS 259</h3>\n\n<p>Abstract:</p>\n\n<p>This talk will be a presentation of a paper\nby Fedor V. Fomin, Fabrizio Grandoni, and Dieter Kratsch from ICALP 2005.</p>\n\n<p>Davis-Putnam-style exponential-time backtracking algorithms are the most\ncommon algorithms used for finding exact solutions of NP-hard\nproblems. The analysis of such recursive algorithms is based on the\nbounded search tree technique: a measure of the size of the subproblems\nis defined; this measure is used to lower bound the progress made by the\nalgorithm at each branching step.  For the last 30 years the research on\nexact algorithms has been mainly focused on the design of more and more\nsophisticated algorithms. However, measures used in the analysis of\nbacktracking algorithms are usually very simple. In this paper we stress\nthat a more careful choice of the measure can lead to significantly\nbetter worst case time analysis.  As an example, we consider the minimum\ndominating set problem. The currently fastest algorithm for this problem\nhas running time O(2<sup>0.850n</sup>) on n-nodes graphs. By measuring the\nprogress of the (same) algorithm in a different way, we refine the time\nbound to O(2<sup>0.598n</sup>). A good choice of the measure can provide such a\n(surprisingly big) improvement; this suggests that the running time of\nmany other exponential-time recursive algorithms is largely\noverestimated because of a \"bad\" choice of the measure.</p>\n\n</body>\n</html>\n", "encoding": "ascii"}