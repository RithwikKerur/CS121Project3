{"url": "https://www.ics.uci.edu/~dechter/publications/r116.html", "content": "  <html>\r\n    <head>\r\n      <title>\r\n        Dr. Rina Dechter @ UCI\r\n      </title>\r\n      <LINK REL=\"Stylesheet\" HREF=\"/~dechter/basic.css\">\r\n    </HEAD>\r\n\r\n  <BODY bgcolor=\"#ffffff\" alink=\"00aaaa\" link=\"008080\" vlink=\"008080\">\r\n\r\n  <!-- Begin Header -->\r\n    [an error occurred while processing this directive]\r\n  <!-- End Header -->\r\n\r\n\r\n  <!-- Begin Body -->\r\n\r\n  <br><br><center>\r\n<table width=90%>\r\n<tr>\r\n<td class=title>Publications & Technical Reports</td>\r\n<tr>\r\n  <td colspan=2><img width=\"100%\" height=\"2\"  src=\"/~dechter/images/black-fill.gif\"></td>\r\n</tr>\r\n</tr>\r\n</table>\r\n</center> \r\n<center>\r\n<table width=\"80%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n<tr valign=top>\r\n<td><b>R116</td>\r\n | \r\n<br></td>\r\n</tr>\r\n\r\n<tr>\r\n<td colspan=2>\r\n<div class=title>Approximation Algorithms for Probabilistic Reasoning:\r\nSampling and Iterative Inference</div>\r\n\r\n<TT>Bozhena Bidyuk</tt>\r\n\r\n<div class=abstract>\r\n<b>Abstract</b><BR> The complexity of the exact inference increases exponentially with size and complexity of the network. As a result,\r\nthe exact inference methods become impractical for large networks and we seek to approximate the results. A\r\nvariety of approximation methods exist. This research focuses on two approximation methods for finding posterior\r\nmarginals P(xije) in Bayesian networks: iterative belief updating (defined by Pearl [Pearl 1988]) and sampling.\r\n<br>\r\nThe belief updating is an exact inference method for singly-connected networks. It can be applied to loopy networks\r\nto obtain approximate answers. The algorithm is based on message passing: in some order, each node computes and\r\nsends messages to its neighbors incorporating the latest messages it recieved. In a singly-connected network, we can\r\norder nodes so that it will be sufficient for each node to pass one messages in each direction. In a loopy network, the\r\nnodes compute several iterations of messages to achieve convergence (or to demonstrate the lack of convergence).\r\nThus, belief updating in loopy networks is often referred to as Iterative Belief Propagation or IBP. Although IBP\r\ngenerally computes only approximate answers, it is known to perform extremely well in several special classes of\r\nnetworks such as coding networks and noisy-or networks. At the same time, we know that in some instances IBP\r\ndoes not converge or generates approximate answers far from correct. Currently, we do not have any methodology\r\nthat would allow us in general case to predict the convergence of IBP or provide some practical error bounds on\r\nthe approximate marginals it computes. In this research work, we examine the influence of the \u000f-cutset criteria\r\non the convergence and quality of approximate marginals computed by IBP. We conjecture the \u000f-cutset (defined\r\nas a cycle-cutset with extreme posterior marginals) has effect similar to an observed cycle-cutset which breaks the\r\nloops and leaves the network singly-connected. We prove that the conjecture is true for Bayesian networks without\r\nevidence and show that the error in the approximate marginals computed by IBP converges to 0 as \u000f tends to 0. We\r\nprovide empirical support for instances of Bayesian networks with evidence.\r\n<br>\r\nThe idea behind the sampling methods for Bayesian networks is to generate a set of samples (where a sample in a\r\nvector space X = fX1; :::;XNg is just an assignment of values to the elements of vector X) and then estimate the\r\nposterior marginals of interest from samples. In general, the quality of the approximate answers depends primarily\r\non the number of samples generated and the approximate values converge to the exact values as number of samples\r\nincreases. However, the sampling variance increases with the size of the sampling space. In this research work, we\r\nfocus on the the variance reduction techniques on the example of the Gibbs sampler for Bayesian networks. It is obvious\r\nthat we can achieve the reduction in variance by sampling only a subset of variables. However, the implication\r\nis that we have to carry out a lot more analytical computations which may render the whole approach impractical.\r\nWe demonstrate that we can reduce sampling space efficiently if we take into consideration the underlying network\r\nstructure. The time/space complexity of the exact inference in Bayesian networks is exponential in the induced\r\nwidth of the graph. In our sampling scheme, called w-cutset sampling, we sample a subset of variables (called a\r\ncutset) that is carefully chosen to reduce the complexity of the graph bounded by the induced width w. We analyze\r\nthe problem of finding an optimal w-cutset of a graph (NP-hard in general case) and provide a heuristic algorithm\r\nfor finding w-cutset in practice. We show empirically that w-cutset sampling typically finds better approximate\r\nanswers than standard Gibbs sampler for a range of w values although its performance eventually deteriorates as w\r\nincreases.\r\n</div>\r\n<P>\r\n<A href=\"r116.pdf\"><b>PDF</b></a> <br>\r\n<!-- <A href=\"http://www.ics.uci.edu/~csp/r110.ps\"><b>PS</b></a> -->\r\n\r\n</td></tr></table></center>\r\n<br><br>\r\n\r\n\r\n<!-- End Body-->\r\n\r\n<!--- Begin Footer -->\r\n     <div id=\"footer\"><centeR>\r\n<A HREF=\"http://www.ics.uci.edu\">School of Information and Computer Science</A>\r\n<A HREF=\"http://www.uci.edu\">University of California, Irvine, CA 92697-3435</a>\r\n<A HREF=\"http://www.ics.uci.edu/~dechter\">Dr. Rina Dechter</A>\n\r\n<A HREF=\"mailto:dechter_at_ics.uci.edu\">dechter at ics.uci.edu</A>\r\n\n</center></div>\r\n<!--- End Footer -->\r\n</body>\r\n<html>\r\n\r\n</body>\r\n<html>\r\n", "encoding": "ascii"}