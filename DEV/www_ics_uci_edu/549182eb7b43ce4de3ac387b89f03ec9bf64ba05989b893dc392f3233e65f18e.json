{"url": "https://www.ics.uci.edu/~eppstein/161/960201.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Deterministic selection</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for February 1, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Graph algorithms</h1>\n\n<a name=\"graph\">What is a graph?</a> \n\n<p>It's an abstract notion, used to represent the idea of some kind\nof connection between pairs of objects. A graph consists of:</p>\n\n<ul>\n<li>A collection of \"vertices\", which I'll usually draw as small\ncircles on the blackboard, and</li>\n\n<li>A collection of \"edges\", each connecting some two vertices.\nI'll usually draw these as curves on the blackboard connecting the\ngiven pair of vertices.</li>\n</ul>\n\nFor this definition it doesn't matter what the vertices or edges\nrepresent -- that will be different depending on what application\nthe graph comes from. It also doesn't matter how I draw the graph,\nthe only part that matters is which pairs of vertices are connected\nwith each other. \n\n<p> There are two different types of graph that we'll commonly\nsee.</p>\n\n<ul>\n<li style=\"list-style: none\"><a name=\"ug\"></a></li>\n\n<li>In one type, known as an <i>undirected graph</i> it doesn't\nmatter which end of the graph is which -- the relation between the\ntwo is symmetric. In that case I'll just draw the edges as an\nundecorated curve. \n\n<p>For instance, suppose we draw a graph with one vertex for every\nperson in the U.S., and one edge connecting any two people who have\nshaken hands with each other. If I've shaken hands with you, you've\nshaken hands with me, so each edge is symmetric. It seems to be\ntrue that graphs like this have very small {\\em diameter}: you can\nconnect any two people with a very short chain of handshakes.</p>\n\n<p><a name=\"dg\"></a></p>\n</li>\n\n<li>In the other type of graph, known as a <i>directed graph</i>,\nan edge goes from one of the vertices, towards the other. In this\ncase I'll draw an arrowhead at the vertex towards which the edge is\ngoing. If we drew a graph like the handshake graph described above,\nbut instead connected two people if one had written a letter to the\nother, the result would be directed: if I've written a letter to\nyou, you may not have written a letter back to me.</li>\n</ul>\n\n<p>Whenever we talk about graphs, we'll use <i>n</i> to denote the\nnumber of vertices, and <i>m</i> to denote the number of edges.\nThis notation is confusing, but I'll stick with it because it's\nvery standard. We'll also use V(G) to denote the set of vertices of\na graph G, and E(G) to denote the set of edges.</p>\n\n<h2>Some more examples of graphs</h2>\n\n<ul>\n<li style=\"list-style: none\"><a name=\"acyc\"></a></li>\n\n<li>Any tree is a graph. In fact trees can be defined as the\nundirected graphs that are connected (there is a path between any\ntwo vertices) and acyclic (there aren't any sequences of edges that\ngo around in a loop). (It is also possible to define trees in terms\nof directed graphs.) Any connected graph has at least <i>n</i>-1\nedges, and any acyclic graph has at most <i>n</i>-1 edges, so any\ntree has exactly <i>n</i>-1 edges. \n\n<p><a name=\"compgraph\"></a></p>\n</li>\n\n<li>In the last lecture, we saw a notation for decision trees in\nwhich I connected two circles (representing objects to be sorted)\nby a downward edge if a comparison had been done and had found that\none was greater than the other. This is an example of a directed\ngraph. \n\n<p>The same comparison graph notation is useful in solving homework\n3.19, which asked you to find an element among the smallest <i>\nk</i> values among an <i>n</i>-value set. A simple method which\nuses exactly <i>n</i>-<i>k</i> comparisons is just to find the\nminimum of the first <i>n</i>-<i>k</i>+1 values in the list. To\nprove this is optimal, suppose we have found some value <i>x</i>\nthat we know to be in the smallest <i>k</i> values. Therefore, we\nmust be able to show from the comparisons we have made that at\nleast <i>n</i>-<i>k</i> other values are larger than it. So if we\ndraw the comparison graph of the comparisons we've made, <i>x</i>\nmust be part of a connected region of <i>n</i>-<i>k</i>+1 values\n(counting <i>x</i> itself), so this region has to have at least <i>\nn</i>-<i>k</i> edges and we must have made that many\ncomparisons.</p>\n\n<p><a name=\"famtree\"></a></p>\n</li>\n\n<li>Our third example of a graph is a \"family tree\". We draw a\nvertex for each member of a family, and a directed edge from each\nparent to each child. This is not really a tree [exercise: give two\nreasons why not] but in many families has a structure similar to a\ntree. Typical graph algorithm problems for this example would be to\ndetermine how two people are related, or to draw this graph as a\nnice picture of the family tree. \n\n<p><a name=\"air\"></a></p>\n</li>\n\n<li>We can represent airline schedules as a graph, in which the\nvertices correspond to airports, and edges correspond to flights.\nUnlike the previous examples, there is some extra information here\nthat we can attach to the graph; for instance we might store the\ntime, cost, and airline for each flight. A typical graph algorithm\nproblem here might be to find the cheapest route from city A to\ncity B, allowing stops in between. \n\n<p><a name=\"vlsi\"></a></p>\n</li>\n\n<li>A VLSI circuit design consists of a collection of components\n(gates or transistors) connected by wires. We can think of the\ncomponents as vertices, and the wires as edges. A typical problem\nis to translate the list of electrical connections into the design\nfor a chip. \n\n<p><a name=\"pcb\"></a></p>\n</li>\n\n<li>Another problem in electronics comes from printed circuit board\nmanufacturing. A PC board is made of fiberglass, with wires\nconnecting places for chips to be plugged in. One step of making PC\nboards involves drilling holes where the chips go, and other holes\nconnecting wires in different layers of the board. We want to be\nable to plan the movement of a drill from one hole to the next, so\nit doesn't spend too much time moving. \n\n<p>We can make a graph, in which the vertices represent holes, and\neach two vertices are connected by an edge labeled with the amount\nof time it would take to move the drill from one hole to the other.\nThis is a <i>complete graph</i> since it has every possible edge\nthat could be there. In this representation, our drill scheduling\nproblem becomes one of finding the shortest path through this graph\nthat visits every vertex exactly once. This is a famous problem,\nthe <i>traveling salesman problem</i>. As we will see much later in\nthe class, this is an example of an <i>NP-complete problem</i>,\nwhich is evidence that there doesn't exist a good algorithm to\nsolve it exactly. However you can find a path that's pretty close\nto the optimal solution, using <i>minimum spanning trees</i> (to be\nexplained next week).</p>\n\n<p><a name=\"reg\"></a></p>\n</li>\n\n<li>In compiler design, one problem that a compiler must deal with\nis, for each variable in your code, to find a machine register that\ncan store the value of that variable. Typically you can't just\nassign them one-to-one because there are likely to be more\nvariables than registers. But if two variables are used in\ndifferent parts of code, it's safe to combine them and store them\nboth in the same register. We can draw a graph in which variables\ncorrespond to vertices, and an edge shows that two variables exist\nat the same time. Then register assignment becomes a <i>graph\ncoloring</i> problem. Again this is NP-complete but reasonable\nheuristics are known for the graphs occurring in this application. \n\n<p><a name=\"sched\"></a></p>\n</li>\n\n<li>Our final example comes from task scheduling -- suppose you are\nmanaging a project consisting of a sequence of several tasks to be\nperformed (the example I used in class was writing, revising,\nhanding out and grading a midterm; another comes from compiler\ndesign again: determining what order to perform a sequence of\ninstructions). Some tasks have to be performed before others, but\nit is not always the case that there is one fixed global order that\nyou have to do everything in. Here we can draw a graph in which a\nvertex represents a task, and a directed edge says we have to do\none task before the other. This is pretty similar in some ways to\nthe comparison graph we drew when analyzing decision trees. One\nimportant graph problem (<i>topological sorting</i> consists of\nfinding some global ordering consistent with these local\nconstraints. Another is determining, if you had enough people to\nwork on as many different tasks as possible at once, how much time\nwould it take to do the whole job; this is equivalent to finding a\n<i>longest path</i> in the graph. We'll see later a longest path\nalgorithm that uses topological sorting as an important\nsubroutine.</li>\n</ul>\n\nAs we have seen, all of these many problems can be reformulated as\ngraph problems. This hides the unnecessary complications of the\nproblem, making the important structure more obvious and making it\neasier to find a solution. Translation to a graph problem also has\nthe advantaged that several different real world problems could end\nup looking like the same graph problem, so finding a good algorithm\nfor that one problem could have many applications. \n\n<h2>Representation of Graphs</h2>\n\nIn order to perform graph algorithms in a computer, we have to\ndecide how to store the graph. When I talk about graphs to you,\nI'll draw a diagram with circles and lines on the blackboard, but\ncomputers aren't very good at interpreting that sort of input.\nInstead we need a representation closer to the abstract definition\nof a graph. There are several possibilities, with different uses. I\ndidn't go over all of them in the lecture (I left out the incidence\nlist and incidence matrix.) \n\n<ul>\n<li style=\"list-style: none\"><a name=\"obj\"></a></li>\n\n<li><b>Object oriented representation</b>. The most obvious thing\nto do is just to copy the definition I gave of a graph. Have some\nstructure for each vertex (representing whatever information you\nwant to store there), and another structure for each edge (with\npointers to the two vertices it connects). This representation is a\nlittle difficult to work with, because unless the edges are ordered\nmore carefully it will be difficult to find the ones you want. \n\n<p>As an example we might have a graph with four vertices (which\nI'll call A, B, C, and D) and four edges: (A,B), (C,D), (B,C),\n(C,A). The object oriented representation would just have a list or\narray of structures for each of these objects.</p>\n\n<p>The total space used by this representation is just O(m+n),\nsince there is a constant amount of space (one structure) per\nvertex or edge. Most operations in this representation involve\nscanning the whole list of edges, and take time O(m).</p>\n\n<p><a name=\"adjlist\"></a></p>\n</li>\n\n<li><b>Adjacency list</b> In the adjacency list representation,\neach vertex keeps a linked list of the neighboring vertices. The\nedges don't really appear at all. In the same graph above, the\nlists for each vertex would be: \n\n<pre>\n    A: B, C\n    B: A, C\n    C: D, B, A\n</pre>\n\nThis representation makes it much easier to find the edges\nconnected to any particular vertex. Its space is still also small:\nO(m+n), since the total length of all the lists is 2m (each edge\nappears twice, once for each of its endpoints). It is also quite\nfast for many applications. The slowest operation that might\ncommonly be used is testing whether a pair of vertices is connected\nby an edge; this has to be done by scanning through one of the\nlists, but you could speed it up by sorting the adjacency lists and\nusing binary search. Another disadvantage is that each edge is\nlisted twice, so if the edges carry any extra information such as a\nlength it may be complicated to keep track of both copies and make\nsure they have the same information. \n\n<p><a name=\"inclist\"></a></p>\n</li>\n\n<li><b>Incidence list</b>. By combining the adjacency list and the\nobject oriented representation, we get something with the\nadvantages of both. We just add to the object oriented\nrepresentation a list, for each vertex, of pointers to the edges\nincident to it. If I don't specify a representation, this is\nprobably what I have in mind. The space is a little larger than the\nprevious two representations, but still O(m+n). \n\n<p><a name=\"adjmat\"></a></p>\n</li>\n\n<li><b>Adjacency matrix</b>. In some situations we are willing to\nuse a somewhat larger data structure so that we can test very\nquickly whether an edge exists. We make an n x n matrix M[i,j],\nwith rows and columns indexed by vertices. If edge (u,v) present,\nwe put a one in cell M[u,v]; otherwise we leave M[u,v] zero.\nFinding the neighbors of a vertex involves scanning a row in O(n)\ntime, but to test if an edge (u,v) exists just look in that entry,\nin constant time. To store extra information like edge lengths, you\ncan just use more matrices. For the same graph above, we'd have a\nmatrix \n\n<pre>\n    0 1 1 0\n    1 0 1 0\n    1 1 0 1\n    0 0 1 0\n</pre>\n\nThis is occasionally useful for performing graph computations by\nlinear algebra. For instance, if you take the kth power of this\nmatrix, an entry will be nonzero if and only if the corresponding\nvertices are connected by a path of k edges. For undirected graphs,\nthe matrix will be symmetric. \n\n<p><a name=\"incmat\"></a></p>\n</li>\n\n<li><b>Incidence matrix</b>. This is another matrix, but generally\nit is rectangular rather than square (n x m); the rows are indexed\nby vertices and the columns by edges. Just like the adjacency\nmatrix, we put a one in a cell when the corresponding vertex and\nedge are incident. Therefore every column will have exactly two\nones in it. For a directed graph, you can make a similar matrix in\nwhich every column has one +1 and one -1 entry. This matrix is\nusually not symmetric. For the same graph, the incidence matrix is \n\n<pre>\n    1 0 1 0\n    1 1 0 0\n    0 1 1 1\n    0 0 0 1\n</pre>\n</li>\n</ul>\n\n<h2>Connectivity</h2>\n\nAs a warmup to graph algorithms, we'll start with a simple problem:\nis a graph connected? Is there always an airline route from A to B?\nIs everyone in the family tree related to everyone else? Can your\nscheduling task be split into two parts that can be done by\ndifferent departments of your company? \n\n<p>As a rough outline, we start with some vertex x, and build a\nlist of the vertices you can get to from x. Each time we find a new\nvertex to be added to this list, we check its neighbors to see if\nthey should be added as well. Finally, we check whether the list\ncovers the whole graph. In pseudocode:</p>\n\n<pre>\n    test-connected(G)\n    {\n    choose a vertex x\n    make a list L of vertices reachable from x,\n    and another list K of vertices to be explored.\n    initially, L = K = x.\n\n    while K is nonempty\n        find and remove some vertex y in K\n        for each edge (y,z)\n            if (z is not in L)\n            add z to both L and K\n\n    if L has fewer than n items\n        return disconnected\n    else return connected\n    }\n</pre>\n\nTo analyze the algorithm, first notice that the outer loop happens\nn times (once per vertex). The time for the inner loop (finding all\nunreached neighbors) is more complicated, and depends on the graph\nrepresentation. One key step (testing whether z is in L) seems like\nit might be slow, but can be done quickly by keeping a bit on each\nvertex that says whether it's in L or not. \n\n<ul>\n<li>For the object oriented representation, each execution of the\ninner loop involves scanning through all m edges of the graph. So\nthe total time for the algorithm is O(mn).</li>\n\n<li>For the adjacency matrix representation, each execution of the\ninner loop involves looking at a single row of the matrix, in time\nO(n). So the total time for the algorithm is O(n^2).</li>\n\n<li>In the adjacency list (or incidence list) representation, each\nelement on each list is scanned through once. So the total time on\nall executions of the inner loop is the same as the total length of\nall adjacency lists, which is 2m. Note that we don't multiply this\nby n, even though this is a nested loop -- we just add up the\nnumber of times each statement is executed in the overall\nalgorithm. The total time for the algorithm is O(m+n)</li>\n</ul>\n\nAt the end of the algorithm, the list L tells you one <i>connected\ncomponent</i> of the graph (how much of the graph can be reached\nfrom x). With some more care, we can find all components of G\nrather than just one component. \n\n<p>If graph is connected, we can modify the algorithm to find a\ntree in G covering all vertices (a <i>spanning tree</i>): For each\nz, let parent(z) be the vertex y corresponding to the time at which\nwe added z to L. This gives a graph in which each vertex except x\nis connected to some previous vertex, and any such graph must be a\ntree.</p>\n\n<p>When analyzing the algorithm, we didn't pay attention to what\norder we put vertices into K and removed them again, but different\norders produce different trees. If we put vertices at end of K,\ntake them off front (so K acts like a queue) we get \"breadth first\nsearch\". Then the parent of z is always as close to x as possible\nso this gives shortest paths from x to everything else, and the\ntree is short and bushy. If instead we add and remove vertices at\nthe same end of K (so K acts like a stack) we get \"depth first\nsearch\". This tends to produce long stringy spanning trees with\nsome useful properties that we'll see later.</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960201.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}