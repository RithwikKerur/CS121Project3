{"url": "https://www.ics.uci.edu/~dan/pubs/DC-Sec4.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 4</TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_4\">\n<H2> 4.  ADAPTIVE HUFFMAN CODING</H2> </a>\n\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tAdaptive Huffman coding was first conceived independently\nby Faller and Gallager [Faller 1973; Gallager 1978].  Knuth \ncontributed improvements to the original algorithm [Knuth 1985] and the \nresulting algorithm is referred to as algorithm FGK.\nA more recent version of adaptive Huffman coding\nis described by Vitter [Vitter 1987].  All of these methods are \ndefined-word schemes which determine\nthe mapping from source messages to codewords based upon \na running estimate of the source message probabilities.  The\ncode is adaptive, changing so as to remain optimal for the\ncurrent estimates.  In this way, the adaptive Huffman codes\nrespond to locality.  In essence, the encoder is \"learning\" the\ncharacteristics of the source.  The decoder must learn along\nwith the encoder by continually updating the Huffman tree so\nas to stay in synchronization with the encoder.\n<P>\nAnother advantage of these systems is that\nthey require only one pass over\nthe data.  Of course, one-pass methods are not very interesting if\nthe number of bits they transmit is significantly greater than that\nof the two-pass scheme.  Interestingly, the performance of these methods, in terms \nof number of bits transmitted, can be better than that of static\nHuffman coding.  This does not contradict the optimality of the\nstatic method as the static method is optimal only over all methods which\nassume a time-invariant mapping.  The performance of the adaptive\nmethods can also be worse than that of the static method.  Upper \nbounds on the redundancy of these methods are presented in this section.\nAs discussed in the introduction, the adaptive method of Faller, Gallager\nand Knuth is the basis for the UNIX utility <EM>compact</EM>.  The\nperformance of <EM>compact</EM> is quite good, providing typical compression\nfactors of 30-40%.\n\n<a name=\"Sec_4.1\">\n<H3> 4.1  Algorithm FGK</H3> </a>\n\n\tThe basis for algorithm FGK is the Sibling Property, defined\nby Gallager [Gallager 1978]:  A binary code tree has the sibling\nproperty if each node (except the root) has a sibling and if the nodes\ncan be listed in order of nonincreasing weight with each node \nadjacent to its sibling.  Gallager proves that a\nbinary prefix code is a Huffman code if and only if the code tree \nhas the sibling property.  In algorithm FGK, both sender and receiver\nmaintain dynamically changing Huffman code trees.  The leaves of the\ncode tree represent the source messages and the weights of the\nleaves represent frequency counts for the messages.  At any point in\ntime, <VAR>k</VAR> of the <VAR>n</VAR> possible source messages have occurred in the\nmessage ensemble.  \n<P>\n<IMG SRC=\"DC-fig41.gif\" ALT=\"[FIGURE 4.1]\">\n<P>\nFigure 4.1 -- Algorithm FGK processing the ensemble \n<VAR>EXAMPLE</VAR> (a) Tree after processing \"<VAR>aa bb</VAR>\";\n11 will be transmitted for the next <VAR>b</VAR>.\n(b) After encoding the third <VAR>b</VAR>;\n101 will be transmitted for the next <VAR>space</VAR>;\nthe tree will not change;\n100 will be transmitted for the first <VAR>c</VAR>.\n(c) Tree after update following first <VAR>c</VAR>.\n<P>\n\tInitially, the code tree consists of a single\nleaf node, called the 0-node.  The 0-node is a special node used\nto represent the <VAR>n</VAR>-<VAR>k</VAR> unused messages.  For each message transmitted,\nboth parties must increment the corresponding weight and recompute\nthe code tree to maintain the sibling property.  At the point in\ntime when <VAR>t</VAR> messages have been transmitted, <VAR>k</VAR> of them distinct,\nand <VAR>k</VAR> &lt; <VAR>n</VAR>, the tree is a legal Huffman code tree with <VAR>k</VAR>+1 leaves,\none for each of the <VAR>k</VAR> messages and one for the 0-node.  If the \n(<VAR>t</VAR>+1)st message is one of the <VAR>k</VAR> already seen, the algorithm \ntransmits <VAR>a</VAR>(<VAR>t</VAR>+1)'s current code, increments the appropriate\ncounter and recomputes the tree.  If an unused message occurs, \nthe 0-node is split to create a pair of leaves, one for <VAR>a</VAR>(<VAR>t</VAR>+1),\nand a sibling which is the new 0-node.  Again the tree is recomputed.  \nIn this case, the code for the 0-node is sent; in addition, the\nreceiver must be told which of the <VAR>n</VAR>-<VAR>k</VAR> unused messages has\nappeared.\nAt each node a count of occurrences of the corresponding\nmessage is stored.  Nodes are numbered indicating their position in\nthe sibling property ordering.  The updating of the tree can be\ndone in a single traversal from the <VAR>a</VAR>(<VAR>t</VAR>+1) node to the root.\nThis traversal must increment the count for the <VAR>a</VAR>(<VAR>t</VAR>+1) node\nand for each of its ancestors.  Nodes may be exchanged to maintain\nthe sibling property, but all of these exchanges involve a\nnode on the path from <VAR>a</VAR>(<VAR>t</VAR>+1) to the root.\nFigure 4.2 shows the final code tree formed by this process on\nthe ensemble <VAR>EXAMPLE</VAR>.\n<P>\n<IMG SRC=\"DC-fig42.gif\" ALT=\"[FIGURE 4.2]\">\n<P>\nFigure 4.2 -- Tree formed by algorithm FGK for ensemble <VAR>EXAMPLE</VAR>.\n<P>\n\tDisregarding overhead, the number of bits transmitted by \nalgorithm FGK for the <VAR>EXAMPLE</VAR> is 129.  The \nstatic Huffman algorithm would transmit 117 bits in processing the \nsame data. The overhead\nassociated with the adaptive method is actually less than that\nof the static algorithm. In the adaptive case the only overhead\nis the <VAR>n</VAR> lg <VAR>n</VAR> bits needed to represent each of the <VAR>n</VAR> different \nsource messages when they appear for the first time.  (This is in fact\nconservative; rather than transmitting a unique code for each of the\n<VAR>n</VAR> source messages, the sender could transmit the message's position\nin the list of remaining messages and save a few bits in the average\ncase.)  In the static case, the source messages need to be\nsent as does the shape of the code tree.  As discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>,\nan efficient representation of the tree shape requires 2<VAR>n</VAR> bits.\nAlgorithm FGK compares well with static Huffman coding on this ensemble\nwhen overhead is taken into account.\nFigure 4.3 illustrates an example on which algorithm FGK performs\nbetter than static Huffman coding even without taking overhead\ninto account. Algorithm FGK transmits 47 bits for this ensemble\nwhile the static Huffman code requires 53.\n<P>\n<IMG SRC=\"DC-fig43.gif\" ALT=\"[FIGURE 4.3]\">\n<P>\nFigure 4.3 -- Tree formed by algorithm FGK for ensemble\n\"<VAR>e eae de eabe eae dcf</VAR>\".\n<P>\nVitter has proved that\nthe total number of bits transmitted by algorithm FGK for a message\nensemble of length <VAR>t</VAR> containing <VAR>n</VAR> distinct messages is bounded below \nby <VAR>S - n</VAR> + 1, where <VAR>S</VAR> is the performance of the static method, and \nbounded above by 2<VAR>S</VAR> + <VAR>t</VAR> - 4<VAR>n</VAR> + 2 [Vitter 1987].  So the performance \nof algorithm FGK is never much worse than twice optimal.  \nKnuth provides a complete implementation of algorithm FGK \nand a proof that the time required for each encoding or decoding \noperation is <VAR>O</VAR>(<VAR>l</VAR>), where <VAR>l</VAR> is the current length of the codeword [Knuth 1985].\nIt should be noted that since the mapping is defined dynamically, during\ntransmission, the encoding and decoding algorithms stand alone; there is\nno additional algorithm to determine the mapping as in static methods.\n\n<a name=\"Sec_4.2\">\n<H3> 4.2  Algorithm V</H3> </a>\n\n<P>\n<IMG SRC=\"DC-fig44.gif\" ALT=\"[FIGURE 4.4]\">\n<P>\nFigure 4.4 -- FGK tree with non-level order numbering.\n<P>\n\tThe adaptive Huffman algorithm of Vitter (algorithm V)\nincorporates two improvements over algorithm FGK.  First, the \nnumber of interchanges in which a node is moved upward in the \ntree during a recomputation is limited to one.  This \nnumber is bounded in algorithm FGK only by l/2 where <VAR>l</VAR> is the \nlength of the codeword for <VAR>a</VAR>(<VAR>t</VAR>+1) when the recomputation begins.  \nSecond, Vitter's method minimizes the values of SUM{ <VAR>l</VAR>(<VAR>i</VAR>) } and \nMAX{<VAR>l</VAR>(<VAR>i</VAR>)} subject to the requirement of minimizing\nSUM{ <VAR>w</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) }.\nThe intuitive explanation of algorithm V's advantage over algorithm\nFGK is as follows:  as in algorithm FGK, the code tree constructed\nby algorithm V is the Huffman code tree for the prefix of the \nensemble seen so far. The adaptive methods do not assume that the\nrelative frequencies of a prefix represent accurately the symbol \nprobabilities over the entire message. Therefore, the fact that algorithm V\nguarantees a tree of minimum height (height = MAX{ <VAR>l</VAR>(<VAR>i</VAR>) } \nand minimum external path length (SUM{ <VAR>l</VAR>(<VAR>i</VAR>) }) implies that it is\nbetter suited for coding the next message of the ensemble, given that\nany of the leaves of the tree may represent that next message.\n<P>\nThese improvements are accomplished through the use of a new system\nfor numbering nodes.  The numbering, called an implicit numbering,\ncorresponds to a level ordering of the nodes (from bottom to top\nand left to right).  Figure 4.4 illiustrates that the numbering of\nalgorithm FGK is not always a level ordering.  The following invariant\nis maintained in Vitter's algorithm:  For each weight <VAR>w</VAR>, all leaves\nof weight <VAR>w</VAR> precede (in the implicit numbering) all internal nodes\nof weight <VAR>w</VAR>.  Vitter proves that this invariant enforces the \ndesired bound on node promotions [Vitter 1987].  The invariant\nalso implements bottom merging, as discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>, to\nminimize SUM{ <VAR>l</VAR>(<VAR>i</VAR>) } and MAX{ <VAR>l</VAR>(<VAR>i</VAR>) }.  The difference between Vitter's\nmethod and algorithm FGK is in the way the tree is updated between\ntransmissions.  In order to understand the revised update operation,\nthe following definition of a block of nodes is necessary:  Blocks\nare equivalence classes of nodes defined by\n<VAR>u</VAR> is equivalent to <VAR>v</VAR> iff <VAR>weight(u)</VAR> =\n<VAR>weight(v)</VAR> and <VAR>u</VAR> and <VAR>v</VAR> are either both leaves or both internal nodes.\nThe leader of a block is the highest-numbered (in the implicit numbering)\nnode in the block.  Blocks are ordered by increasing weight with the\nconvention that a leaf block always precedes an internal block of the\nsame weight.  When an exchange of nodes is required to maintain\nthe sibling property, algorithm V requires that the node being\npromoted be moved to the position currently occupied by the\nhighest-numbered node in the target block.\n<P>\nIn Figure 4.5, the Vitter tree corresponding to Figure 4.1c is   \nshown.  This is the first point in <VAR>EXAMPLE</VAR> at which \nalgorithm FGK and algorithm V differ significantly.  At this\npoint, the Vitter tree has height 3 and external path length 12 while\nthe FGK tree has height 4 and external path length 14.  Algorithm V\ntransmits codeword 001 for the second <VAR>c</VAR>; FGK transmits 1101.\nThis demonstrates the intuition given earlier that algorithm V\nis better suited for coding the next message.\nThe Vitter tree corresponding to Figure 4.2, representing the final\ntree produced in processing <VAR>EXAMPLE</VAR>, is only different from\nFigure 4.2 in that the internal node of weight 5 is to the right\nof both leaf nodes of weight 5.\nAlgorithm V transmits 124 bits in processing <VAR>EXAMPLE</VAR>, as compared\nwith the 129 bits of algorithm FGK and 117 bits of static Huffman \ncoding.  It should be noted that these figures do not include overhead\nand, as a result, disadvantage the adaptive methods.\n<P>\n<IMG SRC=\"DC-fig45.gif\" ALT=\"[FIGURE 4.5]\">\n<P>\nFigure 4.5 -- Algorithm V processing the ensemble \"<VAR>aa bbb c</VAR>\".\n<P>\nFigure 4.6 ilustrates the tree built by Vitter's method for the ensemble\nof Figure 4.3.  Both SUM{<VAR>l</VAR>(<VAR>i</VAR>)} and MAX{<VAR>l</VAR>(<VAR>i</VAR>)} are \nsmaller in the tree of Figure 4.6.  The number of bits transmitted\nduring the processing of the sequence is 47, the same used by \nalgorithm FGK.  However, if the transmission continues with <VAR>d,b,c,f</VAR> \nor an unused letter, the cost of algorithm V will be less than\nthat of algorithm FGK.  This again illustrates the benefit of\nminimizing the external path length SUM{<VAR>l</VAR>(<VAR>i</VAR>)}\nand the height MAX{<VAR>l</VAR>(<VAR>i</VAR>)}.\n<P>\n<IMG SRC=\"DC-fig46.gif\" ALT=\"[FIGURE 4.6]\">\n<P>\nFigure 4.6 -- Tree formed by algorithm V for the ensemble of Fig. 4.3.\n<P>\nIt should be noted again that the strategy of minimizing external path \nlength and height is optimal under the assumption that any source\nletter is equally likely to occur next.  Other reasonable strategies\ninclude one which assumes locality.  To take advantage of locality,\nthe ordering of tree nodes with equal weights could be determined\non the basis of recency.\n  Another reasonable assumption about adaptive coding\nis that the weights in the current tree correspond closely to the\nprobabilities associated with the source.  This assumption becomes\nmore reasonable as the length of the ensemble increases.\nUnder this assumption, the expected cost of transmitting the next letter\nis SUM{ <VAR>p</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) } which is approximately\nSUM{ <VAR>w</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) }, so that neither algorithm FGK nor \nalgorithm V has any advantage.\n<P>\nVitter proves that the performance of his \nalgorithm is bounded by <VAR>S - n</VAR> + 1 from below and <VAR>S + t</VAR> - 2<VAR>n</VAR> + 1 \nfrom above [Vitter 1987].  At worst then, Vitter's adaptive method\nmay transmit one more bit per codeword than the static Huffman \nmethod.  The improvements made by Vitter do not change the complexity of\nthe algorithm; algorithm V encodes and decodes in <VAR>O</VAR>(<VAR>l</VAR>) time as does\nalgorithm FGK.\n\n<P>\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}