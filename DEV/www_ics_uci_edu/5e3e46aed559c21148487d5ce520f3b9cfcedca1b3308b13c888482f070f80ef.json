{"url": "https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/paper4", "content": ".EQ\ndelim $$\n.EN\n.ls 1\n.ce\nPROGRAMMING BY EXAMPLE REVISITED\n.sp\n.ce\nby John G. Cleary\n.ce\nMan-Machine Systems Laboratory\n.ce\nUniversity of Calgary.\n.sp\n.sh \"Introduction\"\n.pp\nEfforts to construct an artificial intelligence have relied on\never more complex and carefully prepared programs.  While useful in\nthemselves, these programs\nare unlikely to be useful in situations where ephemeral and\nlow value knowledge must be acquired.  For example a person (or robot)\nworking in a normal domestic environment knows a lot about which\ncupboards have sticky doors and where the marmalade is kept.  It seems\nunlikely that it will ever be economic to program such knowledge \nwhether this be via a language or a discourse with an expert system.\n.pp\nIt is my thesis, then, that any flexible robot system working in the\nreal world must contain a component of control intermediate\nbetween hard wired 'reflex' responses and complex intellectual \nreasoning.  Such an intermediate system must be adaptive, be able\nto carry out complex patterned responses and be fast in operation.\nIt need not, however, carry out complex forward planning or be capable\nof introspection (in the sense that expert systems are able to explain\ntheir actions).\n.pp\nIn this talk I will examine a system that acquires knowledge by \nconstructing a model of its input behaviour and uses this to select its\nactions.  It can be viewed either as an automatic adaptive system  or\nas an instance of 'programming by example'.  Other workers have\nattempted to do this, by constructing compact models in some appropriate\nprogramming language:e.g. finite state automata [Bierman, 1972], \n[Bierman and Feldman, 1972]; LISP [Bierman and Krishnaswamy, 1976]; \nfinite non-deterministic\nautomata [Gaines,1976], [Gaines,1977],\n[Witten,1980]; high level languages [Bauer, 1979], [Halbert, 1981].\nThese efforts, however, suffer from\nthe flaw that for some inputs their computing time is \nsuper-exponential in the number\nof inputs seen.  This makes them totally impractical in any system which\nis continuously receiving inputs over a long period of time.\n.pp\nThe system I will examine comprises one or more simple independent\nmodels.  Because of their simplicity and because no attempt is made to \nconstruct models which are minimal,\nthe time taken to store new information and to make \npredictions is constant and independent of the amount of information stored\n[Cleary, 1980].  This leads to a very integrated and responsive environment.\nAll actions by the programmer are immediately incorporated into the program\nmodel. The actions are also acted upon so that their consequences are \nimmediately apparent.\nHowever, the amount of memory used could grow \nlinearly with time. [Witten, 1977] introduces a modelling system related\nto the one here which does not continually grow and which can be updated\nincrementally.\n.pp\nIt remains to be shown that the very simple models used are capable \nof generating any\ninterestingly complex behaviour.\nIn the rest of this\ntalk I will use the problem of executing a subroutine to illustrate\nthe potential of such systems.\nThe example will also illustrate some of the techniques which have been\ndeveloped for combining multiple models, [Cleary, 1980], [Andreae\nand Cleary, 1976], [Andreae, 1977], [Witten,1981].  It has also been\nshown in [Cleary, 1980] and in [Andreae,1977] that such systems can\nsimulate any Turing machine when supplied with a suitable external memory.\n.sh \"The modelling system\"\n.pp\nFig. 1 shows the general layout of the modeller.  Following the flow\nof information through the system it first receives a number of inputs\nfrom the external world.  These are then used to update the current\ncontexts of a number of Markov models.  Note, that each Markov model\nmay use different inputs to form its current context, and that they\nmay be attempting to predict different inputs.  A simple robot\nwhich can hear and move an arm might have two models; one, say, in\nwhich the last three sounds it heard are used to predict the next\nword to be spoken, and another in which the last three sounds and the last\nthree arm movements are used to predict the next arm movement. \n.pp\nWhen the inputs are received each such context and its associated \nprediction (usually\nan action) are added to the Markov model.  (No\ncounts or statistics are maintained \\(em they are not necessary.)  When the\ncontext recurs later it will be retrieved along with all the predictions\nwhich have been stored with it.\n.pp\nAfter the contexts have been stored they \nare updated by shifting in the new inputs. These new contexts are then\nmatched against the model and all the associated predictions are retrieved.\nThese independent predictions from the individual Markov models\nare then combined into a single composite \nprediction.\n(A general theory of how to do this has been\ndeveloped in [Cleary, 1980]).  \n.pp\nThe final step is to present this \ncomposite prediction to a device I have called the 'choice oracle'.\nThis uses whatever information it sees fit to choose the next action.\nThere are many possibilities for such a device.  One might be to choose\nfrom amongst the predicted actions if reward is expected and to choose\nsome other random action if reward is not expected.  The whole system then \nlooks like\na reward seeking homeostat.  At the other extreme the oracle might be\na human programmer who chooses the next action according to his own\nprinciples.  The system then functions more like a programming by\nexample system \\(em [Witten, 1981] and [Witten, 1982] give examples of such \nsystems.\n[Andreae, 1977] gives an example of a 'teachable' system lying between\nthese two extremes.\n.pp\nAfter an action is chosen this is\ntransmitted to the external world and the resultant inputs are used\nto start the whole cycle again.  Note that the chosen action will\nbe an input on the next cycle.\n.sh \"Subroutines\"\n.pp\nAn important part of any programming language is the ability to write a \nfragment of a program and then have it used many times without it having\nto be reprogrammed each time.  A crucial feature of such shared code is\nthat after it has been executed the program should be controlled by the\nsituation which held before the subroutine was called. A subroutine can be \nvisualised as a black box with an unknown and arbitrarily complex interior.\nThere are many paths into the box but after passing through each splits again\nand goes its own way, independent of what happened inside the box.\n.np\nAlso, if there are $p$ paths using the subroutine and $q$ different sequences\nwithin it then the amount of programming needed should be proportional to\n$p + q$ and not $p * q$.  The example to follow possess both these properties\nof a subroutine.\n.rh \"Modelling a Subroutine.\"\nThe actual model we will use is described in Fig. 2.  There are two Markov\nmodels (model-1 and model-2) each seeing and predicting different parts of\nthe inputs.  The inputs are classified into four classes; ACTIONs that\nmove a robot (LEFT, RIGHT, FAST, SLOW), patterns that it 'sees' (danger,\nmoved, wall, stuck) and two types of special 'echo' actions, # actions\nand * actions (*home, #turn).  The # and * actions have no effect on the \nenvironment,\ntheir only purpose is to be inputs and act as place keepers for relevant\ninformation.  They may be viewed as comments which remind the system of\nwhat it is doing.  (The term echo was used in [Andreae,1977], where the\nidea was first introduced, in analogy to spoken words of which one\nhears an echo.)\n.pp\nModel-2 is a Markov model of order 2 and uses only # actions in its\ncontext and seeks to predict only * actions.  Model-1 is a Markov model \nof order 3 and uses all four classes of inputs in its context.  It\nseeks to predict ACTIONs, # actions and * actions.  However, * actions\nare treated specially.  Rather than attempt to predict the exact * action\nit only stores * to indicate that some * action has occurred.  This\nspecial treatment is also reflected in the procedure for combining the\npredictions of the two models.  Then the prediction of model-2 is used,\nonly if model-1 predicts an *.  That is, model-1 predicts that some \n* action will occur and model-2 is used to select which one. If model-1\ndoes not predict an * then its prediction is used as the combined prediction\nand that from model-2 is ignored.\n.pp\nThe choice oracle that is used for this example has two modes.  In\nprogrammer mode a human programmer is allowed to select any action\nshe wishes or to acquiesce with the current prediction, in which case\none of the actions in the combined prediction is selected.  In\nexecution mode one of the predicted actions is selected and the\nprogrammer is not involved at all.\n.pp\nBefore embarking on the actual example some points about the predictions\nextracted from the individual Markov models should be noted.  First, if \nno context can be found stored in the memory which equals the current\ncontext then it is shortened by one input and a search is made for any\nrecorded contexts which are equal over the reduced length.  If necessary\nthis is repeated until the length is zero whereupon all possible\nallowed actions are predicted.\n.pp\nFig. 3 shows the problem to be programmed.  If a robot sees danger it\nis to turn and flee quickly.  If it sees a wall it is to turn and return\nslowly.  The turning is to be done by a subroutine which, if it gets \nstuck when turning left, turns right instead.\n.pp\nFig. 4 shows the contexts and predictions stored when this is programmed.\nThis is done by two passes through the problem in 'program' mode: once\nto program the fleeing and turning left; the other to program the wall\nsequence and the turning right.  Fig. 5 then shows how this programming\nis used in 'execute' mode for one of the combinations which had not been\nexplicitly programmed earlier (a wall sequence with a turn left).  The\nfigure shows the contexts and associated predictions for each step.\n(Note that predictions are made and new contexts are stored in both\nmodes.  They have been omitted from the diagrams to preserve clarity.)\n.sh \"Conclusion\"\n.pp\nThe type of simple modelling system presented above is of interest for a\nnumber of reasons.  Seen as a programing by example system, \nit is very closely \nintegrated. Because it can update its models incrementally in real time\nfunctions such as input/output, programming, compilation and execution\nare subsumed into a single mechanism. Interactive languages such as LISP\nor BASIC gain much of their immediacy and usefulness by being interpretive \nand not requiring a separate compilation step when altering the source\nprogram. By making execution integral with the process of program entry\n(some of) the consequencs of new programming become immediately apparent.\n.pp\nSeen as an adaptive controller, the system has the advantage of being fast\nand being able to encode any control strategy. Times to update the model\ndo not grow with memory size and so it can operate continuously in real time.\n.pp\nSeen as a paradigm for understanding natural control systems, it has the\nadvantage of having a very simple underlying storage mechanism. Also,\nthe ability to supply an arbitrary choice oracle allows for a wide\nrange of possible adaptive strategies.\n.sh \"References\"\n.in +4m\n.sp\n.ti -4m\nANDREAE, J.H. 1977\nThinking with the Teachable Machine.  Academic Press.\n.sp\n.ti -4m\nANDREAE, J.H. and CLEARY, J.G. 1976\nA New Mechanism for a Brain.  Int. J. Man-Machine Studies\n8(1):89-119.\n.sp\n.ti -4m\nBAUER, M.A. 1979 Programming by examples. Artificial Intelligence 12:1-21.\n.sp\n.ti -4m\nBIERMAN, A.W. 1972\nOn the Inference of Turing Machines from Sample Computations.\nArtificial Intelligence 3(3):181-198.\n.sp\n.ti -4m\nBIERMAN, A.W. and FELDMAN, J.A. 1972\nOn the Synthesis of Finite-State Machines from Samples of\ntheir Behavior.  IEEE Transactions on Computers C-21, June:\n592-597.\n.sp\n.ti -4m\nBIERMAN, A.W. and KRISHNASWAMY, R. 1976 Constructing programs from example \ncomputations. IEEE transactions on Software Engineering SE-2:141-153.\n.sp\n.ti -4m\nCLEARY, J.G. 1980\nAn Associative and Impressible Computer. PhD thesis, University\nof Canterbury, Christchurch, New Zealand.\n.sp\n.ti -4m\nGAINES, B.R. 1976\nBehaviour/structure transformations under uncertainty.\nInt. J. Man-Machine Studies 8:337-365.\n.sp\n.ti -4m\nGAINES, B.R. 1977\nSystem identification, approximation and complexity.\nInt. J. General Systems, 3:145-174.\n.sp\n.ti -4m\nHALBERT, D.C. 1981\nAn example of programming by example. Xerox Corporation, Palo Alto, \nCalifornia.\n.sp\n.ti -4m\nWITTEN, I.H. 1977\nAn adaptive optimal controller for discrete-time Markov\nenvironments.  Information and Control, 34, August: 286-295.\n.sp\n.ti -4m\nWITTEN, I.H. 1979\nApproximate, non-deterministic modelling of behaviour\nsequences.  Int. J. General Systems, 5, January: 1-12.\n.sp\n.ti -4m\nWITTEN, I.H. 1980\nProbabilistic behaviour/structure transformations using\ntransitive Moore models.  Int. J. General Systems, 6(3):\n129-137.\n.sp\n.ti -4m\nWITTEN, I.H. 1981\nProgramming by example for the casual user: a case study.\nProc. Canadian Man-Computer Communication Conference, Waterloo,\nOntario, 105-113.\n.sp\n.ti -4m\nWITTEN, I.H. 1982\nAn interactive computer terminal interface which predicts user \nentries. Proc. IEE Conference on Man-Machine Interaction,\nManchester, England.\n.in -4m\n", "encoding": "ascii"}