{"url": "https://www.ics.uci.edu/~eppstein/161/960109.html#dynprog", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Fibonacci Numbers</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 9, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Introduction to analysis of algorithms</h1>\n\n<a name=\"pseudocode\">An algorithm is just the outline or idea\nbehind a program. We express algorithms in <i>pseudo-code</i>:\nsomething resembling C or Pascal, but with some statements in\nEnglish rather than within the programming language. It is expected\nthat one could translate each pseudo-code statement to a small\nnumber of lines of actual code, easily and mechanically.</a> \n\n<p><a name=\"analysis\">This class covers the design of algorithms\nfor various types of problems, as well as a mathematical analysis\nof those algorithms done independently of any actual computational\nexperiments. The purpose of design of algorithms is obvious: one\nneeds an algorithm in order to write a program. Analysis of\nalgorithms is less obviously necessary, but has several\npurposes:</a></p>\n\n<ul>\n<li>Analysis can be more reliable than experimentation. If we\nexperiment, we only know the behavior of a program on certain\nspecific test cases, while analysis can give us guarantees about\nthe performance on all inputs.</li>\n\n<li>It helps one choose among different solutions to problems. As\nwe will see, there can be many different solutions to the same\nproblem. A careful analysis and comparison can help us decide which\none would be the best for our purpose, without requiring that all\nbe implemented and tested.</li>\n\n<li>We can predict the performance of a program before we take the\ntime to write code. In a large project, if we waited until after\nall the code was written to discover that something runs very\nslowly, it could be a major disaster, but if we do the analysis\nfirst we have time to discover speed problems and work around\nthem.</li>\n\n<li>By analyzing an algorithm, we gain a better understanding of\nwhere the fast and slow parts are, and what to work on or work\naround in order to speed it up.</li>\n</ul>\n\n<h2>The Fibonacci numbers</h2>\n\nWe introduce algorithms via a \"toy\" problem: computation of\nFibonacci numbers. It's one you probably wouldn't need to actually\nsolve, but simple enough that it's easy to understand and maybe\nsurprising that there are many different solutions. \n\n<p> <b>The Fibonacci story:</b></p>\n\n<p> <a href=\"people.html#fibonacci\">Leonardo of Pisa</a> (aka\nFibonacci) was interested in many things, including a subject we\nnow know as <i>population dynamics</i>: For instance, how quickly\nwould a population of rabbits expand under appropriate\nconditions?</p>\n\n<p> As is typical in mathematics (and analysis of algorithms is a\nform of mathematics), we make the problem more abstract to get an\nidea of the general features without getting lost in detail:</p>\n\n<ul>\n<li>We assume that a pair of rabbits has a pair of children every\nyear.</li>\n\n<li>These children are too young to have children of their own\nuntil two years later.</li>\n\n<li>Rabbits never die.</li>\n</ul>\n\n(The last assumption sounds stupid, but makes the problem simpler.\nAfter we have analyzed the simpler version, we could go back and\nadd an assumption e.g. that rabbits die in ten years, but it\nwouldn't change the overall behavior of the problem very much.) \n\n<p>We then express the number of pairs of rabbits as a function of\ntime (measured as a number of years since the start of the\nexperiment):</p>\n\n<ul>\n<li>F(1) = 1 -- we start with one pair</li>\n\n<li>F(2) = 1 -- they're too young to have children the first\nyear</li>\n\n<li>F(3) = 2 -- in the second year, they have a pair of\nchildren</li>\n\n<li>F(4) = 3 -- in the third year, they have another pair</li>\n\n<li>F(5) = 5 -- we get the first set of grandchildren</li>\n</ul>\n\nIn general F(n) = F(n-1) + F(n-2): all the previous rabbits are\nstill there (F(n-1)) plus we get one pair of children for every\npair of rabbits we had two years ago (F(n-2)). \n\n<p>The algorithmic problem we'll look at today: how to compute\nF(n)?</p>\n\n<h2>Formulas and floating point</h2>\n\nYou probably saw in Math 6a that F(n) = (x^n - (1-x)^n)/(x - (1-x))\nwhere x = (1+sqrt 5)/2 ~ 1.618 is the golden ratio. This solution\nis often used as a standard example of the method of \"generating\nfunctions\". \n\n<p>So this seems to be an algorithm: compute 1.618^n - 0.618^n.\nProblem: how accurately do you have to know x to get the right\nanswer? e.g. if you just use x=1.618, you get</p>\n\n<ul>\n<li>F(3)=1.99992 -- close enough to 2?</li>\n\n<li>F(16)=986.698 -- round to 987?</li>\n\n<li>F(18)=2583.1 -- should be 2584</li>\n</ul>\n\nInstead since F(n) is defined in integers it saves some problems to\nstick with integers. \n\n<h2>A recursive algorithm</h2>\n\nThe original formula seems to give us a natural example of\nrecursion: \n\n<p><b>Algorithm 1:</b></p>\n\n<pre>\n    int fib(int n)\n    {\n    if (n &lt;= 2) return 1\n    else return fib(n-1) + fib(n-2)\n    }\n</pre>\n\nAn example of the sort of basic question we study in this class is,\nhow much time would this algorithm take? How should we measure\ntime? The natural measure would be in seconds, but it would be nice\nto have an answer that didn't change every time Intel came out with\na faster processor. We can measure time in terms of machine\ninstructions; then dividing by a machine's speed (in\ninstructions/second) would give the actual time we want. However,\nit is hard to guess from a piece of pseudo-code the exact number of\ninstructions that a particular compiler would generate. To get a\nrough approximation of this, we try measuring in terms of lines of\ncode. \n\n<p>Each call to fib returns either one or two lines. If n &lt;= 2,\nwe only execute one line (the if/return). if n = 3, execute 2 for\nfib(2), plus one each for fib(1) and fib(0): 4</p>\n\n<p>It's like the rabbits! Except for the two lines in each call,\nthe time for n is the sum of the times for two smaller recursive\ncalls.</p>\n\n<pre>\n    time(n) = 2 + time(n-1) + time(n-2)\n</pre>\n\n<a name=\"recurrence\">In general, any recursive algorithm such as\nthis one gives us a <i>recurrence relation</i>: the time for any\nroutine is the time within the routine itself, plus the time for\nthe recursive calls. This gives in a very easy mechanical way an\nequation like the one above, which we can then solve to find a\nformula for the time. In this case, the recurrence relation is very\nsimilar to the definition of the Fibonacci numbers.</a> \n\n<p> With some work, we can solve the equation, at least in terms of\nF(n): We think of the recursion as forming a tree. We draw one\nnode, the root of the tree, for the first call then any time the\nroutine calls itself, we draw another child in the tree.</p>\n\n<pre>\n        F(5)\n           /    \\\n       F(4)      F(3)\n      /    \\    /    \\\n       F(3)   F(2) F(2)  F(1)\n      /    \\\n   F(2)    F(1)\n</pre>\n\nThe four internal nodes of this tree for fib(5) take two lines\neach, while the five leaves take one line, so the total number of\nlines executed in all the recursive calls is 13. \n\n<p>Note that, when we do this for any call to fib, the Fibonacci\nnumber F(i) at each internal node is just the number of leaves\nbelow that node, so the total number of leaves in the tree is just\nF(n). Remember that leaves count as one line of code, internal\nnodes 2. To count internal nodes, use basic fact about binary trees\n(trees in which each node has 2 children): the number of internal\nnodes always equals the number of leaves minus one. (You can prove\nthis by induction: it's true if there's one leaf and no internals,\nand it stays true if you add 2 children to a leaf.)</p>\n\n<p> So there are F(n) lines executed at the leaves, and 2F(n)-2 at\nthe internal nodes, for a total of 3F(n)-2. Let's double check this\non a simple example: time(5) = 3F(5) - 2 = 3(5)-2 = 13.</p>\n\n<p> This is kind of slow e.g. for n=45 it takes over a billion\nsteps. Maybe we can do faster?</p>\n\n<h2>Dynamic programming</h2>\n\nOne idea: the reason we're slow slow is we keep recomputing the\nsame subproblems over and over again. For instance, the tree above\nshows two computations of F(3). The second time we get to F(3),\nwe're wasting effort computing it again, because we've already\nsolved it once and the answer isn't going to change. Instead let's\nsolve each subproblem once and then look up the solution later when\nwe need it instead of repeatedly recomputing it. \n\n<p>This easy idea leads to some complicated algorithms we'll see\nlater in the section on dynamic programming, but here it's pretty\nsimple:</p>\n\n<p><b>Algorithm 2:</b></p>\n\n<pre>\n    int fib(int n)\n    {\n    int f[n+1];\n    f[1] = f[2] = 1;\n    for (int i = 3; i &lt;= n; i++)\n        f[i] = f[i-1] + f[i-2];\n    return f[n];\n    }\n</pre>\n\nThis is an <a name=\"iterative\">iterative algorithm</a> (one that\nuses loops instead of recursion) so we analyze it a little\ndifferently than we would a recursive algorithm. Basically, we just\nhave to compute for each line, how many times that line is\nexecuted, by looking at which loops it's in and how many times each\nloop is executed. \n\n<p>Three lines are executed always. The first line in the loop is\nexecuted n-1 times (except for n=1) The second line in loop\nexecuted n-2 times (except for n=1) so time(n) = n-1 + n-2 + 3 = 2n\n(except time(1)=4).</p>\n\n<p>As an example for n=45 it takes 90 steps, roughly 10 million\ntimes faster than the other program. Even if you don't do this very\noften this is a big enough difference to notice, so the second\nalgorithm is much better than the first.</p>\n\n<h2>Space complexity</h2>\n\nRunning time isn't the only thing we care about, or the only thing\nthat can be analyzed mathematically. Programmer time and code\nlength are important, but we won't discuss them here -- they're\npart of the subject of software engineering. However we will often\nanalyze the amount of memory used by a program. If a program takes\na lot of time, you can still run it, and just wait longer for the\nresult. However if a program takes a lot of memory, you may not be\nable to run it at all, so this is an important parameter to\nunderstand. \n\n<p>Again, we analyze things differently for recursive and iterative\nprograms. For an iterative program, it's usually just a matter of\nlooking at the variable declarations (and storage allocation calls\nsuch as malloc() in C). For instance, algorithm 2 declares only an\narray of n numbers. Analysis of recursive program space is more\ncomplicated: the space used at any time is the total space used by\nall recursive calls active at that time. Each recursive call in\nalgorithm 1 takes a constant amount of space: some space for local\nvariables and function arguments, but also some space for\nremembering where each call should return to. The calls active at\nany one time form a path in the tree we drew earlier, in which the\nargument at each node in the path is one or two units smaller than\nthe argument at its parent. The length of any such path can be at\nmost n, so the space needed by the recursive algorithm is again\n(some constant factor times) n. We abbreviate the \"some constant\nfactor times\" using \"O\" notation: O(n).</p>\n\n<p>It turns out that algorithm 2 can be modified to use a much\nsmaller amount of space. Each step through the loop uses only the\nprevious two values of F(n), so instead of storing these values in\nan array, we can simply use two variables. This requires some\nswapping around of values so that everything stays in the\nappropriate places:</p>\n\n<p><b>Algorithm 3:</b></p>\n\n<pre>\n    int fib(int n)\n    {\n    int a = 1, b = 1;\n    for (int i = 3; i &lt;= n; i++) {\n        int c = a + b;\n        a = b;\n        b = c;\n    }           \n    return b;\n    }\n</pre>\n\nHere c represents f[i], b represents f[i-1], and a represents\nf[i-2]. The two extra assignments after the sum shift those values\nover in preparation for the next iteration. This algorithm uses\nroughly 4n lines to compute F(n), so it is slower than algorithm 2,\nbut uses much less space. \n\n<h2>Big \"O\" notation</h2>\n\nThere are better algorithms for Fibonacci numbers, but before we\ninvestigate that, let's take a side track and make our analysis a\nlittle more abstract. \n\n<p> A problem with the analysis of the two algorithms above: what\nis a line of code? If I use whitespace to break a line into two, it\ndoesn't change the program speed but does change the number of\nlines executed. And as mentioned before, if I buy a faster\ncomputer, it does change program speed but doesn't change the\nanalysis.</p>\n\n<p> To avoid extraneous details like whitespace and computer type,\nwe use \"big O\" notation. The idea: we already write the times as a\nfunction of n. Big O notation treats two functions as being roughly\nthe same if one is c times the other where c is a constant\n(something that doesn't depend on n). So for instance we would\nreplace 3F(n)-2 by O(F(n)) and both 2n and 4n by O(n).</p>\n\n<p>Formally, we say that</p>\n\n<pre>\n    f(n)=O(g(n))\n</pre>\n\nIf there is some constant c such that \n\n<pre>\n    f(n) &lt;= c g(n)\n</pre>\n\nIt is true that 4n=O(n), but it is also true that n=O(4n). However\nnote that this is not always a symmetric relation; n=O(F(n)) but it\nis not true that F(n)=O(n). In practice we will usually only use O\nnotation to simplify formulas, by ignoring constant factors and\nother extraneous details. \n\n<p>What is the point of O notation? First, it makes life easier by\nallowing us to be less careful of all the fine details of an\nalgorithm's behavior. But also, it allows us to compare two\nalgorithms easily. Algorithm 2 and algorithm 3 are both O(n).\nAccording to the number of lines executed, one is twice as fast as\nthe other, but this ratio does not change as a function of n. Other\nfactors (like the amount of time needed to allocate the large array\nin algorithm 2) may mean that in actual time, the algorithms are\ncloser to each other; a more careful analysis is needed to\ndetermine which of the two to use. On the other hand, we know that\n4n is much better than 3F(n)-2 for any reasonable value of n -- but\nthis doesn't depend on the factor of 4 in the 4n time bound, it\nwould be the same for 7n or 12n. For larger and larger n, the ratio\nof n to F(n) gets very large, so that very quickly any O(n) will be\nfaster than any O(F(n)). Replacing 4n by O(n) is an abstraction\nthat lets us compare it to other functions without certain details\n(the 4) getting in the way.</p>\n\n<h2>Recursive powering</h2>\n\nAlgorithms 3 and 4 above aren't the best! \n\n<p>Here's a mathematical trick with matrices:</p>\n\n<pre>\n    [ 1 1 ] n      [ F(n+1) F(n)   ]\n    [ 1 0 ]    =   [ F(n)   F(n-1) ]\n</pre>\n\n(You don't have to remember much linear algebra to understand this\n-- just the formula for multiplying two symmetric 2x2 matrices: \n\n<pre>\n    [ a b ] [ d e ]   [ ad + be  bd + ce ]\n    [ b c ] [ e f ] = [ bd + ce  be + cf ]\n</pre>\n\nYou can then prove the result above by induction: Let \n\n<pre>\n    [ 1 1 ]\n    A = [ 1 0 ]\n</pre>\n\nassume by induction that the equation above is is true for some n,\nmultiply both sides by another power of A using the formula for\nmatrix multiplication, and verify that the terms you get are the\nsame as the formula defining the Fibonacci numbers.) \n\n<p>We can use this to define another iterative algorithm, using\nmatrix multiplication. Although I will write this in C syntax, we\nare starting to get to pseudo-code, since C does not have matrix\nmultiplication built in to it the way I have it written below. The\nfollowing algorithm initializes a matrix M to the identity matrix\n(the \"zeroth power\" of A) and then repeatedly multiplies M by A to\nform the (n-1)st power. Then by the formula above, the top left\ncorner holds F(n), the value we want to return.</p>\n\n<p><b>Algorithm 4:</b></p>\n\n<pre>\n    int fib(int n)\n    {\n    int M[2][2] = {{1,0},{0,1}}\n    for (int i = 1; i &lt; n; i++)\n        M = M * {{1,1},{1,0}}\n    return M[0][0];\n    }\n</pre>\n\nThis takes time O(n) (so much better than algorithm 1) but is\nprobably somewhat slower than algorithm 2 or algorithm 3. (The big\nO notation hides the difference between these algorithms, so you\nhave to be more careful to tell which is better.) Like algorithm 3,\nthis uses only O(1) space. \n\n<p> But we can compute M^n more quickly. The basic idea: if you\nwant to compute e.g. 3^8 you can multiply 8 3's together one at a\ntime (3*3*3*3*3*3*3*3) or you can repeatedly square: square 3^2 =\n9, 9^2 = 3^4 = 81, 81^2 = 3^8 = 6561. The squaring idea uses many\nfewer multiplications, since each one doubles the exponent rather\nthan simply adding one to it. With some care, the same idea works\nfor matrices, and can be extended to exponents other than powers of\ntwo.</p>\n\n<p><b>Algorithm 5:</b></p>\n\n<pre>\n    int M[2][2] = {{1,0}{0,1}}\n\n    int fib(int n)\n    {\n    matpow(n-1);\n    return M[0][0];\n    }\n\n    void matpow(int n)\n    {\n    if (n &gt; 1) {\n        matpow(n/2);\n        M = M*M;\n    }\n    if (n is odd) M = M*{{1,1}{1,0}}\n    }\n</pre>\n\nBasically all the time is in matpow, which is recursive: it tries\nto compute the nth power of A by squaring the (n/2)th power.\nHowever if n is odd, rounding down n/2 and squaring that power of A\nresults in the (n-1)st power, which we \"fix up\" by multiplying one\nmore factor of A. \n\n<p>This is a recursive algorithm, so as usual we get a recurrence\nrelation defining time, just by writing down the time spent in a\ncall to matpow (O(1)) plus the time in each recursive call (only\none recursive call, with argument n/2). So the recurrence is</p>\n\n<pre>\n    time(n) = O(1) + time(n/2)\n</pre>\n\nIt turns out that this solves to O(log n). For the purposes of this\nclass, we will use logarithms base 2, and round all logarithms to\nintegers, so log n is basically the number of bits needed to write\nn down in binary. An equivalent way of defining it is the smallest\nvalue of i such that n &lt; 2^i. But clearly if n &lt; 2^i, n/2\n&lt; 2^(i-1) and conversely, so log n satisfies the recurrence\nlog(n) = 1 + log(n/2). The recurrence defining the time for matpow\nis basically the same except with O(1) instead of 1. So the\nsolution to the recurrence is just the sum of log n copies of O(1),\nwhich is O(log n). \n\n<p>If n is 1 billion, log n would only be 30, and this algorithm\nwould be better than algorithms 2 and 3 in the same way that they\nare better than algorithm 1.</p>\n\n<p>(This is actually somewhat cheating: to be able to use this for\nn equal to a billion you need to be able to write down the answer\nwhich will have O(n) digits, and you need to be able to store\nvariables with that many digits. Manipulating such large numbers\nwould take more like O(n) steps per operation, where here we are\nonly counting one step per integer multiplication or addition. But\neven if you used a special library for dealing with large numbers,\nalgorithm 4 would be much faster than the other ones.)</p>\n\n<p>Actually you can get the original formula 1.618^n to work using\na similar repeated squaring trick, also with time O(log n). So to\ntell which is better you have to be more careful and not just use\nO-notation -- dealing with an integer matrix is somewhat simpler\nthan having to compute floating point square roots so it wins.</p>\n\n<p>Which is the sort of comparison that analysis of algorithms is\nall about...</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960109.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}