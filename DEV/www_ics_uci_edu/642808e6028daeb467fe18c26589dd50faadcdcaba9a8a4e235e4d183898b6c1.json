{"url": "https://www.ics.uci.edu/~eppstein/280/regress.html", "content": "<HTML><HEAD>\n<TITLE>Computational Statistics: Regression</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Regression</H2>\n\nThe data model for linear regression is that the data points\nare constrained to lie on some hyperplane (i.e., there is a linear\nrelation between their coordinates).  Another way of interpreting this\nis to treat one of the coordinates as \"dependent\" and the rest as\n\"independent\": the dependent coordinate is then a linear function of the\nindependent ones.  The data may be chosen according to some probability\ndistribution on the line, or there may be some known pattern of\nindependent coordinates (e.g. they are chosen on the grid), at each\npoint of which one measures the dependent coordinate.\nFor the probability distribution models, it doesn't much matter which is\nthe dependent and which the independent variables (the only restriction\nis that the hyperplane must not be parallel to the dependent coordinate axis).\nBut this choice may make a difference if the independent coordinates are\nnonrandom or if the noise model adds noise only to the dependent coordinate.\n\n<P>\nData models in which the data points must satisfy multiple linear\nrelations (i.e. they lie on a lower dimensional subspace) can often\nbe handled by treating each dependent coordinate separately,\nbut this is only appropriate for noise models in which the coordinates\nare independent (e.g. Gaussian noise).\n\n<H3>Projective Duality</H3>\n\nThere's another picture of regression that is mathematically equivalent\nbut may help human intuition.\nInterpret the coordinates (a,b) defining\nany line y=ax+b to instead define a point in a \"dual\" a-b plane.\nThen the set of lines through any point (x,y) in the primal plane\nturns into a set of points b=(-x)a+y in the dual plane; this set is just\na line.\n\n<P>So if we have a collection of points (x<sub>i</sub>,y<sub>i</sub>)\ndefining a regression problem, we can think of them instead as\neach defining a different line b=(-x<sub>i</sub>)a+y<sub>i</sub>\nin the a-b plane.  What we want to find is a point in the a-b plane\nthat is \"near\" all of these lines; that is, we expect the lines\nto roughly look like they all go through a common point, and we want to\nfind that common point.\n\n<P>Similar methods transform any higher dimensional regression problem\nof fitting a single hyperplane to a set of data points\ninto an equivalent problem of fitting a single point\nto an arrangement of hyperplanes.\n\n<H3><A NAME=\"Lp\">L<sub>p</sub> Models for Dependent Noise</A></H3>\n\nLet's first consider the case in which the noise is a random variable\nadded only to the independent variable.\nThe most common algorithm for fitting this model is least squares:\nIf we are trying to find a linear model\ny=m.x+b (where y is the dependent variable, x is the vector of\nindependent variables, and m and b are the parameters we're trying to find)\nthen we measure the error of fit by\n<DIV ALIGN=CENTER>\nE = sum (m.x<sub>i</sub>+b-y<sub>i</sub>)<sup>2</sup>\n</DIV>\nThe least squares fit is the one minimizing this error estimate.\n\n<P>\nThe big advantage of least squares is that it's easy to solve:\nE is just a positive definite quadratic form, so it has a unique\nminimum.  Since E is a smooth function of m and b, its minimum\noccurs at the point where its partial derivatives in each coordinate\nare all zero.  This gives a system of d equations in d unknowns\nwhich one can solve by Gaussian elimination, or one can\ninstead use local improvement methods to approximate the optimal fit.\nThus the time is linear in the number of unknowns, and at worst\ncubic in the dimension.\n\n<P>\nMore generally the L<sub>p</sub> error is formed by using pth powers\nin place of squares in the definition of E.\nThe limit of this, the L<sub>infinity</sub> norm, is\njust the maximum error of any individual data point.\nThe most commonly used of these are the L<sub>1</sub> norm\n(also known as \"least absolute deviation\" or LAD regression)\nwhich is less sensitive to outliers than least squares,\nand the L<sub>infinity</sub> norm,\nwhich is more sensitive to outliers and therefore useful\nin contexts where one wants to detect them e.g. measuring flatness of a\nsurface.\n\n<P>The L<sub>infinity</sub> estimator can be expressed as a low\ndimensional linear program: find (m,b,e), minimizing e,\nand satisfying the linear constraints\n-e&nbsp;<U>&lt;</U>&nbsp;m.x<sub>i</sub>+b-y<sub>i</sub>&nbsp;<U>&lt;</U>&nbsp;e.\nThe dimension (number of variables to be found) is just d+1.\nTherefore, it can be solved in time linear in the number of data points\n(but exponential in the dimension)\nusing low-dimensional linear programming techniques.\n[citations to be filled in later].\nThere also exists a similar linear time bound for L<sub>1</sub> estimation\n[<A HREF=\"bib.html#Z84\">Z84</A>].\nAll L<sub>p</sub> estimators are invariant to coordinate changes\nand affine transformations among only dependent variables.\n\n<P>One advantage of assuming dependent noise is that\nit is preserved by the duality transformation described above:\nthe vertical distance of a point (x<sub>i</sub>,y<sub>i</sub>)\nfrom the line y=ax+b is the same as the vertical distance\nof the point (a,b) from the line\nb=(-x<sub>i</sub>)a+y<sub>i</sub>.\n(Just work out the equations, they're the same!)\n\n<P>In the discussion, one of the students suggested using a convex combination\nof different L<sub>p</sub> metrics, which might e.g. come from a truncated\npower series for some harder-to-estimate metric.  Alternatively, the\ncoefficients in the convex combination might themselves come\nfrom some sort of statistical estimation, to fit the noise\nactually apparent in the data.  However, I don't know of any references\nrelated to these ideas.\n\n<H3><A NAME=\"tls\">L<sub>p</sub> Models for Independent Noise</A></H3>\n\nIf the noise is added to all coordinates (not just the dependent ones)\nit may be appropriate to use the Euclidean distance from the regression\nhyperplane in place of the vertical distance m.x+b-y used above.\nAlternatively, the same considerations as in\n<A HREF=\"point.html#dist\">single point estimation</A>\nmay make a different distance function appropriate.\n\nL<sub>2</sub> estimation with Euclidean distance is known as \"total least\nsquares\"\n[<A HREF=\"bib.html#GL80\">GL80</A>,\n[<A HREF=\"bib.html#M59\">M59</A>,\n[<A HREF=\"bib.html#P01\">P01</A>].\nThe optimal estimator must go through the centroid of the\ndata (this is not too hard to see if one applies duality) but the\nalgebra describing the estimator quality as a function of its parameters\nis messy and hard to invert.  However, the problem can\napparently be solved by singular value decomposition.\n\n<P>There's a nice connection between L<sub>1</sub> estimation in the\nindependent noise model\nand the famous \"k-sets\" or \"k-level\" problem from computational\ngeometry: if you fix a choice of slope, the problem is just a\none-dimensional L<sub>1</sub> problem which can be solved by the median.\nIf we use projective duality,\nwhat we want to know is the median among the points where the dual\nhyperplanes cross each possible vertical line;\nthis sequence of medians forms a surface in the arrangement and is known\nas the \"k-level\" (here k=n/2).  In two dimensions, this surface is just\na polygonal path, formed by walking left-to-right through the\narrangement and turning up or down at every crossing point you pass through.\nA famous problem posed by Erd&ouml;s\nand Lov&aacute;sz is on the number of edges this path can have;\nknown bounds are Omega(n&nbsp;log&nbsp;k) and O(nk<sup>1/3</sup>).\n\n<P>L<sub>infinity</sub> estimation is equivalent to computing the width of\na set (minimum distance between parallel supporting hyperplanes);\nit can be computed in O(n log n) time in the plane, but the best time\nknown in three dimensions is O(n<sup>3/2</sup>) [citations to be filled\nin later].\n\n<P>These independent noise estimators are generally invariant\nunder arbitrary translations or rotations of the coordinate system,\nhowever they are not invariant to general affine transformations.\n\n<H3>Dual-Independent Noise</H3>\n\nIf one has in mind some physical meaning to the dual interpretation,\nin which the data define lines and the estimator one is trying to find\nis a point near all these lines, then it may make sense to use Euclidean\ndistance in the dual plane instead of the primal.\nI.e., we define the distance from the estimator point (a,b)\nto a data line b=(-x<sub>i</sub>)a+y<sub>i</sub>.\nto be the usual Euclidean distance of that point from that line.\n\n<P>With that definition, one can make any sort of L<sub>p</sub>\nregression etc.  The L<sub>1</sub>, L<sub>2</sub>, and\nL<sub>infinity</sub> regressions can all be solved easily using\nsimple modifications of the algorithms for dependent noise models.\n(This is one big advantage of dual independence over primal independence.)\n\n<P>I don't know of any references, or work by actual statisticians,\non this sort of model.\n\n<H3><A NAME=\"LMS\">Least Median of Squares</A></H3>\n\nThe LMS estimator, defined by Rousseeuw\n[<A HREF=\"bib.html#R84\">R84</A>],\nminimizes the median error (rather than the sum of errors or squared\nerrors as in L<sub>1</sub> or L<sub>2</sub> estimation).\nEquivalently, we seek a planar strip of minimum possible\nvertical width that covers at least half the data points.\n\n<P>It is commonly claimed that\nthis estimator provides the maximum possible robustness to outliers\n-- up to half the data may be arbitrarily corrupted.\nThis claim seems to rely on an assumption that the data\nis well spread out along the independent coordinates,\nrather than being tightly clustered along lower dimensional subspaces in\na way that would\nallow an adversary to set up false estimates using large\nnumbers of valid data points.\nBut the same assumption must be made for any other robust regression method.\n\n<P>\nUnfortunately algorithms for computing the LMS estimator are relatively slow:\nO(n<sup>2</sup>) even in two dimensions\n[<A HREF=\"bib.html#SS87\">SS87</A>,\n<A HREF=\"bib.html#ES90\">ES90</A>].\nThere are theoretical reasons for believing that no exact\nalgorithm can be faster than this.\nHowever Mount et al\n[<A HREF=\"bib.html#MNRSW97\">MNRSW97</A>]\nprovide an O(n log n) time approximation as well as\na randomized algorithm that they believe should work well in practice\n(although it lacks theoretical performance guarantees).\n\n<P>In the dual framework, we are given an arrangement of lines,\nand we wish to find the shortest possible vertical line segment\nthat crosses at least half of the lines.\n\n<P>This estimator assumes a dependent noise model;\none can define LMS-like estimator in similar ways for\nindependent noise (in which case one is trying to find \na line minimizing the median distance to the points)\nor for dual-independent noise\n(in which case one is trying to find a minimum-radius circle\nthat crosses at least half of the lines).\n\n<H3><A NAME=\"repmed\">Repeated Median</A></H3>\n\nI don't know what this is, but it's the subject of\n[<A HREF=\"bib.html#MMN98\">MMN98</A>].\n\n<P>Here's my guess, based on the name -- I haven't looked the paper\nitself up yet.  LMS estimation can be interpreted as finding a set of\noutliers to throw away and ignore (namely the points further away\nthan the median distance it finds).  But once it throws away those\npoints, it then does something fairly stupid with the rest of the points:\njust the L<sub>infinity</sub> estimator, which is very sensitive to outliers.\nSo if the data that's left after throwing away half the points is still\nsuspect, it seems more reasonable to instead apply the same outlier\ndetection strategy recursively.\n\n<P>There's not much to say about algorithms for this, at least not\nwithout more progress on algorithms for the LMS estimator.\nThe reason is that if you have a quadratic algorithm for LMS,\nand apply it recursively as above, you can describe the recursion's time\nby the recurrence T(n)=T(n/2)+O(n<sup>2</sup>) which is again quadratic.\nSo asymptotically, the time for this recursive approach would be the\nsame as the time for simple LMS estimation.\n\n<H3><A NAME=\"medslope\">Median Slope</A></H3>\n\nThe so-called Thiel-Sen estimator (for planar data only) finds\na line y=mx+b\nby choosing m\nto be the median among the n(n-1)/2 slopes of lines determined\nby pairs of data points.  The remaining parameter b can then be\nfound by choosing the median among the values y-mx.\n\n<P>In the dual framework, we are given an arrangement of lines\nin the a-b plane.  Each pair of lines determines a point where\nthe two lines cross; we wish to find the median a-coordinate\namong these (n&nbsp;choose&nbsp;2) crossings points.\n\n<P>This method is only robust to n(1-1/sqrt(2)) ~ 0.29n outliers rather\nthan n/2 for the\nLMS estimator (proof: if there are n/sqrt(2) valid data points, they define\nroughly n^2/4 slopes, too many for the outliers to perturb the median).\nHowever, it can be computed more quickly: O(n log n) time by\nany of several algorithms\n[<A HREF=\"bib.html#BC98\">BC98</A>,\n<A HREF=\"bib.html#CSSS89\">CSSS89</A>,\n<A HREF=\"bib.html#DMN92\">DMN92</A>,\n<A HREF=\"bib.html#KS93\">KS93</A>,\n<A HREF=\"bib.html#M91b\">M91b</A>].\n\n<P>I guess in higher dimensions one could apply any robust single point\nestimation technique (e.g. centerpoint or least median of squares) to\nthe normal vectors to hyperplanes defined by d-tuples of points.\nBut I don't know of any work on such generalizations.\n\n<H3><A NAME=\"depth\">Regression Depth</A></H3>\n\nA \"nonfit\" (in the context of dependent-noise regression) is defined to\nbe a vertical hyperplane; these are bad because they can't be used to\npredict the values of the dependent variable.  Rousseeuw and Hubert [<A\nHREF=\"bib.html#RH99\">HR99</A>] define the \"depth\" of a hyperplane to be\nthe minimum number of points it must cross on any continuous motion that\nmoves it to a nonfit.  It doesn't make any difference if you restrict\nthe class of continuous motions to be rotations around some lower\ndimensional axis.  Equivalently, one can say that a plane\npartitions the data into two subsets; the depth is the \nminimum number of points that would have to change from one subset to\nthe other to get a partition achievable by a vertical hyperplane.\nThe depth of a plane is a lower bound on the number\nof outliers that would have to have occurred to make that plane\ninaccurate.\n\n<P>The dual version of this is much easier to understand.\nThe depth of a point in an arrangement is just the minimum\nnumber of lines (or hyperplanes) that you would need to cross\nto get from the point out to infinity.  A good fit according\nto this measure is then just a point with high depth;\nthat is, one that's enclosed by a large number of hyperplanes\nin every possible direction.\n\n<P>In one dimension, a hyperplane is just a point, a vertical hyperplane\nis just a point at infinity, and so the median of a point set\ngives optimal depth n/2.\nIn two dimensions, a very simple construction, the \"catline\"\n[<A HREF=\"bib.html#HR98\">HR98</A>] has depth n/3.\nThis is optimal, since one can find sets of points for which\nno line is better than n/3.\nIt turns out that any point set in any dimension has a hyperplane of depth\nat least n/(d+1), exactly matching the quality bounds known for centerpoints\n[<A HREF=\"bib.html#ABET98\">ABET98</A>].\nHowever the proof is nonconstructive.\n\n<P>Just as with the centerpoint, you probably want the deepest possible\nplane not just any deep plane.  This can be found efficiently in the\nplane\n[<A HREF=\"bib.html#KMRSSS99\">KMRSSS99</A>]\nand less efficiently in higher dimensions.\nOne can also efficiently approximate the deepest plane\nin linear time in any dimension using\n<A HREF=\"cluster.html#eps\">geometric sampling</A> techniques\n[<A HREF=\"bib.html#SW98\">SW98</A>].\n\n<P>Because (the primal formulation of) this estimator depends on a definition\nof a vertical direction, it makes sense for the dependent or dual-independent\nnoise models but not for independent noise.\nIt is invariant under affine transformations among only\nindependent variables, arbitrary affine transformations of the dual space,\nor more generally any continuous deformation of space that\ndoesn't ever flatten a tetrahedron (produce d+1 coplanar points) or make\na vertical triangle (d points forming a vertical plane).\n\n<H2><A HREF=\"cluster.html\">NEXT: Clustering</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"regress.html\" --></SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}