{"url": "https://www.ics.uci.edu/~eppstein/161/960305.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Three Dynamic Programs</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for March 5, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Three Dynamic Programs</h1>\n\nAs we saw last time, the basic idea of dynamic programming is\nsimple: \n\n<ul>\n<li>Start with an inefficient recursive algorithm.</li>\n\n<li>Speed it up by only solving each subproblem once and\nremembering the solution for later use.</li>\n\n<li>Maybe rearrange the order of subproblem computation to be more\nconvenient (a loop instead of a recursion).</li>\n</ul>\n\nI promised you I'd talk about regular expression matching, so we'll\nstart with that. I'll also describe algorithms for partitioning\nsets into equal-size subsets, and for finding the optimal order of\nmultiplying matrices. \n\n<h2>Regular expressions</h2>\n\nA regular expression is a string such as ba((na+bo)*)no. These\nstrings are interpreted as patterns that match longer strings,\nusing three basic operations: \n\n<ul>\n<li><b>Concatenation</b>. If pattern p1 matches string s1, and p2\nmatches s2, then p1p2 matches s1s2. The pattern above concatenates\nthree subpatterns: ba, (na+bo)*, and no. In each of these patterns,\nthe two-letter combinations are concatenations of single\nletters.</li>\n\n<li><b>Iteration</b>. If pattern p matches string s, then pattern\np* matches any number of copies of s, all concatenated together.\n\"Any number\" can be zero, so p* also matches the empty string.</li>\n\n<li><b>Alternation</b>. If pattern p1 matches string s1, and p2\nmatches s2, then p1+p2 matches s1 or s2. In the pattern above,\n(na+bo) matches either \"na\" or \"bo\", but not combinations such as\n\"nabo\".</li>\n</ul>\n\nFor instance, the pattern above matches strings bano, banano,\nbabono, banabobonano, etc. \n\n<p>The algorithms I'm going to describe test whether some string\nmatches a regular expression. Generally, we don't want to know that\nso much as whether a substring matches. We can solve this problem\nby introducing \"wildcards\", which I'll denote by a question mark.\nSo ?*ba((na+bo)*)no?*. would match any string having a substring\nmatching ba((na+bo)*)no.</p>\n\n<h2>Graphs from regular expressions</h2>\n\nGiven any regular expression, there's a natural way of transforming\nit into a certain kind of graph. We make a subgraph for each piece\nof the expression, with one incoming and one outgoing edge. To form\nthe overall graph, we hook these subgraphs together in certain\nways: one after the other for concatenation, side by side for\nalternation, or looping back on itself for iteration. Here's the\ngraph for our original pattern ba((na+bo)*)no: \n\n<center><img src=\"banabono.gif\" width=\"509\" height=\"167\" alt=\n\"pattern matching graph\"></center>\n\n<p> The strings matched by the pattern then correspond exactly to\nthe sequences of letters you go through at the vertices of paths in\nthis graph. So we can think of the regular expression matching\nproblem as one of finding an appropriate path in the graph, one\nthat goes through the right vertices in the right order.</p>\n\n<p>To apply dynamic programming, we'll start with a recursive\nalgorithm for this problem, that tests for a given vertex in the\ngraph, and a suffix of the input, whether there is a path from the\nstart to that vertex that matches that substring.</p>\n\n<pre>\n    recursive_match(graph,vertex,string,strlen)\n    {\n        for (each vertex w)\n            if (recursive_match(graph, w, string, strlen-1) &amp;&amp;\n                    (w -&gt; v matches string[strlen-1]))\n                return TRUE;\n        return FALSE;\n    }\n</pre>\n\nTo turn this into a dynamic program, we just remember the solutions\nfor each value of the vertex and strlen parameters. There are O(m)\nchoices of the vertex (where m denotes the length of the pattern)\nand O(n) choices of strlen (where n is the length of the string to\nbe matched), so there are O(mn) subproblems. Each takes O(m) time\nto compute (it involves a loop through each other vertex) so the\ntotal time is O(m^2 n). \n\n<p>To think about how to speed this up, let's turn it into a graph\npath problem as before. We'll make a new big graph; think of it as\nbeing the cartesian product of the string with the smaller pattern\ngraph we already constructed. A vertex of the new graph will\ncorrespond to pair (v,i) where v is vertex of the pattern graph and\ni is index into string. We add an edge (v,i) -&gt; (w,i) if v can\nget to w without processing any more characters, and an edge (v,i)\n-&gt; (w,i+1) if you can get from v to w while matching the single\ncharacter string[i].</p>\n\n<p>For instance, for the string \"bano\" this product graph looks\nlike four copies of the original pattern graph. Each copy keeps\nonly the edges that don't involve matching a character, and\ndifferent copies are connected by edges that depend on the\ncorresponding positions of the string \"bano\".</p>\n\n<center><img src=\"bano.gif\" width=\"487\" height=\"731\" alt=\n\"product of pattern graph and text\"></center>\n\n<p> The pattern matching problem then turns out to be equivalent to\nreachability in this graph. Since the graph has O(mn) vertices and\nedges, we can test reachability and solve the regular expression\nmatching problem in time O(mn).</p>\n\n<h2>Partition problem</h2>\n\nSuppose you have 20 files to store, with lengths measured in\nmegabytes. The total size is 200 MB, and you have two 100-MB\ndrives. \n\n<p>Can you fit them all onto the disks?</p>\n\n<p>The answer is sometimes yes, sometimes no. E.g. if the file\nsizes are 18x5, 1x47, 1x63 it's not possible. The two big files\ncan't be together because they're too big, and they can't be\nseparate because then neither side could be a multiple of ten. But\n4x38, 16x3 is possible: put 2x38 and 8x3 on each side.</p>\n\n<p>How to tell automatically when this problem can be solved?</p>\n\n<p>As usual, let's start with a recursive algorithm. Given a\nsequence x1,x2,x3,...xn of positive integers and a target sum T\n(here n=20, T=100) then if some group of values adds to T, either\nxn is in the group or it isn't. So the problem is solvable if\neither the same sum is solvable for x1...x(n-1), or if T-xn is\nsolvable for x1...x(n-1).</p>\n\n<pre>\n    partition(X,n,T)\n    {\n        if (T=0) return true else\n        return (partition(X,n-1,T) or partition(X,n-1,T-X[n])\n    }\n</pre>\n\nAs often happens, this simple recursive algorithm is very\ninefficient. A problem with n values leads to two subproblems with\nn-1 values, so there are 2^n recursive calls. We can make some\nminor improvement if we test whether T&lt;0 and return false\nimmediately, but that still won't be enough to really speed up the\nalgorithm. \n\n<p>Now let's turn it into a dynamic program. As usual, we start\nwith the memoizing version, based on the same recursion, but\nstoring subproblem solutions. We store them in an array M[n,T]\ncontains three values: true, false, undefined. Initially all values\nare assumed to be initialized to undefined.</p>\n\n<pre>\n    partition(X,n,T)\n    {\n        if T&lt;0 return false\n        else if T=0 return true\n        else if M[n,T] is undefined\n            M[n,T] = partition(X,n-1,T) or partition(X,n-1,T-X[n])\n        return M[n,T]\n    }\n</pre>\n\nAnalysis: each call takes constant time unless we fill in a new\narray value. There are nT array values to fill in, each of which\ninvolves constant time plus O(1) subroutine calls. So the total is\nO(nT) time. \n\n<p>As usual, we can simplify this by reordering it and getting a\nbottom up dynamic program:</p>\n\n<pre>\n    partition(X,n,T)\n    {\n        array M[n,T] of boolean values\n        for i = 1 to n\n        {\n            M[i,0] = true\n            for j = 1 to T\n            {\n                if (X[i] &gt; j) M[i,j] = M[i-1,j]\n                else M[i,j] = M[i-1,j] or M[i-1,j-X[i]]\n            }\n        }\n        return M[n,T]\n    }\n</pre>\n\nAnd as in the longest common subsequence problem, reordering saves\nspace and simplifies the method even further: \n\n<pre>\n    partition(X,n,T)\n    {\n        array M[T] of boolean\n        M[0] = true\n        for i = 1 to n\n            for j = X[i] to T\n                M[j] = M[j] or M[j-X[i]]\n        return M[T]\n    }\n</pre>\n\nLike the previous two dynamic programs, this can be turned into a\ngraph reachability problem by making one vertex per subproblems and\nconnecting two vertices by an edge when one depends on the other.\nLet's finish by looking at a problem, matrix multiplication, that\nis different: as far as I can tell it isn't related to paths in\ngraphs. It's also a good example of a dynamic program in which each\nrecursive call is more than constant time. \n\n<h2>Optimum matrix multiplication</h2>\n\nTwo facts you need to know about matrices: multiplying an i*j\nmatrix by a j*k matrix produces an i*j*k matrix and takes time\nO(ijk). (Actually there are somewhat faster algorithms based on\ncomplicated divide and conquer schemes but that doesn't affect the\nrest of this section.) \n\n<p>Given a sequence of integers</p>\n\n<pre>\n    (x1,x2,x3,...xn)\n</pre>\n\ncorresponding to matrices \n\n<pre>\n    M[x1,x2], N[x2,x3], O[x3,x4] ...\n</pre>\n\nthere are several ways to do the multiplication: \n\n<pre>\n    (M N) O   or  M (N O)\n</pre>\n\ntaking different total times: \n\n<pre>\n    O(x1x2x3 + x1x3x4)  or  O(x1x2x4 + x2x3x4)\n</pre>\n\nWhat is the best way to multiply the matrices? how much time does\nit take? \n\n<p>If there are only four integers in the sequence (and so only\nthree matrices) you can just compare both possibilities, but for\nlonger sequences there may be exponentially many different\nmultiplication orders to test. We'd like to find the best one\nwithout testing them all.</p>\n\n<p>The basic idea of the algorithm is to look at the last\nmultiplication you do:</p>\n\n<pre>\n    (M N O ... Q) (R S T ... Z)\n</pre>\n\nNote that the two groups should be optimally multiplied inside\nthemselves. So if you only knew where to split them, you could\nsolve the problem using two recursive subproblems. Fortunately, it\nisn't allowed to reorder the matrices, so there are only O(n)\ndifferent possible splits. We simply try them all. \n\n<p>It's easier to understand the solution if we instead look at\nlist of dimensions. This is again splits into two groups,</p>\n\n<pre>\n    (x1 x2 ... xk)\n    (xk x(k+1) ... xn)\n</pre>\n\n(note that the two overlap by one integer). We want to find the\noptimal value of k; we can try all possible values and choose the\none that gives the best total matrix multiplication cost. \n\n<p>As usual, we start with a recursive procedure.</p>\n\n<pre>\n    mtime(X,i,j)\n    {\n        if (j&lt;=i+1) return 0;\n        cost = +infinity;\n        for (k = i+1; k &lt;= j-1; k++)\n            cost = min(cost, mtime(X,i,k)+mtime(X,k,j)+X[i]X[j]X[k])\n        return cost\n    }\n</pre>\n\nThis is inefficient (it takes exponential time). But there are only\nO(n^2) subproblems being solved: that's how many ways there are of\nchoosing i and j. We can memoize by storing a solution T[i,j]: \n\n<pre>\n    array T[n,n] = {-1, -1, ..., -1};\n\n    mtime(X,i,j)\n    {\n        if (j&lt;=i+1) return 0;\n        else if (T[i,j] &gt;=0) return T[i,j];\n        cost = +infinity\n        for (k = i+1; k &lt;= j-1; k++)\n            cost = min(cost, mtime(X,i,k)+mtime(X,k,j)+X[i]X[j]X[k])\n        T[i,j] = cost;\n        return cost;\n    }\n</pre>\n\nEach call takes constant time unless it fills in an entry. But in\nthat case it takes O(n) time. There are O(n^2) entries to fill in,\nso the total time of this memoizing dynamic program is O(n^3). \n\n<p>Finally, let's write a bottom up version that computes the\nanswers in a simpler order. We have to be careful: it would not\nwork to do for(i=1 to n) for(j=1 to n). For one thing, the problem\nonly makes sense when j&gt;i. But also, if we did it in that order\nwe would need recursive values before they were computed. The whole\nidea of the simpler order is not to have to test whether a value\nhas been computed, but instead to know because of the order that it\nalready has been and just look it up. Here one idea that works\npretty well is to compute in order of the difference d=j-i. So:</p>\n\n<pre>\n    mtime(X)\n    {\n        array T[n,n]\n        for(d = 1; d &lt;= n-1; d++)\n            for (i = 1; i &lt;= n-d; i++)\n            {\n                j=i+d\n                if (j&lt;=i+1) T[i,j] = 0\n                T[i,j] = +infinity\n                for (k = i+1; k &lt;= j-1; k++) do\n                    T[i,j] = min(T[i,j], T[i,k]+T[k,j]+X[i]X[j]X[k])\n            }\n        return T[1,n];\n    }\n</pre>\n\nNow it's even more obvious that the total time is O(n^3): just look\nat the three nested loops. \n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960305.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}