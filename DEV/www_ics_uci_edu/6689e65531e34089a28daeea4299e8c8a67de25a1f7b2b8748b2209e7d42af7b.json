{"url": "https://www.ics.uci.edu/~pazzani/CogSci95.html", "content": "<html><Title>Learning Sets of Related Concepts: A Shared Task Model</title></head><body>\n<H1>Learning Sets of Related Concepts: A Shared Task Model</H1>\n<h2>Tim Hume </h2>\n<h4>ICS Dept.,\nUniversity of California, Irvine.\nIrvine, CA 92717</h4>\n<address>\nhume@interplay.com<BR>\n</address>\n<H2><p>\nMichael J. Pazzani </h2>\n<h4>ICS Dept.,\nUniversity of California, Irvine.\nIrvine, CA 92717</h4>\n<address>\npazzani@ics.uci.edu\n(949)824-5888\n<a href=\"http://www.ics.uci.edu/dir/faculty/AI/pazzani\"> http://www.ics.uci.edu/dir/faculty/AI/pazzani</a>\n</address>\n<p>\n<h2>Abstract</h2>\n<p>\nWe investigate learning a set of causally related concepts from examples.  We\nshow that human subjects make fewer errors and learn more rapidly when the set\nof concepts is logically consistent.  We compare the results of these subjects\nto subjects learning equivalent concepts that share sets of relevant features,\nbut are not logically consistent.  We present a shared-task neural network\nmodel simulation of  the psychological experimentation.\n<h2>\nIntroduction</h2>\nResearchers have investigated how the relevant background knowledge of the\nlearner influences the speed or accuracy of concept learning (e.g., Murphy\n&amp; Medin 1985, Nakamura 1985, Pazzani 1991, Wattenmaker <i>et al.</i> 1986).\nHowever, the psychological investigation to date has only explored problems\nwhere subjects learn a single concept and the relevant background knowledge is\neither brought to the experiment by the subject or given in written\ninstructions.  In contrast,  research in machine learning has addressed issues\nthat occur when learning a set of related concepts.  For example, relevant\nbackground concepts might be learned inductively from examples before learning\nconcepts that depend upon this knowledge (Pazzani 1990).  Here, we report on\ntwo experiments in which subjects induce the relevant background knowledge from\nexamples and use this background knowledge to facilitate later learning.  The\nexperiments illustrate the importance of learning the relevance of combinations\nof features, rather than individual features.  We model this experiment with\nshared-task neural networks (Caruana, 1993).  <p>\nIn the first experiment,  subjects first induce the relevant background\nknowledge and then have the opportunity to use this knowledge in later\nlearning.  To more closely simulate the real world, we ran a second experiment\nwherein the subjects induce the relevant background knowledge at the same time\nas learning the concept that depends on this knowledge.  In both experiments,\nsubjects were divided into two groups.  One group, the \"feature consistency\"\ngroup, learned a complex concept that shared relevant features with previously\nlearned related concepts, but was not logically consistent with those concepts.\nAnother group, the \"logical consistency\" group, learned a complex concept that\nwas logically consistent with previously learned related concepts. <b></b>\n<h2>\nInitial Psychological Experimentation</h2>\nIn the first experiment,  subjects were asked to imagine that they work for the\nUS Forest Service and were assigned the task of learning to predict years in\nwhich there is a severe risk of forest fire danger in the fall.  Four concepts\nhad to be learned in the experiment -- one concept in each of four phases.  All\nsubjects learned the same 3 background concepts in phases 1-3.  Then,  for\nphase 4, they were divided into two groups (the logical consistency group and\nthe feature consistency group) to learn one of two separate concepts which\ndepended on the background concepts.<p>\nThe first phase of the experiment was designed to minimize the effects of the\nsubjects' domain-specific pre-existing theories by having every subject learn\nthe same concept.  In this first phase, subjects had to learn when there is a\nsevere risk of forest fires in the fall given data on rain in the spring and\nsummer. An example of these data is shown in Figure 1.  Subjects were given\ndata that indicated that there is a severe risk of forest fires in the fall\nonly when there is both a wet spring and a dry summer.  This rule is consistent\nwith the knowledge of most people who live in Southern California. In the\nremaining phases, when we measure the learning rate and number of errors made\nby subjects,  novel stimuli are used as features to insure that the knowledge\nwas acquired during the experiment.<p>\n<hr>\n<IMG SRC=\"CogSci951.gif\" ALIGN=BOTTOM><b><p>\n<p>\nFigure 1.</b>  An example of the abstract feature stimuli used for the first\nconcept .\n<hr>\nNext, the subjects were told that the US Forest Service needs to do advance\nplanning, so it cannot wait until the end of summer to predict when there will\nbe a severe risk of fire in the fall.  The subjects again examined data from\nseveral years.  This time, however, the data was from five simulated scientific\ninstruments that are used each January to detect the presence of factors that\nmay be useful in predicting the amount of rain.  When one of the instruments\ndetects the presence of a particular factor, it displays a distinctive graph,\nas shown in Figure 2.  Otherwise, a bar is shown to mark the absence of the\ninstrument's graph (see Instrument 3 of Figure 2)  Each instrument displays a\ngraph whose shape differs from that of the other instruments.   In this second\nconcept learning problem, subjects had to learn to predict from the instrument\nreadings when there would be a rainy spring.  All subjects were given data that\nindicated there would be a wet spring when one particular instrument showed a\ndistinctive graph.  All subjects learned a rule of the form \"There will be a\nwet spring when Instrument-A displays a graph,\" with the instrument\ncorresponding to Instrument-A selected randomly.  This concept will serve as\nbackground knowledge for learning the fourth concept.<p>\n<hr>\n<IMG SRC=\"CogSci952.gif\" ALIGN=BOTTOM><b><p>\n                Figure 2.</b>  An example of the stimuli used for the second,\nthird and fourth concepts.\n<hr>\nIn the third concept learning problem, subjects learned another piece of\nbackground knowledge.  Here, subjects had to learn to predict from the\ninstrument readings when there would be a dry summer.  All subjects were shown\ndata derived from the rule \"There will be a dry summer when Instrument-B or\nInstrument-C  displays a graph.\"<p>\nIn the fourth, and final, concept learning problem, subjects had to learn to\npredict from the instrument readings when there would be a severe risk of fire\nin the fall.  Concepts 1-3 served as background knowledge for this concept.\nSubjects in the logical consistency group were given data that indicated there\nwould be a severe risk of fire when Instrument-A displayed a graph and when\neither Instrument-B or Instrument-C (or both) displayed a graph, i.e., A\nand (B or C).  This concept is logically consistent with\nthe first three concepts that were learned.  Subjects in the feature\nconsistency group were given data that indicated there would be a severe risk\nof fire when Instrument-C displayed a graph and when either Instrument-B or\nInstrument-A (or both) displayed a graph, i.e. C and (B\nor A).  Although not consistent with the concepts that were learned,\nthis concept shares relevant features with the logical consistency concept.\n<b><p>\nSubjects. </b>The subjects were 18 male and female undergraduates attending the\nUniversity of California, Irvine who participated in this experiment to receive\nextra credit in an introductory psychology course.  <b><p>\nStimuli. </b>The stimuli consisted of data that were displayed on a computer\nmonitor. In the first concept, since there are two two-valued features, 4\ndistinct stimuli were constructed.  In the remaining three concepts, there were\n32 distinct stimuli since there are five two-valued features.  The stimuli were\npresented in a random order for each subject.<b><p>\nProcedures.</b> Each subject was shown data on the computer from a single year\nand asked to make a prediction (e.g., whether there would be a severe risk of\nfire in the fall) by clicking on a circle next to the word Yes or a circle next\nto the word No (i.e., using a mouse to move a pointer to the circle and\npressing a button on the mouse).  Next, the subject clicked on a box labeled\nCheck Answer. While still displaying the data, the computer indicated to the\nsubject whether his answer was the correct answer.  If the subject's answer was\ncorrect, the subject could click on a box labeled Continue and data from\nanother year was shown.  Otherwise, he selected a different answer and clicked\non Check Answer again.  This process was repeated until the subjects performed\nat a level that ensured they had learned an accurate approximation to the\nconcept (making no more than one error in any sequence of 24 consecutive\ntrials).  The subjects were allowed as much time as they wanted to make their\nprediction and to view the data after the correct answer was shown.  This\nprocess of learning a concept to criteria was repeated for each of the four\nconcepts learned.  We recorded the number of the last trial on which the\nsubject made an error, the total number of errors made by the subject for each\nconcept, and the number of made on each block of 16 trials.  If the subject did\nnot obtain the correct answer after 96 trials, we recorded that the last error\nwas made on trial  96.  <b><p>\nResults. </b>Subjects in the logical consistency group required an average of\n27.6 trials to learn the fourth concept, while subjects in the feature\nconsistency group required an average of 50.4 trials t(16) = 1.91,  p &lt; .05.\nSubjects in the logical consistency group made an average of 6.8 errors, while\nsubjects in the feature consistency group made an average of 14.0 errors t(16)\n= 2.135, p &lt; .05.<b>  </b>\n<h2>\n Multiple Concept Learning</h2>\nIn Experiment 1, subjects accurately induced three relevant background\nconcepts, prior to learning a single concept which depended upon those\nconcepts.  The order of the concepts is the ideal order for subjects to first\nacquire knowledge inductively and then use that knowledge in future learning.\nHowever, the natural world does not have a benevolent teacher who orders\nexperiences for the learner.  To more closely simulate the natural world, in\nthe second experiment, those concepts that had the same stimuli from the first\nexperiment (the last three concepts) are learned at the same time.  For each\npresentation of stimuli, subjects predicted whether there would be a rainy\nspring, a dry summer, and a severe risk of fire in the fall (see Figure 3).\nWith this exception, Experiment 2 was identical to Experiment 1. For the second\nlearning phase, subjects had to click on all three boxes correctly before\nproceeding to the next stimuli.  We recorded the number of the last trial on\nwhich the subject made an error and the total number of errors made by the\nsubject only for the concept that involved predicting whether there would be a\nsevere risk of fire in the fall from the instrument data.  In addition, for\nthis concept, we also recorded the number of errors made by the subject on\nblocks of 16 trials.  If the subject did not obtain the correct answer after\n128 trials, we recorded that the last error was made on trial 128.<p>\n<hr><IMG SRC=\"CogSci953.gif\" ALIGN=BOTTOM><p>\n<b><p>\nFigure 3.</b> An example of the stimuli used in the second phase of Experiment\n2. \n<hr>\n  <b>Results.  </b>The subjects in the logical consistency group required an\naverage of 77.8 trials to predict whether there would be a severe risk of fire\nin the fall from the instrument data, while subjects in the feature consistency\ngroup required an average of 109.9 trials t(16) = 1.81, p &lt; .05.  In\naddition, subjects in the logical consistency group made an average of 29.3\nerrors, while subjects in the feature consistency group made an average of\n41.4. errors.  This last figure is marginally significant t(16) = 1.41, p &lt;\n.1.<b>  </b>The results demonstrate that simultaneously learning a set of\nrelated concepts is easier when the concepts are logically consistent than when\nthe concepts merely share a set of relevant features.  Figure 4 graphs the\npercentage of errors made by the two groups at predicting a severe risk of fire\nin the fall from the instrument data as a function of the number of trials.  It\nshows that subjects in logical consistency and feature consistency groups\nperform similarly until trial 64. After this point, subjects in the logical\nconsistency group make fewer errors than those in the feature consistency\ngroup. <p>\n<hr>\n<IMG SRC=\"CogSci954.gif\"ALIGN=BOTTOM><p>\n<b><p>\nFigure 4</b>  The mean percentage of errors   made by subjects in the logical consistency     and feature consistency groups  as a function  of the trial in Experiment 2.   \n<hr>\n<h2>\nDiscussion</h2>\nThere are three findings of note in these experiments. First, subjects in the\nlogical consistency condition make fewer errors and require fewer trials to\nlearn.  While this finding agrees with our intuition on how people should\nlearn, previous experiments involving background knowledge have not had\nsubjects learn this background knowledge.   Furthermore, current cognitive\nmodels do not perform in this manner and there is no quantitative data on how\nbackground knowledge that is learned  inductively influences the learning rate\nand number of errors made by learners. <p>\nSecond, learning the relevance of individual features cannot account for these\nfindings. Wisniewski and Medin (1994) use  the term selection models to refer\nto learning models that use prior knowledge to determine which features are\nrelevant. Lien and Cheng (1989) present one such model.  Selection models would\nnot be able to explain the results since both the logical consistency and\nfeature consistency groups learn concepts with the same relevant features. <p>\nThird, although the subjects in the logical consistency group learn faster and\nmake fewer errors than subjects in the feature consistency group, they learner\nslower and make more errors than would be predicted by existing computational\nmodels of the influence of prior knowledge such as Explanation-based learning\n(EBL) (Mitchell <i>et al.</i> 1986). EBL is a machine learning method that\nderives concepts from background knowledge. At first, it might seem that EBL\nwould serve as an ideal model of the use of prior knowledge in learning. Its\ninputs correspond exactly to those items learned in Phases 1-3 of the first\nexperiment, and its output correspond exactly to the concept to be learned in\nPhase 4.  However, there are several problems with EBL as a model of human\nlearning.  First,  EBL algorithms would learn more quickly than the logical\nconsistency subjects.  Since the fourth concept can be deductively derived from\nthe preceding three, EBL would make no errors on this data. Second, EBL cannot\nfunction unless the background knowledge is complete.  For example, EBL could\nnot acquire the concepts in Phases 2 and 3 since these are just associations\nbetween stimuli and weather predictions.  \n<h2>\nModeling with Shared-Task Networks</h2>\nHere, we propose a model of the psychological experiments using multi-layer\nneural networks trained with error backpropagation (Rumelhart <i>et al.</i>\n1986) to learn multiple concepts.  First, we would like to make a distinction\nbetween sub-task learning and shared-task learning.  In sub-task learning, some\nof the concepts to be learned serve as background concepts for the other\nconcepts to be learned.  For instance, in poker, learning the hands <i>two\npair</i> and <i>one pair</i> is a sub-task problem because <i>one pair</i> is a\nbackground concept for learning <i>two pair</i>.  Shared-task learning, on the\nother hand, involves learning concepts that share subordinate concepts.  As an\nexample, learning both the hands <i>two pair</i> and <i>full house</i> require\nknowing what <i>one pair</i> is, but <i>two pair</i> and <i>full house</i> do\nnot require knowledge of each other.<p>\nThe network diagrammed on the left side of Figure 5 shows a typical way of\nusing networks to learn sub-task concepts with the network applied to\nExperiment 1.  (Please note that in order to make the diagrams more\ncomprehensible, only some of the connections between nodes are drawn.  In an\nactual network, all the nodes of a hidden layer would be connected to all of\nits input and output nodes.)  The network first learns the section enclosed in\nthe solid line.  The two inputs are analogous to the abstract features shown\nour subjects in the first phase of the experiment.  The output is the network's\nguess at whether or not there will be a severe risk of fire in the fall.\nSecond, the network is trained on the section enclosed in the dashed line.\nThis represents learning the Wet Spring concept.  The five inputs (A-E) on the\nleft represent the five instrument displays shown to the human subjects.  The\noutput is the network's prediction at whether there will be a wet spring.\nThird, the Dry Summer concept is trained on the network section enclosed in the\ndotted line.  The same five inputs are used as were used to learn the previous\nconcept.  The output is the network's guess at whether there will be a dry\nsummer.  The wet spring and dry summer concepts are the sub-tasks the network\nlearns.  The final Fire in the Fall concept is represented by training and\ntesting on the entire network.  The network uses the five inputs to decide if\nthere will be a severe risk of fire in the fall.<p>\n<hr>\n<IMG SRC=\"CogSci956.gif\"><b><p>\n<p>\nFigure 5</b>. Neural network diagrams.  The network on the left is a sub-task\nlearning model.  The network on the right is a shared-task learning model. \n<hr>\nA system such as KBANN (Towell <i>et al.</i> 1990) could set up a network like\nthe one on left side of Figure 5, given symbolic inferences rules that\nrepresent the knowledge acquired in the first three phases of the experiment. A\nproblem with this method in modeling the experiment is that since the network\nwould already be trained on the three background concepts, it would not require\nany training to learn the final concept in the logical consistency group of our\nexperiment.  This is the same problem that EBL suffers from.<p>\nCaruana (1993) has done work on shared-task learning using networks with one\nhidden layer.  The network on the right of Figure 5 is a representation of such\na network.  A major advantage of this model is that the hidden layer can create\nnew features which can be shared by all of the output units.  To model the\nfirst experiment, the network first uses the five inputs and only the Wet\nSpring output unit is trained, i.e., receives feedback on its performance.\nSecond, the same five inputs are used, but only the Dry Summer output unit is\ntrained.  Third, the Fire in the Fall output unit is trained and tested using\nthe five inputs.  <p>\nWe performed experiments with shared-task neural nets to see if they could\nmodel the results from our psychological experiments since it appeared that\nthis method could learn the combinations of features in addition to feature\nrelevancy.  These networks might also be able to combine features and store the\ncombination in the network just as it stores learned knowledge. In both\nexperiments, the first phase used 2 abstract features as stimuli while the\nlater phases used 5 instrument displays.  Since the network cannot learn\nconcepts with different forms of inputs, it cannot be trained on the first\nphase.  However, the network can be used to learn the other phases of the\nexperiments. To model the sequential experiment (Experiment 1), the network\nfirst uses the 5 inputs and only the Wet Spring output unit is trained, i.e.,\nreceives feedback on its performance.  Second, the same 5 inputs are used, but\nonly the Dry Summer output unit is trained.  Third, the Fire in the Fall output\nunit is trained and tested using the 5 inputs.  Modeling the simultaneous\nexperiment (Experiment 2)is done by training all 3 of the output nodes at the\nsame time, but only using the Fire in the Fall unit for testing.<p>\nThe logical form of the data was the same as used in the psychological\nexperiments.  The first output unit had a value of one when one random feature,\nsay A, had a value of 1.  The second output unit had a value of 1 when either\n(or both) of two other randomly selected features, say B and C, had values of\n1.  To model the logical consistency group, the third output unit had a value\nof 1 when feature A had a value of 1 and either feature B or feature C (or\nboth) had a value of 1, i.e. A and (B or C).  The network\nused was a feed-forward system with one layer of 20 hidden units.  The\ngeneralized delta rule was used for training and the logistic function was used\nfor activation.  At testing, a network output value greater than 0.5 was\ntreated as a 1 and a value below  0.5 was treated as a 0 to model the forced\nguessing that was applied to the human subjects.  Momentum was set at 0.90 and\nthe learning rate was set at 0.25.<p>\nTo model Experiment 1, we trained the network to sequentially learn each of the\n3 concepts: wet spring, dry summer, and fire in the fall.  We first trained the\nnetwork to learn when an example was a positive example of the wet spring\nconcept, i.e. when the first output unit would have a value of 1 as a function\nof the 5 features.  After each epoch through the training data, the network was\ntested to see if it could correctly predict the value of the first output unit\non at least 31 of the 32 examples.  If it could, the network was then trained\non learning when the second output unit (dry summer) was true as a function of\nthe 5 features.  If it could not reliably predict the first feature, it was\ntrained on another epoch through the data.  After it had learned to reliably\npredict the second output unit, it was trained to predict the third output unit\n- the fire in the fall concept.  Data was recorded on how many epochs the\nnetwork took to learn the final concept.  The process of learning each concept\nsequentially was repeated 50 times.<p>\nThe network required an average of 5.96 epochs, or 190.72 trials, to learn the\nlogical consistency set, while it took significantly longer, 8.50 epochs or\n272.00 trials, to learn the feature consistency set, t(98) = 6.06, p &lt; .05.\nSimilar to the human subjects, this network sequentially learned the set of\nconcepts more easily when it was logically consistent than when the concepts\nmerely share features.  <p>\nTo model Experiment 2, we trained the network to simultaneously learn all three\nconcepts. The network was trained on all 3 of the concepts, but was tested only\non the third concept.  After each epoch through the training data, the network\nwas tested to see if it could correctly predict the value of the third feature\non at least 31 of the 32 examples.  If it could, then training stopped;\notherwise, it was trained for another epoch.  Data was kept on how many errors\nthe network made on each epoch and on which epoch the network learned the final\nconcept.  The process of learning the concepts was repeated 50 times.<p>\nThe neural net required an average of 7.12 epochs, or 227.84 trials, to learn\nthe logical consistency set, while it took significantly longer, 9.66 epochs or\n309.12 trials, to learn the feature consistency set, t(98) = 5.039, p &lt; .05.\nSimilar to the human subjects, this network simultaneously learned the set of\nconcepts more easily when it was logically consistent than when the concepts\nmerely share features.  Figure 6 graphs the percentage of errors made on the\ntwo sets as a function of the number of epochs.  It shows that after the second\nepoch, the graph is similar to Figure 4.  On the logically consistent\ncondition, the network becomes accurate with fewer training epochs.<p>\n<hr>            \n<IMG SRC=\"CogSci955.gif\"ALIGN=BOTTOM>\n<b><p>Figure 6</b>  The mean percentage of errors   made by the  neural network in the logical consistency and feature consistency groups as a function of the epoch.\n<hr>\nShared task networks are able to model these results because they can create\nnew abstract features and use these features to influence learning other\nconcepts.  The network requires some training to determine how to use these\nabstract features, but less training than would be required if new concepts\nwere not consistent with the concepts learned earlier. The shared task network\nis an example of what Wisniewski and Medin (1994)  call a tightly coupled\nmodel.  Prior knowledge, in this case created by prior learning, selects the\nrelevance of features (by having higher weights on some connections), and\ncreates new features (as represented in the hidden units).  Furthermore,\nfeedback during learning one concept can change the features or strengths of\nthe hidden units used by other concepts.  \n<h2>\nConclusions</h2>\nAlthough the general topic of learning a series of concepts has been discussed,\nprevious research has focused on attentional phenomena such as the\nintradimensional and extradimensional shift in which subsequent concepts share\nrelated features with prior concepts.  However, these approaches consider sets\nof arbitrary groups of concepts rather than concepts that are causally related.\nWaldmann and Holyoak (1990) argue that the causal induction process differs\nfrom the learning process used to acquire arbitrary concepts.  In particular,\nwe show that concepts acquired by induction in one phase of an experiment\ninfluence later learning in much the same manner as concepts acquired by\nreading written instructions or  prior background concepts.<p>\nWe have focused on how prior knowledge facilitates learning. We should also\npoint out that incorrect prior knowledge may also hinder learning by providing\nmisconceptions (Chi, Slotta &amp; de Leeuw, 1994). It is only when prior\nknowledge is compatible with the new knowledge to be acquired that we\nanticipate a positive effect.<p>\nClassical concepts that consistent of sets of necessary and sufficient features\nhave several flaws. Few concepts people encounter have such rigorous logical\ndefinitions (Rosch, 1978).   More recently, it has become apparent that\nconcepts do not exist and are not learned in isolation. Here, we have presented\nquantitative results on how induced background knowledge influence the rate of\nlearning and the number of errors made during learning.  While we have found\nthat having relevant, correct background knowledge facilitates learning, it\ndoes not eliminate the need for learning. That is, unlike previous learning\nmodels, when subjects have learned rules corresponding to  \"A -&gt; WetSpring,\"\n\"B or C -&gt;  DrySummmer\" and \"WetSpring and DrySummmer\n-&gt;  FireInFall\"  they do not automatically know that  \"A and (B\nor C) -&gt;  FireInFall.\"  We believe that one flaw in previous\nlearning models that use prior knowledge is that the equate an explanation with\na logical proof, and use rules that have necessary and sufficient\npreconditions.  Such rules may be as rare in the real world and as cognitively\nimplausible as concepts that consistent of necessary and sufficient\ndefinitions. \n<h2>\nAcknowledgments</h2>\nThe research reported here was supported in part by NSF grant IRI-9310413.  We\nthank Kamal Ali, Dorrit Billman, Cliff Brunk, Piew Datta, Dennis Kibler, Chris\nMerz, Brian Ross, and David Schulenburg for comments on various phases of this\nwork.<p>\n                      \n\n<h2>\nReferences</h2>\nCaruana, R. (1992).  Multitask learning: A knowledge-based source of inductive\nbias.  <i>Proceedings of the Tenth International  Machine Learning\nConference</i>  (pp. 41-48).  San Mateo, CA: Morgan Kaufman.<p>\nChi, M., Slotta, J. &amp; de Leeuw,  N. (1994). From theories to processes: A\ntheory of conceptual changes for learning science concepts.    <i>Learning and\nInstruction, 4, </i>27-43.<p>\nLien, Y., &amp; Cheng, P. (1989).  A framework for psychological induction:\nIntegrating the power law and covariation views. <i> The Eleventh Annual\nConference of the Cognitive Science Society </i> (pp. 729-733).  Ann Arbor, MI:\nLawrence Erlbaum Associates, Inc.<p>\nMitchell,  T., Keller,  R.,  &amp; Kedar-Cabelli, S.  (1986).\nExplanation-based learning: A unifying view. <i>Machine Learning, Vol.\n1(1).</i><p>\nMurphy,  G.,  &amp;  Medin,  D.  (1985).  The role  of theories in conceptual\ncoherence.  <i>Psychological Review, 92,</i> 3.<p>\nNakamura, G. (1985).  Knowledge-based classification of ill-defined categories.\n<i>Memory &amp; Cognition. 13, </i>377-84.<p>\nPazzani,  M. (1990)  <i>Creating a memory of  causal  relationships: An\nintegration of empirical and explanation-based  learning methods</i>.\nHillsdale, NJ: Lawrence Erlbaum Associates.<p>\nPazzani, M. (1991).  The influence of prior knowledge on concept acquisition:\nExperimental and computational results.  <i>Journal of Experimental Psychology:\nLearning, Memory &amp; Cognition, 17, 3, </i>416-32.<p>\nRosch  E.    (1978).    Principles  of  categorization.    In   <i>Cognition\nand categorization</i>  (Ed.),  Rosch  E.  &amp;  Lloyd  B..  Hillsdale, NJ.:\nLawrence Erlbaum Associates. <p>\nRumelhart, D.,  Hinton, G., &amp; Williams, R. (1986).  Learning internal\nrepresentations by backpropagating errors.  In: Rumelhart, D., McClelland, J.\n(eds.), <i>Parallel Distributed Processing</i>, Cambridge, MA: MIT Press.<p>\nTowell, G. Shavlik, J. &amp; Noordewier, M. (1990).  Refinement of approximate\ndomain theories by knowledge-based neural networks.  <i>Proceedings of the\nEighth National Conference on Artificial Intelligence</i> (pp. 861-66).\nCambridge, MA: MIT Press. <p>\nWaldmann, M. &amp; Holyoak, K. (1990). Can causal induction be reduced to\nassociative learning?  <i>Proceedings of the Twelfth  Annual Conference of the\nCognitive Science Society</i>   Cambridge, MA: Lawrence Erlbaum.<p>\nWattenmaker, W., Dewey, G., Murphy, T., &amp; Medin, D. (1986).  Linear\nseparability and concept learning: Context, relational properties and concept\nnaturalness.  Cognitive Psychology, 18, 158-194.<p>\nWisniewski, E. &amp; Medin,  D. (1994). On the interaction of data and theory\nin concept learning. Cognitive Science, 18, 221-282.\n</body></html>", "encoding": "ascii"}