{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/sorting2.txt", "content": "\t\tSorting: O(N Log2 N) Sorts and Lower Bounds\r\n\r\nIn this lecture we will discuss three O(N Log2 N) sorting algorithms (although\r\nfor quicksort, this bound is the average case, not the worst case). We will\r\nalso  discuss a non-trivial lower bound for sorting (to me, this is especially\r\ninteresting and surprising).\r\n\r\n\r\nHeapSort:\r\n\r\nFirst we discuss HeapSort. As we have seen with Heaps for priority queues, to\r\nsort N values we can add each value into the Heap (assume the lowest value has\r\nthe highest priority) and then remove the values in order (highest one first:\r\nuse a Max-Heap). The complexity of the \"online\" algorithm is NxO(Log2 N) +\r\nNxO(Log2 N) = 2NxO(Log2 N) = O(N Log2 N). The complexity of the \"offline\"\r\nalgorithm is O(N) + NxO(Log2 N) = O(N+NLog2 N) = O(N Log2 N). So HeapSort takes\r\nthis amount of work even in the best case.\r\n\r\n  1) Worst/Best/Average case is O(N Log2 N)\r\n  2) In-place (all data is stored in the array that is the heap); when we\r\n       remove values (biggest first) we swap it with the last used location\r\n       in the array, and then do not consider that location as part of the\r\n       heap. The result at the end is an array filled in sorted order.\r\n  3) O(N Log2 N) comparisons; O(N Log2 N) swaps in the worst case\r\n  4) Unstable (percolating values up and down the tree -across many indexes\r\n       in the array- produces instability).\r\n\r\n\r\nMergeSort:\r\n\r\nNext we will discuss MergeSort. This is a \"divide and conquer\" sort,\r\nimplemented simply via recursion. We use recursion to divide up the problem\r\nand merging to do the \"actual sorting\". The array form of this sort is written\r\nas\r\n  template<class T>\r\n  void merge_sort(T a[], int length) {\r\n    merge_sort(a, 0, length-1);\r\n  }\r\n\r\ncalling an overloaded merge_sort method that specifies the minimum and maximum\r\nindex to use when sorting the (sub)array (in the call from the method above, we\r\nspecify all indexes). This method can be written recursively\r\n\t\r\n  template<class T>\r\n  void merge_sort(T a[], int low, int high) {\r\n    if (low >= high)                  //Base case: 1 value to sort is sorted\r\n      return;                         //(0 possible only on initial call)\r\n    else {\r\n      int mid = (low + high)/2;       //Approximate midpoint*\r\n      merge_sort(a, low, mid);        //Sort low to mid part of array\r\n      merge_sort(a, mid+1, high);     //Sort mid+1 to high part of array\r\n      merge(a, low, mid, mid+1,high); //Merge sorted subparts of array\r\n    }\r\n  }\r\n\r\nNote that if low and high are adjacent, say 4 and 5, then mid = 4 and the\r\nrecursive calls are mergeSort(a, 4,4) and mergeSort(a, 5,5), which are both\r\nbase cases, requiring no work.\r\n\r\n*(low+high)/2 when sorting arrays whose size is near the maximum int can cause\r\narithmetic overflow on addition (even though the final result should be between\r\nlow and high). This \"error\" was in many libraries, but typically did not\r\nmanifiest itself on array sizes less than huge. A better way to compute the\r\nclosest value to the average without overflow is low/2 + high/2 + 1(if both are\r\nodd: which would result in double .5 truncation in the / operators).\r\n\r\nAll the \"sorting\" is done in the merge method: merge_sort just recursively\r\ncomputes the positions in each part of the array to merge sort (and stops at\r\n1 element arrays as the base case, which are by definition sorted).\r\n\r\nSuppose that we write an original array of 16 values as follows. We choose\r\n16 because it is a perfect power of 2, but all other sizes work as well.\r\n\r\n 7   10    3    2    6    13   15   16   12    1    5    9    14    4   11   8 \r\n\r\nThe first level of recursive calls will split it into 2 arrays of 8 values each\r\n(see the | character)\r\n\r\n 7   10    3    2    6    13   15   16 | 12    1    5    9    14    4   11   8 \r\n\r\nThe next level of recursive calls will split it into 4 arrays of 4 values each.\r\n\r\n 7   10    3    2  | 6    13   15   16 | 12    1    5    9  | 14    4   11   8 \r\n\r\nThe next level of recursive calls will split it into 8 arrays of 2 values each.\r\n\r\n 7   10 |  3    2  | 6    13 | 15   16 | 12    1 |  5    9  | 14    4 | 11   8 \r\n\r\nThe bottom level of recursive calls will split it into 16 arrays of 1 value\r\neach.\r\n\r\n 7 | 10 |  3 |  2 |  6 |  13 | 15 | 16 | 12 |  1 |  5 |  9 |  14 |  4 | 11 | 8 \r\n\r\nNow each pair of adjacent 1 value sorted arrays is merged into 8 sorted arrays\r\nof 2 values each.\r\n\r\n 7   10 |  2    3 |  6    13 | 15   16 |  1   12 |  5    9  |  4   14 |  8 | 11\r\n\r\nNow each pair of adjacent 2 value sorted arrays is merged into 4 sorted arrays\r\nof 4 values each.\r\n\r\n 2    3    7   10 |  6    12   15   16 |  1    5    9   12  |  4 |  8   11   14\r\n\r\nNow each pair of adjacnet 4 value sorted arrays is merged into 2 sorted arrays\r\nof 8 values each.\r\n\r\n 2    3    6    7   10    12   15   16 |  1    4    5    8     9   11   12  14\r\n\r\nFinally, the remaining pair of 8 value sorted arrays is merged into 1 sorted\r\narrays of 16 values.\r\n\r\n 1    2    3    4    5     6    7    8    9   10   11   12    13   14   15  16\r\n\r\nNote that recursive calls do O(1) work; there are Log2 N levels for O(Log2 N)\r\nnet work. Merging each level requires O(N) work (justified below), so the\r\ntotal amount of work is Log2 N x O(N) or O(N Log2 N).\r\n\r\n  1) Worst/Best/Average case is O(N Log2 N)\r\n  2) Not in-place (requires an equal sized array; see merge below)\r\n  3) O(N Log2 N) comparisons; O(N Log2 N) movement in the worst case\r\n  4) Stable: when we merge left and right arrays, equal values should be moved\r\n       first from the left subarray (which were originally to the left of all\r\n       the equal values on the right subarray, ensuring stability).\r\n\r\nHere is pseudo-code for merging\r\n\r\ntemplate<class T>\r\nvoid merge (T a[],\r\n            int left_low , int left_high,\r\n            int right_low, int right_high) {\r\n\r\n  Create a temporary array that is the same size as \"a\"\r\n    (this extra storage is why the algorithm is not in-place;\r\n     technically, we need a temporary array storing only\r\n     right_high - left_low + 1 values)\r\n  for every temporary array value from left_low to right_high\r\n    if there are no more \"left\" values\r\n      copy to the temporary array the next \"right\" value\r\n    else if there are no more \"right\" values\r\n      copy to the temporary array the next \"left\" value \r\n    else if the next \"left\" value <= the next \"right\" value\r\n      copy to the temporary array the next \"left\" value \r\n    else\r\n      copy to the temporary array the next \"right\" value\r\n\r\n  copy the temp array back into \"a\": into the positions left_low to right_high\r\n}\r\n\r\nThe Merge method merges two sorted arrays (both in a) of size about N/2 into\r\none sorted  array of size N (temp). The main loop puts a new value in the temp\r\narray on every iteration, sometimes from the sorted left part of \"a\" and\r\nsometimes from the sorted right part of \"a\". So, the loop iterates N times with\r\nO(1) work done during each iteration, doing O(N) work\r\n\r\nThe first two ifs test whether all the values from the left/right have been\r\nmoved, and if so it moves a value from the other one. If there are values in\r\nboth, it compares them and moves the smallest (using the left of \"a\" when they\r\nare equal). Finally, all the values are copied back from the temp array into\r\n\"a\": another O(N) of work.\r\n\r\nThis method is easy to do with linked lists as well: although dividing the\r\nlinked list is half takes O(N) time, merging also takes O(N) times as well,\r\nso the O(N Log2 N) complexity bound still holds for linked lists.\r\n\r\nFinally there are iterative (non-recursive) implementations. Such code is more\r\ncomplicated but not unreasonable for advanced students to write (recursion is\r\n\"simulated\" by iteration and use of an explicit stack). Also, sometimes other\r\nalgorithms are faster for small N (say N <= c). So whenever the base case is an\r\narray size <= c, we can instead call the other sorting method to sort the\r\nsubarray, instead of calling merge sort recursively all the way down to\r\nsingle-valued subarrays. Unfortunately, this is all technology based and there\r\nis no easy way to know for a specific machine and compiler what c's value is;\r\nwe would have to test the code to get this value empirically.\r\n\r\n\r\nQuickSort:\r\n\r\nFinally, we will discuss QuickSort, which is also a \"divide and conquer\" sort,\r\nimplemented simply via recursion. We use partitioning to divide up the problem.\r\nThe array form of this sort is written much like merge_sort was, first\r\n\r\n  template<class T>\r\n  void quick_sort(T a[], int length) {\r\n    quick_sort(a, 0, length-1);\r\n  }\r\n\r\ncalling an overloaded quick_sort method that specifies the minimum and maximum\r\nindex to use when sorting the array (here, all of them). This method can be\r\nwritten recursively\r\n\t\r\n  template<class T>\r\n  void quick_sort(T a[], int low, int high) {\r\n    if (low >= high)  //Base case: 0 or 1 value to sort is sorted\r\n      return;         //(0 possible on initial call and recursion)\r\n    else {\r\n      int pivot_index = partition(a,low,high);//Partition and return Pivot index\r\n      quick_sort(a, low, pivot_index-1);      //Sort values to left of pivot\r\n      quick_sort(a, pivot_index+1, high);     //Sort values to right of pivot\r\n\r\n      //Note that after partitioning, all values to the left of the pivot are\r\n      // < the pivot and all values to the right of the pivot are >= to the\r\n      // pivot, so if the left/right values are sorted recursively (with the\r\n      // pivot remaining between them), the entire array is sorted.\r\n    }\r\n  }\r\n\r\nHere we call partition before the two recursive calls; in merge sort we\r\nperformed the two recursive calls first, followed by merging them together.\r\n\r\nThe partition method chooses the pivot value, then parititions the array into\r\nthose values < pivot (on the left) and those values >= pivot (on the right),\r\nfinally putting the pivot at an index in between these two. It returns the\r\npivot's index (so the recursive calls know which parts of the array need to be\r\nsorted together).\r\n\r\nSimilar to MergeSort, all the sorting is done in the pivot method: quick_sort\r\ncalls partition and figures out, based on the pivot_index, where to do the\r\nrecursive calls for more paritioning (and stops at 0 or 1 element arrays, which\r\nare by definition sorted).\r\n\r\nThe pseudo-code for partition is\r\n  Choose the pivot value (see the discussion below) and swap the pivot value\r\n     with the value in a[high], putting it back where it belongs at the end\r\n  Start with l = low and r = high;\r\n  while (l<r) {                    //Are there some values to examine?\r\n    while (l<r && a[l] <  pivot)   //Find a left value >= the pivot\r\n      ++l;\r\n    while (l<r && a[r] >= pivot)   //Find a right value < the pivot\r\n      --r;\r\n    if (l<r)\t\t\t   //If found both, swap them\r\n      swap(a[l],a[r]);\r\n    //if not, the while loop will terminate on the next iteration\r\n  }\r\n\r\n  swap(a,l,high);  //swap the pivot back where it belongs a[l] is > pivot\r\n  return l;        //the position of the pivot\r\n\r\nLet's look at an example of how this work. Suppose that we write an original\r\narray of 16 values as follows.\r\n\r\n 7   10    3    2    6    13   15   12   16    4    5    9    14    1   11   8 \r\n\r\nLet's just choose the last value (8) as the pivot.\r\n\r\n 7   10    3    2    6    13   15   12   16    4    5    9    14    1   11   8 \r\nlow                                                                        high\r\n l                                                                          r \r\n\r\nIt scans l forwards until it indexes a value >= 8; it scans r backwards until\r\nit indexes value < 8.\r\n\r\n 7   10    3    2    6    13   15   12   16    4    5    9    14    1   11   8 \r\nlow   l                                                             r      high\r\n\r\nNow it swaps those values.\r\n\r\n 7    1    3    2    6    13   15   12   16    4    5    9    14   10   11   8 \r\nlow   l                                                             r      high\r\n\r\nIt scans l forwards until it indexes a value >= 8; it scans r backwards until\r\nit indexes value < 8.\r\n\r\n 7    1    3    2    6    13   15   12   16    4    5    9    14   10   11   8 \r\nlow                        l                        r                      high\r\n\r\nNow it swaps those values.\r\n\r\n 7    1    3    2    6     5   15   12   16    4   13    9    14   10   11   8 \r\nlow                        l                        r                      high\r\n\r\nIt scans l forwards until it indexes a value >= 8; it scans r backwards until\r\nit indexes value < 8.\r\n\r\n 7    1    3    2    6     5   15   12   16    4   13    9    14   10   11   8 \r\nlow                             l              r                           high\r\n\r\nNow it swaps those values.\r\n\r\n 7    1    3    2    6     5    4   12   16   15   13    9    14   10   11   8 \r\nlow                             l              r                           high\r\n\r\nIt scans l forwards until it indexes a value >= 8; it scans r backwards until\r\nit indexes value < 8 -but stops these indexes when they are equal.\r\n\r\n 7    1    3    2    6     5    4   12   16   15   13    9    14   10   11   8 \r\nlow                                  l                                     high\r\n                                     r\r\n\r\nSo, now r=l, so it doesn't swap those values. instead is swaps index l with\r\nindex high, putting the pivot after the values smaller than it and at the\r\nbeginning of the values greater than or equal to it.\r\n\r\n 7    1    3    2    6     5    4    8   16   15   13    9    14   10   11  12\r\nlow                                  l                                     high\r\n                                     r\r\n\r\nThe partitioned array would look like the following (with the pivot in |...|).\r\n\r\n 7    1    3    2    6     5    4  | 8 | 16   15   13    9    14   10   11  12\r\n\r\nThe partition method returns 7 (the index of the pivot 8 in the array). Again,\r\nnote that values to the left of the pivot are < the value of the pivot and the\r\nvalues to the right of the pivot are >= the value of the pivot.\r\n\r\nYou should understand the details of how the partition method works, by hand\r\nsimulating it on other 16 element arrays.\r\n\r\nHere we were lucky, as 8 was the middle value in the array. As with merging,\r\npartitioning requires a total of O(N) operations to compute all the partitions\r\nneeded for each level. If we continue choosing \"middle\" values as pivots, there\r\nwould have been a total of Log2 N levels, just like with MergeSort. Leading to\r\na best case complexity of O(N Log2 N).\r\n\r\nNow we recursively partition the left part (indexes 0 to 6) and right part\r\n(indexes 8 to 15). In both we again choose the last value as the pivot: 4 for\r\nthe left range, 12 for the right range; in  both these cases the choices\r\nis fortunate, as these values are near the middle, of each range. After each\r\nrange is partitioned, it looks as follows (with the pivots in ||).\r\n\r\n2    1    3  | 4 |   6     5    7  | 8 | 11   10    9  |12 |   14   15   16  13\r\n\r\nThe result is that we still have arrays of size 3, 3, 3, and 4 to partition. If\r\nwe keep choosing such good pivots, there would be Log N levels, meaning the\r\nbest case complexity class for QuickSort would be O(N Log2 N): N levels each\r\nrequiring O(N) work.\r\n\r\nStarting over again, here is an example of an array that would continually\r\nsupply the the worst partition choice (the biggest value in the array).\r\n\r\n1    2    3    4    5     6    7    8    9   10   11   12    13   14   15   16 \r\n\r\nThis results in the following array after partitioning\r\n\r\n1    2    3    4    5     6    7    8    9   10   11   12    13   14   15  |16|\r\n\r\nwhich has taken 16 operations to partition the array but has not changed it.\r\nNow the recursive calls work on an array of size 15 and of size 0. If we\r\ncontinue to choose the worst partition, the next recursive call would take 15\r\noperations to partition the array but would not changed it. This would continue\r\nrequiring 16 + 15 + 14 + .... + 1 operations, or O(N^2), which is why in the\r\nworst case this method is O(N^2).\r\n\r\nNow let's look at the in-between cases. If partition always leaves 2 values on\r\nthe right and the remaininng values on the left (or vice-versa), then there\r\nwill be N/2 levels, also yielding O(N^2/2) = O(N^2). Likewise, if partition\r\nalways leaves 3 values on the right and the remaininng values on the left (or\r\nvice-versa), then there will be N/3 levels, also yielding O(N^2/3( = O(N^2). In\r\nfact, if partition always leaves M values on the right and the remaininng\r\nvalues on the left (or vice-versa), then there will be N/M levels, also\r\nyielding O(N^2/M) = O(N^2) since M is constant.\r\n\r\nIf partition always leaves 1/3 values on the right and the remaininng 2/3\r\nvalues on the left (or vice-versa), then there will be Log 3/2 N (Log, base 3/2,\r\nof N) yielding O(NLogN) (but to a base less than 2, so the Log to that base\r\nwill be bigger: if (3/2)^y = 2^z then y > z). Likewise, if partition always\r\nleaves 1/4 values on the right and the remaininng 3/4 values on the left (or\r\nvice-versa), then there will be Log 4/3 N (Log, base 4/3, of N) yielding\r\nO(NLogN) (but to a base less than 2 or 3/2, so the Log to that base will be\r\nbigger: if (4/3)^x = (3/2)^x = 2^y then x > y > z). In fact, if partition\r\nalways leaves (1/M) values on the right and (M-1)/M values on the left\r\n(or vice-versa) , then there will be Log M/(M-1) N levels, also yielding\r\n O(NLogN). How big do the logarithms get.\r\n\r\nThat is, if the bigger part of the partition array is always 2/3 of the original\r\nsize, solving N(2/3)^x = 1 is equivalent to N = (3/2)^x, or taking\r\nLog (base 3/2) of each side, x = Log (base 3/2) N\r\n\r\nHere is table of values to illustrate.\r\n\r\n  Base |  Log Base of 10^6\r\n-------+-------------------\r\n  2/1  |       20\r\n  3/2  |       34\r\n  4/3  |       48\r\n  5/4  |       62\r\n  6/5  |       76\r\n  7/6  |       90\r\n  8/7  |      103\r\n  9/8  |      117\r\n 10/9  |      131\r\n\r\nSo, if the partition divides the arrary in 1/2, it will take 20 divisions to\r\nget to an array of size 1. If the partition divides the arrary in 1/3 and 2/3,\r\nit will take 34 divisions to get to an array of size 1....If the partition\r\ndivides the arrary in 1/10 and 9/10, it will take 131 divisions to get to an\r\narray of size 1.\r\n\r\nSo, as long as the partition method leaves a certain percentage of values on\r\neach side of the partition, the algorithm will be O(N Log N) -for some base,\r\npossibly a base a bit bigger than 1, causing a \"large\" logarithme. If there are\r\na fixed number of values on one side the algorithm will be O(N^2).\r\n\r\n\r\nWe summarize this information as follows:\r\n\r\n  1) Worst is O(N^2), best and average are O(N Log2 N)\r\n  2) In-place on average case (requires O(Log N) stack space for recursive\r\n       calls) but not in-place in worst case (requires N stack space for\r\n       recursive calls).\r\n  3) O(N^2) comparisons and movement worst case (O(N Log N) and O(N Log N)\r\n       average)\r\n  4) Unstable: partition swaps values over large distances in the array\r\n\r\nTo summarize, QuickSort's work is between O(N Log2 N) and O(N^2). It can be\r\nmuch more often O(N Log2 N), and its constant is lower than those for either\r\nHeapSort or MergeSort. The difference between good and bad behavior is\r\ncheaply picking a good pivot (discussed further below).\r\n\r\nSo, cheaply picking a good pivot is important. Sometimes pivots are chosen\r\nfrom the start, middle, or end of the array. By choosing the middle, if the\r\narray is already sorted, the pivot will be optimal (choosing either end will\r\nresult in O(N^2) time for a sorted array). One can also choose a pivot from a\r\nrandom position in the array.  Obviously the best pivot is the median value in\r\nthe array, which will split it in half. But it would take too much work (O(N) to\r\nfind the true median), over and over again in each call to partition. So, we\r\ncan find an approximate median by choosing the pivot as the median of any 3\r\nvalues in the array to approximate the actual median (picking the first,\r\nmiddle, last values to compare, or picking three values in random indexes).\r\n\r\nOf course, we could better approximate the median by looking at even more\r\nvalues (say Median of 5), but the time to find such a median is bigger. We\r\nneed a tradeoff between how long it takes to choose a pivot and how good the\r\npivot chosen is. It has been found in practice (empirically) that Median of 3\r\ngives the best overall results based on getting close to the median, and running\r\nquickly.\r\n\r\nEmpirically I found that if we just pick the pivot as the last position in an\r\narray of random integers, on average the largest remaining array size is 75%\r\n(3/4). This value makes sense because the possible sizes of the largest arrays\r\nare 50% (if the median was picked) and 100% (really N-1/N percent) if the\r\nlargest or smallest value is picked. When using the median-of-3 as the pivot,\r\non average the largest remaining array size is 66% (2/3). Finally, when using\r\nthe median-of-5 as the pivot, on average the largest remaining array size is\r\n64% (which is only a few percent better that median-of-3, but more expensive\r\nto compute). See the table above for Log (base 3/2) of 10^ 6 = 34, which is\r\nless than 2 times Log (base 2) 10^6, so the average running time will be about\r\n2 times the best running time, which is O(N Log2 N) if the data is random.\r\n\r\nNote, if the data might not be random, but not random in a strange way, we can\r\nspend O(N) time randomizing it, so that QuickSort can run quickly. This is one\r\nexample (of many algorithms) where randomizing the data may actually improve the\r\nrunning time of the algorithm.\r\n\r\nWith a good pivot, this algorithm also requires only O(Log N) extra stack\r\nspace to store data for the recursive calls, which we consider in-place. Unlike\r\nMerge sort, we do all the data movement in the original array, with no\r\nrequirement to copy.\r\n\r\nFinally, to speed up QuickSort, as we did with MergeSort, the many small arrays\r\nat the bottom of the recursion are sorted via a faster sort for small arrays.\r\nOr in the case of QuickSort,  not sorted at all (which is certainly fast)! Say\r\nthat we leave arrays of size 4 or less unsorted. After most parts of the array\r\nare sorted, we do one call to Insertion Sort. This method runs in O(N) if the\r\ndata is mostly sorted (which it will be, after lots of partitioning: values\r\nwill be within 4 of the final index) and doing so is often faster than\r\ncompletely sorting down to arrays of size 1 via QuickSort. Depending on your\r\nmachine/compiler, you might discover an optimal minimal size (that is bigger or\r\nsmaller) for recursively calling QuickSort.\r\n\r\n\r\nFinal Words on O(N Log2 N) sorting:\r\n\r\nThe following analysis is based on my implementation of these sorting methods.\r\n\r\nHeapSort is guaranteed to run in O(N Log2 N) and typically runs slowest of the\r\nO(N Log2 N) sorts; it CAN be done in place, but it is NOT stable.\r\n\r\nMergeSort is guaranteed to run in O(N Log2 N) but typically runs slower than\r\nQuickSort; it CANNOT be done in place (requiring an extra N in space), but it\r\nIS stable.\r\n\r\nQuickSort is NOT guaranteed to run in O(N Log2 N) -running O(N^2) in bad\r\ncases, but if we choose the partition carefully, it almost always runs in\r\nO(N Log2 N) and does it faster than MergeSort or HeapSort (with a smaller\r\nconstant). It CAN be done in place (for well chosen pivots), using much less\r\nthan the extra N space needed by MergeSort. Finally, it is NOT stable.\r\n\r\nRecently (2011) a programmer named Tim Peters developed a sorting algorithm\r\n(he named it TimSort) that is based on merge sort and insertion sort. It is\r\nstable, and at worst runs in O(N Log2 N) but often runs faster, sometimes as\r\nfast as O(N), when the data is not completely random, but partially sorted\r\n(which is often the case). It does take up some extra space, but not a lot. To\r\nget this performance the method is highly engineered/tuned and takes lots of\r\ncode. But since sorting is done so often, TimSort is now the standard sorting\r\nalgorithm in Python (where it was developed) and Java (and becoming the standard\r\nin C++).\r\n\r\nThere are hundreds of sorting algorithms; The more you know about how your data\r\nis distributed (if it isn't totally random) the better choice of sorting\r\nalgorithm you can make. Generally, QuickSort (now TimSort) was built into\r\nlibraries, if they contained just one sorting method.\r\n\r\n\r\nLower Bounds for Comparison Sorting Methods:\r\n\r\nCertainly we must look at every value in the array when sorting it (if we\r\nleft one out, and therefore didn't move it, it might be in the wrong spot). So\r\nwe have a trivial Omega(N) lower bound for sorting algorithms when using\r\ncomparisons. But we can use the idea of a Comparison Tree to compute a much\r\nmore interesting and useful lower bound for sorting using comparisons.\r\n\r\nFor every comparision-based algorithm that we develop for sorting, we can \r\ntranslate it into a Comparison Tree, by looking at which values in the array\r\nit compares to which other values in the array. After a series of comparisons,\r\nwe know what swaps to make to sort the array.\r\n\r\nThus, the entire tree specifies how comparisons are made for every possible\r\ninput (which is just a different form of a sorting algorithm). Each internal\r\nnode of the tree specifies a comparison to make; each leaf shows the order of\r\nall the values derived from the results of the comparisions. Here is a\r\nComparison Tree for an algorithm that sorts the three values x1, x2, x3. I took\r\nthis tree from David Eppstein's ICS 163 Notes (so, you might see a similar\r\nproof again, at a more sophisticated level, when you are more sophisticated).\r\n\r\nHere x1:x2 means compare x1 to x2. Each left subtree (labeled <) shows what\r\ncomparisons to peform if x1 < x2. Likewise, each right subtree (labeled >)\r\nshows what comparisons to peform if x1 > x2. Here we assume unique values, but\r\nwe could choose one side as <= or the other >=.\r\n                \r\n                 x1:x2\r\n                /     \\\r\n             < /       \\ >\r\n              /         \\\r\n          x2:x3        x1:x3\r\n           / \\           / \\\r\n        < /   \\>      < /   \\ >\r\n         /     \\       /     \\\r\n    x1,x2,x3  x1:x3 x2,x1,x3 x2:x3\r\n               / \\           / \\\r\n            < /   \\ >     < /   \\ >\r\n             /     \\       /     \\\r\n      x1,x3,x2 x3,x1,x2  x2,x3,x1 x3,x2,x1\r\n\r\nAt the root we know nothing about the ordering of x1, x2, and x3. At every\r\ninternal node we perform one comparison (different sorting methods do these\r\ncomparisons in different orders; some will perform more than the minimum\r\nnumber of comparisons). After each comparison we know a bit more about the\r\nordering of the values. After we accumulate enough information (do enough\r\ncomparisons on any path from the root downward), we know the exact ordering of\r\nthe values and can swap them appropriately to put the array in order.\r\n\r\nSo, if for example, we follow from the root and if we find that\r\n  x1 < x2, then find that x2 < x3; we now know the complete order: x1 < x2 < x3.\r\n\r\nLikewise, if we follow from the root and find that\r\n  x1 < x2, and then find that x2 > x3, we know x2 is biggest, but we still\r\n  don't have any information about how x1 and x3 compare: all we know is that\r\n  both x1 and x3 are less than x2. If we then find that x1 < x3 we now know the\r\n  complete order: x1 < x3 < x2.\r\n\r\nIn the worst case for an input, a Comparison Tree must perform one comparison\r\nfor each DEPTH in the tree, and thus in the worst case it performs a number of\r\ncomparisons equal to the tree's HEIGHT. So, if we know the height of a\r\ncomparison tree, the complexity class of its algorithm is equal to its height:\r\nthere is some input that requires that many comparisons (although other inputs\r\nmay require fewer comparisions: here we are still interested in the worst case).\r\n\r\nWe can also use what we know about tree heights to get an interesting lower\r\nbound, by knowing how many leaves must be in any Comparison Tree.\r\n\r\nWhen sorting an N value array, there are N! (N factorial) different\r\narrangements of these values. Each arrangement must occur at least once in the\r\nComparision Tree; so, in the Comparision Tree there are at least N! leaves.\r\n\r\nFor example, for 3 values, there are 6 (1x2x3) different arrangements of values:\r\n\r\n  1) x1 < x2 < x3\r\n  2) x1 < x3 < x2\r\n  3) x2 < x1 < x3\r\n  4) x2 < x3 < x1\r\n  5) x3 < x1 < x2\r\n  6) x3 < x2 < x1\r\n\r\nall of which occur once in this (optimal) Comparison Tree.\r\n\r\nNote that if there are N different choices for the smallest value, and (N-1)!\r\narrangements for the remaining values, yielding N*(N-1)! = N! different\r\narrangements.\r\n\r\nBased on a Comparison Tree having at least N! leaves, we can prove that the\r\nheight of the Comparison Tree (the number of comparisons performed for the\r\nworst case input) is bounded below by N Log2 N, so it is Omega(N Log2 N).  Here\r\nis a chain of inequalities that allow us to prove this fact.\r\n\r\nNote first that each Comparison Tree is a binary tree.\r\n\r\n1) A Comparison Tree is a binary tree that has at least N! leaves.\r\n\r\n2) A binary tree with N! leaves has more nodes than a binary tree with N!\r\n   nodes - because it must have many internal nodes besides the leaves.\r\n\r\n3) The height of a binary tree with N nodes is at least Log2 N.  So a binary\r\n   tree with N! nodes must have a height at least Log2 N!.\r\n\r\n4) N! =  N  * (N-1) * (N-2)* (N-3)*....*(N/2) * (N/2-1) * (N/2-2) *....* 2 * 1\r\n   N! > N/2 *  N/2  *  N/2 *  N/2 *....* N/2  *     2   *    2    *....* 2 * 2\r\n\r\n   Here we replaced the first N/2 terms by N/2 and replace the second N/2 terms\r\n   by 2. Technically the last replacement is wrong (because 1 < 2) but for large\r\n   N we can find a second factor of 2 in one of the numbers we are replacing by\r\n   2 to get the inequality. For example, if N > 8, 4 is replaced by 2; if we\r\n   left it at 4, there is where the extra factor of 2 can come from.\r\n\r\n   Thus, N! > N^(N/2)      \r\n   Taking Log2 of each side, Log2 N! > N/2 * Log2 N or reversing the inequality\r\n     N/2 * Log2 N  < Log2 N!\r\n\r\n   so Log2 N! is bounded frrom below by N/2 * Log2 N so it is Omega(N Log N)\r\n\r\nIn summary, we showed\r\n  Height of Comparison Tree (with N! leaves)\r\n    > Height of a binary tree  (with N! nodes)\r\n    > Log2 N!\r\n    > N/2 * Log2 N\r\n    which is Omega(N Log N)\r\n\r\n  So, for any Comparison Tree that sorts N values, its height must grow at\r\n  least as fast as N Log N. The Worst Case for sorting is the height of the\r\n  tree, so the worst case behavior grows as Omega(N Log2 N) comparisons to find\r\n  the correct ordering  (there is an ordering that requires a number of\r\n  comparisons at least the height of the tree).\r\n\r\nAlso note that Log2 N! = Log2 1 + Log2 2 + Log2 3 + ... + Log2 N which can\r\nbe accurately approximated by integrating Log x dx between 1 and N. Stirling's\r\napproximation for N! is sqrt(2*pi*N)* N^N * e^(-N). As N gets big, e^(-N) gets\r\nvery close to 1, so N! is Omega(sqrt(N) * N^N); when taking Logs base e we have\r\nLog N! is Omega (Log(base e) sqrt(N) + N Loge N - N) or just Omega(N Loge N), by\r\ndropping the sqrt and linear terms, which we can do for lower bounds since\r\nN Loge N < N Log2 N + other terms.\r\n\r\nSince we know sorting with comparisons is Omega(N Log N) and we have multiple\r\nalgorithms that are O(N Log N) -HeapSort and MergeSort- we have \"optimally\r\nsolved\" the sorting problem - at least according to its complexity class. Other\r\nalgorithms based on comparisons might have smaller constants (which affect the\r\nactual speed), but none will be in a smaller complexity class in the worst case.\r\n\r\nNext we will examine two sorting algorithms that seem to violate this lower\r\nbound, but they do so by not using comparisons to sort their values! These are\r\nstrange algorithms, that are useful for certain kinds of data and have \r\ninteresting upper and lower bounds that we will explore in more detail (and\r\nsee that they don't really violate the lower bound)\r\n", "encoding": "ascii"}