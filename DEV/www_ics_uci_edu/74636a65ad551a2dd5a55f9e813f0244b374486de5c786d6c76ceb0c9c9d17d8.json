{"url": "https://www.ics.uci.edu/~theory/269/190201a.html", "content": "<!DOCTYPE html>\n<html>\n<head>\n<title>Theory Seminar, February 1, 2019</title>\n<link rel=\"stylesheet\" href=\"../stylesheet.css\" type=\"text/css\">\n<script type=\"text/x-mathjax-config\">\nMathJax.Hub.Config({\n  tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n});\n</script>\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n</script>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n</head>\n<body>\n<a href=\"/~theory/\"><img src=\"http://www.ics.uci.edu/~theory/logo/CATOC2.jpg\"\nalt=\"Center for Algorithms and Theory of Computation\"></a>\n<h2><a href=\"/~theory/269/\">CS 269S, Winter 2019: Theory Seminar</a><br>\nBren Hall, Room 1423, 1pm</h2>\n<hr />\n<h2>Februaru 1, 2019:</h2>\n<h1>Recharging Bandits\n</h1>\n<h2>James Liu</h2>\n\n<p>\nWe introduce a general model of bandit problems in which the expected payout of an arm is an increasing concave function of the time since it was last played. We first develop a PTAS for the underlying optimization problem of determining a reward-maximizing sequence of arm pulls. We then show how to use this PTAS in a learning setting to obtain sublinear regret.\n</p>\n\n<p>\n(Based on a paper by Immorlica and Kleinberg at FOCS 2018)\n<a href=\"http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f309.pdf\">\nhttp://ieee-focs.org/FOCS-2018-Papers/pdfs/59f309.pdf</a></p>\n\n</body></html>\n", "encoding": "ascii"}