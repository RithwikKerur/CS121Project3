{"url": "https://www.ics.uci.edu/~eppstein/180a/990204.html", "content": "<HTML>\n<HEAD>\n<TITLE>ICS 180, February 4, 1999</TITLE>\n<META name=\"Owner\" value=\"eppstein\">\n<META name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</HEAD><BODY>\n<IMG SRC=\"icslogo2.gif\" WIDTH=472 HEIGHT=72 ALT=\"\"><P>\n<A HREF=\"index.html\">\n<H1>ICS 180, Winter 1999:<BR>\nStrategy and board game programming</H1></A>\n\n<H2>Lecture notes for February 4, 1999<BR>\nWhich nodes to search?  Full-width vs. selective search</H2>\n\nAlpha-beta tells us how to search, but we still need to know when to\nexpand a node (search its children) and when to just stop and call the\nevaluation function.\n\n\n<H3>The Horizon Effect</H3>\n\nThe pseudo-code I've shown you so far searches every move out to a given \nfixed depth (this depth is also known as the <I>horizon</I>.  Although this \ncan be quite effective at seeing tactical threats that could be carried out \nwithin the horizon,\nit (obviously) can't detect threats that would take effect past the \nhorizon; for instance a depth-8 search (that is, a search four moves deep) \nwould likely not have much or any information about a forced checkmate in \nfive moves.  What it don't know, it can't defend against, and it would \nsimply ignore those long-term threats.  But this sort of fixed-depth \nsearch can behave even worse when the position contains medium-depth \nthreats in which some bad outcome is forced to occur, but where some lines \nhave that outcome within the search horizon and some don't.  In that case, \nthe program can play horrible pointless moves in an attempt to delay the \nbad outcome long enough that it can't be seen.  This phenomenon is known \nas the <I>horizon effect</I>.\n\n<P>Here's an example.  In the following position, black's bishop is trapped \nby the white pawns.  No matter what black does, the bishop will be taken \nin a few moves; for instance the white rook could maneuver h2-h1-a1-a2 and \ncapture the bishop.  That sequence is 8 plys deep, and suppose that the \nblack program is searching to a depth of 8 plys.  Probably the best move \nfor black in the actual position is to trade off the bishop and pawns,\ne.g. bishop x pawn, pawn x bishop.\nIn the remaining endgame, black's three connected passed pawns may be \nenough to win or draw against the rook.\nBut, a program searching 8 plys will likely instead move the black pawn \nforwards, checking the white king.  White must respond (e.g. by taking the \npawn with his king), but that forced response delays the loss of the bishop \nlong enough that the program can't see it anymore, and thinks the bishop \nis safe.  In fact, in this position, a fixed-depth program can continue \nthrowing away its pawns, delaying the bishop capture a few more moves but \nprobably causing the eventual loss of the game.\n\n<P><CENTER><IMG SRC=\"990204.gif\" WIDTH=292 HEIGHT=292 ALT=\"horizon effect \nexample\"></CENTER>\n\nOne way to counter the horizon effect is to add knowledge to your program: \nif it knows from the evaluation that the bishop is trapped, its search \nwon't try to delay the capture by throwing away pawns.\nAnother is to make the search faster and deeper: the more levels your \nprogram searches, the less likely you are to run across a situation like \nthis where it is possible to delay the loss of the bishop past the horizon.\nBut the most effective general solution is to make the search depth more \nflexible, so that the program searches deeper in the lines where a pawn is \nbeing given away and less deep in other lines where it doesn't need the \ndepth.\n\n<H3>Brute Force and Selectivity</H3>\n\n<P>Shannon's original paper on computer chess listed two possible \nstrategies for adjusting the search depth of a game program.\n\n<P>The most obvious is what the pseudo-code I've shown you so far does:\na full-width, brute force search to a fixed depth.  Just pass in a\n\"depth\" parameter\nto your program, decrement it by one for each level of search, and stop\nwhen it hits zero.  This has the advantage of seeing even wierd-looking\nlines of play, as long as they remain within the search horizon.\nBut the high branching factor means that it doesn't search any line very \ndeeply (bachelor's degree: knows nothing about everything).\nAnd even worse, it falls prey to the horizon effect.\nSuppose, in chess, we have a program searching seven levels deep,\n\n<P>The other method suggested by Shannon was selective pruning:\nagain search to some fixed depth, but to keep the branching factor down\nonly search some of the children of each node (avoiding the \"obviously\nbad\" moves).  So, it can search much more deeply, but there are lines it\ncompletely doesn't see\t(ph.d.: knows everything about nothing).\nShannon thought this was a good idea because it's closer to how humans think.\nTuring used a variant of this idea, only searching capturing moves.\nMore typically one might evaluate the children and only expand the\n<I>k</I> best of them where <I>k</I> is some parameter less than the\ntrue branching factor.\n\t\n<P>Unfortunately, \"obviously bad\" moves are often not bad at all,\nbut are brilliant sacrifices that win the game.  If you don't find one you\nshould have made, you'll have to work harder and find some other way to win.\nWorse, if you don't see that your opponent is about to spring some such\nmove sequence on you, you'll fall into the trap and lose.\n\n<P>Nowadays, neither of these ideas is used in its pure form.\nInstead, we use a synthesis of both: selective extension.\nWe search all lines to some fixed depth, but then extend\nextend some lines deeper than that horizon.\nSometimes we'll also do some pruning (beyond the safe pruning done by\nalpha-beta), but this is usually extremely conservative because it's too\nhard to pick out only the good moves;\nbut we can sometimes pick out and ignore really bad moves.\nFor games other than chess, with higher branching factors,\nit may be necessary to use more aggressive pruning techniques.\n\n<H3>When to extend?</H3>\n\nWhat is the point of extending?\nTo get better (more accurate) evaluations.\nSo, should extend\n<OL>\n<LI>when the current evaluation is likely to be inaccurate, or\n<LI>when the current line of play is a particularly important part of\nthe overall game tree search\n</OL>\n(or some combination of both).\n\n<H3>Quiescence Search</H3>\n\n<LI>In chess or other games in which there are both capturing and \nnon-capturing moves (checkers, go, fanorona), if there are captures to be made,\nthe evaluation will change greatly with each capture.\n\n<P><I>Quiescence search</I> is the idea of, after reaching the main\nsearch horizon, running a Turing-like search in which we only expand\ncapturing moves (or sometimes, capturing and checking) moves.  For games\nother than chess, the main idea would be to only include moves which\nmake large changes to the evaluation.  Such a search must also include\n\"pass\" moves in which we decide to stop capturing.\nSo, each call to the evaluation function in the main alpha-beta search \nwould be replaced by the following, a slimmed down version of alpha-beta \nthat only searches capturing moves, and that allows the search to stop if \nthe current evaluation is already good enough for a fail high:\n\n<PRE>\n    // quiescence search\n    // call this from the main alphabeta search in place of eval()\n\n    quiesce(int alpha, int beta) {\n        int score = eval();\n        if (score >= beta) return score;\n        for (each capturing move m) {\n            make move m;\n            score = -quiesce(-beta,-alpha);\n            unmake move m;\n            if (score >= alpha) {\n                alpha = score;\n                if (score >= beta) break;\n            }\n        }\n        return score;\n    }\n</PRE>\nSome people also include checks to the king inside the quiescence search, \nbut you have to be careful: because there is no depth parameter, \nquiescence can search a huge number of nodes.  Captures are naturally \nlimited (you can only perform 16 captures before you've run out of pieces \nto capture) but checks can go on forever and cause an infinite recursion.\n\n<H3>Selective extensions</H3>\n\nIf the position has been active in the recent past, this may be evidence \nthat further tactics are coming up, or that some of the previous moves \nwere delaying tactics that prevent us from seeing deeply enough to get a \ngood evaluation.\nSo one often increases the search depth if the search passes through\nan \"interesting\" move such as a capture or a check.  In the alpha-beta pseudocode,\nthis would be accomplished by replacing the depth-1 parameter to the\nrecursive call to the search routine by the value\ndepth-1+extension.  You have to be careful not to do this too often,\nthough, or you could end up with a hugely expanded (even possibly\ninfinite!) search tree.\n\n<P>One trick helps make sure this extension idea terminate:\nonly extend by a fraction of a level.  Specifically, make the \"depth\"\ncounter record some multiple of the number of levels you really want to\nsearch, say depth=levels*24.  Then, in recursive calls to alpha-beta\nsearch, pass a value of depth-24+extension.  If the extension is always\nstrictly less than 24, the method is guaranteed to terminate, and you\ncan choose which situations result in larger or smaller extensions.\n\n<P>It may also be useful to include within the evaluation function \nknowledge about how difficult a position is to evaluate, and extend the \nsearch on positions that are too difficult.  My program does this: the \nprogram passes the current depth to the evaluation function.  If the \nposition is complicated, and the depth is close to zero, the evaluation \nreturns a special value telling the search to continue.  But if the depth \nreaches a large negative number, the evaluation function always succeeds, \nso that the search will eventually terminate.\n\n<H3>How to combine accuracy with importance?</H3>\n\nSo far, we've just looked at trying to find the points at which the\nevaluation may be inaccurate.  But maybe we don't care if it's\ninaccurate for unimportant parts of the tree, but we really do care for\nnodes on the principal variation.  How do we take importance into\naccount when performing selective extensions?\n\n<OL>\n<LI>Don't, let alpha-beta sort out importance and just extend based on \naccuracy.\n\n<P><LI>Extend lines that are part of (or near) \nthe principal variation (e.g. singular extensions -- used in Deep Blue \nand/or its predecessors -- if there is one move much better than others in \na position, extend the search on that move).\n\n<P><LI>Moving away from alpha-beta...\nconspiracy number search -- what is the minimum number of positions the \nvalue of which would have to change to force program to make a different \nmove? Search those positions deeper.\n</OL>\n\n<H3>Null-move search</H3>\n\nThis idea fits in with the general theme of the lecture, adjusting search \ndepth in appropriate circumstances, however it works in a different \ndirection.  Instead of extending the search in hard positions, we reduce \nthe search in easy positions.\n\n<P>The idea is based on a piece of knowledge about chess: it's very rare \n(except in the endgame) for it to be a disadvantage to move.  Normally, if \nit's your turn to move, there is something you can do to make your position \nbetter.  Positions in which all possible moves make the position worse are \ncalled \"zugzwang\" (German for move-compulsion), and normally only happen \nin endgames.  In some other games, such as Go-Moku, zugzwang doesn't \nhappen at all.  So, if you changed the rules of chess to allow a \"pass\" \nmove, passing would usually be a mistake and the game wouldn't change much.\n\n<P>So, suppose you have a search node that you expect to fail high\n(i.e., alphabeta will return a score of at least beta).\nThe idea of null-move search is to search the \"pass\" move <I>first</I>, \neven though it's usually <I>not</I> the best move.  If the pass move fails \nhigh, then the true best move is also likely to fail high, and you can \nreturn beta right away rather than searching it.\nTo make this even faster, the depth at which the passing move is searched \nshould be shallower than usual.\n\n<P>You should be careful: this heuristic changes the result of the search,\nand may cause you to miss some important lines of play.  You shouldn't use \nnull moves twice in a row (because then your search will degenerate to \njust returning the evaluation), and you should be careful to only use it \nin situations that are unlikely to be zugzwang.  In chess, that means only \npositions with many pieces left.\n\n<PRE>\n    // alpha-beta search with null-move heuristic\n    alphabeta(int depth, int alpha, int beta) {\n        if (won game or depth <= 0) return score;\n        make passing move;\n        if (last move wasn't null && position is unlikely to be zugzwang &&\n            -alphabeta(depth-3, -beta, -beta+1) >= beta)\n          return beta;\n        for (each possible move m) {\n            make move m;\n            alpha = max(alpha, -alphabeta(depth-1, -beta, -alpha);\n            unmake move m;\n            if (alpha >= beta) break;\n        }\n        return alpha;\n    }\n</PRE>\n\n<P><HR>\n<A HREF=\"/~eppstein/\">David Eppstein,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>,\nMonday, 01-Feb-1999 16:58:05 PST.\n</BODY></HTML>\n", "encoding": "ascii"}