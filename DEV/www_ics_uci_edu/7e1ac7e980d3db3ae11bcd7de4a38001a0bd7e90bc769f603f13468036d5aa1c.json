{"url": "https://www.ics.uci.edu/~skong2/", "content": "<html>\n<head>\n<title>Shu Kong (Aimery) - UC Irvine - Computer Vision</title>\n\n<meta name=\"keywords\" content=\"Computer Vision, Semantic Segmentation, Instance Segmentation, Object Proposal Detection, Object Detection, Keypoint Detection, Fine-grained Classification, Depth Estimation, Camera Pose Estimation, Real-World Scale Estimation,\nImage Aesthetic Rating, Attribute Learning, Data Sampling, Tensor Decomposition, Submodular Function Optimization, Pollen Grain Identification, C. Elegans Segmentation and Counting, Patch Match, Dictionary Learning, Manifold Learning, Pixel Embedding, Recurrent Network, Pixel Grouping, Boundary Detection, Inter-disciplineary Research, Low-Rank Decomposition, Bilinear Pooling, Second Order Statistics, Ubiquitous Fine-Grained Computer Vision, Panoramas, Scene Parsing, Manifold Learning, Sparse Coding, Saliency Detection, High-Order Tensorial Data, Biological Image Processing, Machine Learning\">\n\n\n\n<link rel=\"icon\" href=\"img/profile2.png\" type=\"img/jpg\">\n<style>\nh1 { padding : 0; margin : 0; }\nbody { padding : 0; font-family : Arial; font-size : 16px; background-color :  #eaf2f8; } /* background-image : url('bg.png');}  #EFEFEF*/\n#container { width : 1000px; margin : 20px auto;  background-color :   #fff; padding : 50px; border : 1px solid #ccc; }\n#me { border : 0 solid black; margin-bottom : 0;}\n#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}\n#content { display : block; margin-right : 260px;}\nA { text-decoration : none; }\na:hover { text-decoration : underline; }\na:visited { color : blue; }\na.invisible { color : inherit; text-decoration : inherit; }\n.publogo { margin-right : 25px; height: 50px; width: 50px; float : left; border : 0;}\n.publication { clear : left; padding-bottom : 0px;}\n.publication p { height : 60px; }\n.codelogo { margin-right : 10px; float : left; border : 0;}\n.code { clear : left; padding-bottom : 10px; vertical-align :middle;}\n.code .download a { display : block; margin : 0 15px; float : left;}\n<!-- #simpsons { margin : 5px auto; text-align : center; color : #B7B7B7; } -->\n<!-- \t#erdos { color : #999; text-align : center; font-size : 12px; } -->\n</style>\n<script type=\"text/javascript\">\n\nvar _gaq = _gaq || [];\n    _gaq.push(['_setAccount', 'UA-26193351-1']);\n\t_gaq.push(['_trackPageview']);\n(function() {\nvar ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\nga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\nvar s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n})();\n\n</script>\n</head>\n\n<body>\n<div id=\"container\">\n<div id=\"sidebar\">\n<img src=\"http://www.ics.uci.edu/~skong2/img/profile.jpg\" id=\"me\">\n<!--table>        \n        <tr><td><center><img width=20% src=\"http://www.ics.uci.edu/~skong2/img/profile.jpg\"></center></td></tr>\n        <tr><td><img width=20% src=\"http://www.ics.uci.edu/~skong2/img/profile_pixelEmbedding\"></td></tr>\n        <tr><td><img width=20% src=\"http://www.ics.uci.edu/~skong2/img/profile.jpg\"></td></tr>\n        <tr><td><img width=20% src=\"http://www.ics.uci.edu/~skong2/img/profile.jpg\"></td></tr>\n</table-->\n<br>\n</div>\n\n<div id=\"content\">\n<h1>Shu Kong (Aimery) </h1>\n<!--<p style=\"font-size:14px\"><em>(Aimery is my unofficial name. If you want to know how to pronounce my official name, please click <a href=\"https://translate.google.com/#auto/en/%E5%AD%94%E5%BA%B6\">here</a>.)</em></p>\n-->\n\n<p>I am a PhD candidate at <a href=\"https://www.cs.uci.edu/\">CS</a> | <a href=\"http://www.ics.uci.edu\">ICS</a> | \n<a href=\"http://www.uci.edu\">UCI</a>,\nworking in the <a href=\"http://vision.ics.uci.edu\">Computational Vision Group</a>\nwhere I'm advised by Prof. <a href=\"http://www.ics.uci.edu/~fowlkes/\">Charless Fowlkes</a>. \n<!--font color=ff5061><b><em>I expect to graduate in 2019.</em--> \n<!--a href=\"./slides/phdConsortiumPoster.pdf\">This poster</a> </b></font> is a non-inclusive summary of my work during my PhD.\n</p-->\n<p>\nMy research is motivated by a desire to create intelligent systems that benefit human life, primarily through visual signals and interaction between human and machines, \ne.g., a system understanding the scene for autonomous vehicle, \nan intelligent agent assisting seniors through interaction in daily life, \nand a machine automating microscopy analysis. \n<p>\nMy methodology can be summarized as \"<em>data-driven approach to vision problems through learning</em>\" that includes vision tasks from \nlow-level (e.g., deblur and super-resolution), \nto mid-level (e.g., geometric/odemetric properties and object parts/proposals) and high-level (e.g., semantic instance segmentation and textual grounding). \nBesides performance, I also care about other factors, like model size, inference computation FLOPS, processing speed, etc.\nI also study the learning protocol, roughly concerned of fully/weakly/self supervised learning and how to leverage synthetic data with real data.\n<p>\nIn the meantime, I actively utilize my algorithms to high-throughput microscopy analysis, \nspanning biology, neuronscience and phytology.\nI believe such a practice will not only advance scientific understanding of vision algorithms, \nbut also become increasingly important in developing intelligent robotic systems with broad societal impact and accelerating interdisciplinary scientific research.\n</p>\n<p>\n\n<!--\nAspiring to attacking fine-grained computer vision problems,\nI'm collaborating/working closely with Prof. <a href=\"https://www.cs.cmu.edu/~deva\">Deva Ramanan</a>, Prof. <a href=\"http://cinquin.org.uk/?page_id=108\">Olivier Cinquin</a>, Prof. <a href=\"http://www.life.illinois.edu/plantbio/People/Faculty/Punyasena.htm\">Surangi Punyasena</a>, Prof.  <a href=\"http://zplab.wustl.edu/people.html\">Zachary Pincus</a>, Prof. <a href=\"https://umdearborn.edu/users/fezhou\">Feng Zhou</a>, and Prof. <a href=\"https://yezhouyang.engineering.asu.edu/\">Yezhou Yang</a> on a variety of inter-disciplinary research projects, spanning <em> biology, phytology, physiology and psychology</em>, in which the data provides a good testbed for exploring novel algorithms.\n</p>\n-->\n\n<p>Contact Information</p>\t\n<ul>\n<!--<li>Email: <a href=\"mailto:aimerykong@gmail.com\">aimerykong@gmail.com</a></li>-->\n<li>Email: aimerykong (at) gmail.com</li>\n<li>Office:\n4209 Bren Hall, \n  University of California, Irvine (UCI),\n  Irvine, CA 92697-3435 \n</li>\n</ul>\n\n\n<h5>Other links</h5>\n<ul>\n  <li><!-- a href=\"img/ShuKong_CV.pdf\">CV</a-->\n<a href=\"https://github.com/aimerykong\">Github</a>,\n<a href=\"http://scholar.google.com/citations?user=sm9FdLoAAAAJ&hl=en\">Google Scholar</a>, <!--a href=\"http://www.linkedin.com/in/aimerykong\">LinkedIn</a-->...\n</ul>\n\n\n\n\n\n\n\n\n\n<h3>Recent Update Highlights</h3>\n<ul>\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"https://www.youtube.com/channel/UCAzY2V-MvvZtuF0pvDW1buQ/videos\">Demo videos</a> \nare released for our project\n\"<a href=\"http://www.public.asu.edu/~zfang29/video_grounding_iccv2019/video_grounding.html\">Video-Sentence Grounding with Referring Attention and Weak Supervision</a>\".\n(4/9/2019)\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Research\">\n<p>\nProject page is created for \"<a href=\"https://www.ics.uci.edu/~skong2/mgpff.html\">Multigrid Predictive Filter Flow for Unsupervised Learning \non Videos</a>\";\nsee also  <a href='https://www.youtube.com/playlist?list=PLeUWdu37dSLp68AsgE8RM2x-HJjU_2aEE'>teaser videos at Youtube playlist</a>,\n<a href=\"https://github.com/aimerykong/predictive-filter-flow/tree/master/mgPFF_video\">github code and demo</a>,\nand the <a href=\"http://arxiv.org/abs/1904.01693\">arxiv paper</a>.\n(4/3/2019)\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Research\">\n<p>\nOur paper \"<a href=\"https://arxiv.org/abs/1904.03589\">Modularized Textual Grounding for Counterfactual Resilience</a>\" appears at \n <img src=\"./image2/CVPR2019Logo_local.png\" width=\"150\">,\ncode and data will be released soon!\n(2/24/2019)\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p> joining <img src=\"https://research.fb.com/wp-content/themes/fb-research/images/branding/FB_logo.svg\" width=\"160\"> as summer intern (1/22/2019)</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"pff.html\">Project page</a> is created for \"<a href=\"https://www.ics.uci.edu/~skong2/pff.html\">Image Reconstruction with Predictive Filter Flow</a>\", with released <a href='https://www.ics.uci.edu/~skong2/slides/kf_ff_arxiv2018.pdf'>paper</a>,\n<a href=\"https://docs.google.com/presentation/d/1Vyj1BrVkcE6OxL4tPmkrvyiJsO7mOStkkv5GKnii8JU/edit?usp=sharing\">slides</a>\nand \n<a href='https://github.com/aimerykong/predictive-filter-flow'>demo script</a>. (11/28/2018)\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"PAG.html\">Project page</a> is created for our <img src=\"http://wacv19.wacv.net/wp-content/uploads/whale2a-400x400.png\" width=\"40\"> work \"<a href=\"https://arxiv.org/abs/1805.01556\">Pixel-wise Attentional Gating for Scene Parsing</a>\", which is our Robust Vision Challenge entry for <a href=\"http://www.robustvision.net/leaderboard.php?benchmark=depth\">depth estimation</a> and  <a href=\"http://www.robustvision.net/leaderboard.php?benchmark=semantic\">semantic segmentation</a>. (05/06/2018)\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"DimensionalEmotionModel.html\">Project page</a> is created for \"<a href=\"https://arxiv.org/abs/1805.01024\">Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model</a>\", with released demos, code and models. (05/03/2018)\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Research\">\n<p>\nOur paper <a href=\"https://arxiv.org/abs/1712.08273\">\"Recurrent Pixel Embedding for Instance Grouping\"</a> is accepted by <img src=\"http://cvpr2018.thecvf.com/images/cvpr18logo_3.jpg\" width=\"50\"> as <font color=ff3399><strong>Spotlight Presentation</strong></font>. Read more at the <a href=\"http://www.ics.uci.edu/~skong2/SMMMSG.html\">Project page</a>  for demo, code, models, poster, slides, etc. (02/18/2018) </p>\n</div>\n</li>\n\n<li>\n<div class=\"Research\">\n<p>\nOur paper <a href=\"https://arxiv.org/abs/1705.07238\">\"Recurrent Scene Parsing with Perspective Understanding in the Loop\"</a> is accepted by <img src=\"http://cvpr2018.thecvf.com/images/cvpr18logo_3.jpg\" width=\"50\">. Read more at the <a href=\"http://www.ics.uci.edu/~skong2/recurrentDepthSeg\">Project page</a> for demo/code/models/poster/slides. (02/18/2018) </p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Research\">\n<p>\nThank <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/Googlelogo.png\" width=\"60\"> Google Graduate Student Award for the generous support. (9/2/2017)\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"http://www.ics.uci.edu/~skong2/pano4pose.html\">Project page</a> is created for the google internal project. (9/2/2017) </p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p>\n<a href=\"http://www.ics.uci.edu/~skong2/pollen_BIC.html\">Project page</a> is created for our automated pollen recognition system. (6/2/2017) </p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p> Our paper ''Low-rank Bilinear Pooling for Fine-grained Classification'' is accepted by <img src=\"http://cvpr2017.thecvf.com/images/CVPRLogo3.jpg\" width=\"80\"> See <a href=\"https://github.com/aimerykong/Low-Rank-Bilinear-Pooling/\">github</a> for demo, model and code. (3/2/2017)</p>\n</div>\n</li>\n\n<li>\n<div class=\"Research\">\n<p> joining <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/Googlelogo.png\" width=\"60\"> as summer intern (1/10/2017)</p>\n</div>\n</li>\n\n<li>\n<div class=\"Research\">\n<p> advanced to candidacy [<a href=\"img/candidacy_v0.9.pdf\">slides</a>] (11/30/2016)</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Research\">\n<a href=\"aesthetics.html\">Project page</a> is created for \"deep image aesthetics analysis\" of our <img src=\"./image2/eccv2016-logo.jpg\" width=\"80\"> work, with\ncode, demo and dataset.\n</div>\n</li>\n\n\n<li>\n<div class=\"Research\">\n<p><a href=\"pollen.html\">Project page</a> is created for \"fossilized pollen grain identification\" of our \n<img src=\"./image2/CVPR2016Logo_local.jpg\" width=\"85\"> work, with\n code, demo and dataset.\n</p>\n</div>\n</li>\n\n</ul>\n\n\n<h2>Research Projects</h2>\n\n<ul>\n<ul>\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image2/icon_mgpff_small_dog.gif\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"mgpff.html\">Multigrid Filter Flow for Unsupervised Learning on Videos</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image2/icon_WSRA.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"http://www.public.asu.edu/~zfang29/video_grounding.html\">Language-Video Grounding with Referring Attention and Weak Supervision</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image2/pff_icon_smallSize.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"pff.html\">Image Reconstruction with Predictive Filter Flow</a></b>\n</p>\n</div>\n</li>\n\n\n\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image2/tracklet_icon.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"./tracklet.html\">Tracklet Learning for Long-Term Tracking-by-Detection</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"image2/icon_textualGrounding_cvpr2019.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"https://arxiv.org/abs/1904.03589\">Textual Grounding with Counterfactual Resilience</a></b>\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image2/splashFigure2_icon.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"DimensionalEmotionModel.html\">Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model </a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image/icon_PAG.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"PAG.html\">Pixel-wise Attentional Gating for Scene Parsing</a></b>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"http://www.ics.uci.edu/~skong2/image/icon_pixelEmbedding.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"SMMMSG.html\">Recurrent Pixel Embedding for Instance Grouping</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/rnn_estDepth.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"recurrentDepthSeg.html\">Recurrent Scene Parsing with Perspective Understanding in the loop</a></b>\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/bilinear_sm.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"lr_bilinear.html\">Low-rank Bilinear Pooling for Fine-grained Classification</a></b>\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/imageAesthetics.jpg\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"aesthetics.html\">Deep Understanding of Image Aesthetics</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/pano4pose_small.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"pano4pose.html\">Camera Distance Estimation for 360 Spherical Panoramas </a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/fig_pollen_BIC_lowRes.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"pollen_BIC.html\">Automated Pollen Recognition System </a></b>\n</p>\n</div>\n</li>\n\n\n\n\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/pollen_icon.jpg\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"pollen.html\">Pollen Grains Detection, Segmentation, and Categorization</a></b>\n<!--<a href=\"img/clearness.png\">focus adjustment</a>, <a href=\"img/pollenDetection.jpg\">detection, segmentaion</a>, <a href=\"img/pollenClassification.png\">classification</a>, <a href=\"img/designed_triangulation.gif\">3D modeling</a> and <a href=\"img/patchMatch.jpg\">matching</a> -->\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/worm_icon.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"worm.html\"><em>C. Elegans</em> Detection, Segmentation, and Counting</a></b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/dict_icon.jpg\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b>Sparse Coding, Dictionary Learning, and Applications</b>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/tensor_icon2.png\" class=\"publogo\">\n<div class=\"Research\">\n<p>  <b>Tensor Computation and Applications</b>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<td class=\"pic\"> <img src=\"img/cnn_icon2.png\" class=\"publogo\">\n<div class=\"Research\">\n<p> <b><a href=\"deeplearninggeneral.html\">Theory and Application of Neural Networks -- A Collection of My Projects</a></b>\n</p>\n</div>\n</li>\n\n</ul>\n\n\n\n<h2>Papers</h2>\n<ul>\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Multigrid Predictive Filter Flow for Unsupervised Learning on Videos</font>\",\n<a href=\"http://arxiv.org/abs/1904.01693\">arXiv:1904.01693</a>, 2019.\n<br>\n[<a href=\"https://www.ics.uci.edu/~skong2/mgpff.html\">project page</a>]\n[<a href=\"http://arxiv.org/abs/1904.01693\">arxiv</a>]\n[<a href=\"https://github.com/aimerykong/predictive-filter-flow/tree/master/mgPFF_video\">github</a>]\n[<a href=\"https://github.com/aimerykong/predictive-filter-flow/blob/master/mgPFF_video/demo01_videoSegTrack.ipynb\">demo</a>]\n[<a href=\"https://docs.google.com/presentation/d/1VcA794mt0ukg2ojUnOzbFL8HzpRjEbRp4HXULAIp13Y/edit?usp=sharing\">slides</a>]\n[poster]\n</p>\n</div>\n</li>\n\n\n\n\n\n<li>\n<div class=\"publication\">\n<p><a href=\"http://www.public.asu.edu/~zfang29/\">Zhiyuan Fang</a>, <b>S. Kong</b>, \nC. Fowlkes, <a href=\"https://yezhouyang.engineering.asu.edu/\">Yezhou Yang</a>, \"<font color=#AF7817>Modularized Textual Grounding for Counterfactual Resilience</font>\",\n<a href=\"http://cvpr2019.thecvf.com/\">CVPR</a>, Long Beach, CA, June 2019.\n<br>\n[<a href=\"https://arxiv.org/abs/1904.03589\">paper</a>]\n[<a href=\"http://www.public.asu.edu/~zfang29/\">project page</a>]\n[github]\n[slides]\n[poster]\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p>\nDerek Haselhorst,  <b>Shu Kong</b>,\nCharless Fowlkes, Surangi Punyasena,\n\"<font color=#AF7817> Automated identification of diverse palynological samples using convolutional neural nets</font>\",\n2019. (on the way)\n<br>\n[<a href=\"https://www.ics.uci.edu/~skong2/pollen_BIC.html\">page</a>]\n[<a href=\"https://github.com/aimerykong/pollenDetClsSystem\">code</a>]\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"publication\">\n<p>\nI. Romero, <b>S. Kong</b>,\nC. Fowlkes, M.A. Urban, C. Jaramillo, F. Oboh-Ikuenobe, C. D'Apolito, S.W. Punyasena,\n\"<font color=#AF7817>Automated fossil pollen classification using Airyscan microscopy and convolutional neural networks. Case study: Striatopollis (Amherstieae - Fabaceae)</font>\",\n2019. (on the way)\n<br>\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Image Reconstruction with Predictive Filter Flow</font>\",\n<a href=\"https://arxiv.org/abs/1811.11482\">arXiv:1811.11482</a>, 2018.\n<br>\n[<a href=\"https://www.ics.uci.edu/~skong2/pff.html\">project page</a>]\n[<a href=\"https://www.ics.uci.edu/~skong2/slides/kf_ff_arxiv2018.pdf\">high-res paper (44MB)</a>]\n[<a href=\"https://github.com/aimerykong/predictive-filter-flow\">github</a>]\n[<a href=\"https://docs.google.com/presentation/d/1Vyj1BrVkcE6OxL4tPmkrvyiJsO7mOStkkv5GKnii8JU/edit?usp=sharing\">slides</a>]\n[poster]\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Pixel-wise Attentional Gating for Scene Parsing</font>\",\n<a href=\"http://wacv19.wacv.net\">WACV</a>, Hawaii,2019.\n<br>\n[<a href=\"http://www.ics.uci.edu/~skong2/PAG.html\">project page</a>]\n[<a href=\"https://arxiv.org/abs/1805.01556\">arxiv</a>]\n[<a href=\"https://github.com/aimerykong/Pixel-Attentional-Gating\">github</a>]\n[<a href=\"https://www.ics.uci.edu/~skong2/slides/20180514_AIML_UCI.pdf\">slides</a>]\n[<a href=\"http://www.robustvision.net/leaderboard.php?benchmark=depth\">ROB Entry of Depth Est.</a>]\n[<a href=\"http://robustvision.net/leaderboard.php?benchmark=semantic\">ROB Entry of Segm.</a>]\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>*, F. Zhou*, C. Fowlkes, T. Chen, B. Lei, \"<font color=#AF7817>Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model</font>\",\n<a href=\"https://arxiv.org/abs/1805.01024\">arxiv 1805.01024</a>, 2018.\n<br>\n[<a href=\"http://www.ics.uci.edu/~skong2/DimensionalEmotionModel.html\">project page</a>]\n[<a href=\"https://arxiv.org/abs/1805.01024\">arxiv</a>]\n[<a href=\"https://youtu.be/F8cCXGxGjFQ\">demo</a>]\n[<a href=\"https://drive.google.com/drive/folders/1CVP12ex9q93PsTeredR2nvrMslNubLLk\">models</a>]\n[<a href=\"https://github.com/aimerykong/Dimensional-Emotion-Analysis-of-Facial-Expression\">github</a>]\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Recurrent Pixel Embedding for Instance Grouping</font>\",\n<a href=\"http://cvpr2018.thecvf.com/\">CVPR</a>, 2018 <font color=ff3399><strong>(Spotlight Presentation)</strong></font>.\n<br>\n[<a href=\"SMMMSG.html\">project page</a>]\n[<a href=\"https://arxiv.org/abs/1712.08273\">arxiv</a>]\n[<a href=\"https://github.com/aimerykong/learning-to-group-pixels\">demo</a>]\n[<a href=\"https://drive.google.com/drive/folders/1K2bCmz_mldIhV1e3hCbtBrARZR_0bylm?usp=sharing\">models</a>]\n[<a href=\"https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping\">github</a>]\n[<a href=\"http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_poster.pdf\">poster</a>]\n[<a href=\"http://www.ics.uci.edu/~skong2/slides/pixel_embedding_for_grouping_public_version.pdf\">slides</a>]\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Recurrent Scene Parsing with Perspective Understanding in the Loop</font>\",\n<a href=\"http://cvpr2018.thecvf.com/\">CVPR</a>, 2018.\n<br>\n[<a href=\"recurrentDepthSeg.html\">project page</a>]\n[<a href=\"https://arxiv.org/abs/1705.07238\">technical report</a>]\n[<a href=\"https://github.com/aimerykong/Recurrent-Scene-Parser-with-Perspective-Estimation-in-the-loop\">demo</a>]\n[<a href=\"https://github.com/aimerykong/Recurrent-Scene-Parser-with-Perspective-Estimation-in-the-loop\">model</a>]\n[<a href=\"http://www.ics.uci.edu/~skong2/img/depthGatingSeg_poster.pdf\">poster</a>]\n[<a href=\"http://www.ics.uci.edu/~skong2/img/depthGatingSeg_slides.pdf\">slides</a>]\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, C. Fowlkes, \"<font color=#AF7817>Low-rank Bilinear Pooling for Fine-Grained Classification</font>\",\n<a href=\"http://cvpr2017.thecvf.com/\">CVPR</a>, 2017.\n<br>\n[<a href=\"lr_bilinear.html\">project page</a>]\n[<a href=\"https://arxiv.org/abs/1611.05109\">technical report</a>]\n[<a href=\"img/LRBP_abstract.pdf\">abstract</a>]\n[<a href=\"https://github.com/aimerykong/Low-Rank-Bilinear-Pooling\">demo</a>]\n[<a href=\"https://drive.google.com/open?id=0BxeylfSgpk1MOWt3U1U4WWdmSkk\">model</a>]\n[<a href=\"img/LRBP_poster_v0.4.pdf\">poster</a>]\n[<a href=\"img/fg_CV.pdf\">slides</a>]\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<p> <b>S. Kong</b>, X. Shen, Z. Lin, R. Mech, C. Fowlkes, \"<font color=#AF7817>Photo Aesthetics Ranking Network with Attributes and Content Adaptation</font>\",\n<a href=\"http://www.eccv2016.org/\"><em>ECCV</em></a>, Amsterdam, the Netherlands, (Oct. 2016).\n<br>\n[<a href=\"aesthetics.html\">project page</a>]\n[<a href=\"http://arxiv.org/abs/1606.01621\">paper</a>]\n[<a href=\"https://github.com/aimerykong/deepImageAestheticsAnalysis\">code&demo</a>]\n[<a href=\"https://drive.google.com/open?id=0BxeylfSgpk1MOVduWGxyVlJFUHM\">dataset&model</a>]\n[<a href=\"img/aesthetics_eccv2016.bib\">bibtex</a>]\n[<a href=\"img/eccv2016poster.pdf\">poster</a>]\n[<a href=\"https://docs.google.com/document/d/1HIAvnKbrEAH-lxW7lABKFe81-_LAfHajJKICuwRz7Tc/edit?usp=sharing\">AMT instruction</a>]\n[<u>patent filed</u>]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n\n<p> <b>S. Kong</b>, S. Punyasena, C. Fowlkes, \"<font color=#AF7817>Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification</font>\",\n        <a href=\"http://www.albany.edu/celltracking/CVMI/\"><em>CVPR CVMI Workshop</em></a>, Los Vegas, NV, (July 2016).\n<br>\n<a href=\"pollen.html\">[project page with code&demo]</a> [<a href=\"http://arxiv.org/abs/1605.00775\">paper</a>]\n[<a href=\"img/fossilpollen_cvprcvmi2016.bib\">bibtex</a>]\n[<a href=\"img/fossil_pollen_spotlight_ver3.pdf\">talk</a>]\n[<a href=\"img/fossil_pollen_poster_v5.pdf\">poster</a>]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Zhuolin Jiang, Qiang Yang, \"<font color=#AF7817>Modeling Neuron Selectivity over Simple Mid-Level Features for Image Classification</font>\",\n<em>IEEE Trans. on Image Processing</em>, 2015\n<br>\n<a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7069202&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_Publication_Number%3A83%29%26rowsPerPage%3D50\">[paper]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> Yuetan Lin, <b>Shu Kong</b>, Donghui Wang, Yueting Zhuang, \"<font color=#AF7817>Saliency Detection within a Deep Convolutional Architecture</font>\",\n<a href=\"http://www.aaai.org/Conferences/AAAI/aaai14.php\"><em>AAAI'14 Workshop on Cognitive Computing for Augmented Human Intelligence</em></a>, 2014.\n<br>\n<a href=\"http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8725/8364\">[paper]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>*, Donghui Wang* \"<font color=#AF7817>A Classification-Oriented Dictionary Learning Model: Explicitly Learning the Particularity and Commonality Across Categories</font>\", <em>Pattern Recognition</em>, 2014.\n<br>\n<a href=\"http://www.sciencedirect.com/science/article/pii/S0031320313003245\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/copardic_release_version_1.rar\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Donghui Wang, \"<font color=#AF7817>Learning Exemplar-Represented Manifolds in Latent Space for Classification</font>\", <a href=\"http://www.ecmlpkdd2013.org/\"><em>ECML/PKDD</em></a>, 2013.\n<br>\n<a href=\"http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/21.pdf\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/emlsc.zip\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> Donghui Wang, Xikui Wang, <b>Shu Kong</b>, \"<font color=#AF7817>Integration of Multi-Feature Fusion and Dictionary Learning for Face Recognition</font>\", <em>Image and Vision Computing (IVC)</em>, 2013.\n<br>\n<a href=\"http://www.sciencedirect.com/science/article/pii/S0262885613001509\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/multimodal.rar\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Donghui Wang, \"<font color=#AF7817>Learning Individual-Specific Dictionaries with Fused Multiple Features for Face Recognition</font>\", <a href=\"http://fg2013.cse.sc.edu/\"><em>FG</em></a>, 2013.\n<br>\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/papers/dictionary_fusion_FG2013.pdf\">[paper]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Xikui Wang, Donghui Wang, \"<font color=#AF7817>Multiple Feature Fusion for Face Recognition</font>\", <a href=\"http://fg2013.cse.sc.edu/\"><em>FG</em></a>, 2013.\n<br>\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/papers/multiple_feature_fusion_FG2013.pdf\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/multimodal.rar\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Donghui Wang, \"<font color=#AF7817>A Dictionary Learning Approach for Classification: Separating the Particularity and the commonality</font>\",\n<a href=\"http://eccv2012.unifi.it/\"><em>ECCV</em></a>, 2012.\n<br>\n<a href=\"http://link.springer.com/chapter/10.1007%2F978-3-642-33718-5_14\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/copardic_release_version_1.rar\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Donghui Wang, \"<font color=#AF7817>Transfer Heterogeneous Unlabeled Data for Unsupervised Clustering</font>\", <a href=\"http://www.icpr2012.org/\"><em>ICPR</em></a>, 2012.\n<br>\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/papers/draft_ICPR2012_transfer.pdf\">[paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/thunter.zip\">[code]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Donghui Wang, \"<font color=#AF7817>A Multi-task Learning Strategy for Unsupervised Clustering via Explicitly Separating the Commonality</font>\", <a href=\"http://www.icpr2012.org/\"><em>ICPR</em></a>, 2012.\n<br>\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/papers/draft_ICPR2012_multiTask_full.pdf\">[paper]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> Donghui Wang, <b>Shu Kong</b>, \"<font color=#AF7817>Learning Class-Specific Dictionaries for Digit Recognition from Spherical Surface of a 3D Ball</font>\",  <em>Machine Vision and Applications (MVA)</em>, 2012.\n<br>\n<a href=\"http://link.springer.com/article/10.1007%2Fs00138-012-0463-z\">[paper]\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/download/new_image_3.30.rar\"> [SingleBall_dataset (288MB)]</a>\n<a href=\"http://www.cs.zju.edu.cn/people/wangdh/download/mult-ball_image_dataset.rar\">[MultiBall_dataset (121MB)]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> Donghui Wang, <b>Shu Kong</b>, \"<font color=#AF7817>Feature Selection from High-Order Tensorial Data via Sparse Decomposition</font>\",  <em>Pattern Recognition Letters</em>, 2012.\n<br>\n<a href=\"http://www.sciencedirect.com/science/article/pii/S0167865512001985\"> [paper]</a>\n<a href=\"http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/shopca_version1.zip\">[code]</a>\n</p>\n</div>\n</li>\n</ul>\n\n\n\n\n<h2>Abstract/Workshop</h2>\n<ul>\n\n\n<li>\n<div class=\"publication\">\n<p>Zhiyuan Fang, Shu Kong, Charless Fowlkes ,Yezhou Yang,\n\"<font color=#AF7817> Modularized Textual Grounding for Counterfactual Resilience</font>\",\n<em><a href=\"http://languageandvision.com\">Language And Vision workshop joint with CVPR</a></em>, 2019.\n</p>\n</div>\n</li>\n\n\n\n\n\n<li>\n<div class=\"publication\">\n<p>Surangi W. Punyasena,  <b>Shu Kong</b>, Charless C. Fowlkes, \n\"<font color=#AF7817>Improving the taxonomic accuracy and precision of fossil pollen identifications</font>\",  \n<em><a href=\"https://napc2019.ucr.edu/\">North American Paleontological Convention, Riverside, USA</a></em>, 2019.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"publication\">\n<p>Ingrid Romero,  <b>Shu Kong</b>, Charless C. Fowlkes, Michael A. Urban, Surangi W. Punyasena, \"<font color=#AF7817>Automated Neotropical Fossil Pollen Fabaceae Analysis Using Convolutional Neural Networks</font>\",  <em>GSA Annual Meeting in Indianapolis, Indiana, USA</em>, 2018.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"publication\">\n<p>Zhiyuan Fang,  <b>Shu Kong</b>, Tianshu Yu, Yezhou Yang, \"<font color=#AF7817>Weakly Supervised Attention Learning for Textual Phrases Grounding</font>\",  <em><a href=\"http://languageandvision.com/\">Language and Vision Workshop</a> jointwith CVPR</em>, 2018.\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Charless C. Fowlkes, \"<font color=#AF7817>Low-rank Bilinear Pooling for Fine-Grained Classification</font>\",  <em><a href=\"https://sites.google.com/view/fgvc4/\">the Fourth Workshop on Fine-grained Visual Categorization</a> joint with CVPR</em>, 2017.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Charless C. Fowlkes, \"<font color=#AF7817>Recurrent Scene Parsing with Perspective Understanding in the Loop</font>\",  <em><a href=\"https://sites.google.com/view/socalml17/home\"> Southern California Machine Learning Symposium</a></em>, 2017.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<p> Ingrid Romero, <b>Shu Kong</b>, Charless C. Fowlkes, Michael A. Urban, Carlos D'Apolito, Carlos Jaramillo, OBOH-IKUENOBEA, Francisca E. Oboh-Ikuenobea, Surangi W. Punyasena, \"<font color=#AF7817>NOVEL MORPHOLOGICAL ANALYSIS OF A FOSSIL FABACEAE POLLEN TYPE, STRIATOPOLLIS CATATUMBUS (TRIBE DETARIAE)</font>\",  <em>GSA</em>, 2017.\n<br>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<p> Romero, I.C., <b>S. Kong</b>, C.C. Fowlkes, M.A. Urban, C.A. D'Apolito, C. Jaramillo, F. Oboh-Ikuenobe, and S.W.\nPunyasena, \"<font color=#AF7817>Cenozoic biogeography of Striatopollis catatumbus (Fabaceae  Detariae)</font>\",  <em> AASP-The\nPalynological Society</em>, 2017.\n<br>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"publication\">\n<p> Derek S. Haselhorst, <b>Shu Kong</b>, Charless C. Fowlkes, J. Enrique Moreno, David K. Tcheng, Surangi W. Punyasena, \"<font color=#AF7817>Automating tropical pollen counts using convolutional neural nets: from image acquisition to identification</font>\",  <em>the iDigBio inaugural conference</em>, 2017.\n<br>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> Surangi W. Punyasena, <b>Shu Kong</b>, Charless C. Fowlkes, and Stephen P. Jackson, \"<font color=#AF7817>Reconstructing the extinction dynamics of Picea critchfieldii - the application of computer vision to fossil pollen analysis\n</font>\",  <em>the iDigBio inaugural conference</em>, 2017.\n<br>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"publication\">\n<p> <b>Shu Kong</b>, Charless C. Fowlkes, \"<font color=#AF7817>Low-rank Bilinear Pooling for Fine-Grained Classification</font>\",  <em><a href=\"http://dolcit.cms.caltech.edu/scmls/\">Southern California Machine Learning Symposium</a></em>, 2016.\n<br>\n</p>\n</div>\n</li>\n\n\n</ul>\n\n\n\n<h2>Patents</h2>\n<ul>\n<li><a href=\"https://patentimages.storage.googleapis.com/98/68/f5/208060ce2ce68f/US20180268535A1.pdf\">Utilizing deep learning to rate attributes of digital images</a>, US 2018 / 0268535 A1 </li>\n<li> <a href=\"http://www.freepatentsonline.com/y2017/0294010.html\">UTILIZING DEEP LEARNING FOR RATING AESTHETICS OF DIGITAL IMAGES</a>, US 20170294010</li>\n<li> Method and Apparatus for Image Content Recognition, CN 201410350987.X </li>\n<li> Method and Apparatus for Image Feature Extraction, CN 201410223300.6 </li>\n</ul>\n\n\n\n\n\n\n\n\n\n\n\n<h2>Supported by</h2>\n<ul>\n<li><a href=\"https://projectreporter.nih.gov/project_info_description.cfm?aid=9662175&icde=45479992\">NIA R01AG057748</a> 2019-\n</li>\n<li><img src=\"./image2/CVPR2019Logo_local.png\" width=\"100\"> CVPR PhD Consortium, 2019\n</li>\n<li><a href=\"http://nsf.gov/awardsearch/showAward?AWD_ID=1253538\">IIS-1253538</a> 2016-\n</li>\n<li><a href=\"http://nsf.gov/awardsearch/showAward?AWD_ID=1262547\">NSF DBI-1262547</a> 2015-\n</li>\n<li><img src=\"http://wacv19.wacv.net/wp-content/uploads/whale2a-100x100.png\" width=\"35\">    WACV PhD Consortium, 2019\n</li>\n<li><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/Googlelogo.png\" width=\"60\"> Google Graduate Student Award, 2017\n</li>\n<li><img src=\"https://cdn.shopify.com/s/files/1/0725/0415/files/nvidia_logo_horizontal.png?10377618824155358143\" width=\"80\"> Hardware donation from NVIDIA, 2016\n</li>\n<li><img src=\"https://www.janelia.org/sites/all/themes/janelia7/logo.png\" width=\"80\" style=\"background-color:black;\"> <a href=\"https://www.janelia.org/you-janelia/conferences/junior-scientist-workshop-machine-learning-and-computer-vision\">Janelia Junior Scientist Workshop Travel Grant</a> 2016\n</li>\n<li><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Adobe_Systems_Logo_002.svg/183px-Adobe_Systems_Logo_002.svg.png\" width=\"60\">  Adobe Research Gift 2015\n</li>\n<li><a href=\"https://urop.due.uci.edu/urop/mdp/mdp_web_projects.asp#3\">Multidisciplinary Design Program</a> 2014-2015\n</li>\n</ul>\n\n\n\n\n\n\n\n\n<h2>Presentation/Talk</h2>\n<ul>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Unsupervised Depth Learning from Monocular Videos: Is It Done Right?\"</em>, \n<a href=\"https://research.fb.com/\">Mobile Vision, Oculus, Facebook Research</a>, August 22, 2019.\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Attending to Pixels, Embedding Pixels, Predicting Pixels\"</em>, <a href=\"https://www.ri.cmu.edu/event/attending-to-pixels-embedding-pixels-predicting-pixels/\">CMU VASC Seminar</a>, hosted by Prof. <a href=\"https://www.cs.cmu.edu/~deva/\">Deva Ramanan</a> and bro <a href=\"https://www.cs.cmu.edu/~peiyunh/\">Peiyun Hu</a>, Aug. 6, 2019.\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Attending Pixels, Embedding Pixels, Predicting Pixels\"</em>,\n<a href=\"https://research.fb.com/\">Mobile Vision, Oculus, Facebook Research</a>, July 18, 2019.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Attending Pixels, Embedding Pixels, Predicting Pixels\"</em>, <a href=\"http://cvpr2019.thecvf.com/program/doctoral_consortium\">CVPR PhD Consortium</a> with Prof. <a href=\"https://thoth.inrialpes.fr/~schmid/\">Cordelia Schmid</a>, June 19, 2019.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Attending Pixels, Embedding Pixels, Predicting Pixels\"</em>, \n<a href=\"http://www.vision.caltech.edu/Perona.html\">vision@Caltech, hosted by Prof. Pietro Perona</a> and <a href=\"http://www.vision.caltech.edu/~macaodha/\">Oisin Mac Aodha</a>, June 6, 2019.\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Attention to Pixels, embed pixels, track pixels\"</em>, <a href=\"https://bair.berkeley.edu/\">UC Berkeley BAIR</a> of\nProf. <a href=\"https://people.eecs.berkeley.edu/~efros\">Alyosha Efros</a> and Prof. <a href=\"https://www.ischool.berkeley.edu/people/hany-farid\">Hany Farid</a>, May 24, 2019.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Video Mining by Weakly/Un-supervised Learning\"</em>, <a href=\"https://www.clvrai.com\">CLVR@USC</a> of\nProf. <a href=\"http://www-bcf.usc.edu/~limjj/\">Joseph Lim</a>, May 16, 2019.\n</p>\n</div>\n</li>\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Video Mining: from Sub-pixel to Causality\"</em>, <a href=\"https://vcg.engr.ucr.edu\">Video Computing Group at UC Reverside</a> of \nProf. <a href=\"https://vcg.engr.ucr.edu/people/amit-roy-chowdhury\">Amit Roy-Chowdhury</a>, April 25, 2019.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Predictive Filter Flow: Diving into (Sub)pixels with Unsupervised, Controllable and Interpretable Learning\"</em>, \nhosted by <a href=\"https://people.eecs.berkeley.edu/~efros/\">\"Academic Uncle\" Alyosha Efros</a>@<a href=\"https://bair.berkeley.edu/\">BAIR</a>\nand <a href=\"http://andrewowens.com/\">Andrew Owens</a>, Feb. 18, 2019.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"\"Fine-Grained Visual Understanding and Learning</em>, <a href=\"http://wacv19.wacv.net/\">WACV PhD Consortium</a> of\nProf. <a href=\"http://users.umiacs.umd.edu/~lsd/\">Larry S. Davis</a>, Jan. 8, 2019.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Fine-Grained Image Understanding\"</em>, <a href=\"https://www.traceup.com/\">Traceup</a>, Sep. 14, 2018.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"More to Say About ImageNet Models\"</em>, UCI Computational Vision Group, May 29, 2018.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Pay Attention to the Pixel, Understand the Scene Better\"</em>, <a href=\"https://cml.ics.uci.edu/\">Center for Machine Learning and Intelligent Systems</a>, UCI, May 14, 2018.\n[<a href=\"slides/20180514_AIML_UCI.pdf\">talk</a>]\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"(Dis)entangling Fine-Grained Scene Parsing\"</em>, UCI Computational Vision Group, May 9, 2018.\n</p>\n</div>\n</li>\n\n\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Scene Parsing through Per-Pixel Labeling: a better and faster way\"</em>, <a href='https://yezhouyang.engineering.asu.edu/asu-apg-seminar/'>ASU Active Perception Group Seminar</a>,\nhosted by Prof. <a href=\"https://yezhouyang.engineering.asu.edu/\">Yezhou Yang</a> and bro <a href=\"http://www.public.asu.edu/~zfang29/\">Jacob Fang</a>,\n ASU, March 23, 2018.\n[<a href=\"slides/20180323_APG_ASU.pdf\">talk</a>]\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Towards Human-Object Interaction, and Beyond\"</em>, UCI Computational Vision Group, February 27, 2018.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Learning to Group Pixels into Boundaries, Objectness, Segments and Instances\"</em>, UCI Computational Vision Group, October 31, 2017.\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Predicting Real-World Distance between 360 Photos using Deep Learning\"</em>, Geo, Google, September 5, 2017.\n[talk]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Recurrent Scene Parser with Perspective Estimation in the Loop, and beyond\"</em>, DBH, UCI, April 19, 2017.\n[talk]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Semantic Segmentation: Tricks of the Trade\"</em>, UCI Computational Vision Group, Feb 22, 2017.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Ubiquitous Fine-Grained Computer Vision\n\"</em>, UCI Computational Vision Group, Nov 30, 2016.\n[<a href=\"img/fg_CV.pdf\">talk</a>]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Instance Segmentation\"</em>, UCI Computational Vision Group, Nov 21, 2016.\n\n[<a href=\"img/talk_compvis_2016Fall\">talk</a>]\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Low-rank Bilinear Pooling for Fine-Grained Classification\"</em>,\n<a href=\"http://dolcit.cms.caltech.edu/scmls/\">\nSouthern California Machine Learning Symposium</a>, Caltech, Nov 18, 2016.\n\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Automated Biological Image Analysis using Computer Vision and Machine Learning through Identification, Counting, Detection and Segmetnation \"</em>,\n<a href=\"https://www.janelia.org/you-janelia/conferences/junior-scientist-workshop-machine-learning-and-computer-vision\">\nJunior Scientist Workshop on Machine Learning and Computer Vision</a>, Janelia Research Campus, Oct 2-7, 2016.\n\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification\"</em>, CVPR CVMI Workshop, July 1, 2016.\n[<a href=\"img/fossil_pollen_spotlight_ver3.pdf\">talk</a>][<a href=\"img/fossil_pollen_poster_v5.pdf\">poster</a>]\n\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Geographically Aware Knowledge Mining on Mobile Data\"</em>, <a href=\"https://devpost.com/software/geographically-aware-knowledge-mining-on-mobile-data\">UCI Data Hackathon</a>, May 15, 2016.\n<a href=\"http://www.ics.uci.edu/~skong2/img/UCI_data_hackathon_20160406new.pdf\"> [slides]</a>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Selecting Patches, Matching Species: Fossil Pollen Identification by Spatially Aware Coding\"</em>, UCI Computational Vision Group, Apr. 06, 2016.\n<a href=\"http://www.ics.uci.edu/~skong2/img/CVgourp_20160406.pdf\"> [slides]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"From Linear to Bilinear, and Beyond\"</em>, UCI Computational Vision Group, Jan. 20, 2016.\n<a href=\"http://www.ics.uci.edu/~skong2/img/CVgourp_20160120.pdf\"> [slides]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Deep Understanding Image Aesthetics\"</em>, UCI Computational Vision Group, Sep. 30, 2015.\n<a href=\"http://www.ics.uci.edu/~skong2\"> [slides]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Image Quality and Aesthetics Estimation\"</em>, <a href=\"http://www.adobe.com\"> Adobe Research</a>, Sep. 18, 2015.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Automated Biological Image Analysis using Computer Vision and Machine Learning\"</em>, <a href=\"http://www.urop.uci.edu/mdp.html\">Multi-Disciplinary Project Research Symposium</a>, Calit2 Auditorium, May. 30, 2015.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"Beyond R-CNN detection: Learning to Merge Contextual Attribute\"</em>, UCI Computational Vision Group, UCI, Jan. 29, 2015.\n<a href=\"http://www.ics.uci.edu/~skong2/img/CVgourp_20150129.pdf\"> [slides]</a>\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> <em>\"A Story from Saliency to Objectness and Extension by Deep Neural Network with Perspective and Doubt\"</em>, UCI Computational Vision Group, Nov. 6, 2014.\n<a href=\"http://www.ics.uci.edu/~skong2/img/CVgourp_20141106.pdf\"> [slides]</a>\n</p>\n</div>\n</li>\n\n</ul>\n\n\n\n\n\n\n<h2>Services</h2>\n<ul>\n<h4>Program Committee/Reviewer</h4>\n<ul>\n<li>\n<div class=\"Talks\">\n<p> Conference:\nCVPR2020, AAAI2020, ICLR2020, UAI2019, NeurIPS2019, ICCV2019, ICML2019, CVPR2019, ICLR2019, NIPS2018, ECCV2018, CVPR2018, VCIP2017, ICCV2017, ECCV2016, BMVC2015, CVPR2014, ICCV2013, ICPR2012.</p>\n</div>\n</li>\n<li>\n<div class=\"Talks\">\n<p> Journal: IEEE PAMI (2019), IEEE Access (2019), IEEE CYB (2019), JVLC (2019), Palaeo Electronica (2018), PLOS ONE (2018), IEEE TIP (2018), IEEE CYB (2018), IEEE JBHI (2017), IEEE TIP (2017), PLOS ONE (2017), IEEE TKDE(2017), IEEE CYB (2017), IEEE PAMI (2017), PRLetters (2017), IEEE CYB (2017), IEEE TIP (2016), IEEE THMS (2016), IEEE TIP (2014), MVAP (2014), DSP (2012), IEEE SPLetters (2012). </p>\n</div>\n</li>\n</ul>\n</ul>\n\n\n<ul>\n<h4>Consultant</h4>\n<ul>\n<li>\n<div class=\"Talks\">\n<p> <a href='https://www.traceup.com/'>Trace</a> (2018-2019), \n<a href=\"https://uscabinetsonline.com/\">US Cabinets Online</a> (2018),\n <a href='http://www.paraliantech.com/'>Paralian Tech</a> (2017)</p>\n</div>\n</li>\n</ul>\n</ul>\n\n\n\n\n<ul>\n<h4>Mentorship Program</h4>\n<ul>\n<li>\n<div class=\"Talks\">\n<p> Undergrad GradSchool Q&A Panel (2017), UROP (2015), MDP (2015), Individual Study CompSci299 (2015~2019) </p>\n</div>\n</li>\n</ul>\n</ul>\n\n\n\n\n<ul>\n<h4>Department/School/University Service</h4>\n<ul>\n<li>\n<div class=\"Talks\">\n<p> Student Committee of Faculty Hiring CS-ICS-UCI: 2018, 2019 </p>\n</div>\n</li>\n<li>\n<div class=\"Talks\">\n<p>Graduate Open House Host: 2018, 2019 </p>\n</div>\n</li>\n<li>\n<div class=\"Talks\">\n<p>Panelist@<a href=\"https://www.asuci.uci.edu/president/research-mobilization\">ASUCI Research Mobilization Commission</a>, 2019 </p>\n</div>\n</li>\n</ul>\n</ul>\n\n\n\n\n<ul>\n<h4>Teaching</h4>\n<ul>\n<li>\n<div class=\"Talks\">\n<p> Big Data Image Processing & Analysis Course Information (2017Fall), Computational Photography and Vision (2017Spring), Big Data Image Processing & Analysis Course Information (2016Fall), Graph Algorithms (2016Spring), Machine Learning and Data Mining (2015Winter), Introduction to Graphic Models (2015Fall), Graph Algorithms (2015Spring), Machine Learning and Data Mining (2014Winter), Introduction to Artificial Intelligence (2013Spring), Computer Vision (2012Fall), Logic and Computer Design Fundamentals (2011Fall).</p>\n</div>\n</li>\n</ul>\n</ul>\n\n\n\n\n\n\n\n\n<h2>Misc</h2>\n<ul>\n\n<li>\n<div class=\"Talks\">\n<p> I love mentoring and educating, probably due to my blood that I am a 76th generation descendant of <a href=\"https://en.wikipedia.org/wiki/Confucius\">Confucius</a>, with my family seniority as Ling (\u00e4\u00bb\u00a4).\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> I'm a co-founder of <b>SEED</b> -- a Registered Campus Organization to promote harmony and love within the campus, to bring critical thinking and loving attitude across cultures towards daily lives.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> I'm very slow in responding to messages from all kinds of social media. I'm sort of anti-social-media. So email should be the best way to reach me.\n</p>\n</div>\n</li>\n\n<li>\n<div class=\"Talks\">\n<p> I will get involved in cross-discipline research actively, like <a href=\"http://bigdipa.ccbs.uci.edu/\">Big Data Image Processing and Analysis (Big DIPA).</a>\n</p>\n</div>\n</li>\n\n\n<li>\n<div class=\"Talks\">\n<p> <a href=\"http://sites.uci.edu/joana1/\">Joan Agulilar</a> and I designed \"almighty search\" for <a href=\"https://en.wikipedia.org/wiki/Snake_(video_game)\">Snake</a> game. The \"almighty search\" can always achieve the highest score, see description <a href=\"http://sites.uci.edu/joana1/class-projects/\">here</a>, and technical report <a href=\"http://sites.uci.edu/joana1/files/2016/12/AutomatedSnakeGameSolvers.pdf\">here</a>.\n</p>\n</div>\n</li>\n\n</ul>\n\n<br>\n<br>\n<br>\n<hr>\n<br>\n<br>\n\n\n<br clear=\"both\">\n</div>\n</div>\n\n</body>\n</html>\n", "encoding": "ISO-8859-1"}