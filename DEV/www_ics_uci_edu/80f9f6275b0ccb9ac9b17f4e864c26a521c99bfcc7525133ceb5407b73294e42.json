{"url": "https://www.ics.uci.edu/~smyth/courses/cs274/homeworks/hw4_materials/adam.python", "content": "\ndef get_AdaM_update(alpha_0, grad, adam_values, b1=.95, b2=.999, e=1e-8):\n    adam_values['t'] += 1\n    \n    # update mean\n    adam_values['mean'] = b1 * adam_values['mean'] + (1-b1) * grad\n    m_hat = adam_values['mean'] / (1-b1**adam_values['t'])\n\n    # update variance\n    adam_values['var'] = b2 * adam_values['var'] + (1-b2) * grad**2\n    v_hat = adam_values['var'] / (1-b2**adam_values['t'])\n\n    return alpha_0 * m_hat/(np.sqrt(v_hat) + e)\n\n# Initialize a dictionary that keeps track of the mean, variance, and update counter\nalpha_0 = 1e-3\nadam_values = \\\n    {'mean': np.zeros(beta.shape), 'var': np.zeros(beta.shape), 't': 0}\n\n### Inside the training loop do ###\nbeta_grad = # compute gradient w.r.t. the weight vector (beta) as usual\nbeta_update = get_AdaM_update(alpha_0, beta_grad, adam_values)\nbeta += beta_update\n\n\n", "encoding": "ascii"}