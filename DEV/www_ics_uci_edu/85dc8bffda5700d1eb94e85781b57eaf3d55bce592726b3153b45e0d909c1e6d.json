{"url": "https://www.ics.uci.edu/~eppstein/261/w09-hw1.txt", "content": "ICS 261 -- Spring 2009 -- Homework 1, due Friday, January 23.\n\nRecall from the lecture that the amortized time of an operation in a\ndata structure may be defined by specifying a potential function Phi\nthat maps states of the data structure to numbers, with Phi(x) >= 0 for\nany state x and with Phi(initial state) = 0. Then the amortized time of\nan operation is the actual time, plus a constant times the change to\nphi. Increases to phi cause the amortized time to be larger than the\nactual time, and decreases to phi cause the amortized time to be smaller\nthan the actual time.\n\n1. For a binary heap, let Phi be n log n, where n is the number of items\ncurrently in the heap.\n(a) Show that with this definition, the insert operation in a binary\nheap still takes O(log n) amortized time.\n(b) Show that with this definition, finding and removing the minimum\nelement in a binary heap takes constant amortized time.\nFor both parts, the data structure should be unchanged; we are merely\nanalyzing it differently.\n\n2. Suppose we wish to maintain a resizable vector data structure in\nwhich the allowed operations are reading or writing the item at any\nposition in the vector, growing the length of the vector by one, and\nshrinking the length of the vector by one. We represent the vector as a\nrecord R with three slots R.l, R.a, and R.s, where R.l records the\nlength of the vector, and R.a points to an array of length greater than\nor equal to R.l, and R.s records the length of R.a. Read and write\noperations can be performed by accessing the appropriate position of\nR.a, and grow and shrink operations can usually be performed only by\nchanging R.l, but a grow or shrink might need to replace R.a by a larger\nor smaller array (copying all items to the replacement) whenever R.l and\nR.s become two far apart from each other.\n(a) Suppose that the resizing strategy is to increase the size of the\narray to 2*R.l whenever R.l==R.s (that is, double the space whenever an\nincrease-length operation would overflow the array), and to reduce the\nsize of the array to 2*R.l whenever a R.l <= R.s/3 (to prevent the array\nfrom taking too much unused space). Show that this resizing strategy\ntakes constant amortized time. (Hint: use a potential function that\nmeasures the distance of R.l from R.s).\n(b) Suppose that the resizing strategy is to increase the size of the\narray to 2*R.l whenever a grow operation is performed with R.l==R.s\n(that is, double the space whenever an increase-length operation would\noverflow the array), and to reduce the size of the array to R.l whenever\na shrink operation is performed that would reduce R.l to less than\nR.s/2. Is this algorithm efficient in an amortized sense? Why or why\nnot?\n(c) Suppose that we never shrink the array R.a, and that we always\nresize it to be a square of an integer k*k. Whenever we perform a grow\noperation that would overflow the array (that is, that would make R.l\nlarger than R.s) we increase the size of the array to the next larger\nsquare. Use a potential function of the form R.l - (R.s - R.l)^2 to show\nthat this resizing strategy uses O(sqrt n) amortized time per operation\n\n3. (a) How much memory is required for the priority queue structure in\nDijkstra's algorithm, for a grah with n vertices and m edges, using a\nbinary heap?\n(b) How much memory is required for the priority queue structure in\nDijkstra's algorithm, for a graph with n vertices and m edges, using a\ndata structure in which decrease-key operations are handled by\nperforming a lazy deletion of the item with the old key, followed by an\ninsertion of the same item with the new key?\n", "encoding": "ascii"}