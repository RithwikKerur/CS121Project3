{"url": "https://www.ics.uci.edu/~pazzani/PazzaniIDM.html", "content": "<HTML>\r\n<HEAD>\r\n<META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=windows-1252\">\r\n<META NAME=\"Generator\" CONTENT=\"Microsoft Word 97\">\r\n<TITLE>PROJECT TITLE</TITLE>\r\n<META NAME=\"Template\" CONTENT=\"C:\\Program Files\\Microsoft Office\\Office\\html.dot\">\r\n</HEAD>\r\n<BODY LINK=\"#0000ff\" VLINK=\"#800080\">\r\n\r\n<B><FONT SIZE=6><P>From Computer Data to Human Knowledge: A Cognitive Approach to Knowledge Discovery and Data Mining</P>\r\n</B></FONT><P>Michael J. Pazzani</P>\r\n<P>Department Name:&#9;Information and Computer Science</P>\r\n<P>Institution:&#9;&#9;University of California, Irvine</P>\r\n<B><FONT SIZE=5><P>Contact Information</P>\r\n</B></FONT><P>Michael J. Pazzani</P>\r\n<P>Information and Computer Science</P>\r\n<P>444 Computer Science Bldg.</P>\r\n<P>University of California</P>\r\n<P>Irvine, CA 92697-3425</P>\r\n<P>Phone: (949) 824-7405 </P>\r\n<P>Fax : (949) 824-4035 </P>\r\n<P>Email: pazzani@ics.uci.edu</P>\r\n<P>http://www.ics.uci.edu/~pazzani</P>\r\n<B><FONT SIZE=5><P>WWW PAGE</P>\r\n</B></FONT><U><P>http://www.ics.uci.edu/~pazzani/CognitiveKDD.html</P>\r\n</U><B><FONT SIZE=5><P>Project Award Information</P>\r\n\r\n<UL>\r\n</B></FONT><LI>Award Number: 9731990 </LI>\r\n<LI>Duration: 10/01/1998 \u2013 9/30/2001 </LI>\r\n<LI>Title: From Computer Data to Human Knowledge: A Cognitive Approach to Knowledge Discovery and Data Mining </LI></UL>\r\n\r\n<B><FONT SIZE=5><P>Keywords</P>\r\n</B></FONT><P>Knowledge Discovery in Database; Cognitive Science; Comprehensibility of Learned Models</P>\r\n<B><FONT SIZE=5><P>&nbsp;</P>\r\n<P>Project Summary</P>\r\n</B></FONT><P ALIGN=\"JUSTIFY\">The research is concerned with intelligent decision aids that can be developed by data mining techniques. Experience has shown that such systems can learn accurate models, but that experts in areas where those models are used in decision aids are often reluctant to trust them because they do not use the same tests, intermediate conclusions, or abstractions that the experts have grown to trust. Experts also want models that are stable under small changes in the data being analyzed. Psychologists have uncovered numerous factors that simplify the learning, understanding, and communication of category and process information by humans. This research seeks to explore these psychological principles in light of the output of existing KDD algorithms and to develop and evaluate new KDD algorithms that will provide output that is easy for people to learn, use, and communicate to others. With the results of this research, it should be possible to make such decision aids more \"human centered\", so that they will be used more often and more effectively in practice. </P>\r\n<P ALIGN=\"JUSTIFY\">This is a joint project between Dorrit Billman (Georgia Institute of Technology) and Michael Pazzani (University of California, Irvine). Billman and Pazzani have complementary abilities that will produce the synergy of interdisciplinary work at its best, but also a common pool of assumptions and knowledge to facilitate communication and interaction. Pazzani is a computer scientist who has done psychology experiments, while Billman is a psychologist who has done computational work. This collaboration will bring cognitive principles into the field of Knowledge Discovery and Databases.</P>\r\n<B><FONT SIZE=5><P>Goals, Objectives, and Targeted Activities</P>\r\n</B></FONT><P>Our activities for the project this year fall under two main themes.</P>\r\n<OL>\r\n\r\n<LI>The evaluation of systems for multiple linear regression. Multiple linear regression (e.g., Draper and Smith, 1981) is a technique for finding a linear relationship between a set of explanatory variables (<I>x<SUB>i</I></SUB>) and a dependent variable (<I>y</I>): y<I> = b<SUB>0</I></SUB> + <I>b<SUB>1</SUB>x<SUB>1</I></SUB> + <I>b<SUB>2</SUB>x<SUB>2</I></SUB> + ... + <I>b<SUB>n</SUB>x<SUB>n</I></SUB>. The coefficients, (<I>b<SUB>i</I></SUB>) provide an indication on the influence of an explanatory variable on the dependent variable. Such analysis might help guide future decision-making. For example, many lenders use a credit score to help determine whether to make a loan. This score is a combination of many factors such as income, debt, and past payment history which positively or negatively affect the credit risk of a borrower. However, multiple regression as used in practice can produce models that are unacceptable to experts because factors that should positively affect a decision may have negative coefficients and vice versa. Through psychological investigations, we have verified that users are sensitive to the signs of coefficients used in linear models. We have proposed and evaluated a constrained form of regression (Independent Sign Regression) that produces models that have similar predictive accuracy to multiple linear regression, but whose results are more acceptable to users.</LI>\r\n<LI>The creation and deployment of intelligent agents for text filtering. This activity also seeks to use learning algorithms that produce understandable results. In one system developed (Billsus &amp; Pazzani, 1998), the agent can explain the reasons for classifications, accept feedback on whether the reason is acceptable to the user, and revise the user\u2019s profile based upon feedback to the explanation.</LI></OL>\r\n\r\n<B><FONT SIZE=5><P>Indication of Success</P>\r\n</B></FONT><P>The primary result this year is the creation of a constrained form of regression that produces linear models that generalize as well as those produced by multiple linear regression (see Table 1) yet produces results that subjects are more willing to use (see Table 2).</P>\r\n<B><P ALIGN=\"CENTER\">Table 1. </B>Predictive Mean Squared Error of Regression Routines.</P>\r\n<P ALIGN=\"CENTER\"><CENTER><TABLE BORDER CELLSPACING=1 CELLPADDING=7 WIDTH=333>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<P><B><FONT SIZE=2>Database</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>Multiple Linear Regression</DIR>\r\n</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>Independent Sign Regression</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT SIZE=2><P>Alzheimer</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>0.184</DIR>\r\n</FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>0.166</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT FACE=\"TIMES\" SIZE=2><P>Autompg</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>10.6</DIR>\r\n</FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>10.5</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT SIZE=2><P>Baseball</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>8.74e+5</DIR>\r\n</FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>8.55e+5</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT SIZE=2><P>CS Dept</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>0.244</DIR>\r\n</FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>0.213</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT SIZE=2><P>Housing</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>23.7</DIR>\r\n</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>27.6</DIR>\r\n</FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"40%\" VALIGN=\"TOP\">\r\n<B><FONT FACE=\"TIMES\" SIZE=2><P>Pollution</B></FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<FONT SIZE=2><P>3.53e+3</DIR>\r\n</FONT></TD>\r\n<TD WIDTH=\"30%\" VALIGN=\"TOP\"><DIR>\r\n\r\n<B><FONT SIZE=2><P>1.6e+3</DIR>\r\n</B></FONT></TD>\r\n</TR>\r\n</TABLE>\r\n</CENTER></P>\r\n\r\n<B><P ALIGN=\"CENTER\">Table 2. </B>Average Subjects Ratings for Linear Equations.</P>\r\n<P ALIGN=\"CENTER\"><CENTER><TABLE BORDER CELLSPACING=1 CELLPADDING=7 WIDTH=312>\r\n<TR><TD WIDTH=\"75%\" VALIGN=\"TOP\">\r\n<P ALIGN=\"CENTER\"><B><FONT SIZE=2>Regression Algorithm</B></FONT></TD>\r\n<TD WIDTH=\"25%\" VALIGN=\"TOP\">\r\n<B><FONT SIZE=2><P ALIGN=\"CENTER\">Mean Rating</B></FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"75%\" VALIGN=\"TOP\">\r\n<FONT SIZE=2><P ALIGN=\"CENTER\">Multiple Linear Regression</FONT></TD>\r\n<TD WIDTH=\"25%\" VALIGN=\"TOP\">\r\n<FONT SIZE=2><P ALIGN=\"CENTER\">-0.816</FONT></TD>\r\n</TR>\r\n<TR><TD WIDTH=\"75%\" VALIGN=\"TOP\">\r\n<FONT SIZE=2><P ALIGN=\"CENTER\">Independent Sign Regression</FONT></TD>\r\n<TD WIDTH=\"25%\" VALIGN=\"TOP\">\r\n<FONT SIZE=2><P ALIGN=\"CENTER\">0.603</FONT></TD>\r\n</TR>\r\n</TABLE>\r\n</CENTER></P>\r\n\r\n<B><FONT SIZE=5><P ALIGN=\"CENTER\">Project Impact</P>\r\n</B></FONT><P>Although this research project was initiated on Sept. 30, 1998, it has attracted the attention of several corporations; the PI has been invited to speak at Microsoft Research Laboratories in Redmond, WA and HNC Software in San Diego, CA.</P>\r\n<B><FONT SIZE=5><P>GPRA Outcome Goals</P>\r\n</B></FONT><P>We believe the Independent Sign Regression algorithm represents an advance that will be applicable to a wide variety of situations, particularly those in which people with little knowledge of assumptions behind data mining algorithms apply these algorithms to diverse data sets. </P>\r\n<B><FONT SIZE=5><P>Project References</P>\r\n<OL>\r\n\r\n</B></FONT><LI>Pazzani, M. &amp; Bay, S. (submitted).<I> The Independent Sign Bias: Gaining Insight from Multiple Linear Regression</I>. Annual Meeting of the Cognitive Science Society.</LI>\r\n<LI>Billsus, D. and Pazzani, M. (in press). <I>A Personal News Agent that Talks, Learns and Explains.</I> The Third International Conference on Autonomous Agents (Agents '99), Seattle, Washington, May 1-5, 1999.</LI>\r\n<LI>Intelligent Agent software; NewsDude http://www.ics.uci.edu/~dbillsus/NewsDude</LI></OL>\r\n\r\n<B><FONT SIZE=5><P>Area Background</P>\r\n</B></FONT><P>The goal of data mining is to investigate algorithms for providing insight into some phenomenon by analyzing a database of examples of that phenomenon. The specific focus of our investigation is to constrain and bias algorithms for creating models of data so that these models are understandable and coherent to users of knowledge discovery and data mining systems.</P>\r\n<P>Large databases are being collected in science, business, and medicine due to advances in methods for collecting, storing, and integrating data. The potential benefit of these rich information sources has scarcely been tapped and the societal effects scarcely envisioned. Not only could this provide new discovery methods in science and new decision-making tools for business, but also new bases for policy making in health or economics, new tools for medical diagnosis, new information about products for consumers, and a host of other possibilities. </P>\r\n<P>In response to the availability of these very large databases, a variety of techniques have been developed and applied to recover useful information. These techniques have drawn from statistics, pattern recognition, machine learning, and neural networks to build models describing regularities in the data. The goal of this modeling is to help people understand the data by discovering predictive or descriptive models. However, to date research in data mining has not paid attention to the cognitive factors that make the resulting models coherent, credible, easy to learn, easy to use, and easy to communicate to others. Without attention to the human user, the social benefits of data mining cannot be fully realized.</P>\r\n<P>We anticipate that principles of human learning and reasoning will guide the design of new data mining algorithms to produce models that are easier for users of KDD systems to understand and that properties of learning algorithms will add to the understanding of human psychological processes. </P>\r\n<B><FONT SIZE=5><P ALIGN=\"JUSTIFY\">Area References</P><DIR>\r\n<DIR>\r\n\r\n</B></FONT><P>Davies, J. &amp; Billman, D. (1996). Consistent Contrast in Unsupervised Learning. in Program of the Eighteenth Annual Conference of the Cognitive Science Society. Erlbaum: Hillsdale, NJ. </P>\r\n<P>Draper, N. &amp; Smith, H. (1981). Applied Regression Analysis. John Wiley &amp; Sons.</P>\r\n<P>Fayyad, U.M., Piatetsky-Shapiro, G., &amp; Smyth, P. (1996). From Data Mining to Knowledge Discovery: An Overview. In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, Ramasamy Uthurusamy (Eds.): Advances in Knowledge Discovery and Data Mining (pp. 1-34). AAAI/MIT Press.</P>\r\n<P>Kelley, H. (1971). Causal schemata and the attribution process. In E. Jones, D. Kanouse, H. Kelley, N. Nisbett, S. Valins, &amp; B. Weiner (Eds.), Attribution: Perceiving the causes of behavior (pp 151-174). Morristown, NJ: General Learning Press.</P>\r\n<P>Murphy, G.L. &amp; Allopenna, P.D. (1994). The locus of knowledge effects in concept learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 19, 203-222. </P>\r\n<P>Pazzani, M. (1991a). The influence of prior knowledge on concept acquisition: Experimental and computational results. Journal of Experimental Psychology: Learning, Memory &amp; Cognition, 17, 3.</P>\r\n<P>Spiegelhalter, D., Dawid, P., Lauritzen, S. and Cowell, R. (1993). Bayesian Analysis in Expert Systems. Statistical Science, 8, 219-283.</P>\r\n<P>Tufte, E.R. (1990). Envisioning Information. Connecticut: Graphics Press.</P></DIR>\r\n</DIR>\r\n</BODY>\r\n</HTML>\r\n", "encoding": "Windows-1252"}