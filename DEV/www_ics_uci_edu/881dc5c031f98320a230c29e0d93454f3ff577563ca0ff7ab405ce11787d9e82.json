{"url": "https://www.ics.uci.edu/~dvk/pub/ICDE09_dvk_Speech.html", "content": "<html>\r\n<HEAD>\r\n<TITLE>Using Semantics for Speech Annotation of Images (IEEE ICDE 2009)</TITLE>\r\n<META NAME=\"description\"\r\n CONTENT = \"Speech Annotation,\r\nImage Tagging,\r\nDisambiguation\r\n\">\r\n<META NAME=\"keywords\"\r\n CONTENT = \"Speech Annotation,\r\nImage Tagging,\r\nDisambiguation\r\n\">\r\n</HEAD>\r\n\r\n<BODY   text=#000000\r\n        vLink=#4f4f4f\r\n        aLink=#818501\r\n        link=#818501\r\n        bgColor=#ffffff\r\n        leftMargin=0\r\n        topMargin=0\r\n        rightMargin=2\r\n        VSPACE=\"0\"\r\n        MARGINWIDTH=\"1\"\r\n        MARGINHEIGHT=\"0\">\r\n\r\n<p align=\"center\">\r\n        <TABLE width=\"800\" border=\"0\" ID=\"Table1\">\r\n        <TBODY>\r\n        <TR>\r\n        <TD>\r\n\r\n<H2 align=center>Using Semantics for Speech Annotation of Images.</H2>\r\n<H3 align=center>Appeared in IEEE ICDE Conference, March 2009.</H3>\r\n<BR>\r\n<FONT style=\"FONT-SIZE: 10pt\">\r\n\r\n<p align =center>\r\nChaitanya Desai, <B>Dmitri V. Kalashnikov</B>, Sharad Mehrotra, and Nalini Venkatasubramanian\r\n</p>\r\n\r\n<p align =center>\r\nComputer Science Department<BR>\r\nUniversity of California, Irvine<BR>\r\n</p>\r\n\r\n<center>\r\n<H3>Abstract</H3>\r\n</center>\r\n\r\n<p align =justify>\r\nDigital cameras and multimedia capture devices are becoming\r\nincreasingly popular to take pictures.Annotating these pictures\r\nis important to support their browsing and retrieval. Fully\r\nautomatic image annotation techniques typically rely entirely on\r\nvisual properties of the image. The state of the art image\r\nannotation systems of this kind work well in detecting generic\r\nobject classes: car, horse, motorcycle, airplane, etc. However,\r\ncertain characteristics of the image are hard to capture using\r\nstrictly the visual properties. These include location (Paris,\r\nCalifornia, San Francisco, etc), event (birthday, wedding,\r\ngraduation ceremony, etc), people (John, Jane, brother, etc) and\r\nabstract qualities referring to objects in the image (beautiful,\r\nfunny, sweet, etc) among others. The more conventional method of\r\nannotation that relies completely on human input has several\r\nlimitations as well. Typing tags using the keypads of such devices\r\ncan be cumbersome and error-prone. Secondly, delay in tagging may\r\nresult in a loss of context in which the picture was taken (e.g.,\r\nuser may not remember the names of the people/structures in the\r\nimage).\r\n\r\nThis presents an opportunity for using speech as a modality to\r\nannotate images and/or other multimedia content. Most camera\r\ndevices have a built-in microphone. In principle, some of the\r\nchallenges associated with both, fully automatic annotation as\r\nwell as manual tagging can be alleviated if the user were to use\r\nspeech as a medium of annotation. Ideally, the user would take a\r\npicture and speak the desired tags into the device's microphone. A\r\nspeech recognizer would transcribe the audio signal into text. The\r\nspeech to text transcription can happen either on the device\r\nitself or be done on a remote machine. The transcribed text can be\r\nused as tags for the image, exactly as the user intended. One of\r\nthe biggest bottlenecks facing such systems is the accuracy of the\r\nunderlying speech recognizer. Even speaker dependent recognition\r\nsystems can make mistakes in noisy environments. If the\r\nrecognizer's output is considered as is for annotation, then poor\r\nrecognition will lead to poor quality tags. Our work tries to\r\naddress this issue by incorporating outside semantic knowledge to\r\nimprove interpretation of the recognizer's output, as opposed to\r\nblindly believing what the recognizer suggests. To improve\r\ninterpretation of speech output, we exploit the fact that most\r\nspeech recognizers provide alternate hypotheses for each\r\nutterance.\r\n The main contribution of this paper is our approach for annotating\r\nimages using speech as the input modality. The approach employs a\r\n probabilistic\r\n model for computing the joint probability of a given combination of tags\r\n using a Maximum Entropy solution. The extensive empirical evaluation demonstrates\r\n the advantage of the proposed solution, that leads to a significant improvement\r\n of quality of speech annotation.\r\n\r\n</p>\r\n<BR>\r\n<H3>Categories and Subject Descriptors:</H3>\r\nH.2.m [<B>Database Management</B>]: Miscellaneous - Semantic Image Tagging;<BR>\r\nH.2.8 [<B>Database Management</B>]: Database Applications - Data mining;<BR>\r\nH.3.3 [<B>Information Storage and Retrieval</B>]: Information Search and Retrieval<BR>\r\n<BR>\r\n<BR>\r\n<H3>Keywords:</H3>\r\nSpeech Annotation,\r\nImage Tagging,\r\nDisambiguation\r\n</p>\r\n<BR>\r\n<p>\r\n<H3>Downloadable files:</H3>\r\nPaper: <A href=\"http://www.ics.uci.edu/~dvk/pub/ICDE09_dvk_Speech.pdf\">ICDE09_dvk_Speech.pdf</A><BR>\r\nPresentation: <A href=\"http://www.ics.uci.edu/~dvk/pub/ICDE09_dvk_Speech.ppt\">ICDE09_dvk_Speech.ppt</A><BR>\r\n<IMG SRC=\"https://students.ics.uci.edu/~dvk/fig.cgi?ICDE09_dvk_Speech\" width=1 height=1>\r\n</p>\r\n\r\n<H3>BibTeX entry:</H3>\r\n<pre>@inproceedings{ICDE09::dvk,\r\n   author    = {Chaitanya Desai and Dmitri V.\\ Kalashnikov and Sharad Mehrotra and Nalini Venkatasubramanian},\r\n   title     = {Using Semantics for Speech Annotation of Images},\r\n   booktitle = {Proc.\\ of the 25th IEEE Int\u2019l Conference on Data Engineering (IEEE ICDE 2009)},\r\n   note      = {short publication},\r\n   year      = {2009},   \r\n   month     = {March 29 - April 4},\r\n   address   = {Shanghai, China}   \r\n}\r\n\r\n\r\n\r\n\r\n</pre>\r\n<A href=\"http://www.ics.uci.edu/~dvk/index.html\">Back to Kalashnikov's homepage </A>\r\n<BR>\r\n<BR>\r\n<BR>\r\n<BR>\r\n<BR>\r\n</html>", "encoding": "Windows-1252"}