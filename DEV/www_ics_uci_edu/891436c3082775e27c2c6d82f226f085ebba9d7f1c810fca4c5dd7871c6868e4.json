{"url": "https://www.ics.uci.edu/~eppstein/gina/rajan.html", "content": "<HTML><HEAD>\n<TITLE>Comp. Geom. in VLSI</TITLE>\n<META name=\"Owner\" value=\"eppstein\">\n<META name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</HEAD><BODY>\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<H1>\nComputational Geometry Problems in<BR>\nIntegrated Circuit Design and Layout\n</H1>\n<HR><P>\n\nFrom an invited talk by V. T. Rajan at the\n<A HREF=\"http://ams.sunysb.edu/~held/proc_usb_comp_geo-95.html\">\n5th MSI Worksh. on Computational Geometry</A>,\nStony Brook, October 20, 1995.  Notes by D. Eppstein.\n\n<H2>Problems</H2>\n\nIn his invited talk, Rajan discussed several geometric problems in VLSI\nlayout and analysis:\n\n<UL>\n<LI>Polygon simplification.  In the specific version Rajan gave, one is\ngiven a polygonal input P and must output a simpler polygon C which\ncovers the input, together with (the boundary representation of) the\nset-theoretic difference C-P.  C must have axis-parallel sides, although P need\nnot, and the goal is to choose a C having a given number of edges and\nminimizing the area of C-P.  He didn't say what this was used for, but\nI've seen similar problems applied to PC board simulation.\nHe described a greedy heuristic for this problem but\ncould find no theoretical results.\n\n<LI>Design rule verification.  The usual sort of rule one wants to check\nis that objects of certain types are separated by certain distances, but\nhe also talked at some length about rules requiring that e.g. every 100\nby 100 micron square region of a chip should be covered by metal at a\ndensity of between 30% and 50% (in order for the fabrication process to\nwork well).\n\n<LI>Calculation of capacitance between pairs of polygons\n(either overlapping or adjacent to each other in the layout).\n\n<LI>Logic verification.  Interpretation of layout features as components and nets,\nand verification that the circuit matches the original logic design.\n\n<LI>Process proximity correction.  In the fabrication process, feature shapes will\nbe affected by the presence or absence of nearby objects, and this needs\nto be pre-corrected for in the design.\n</UL>\n\n<H2>Assumptions and Requirements</H2>\n\nThere are several factors relevant here for selection of an algorithm.\nThe input coordinates come from the design process and are\nassumed to be integers.  The integer units do not necessarily\ncorrespond to the feature size, and sometimes one may want to round\nthe input to a larger grid, but this should be done in a way\nthat will avoid introducing unnecessary degeneracy.\nThe results of an algorithm should be exact rather\nthan approximate; in particular floating point computation is only acceptable\nas a way of speeding up exact integer arithmetic, but is not acceptable\nif it leads to round-off.  Therefore one can't in native code work with\nequations of much higher degree than quadratic.\n\n<P>\nRobustness is very important - the program must work well even for\ndegenerate or somewhat malformed input.  The input polygon sides are\nlikely to be mostly axis-parallel, but some diagonals can occur,\nespecially at 45 degree angles.  (There appears to be little in the way\nof theoretical results on such \"almost-orthogonal\" inputs.  Automated\nlayout typically produces axis-parallel layouts but critical regions are\nlikely to have been hand optimized with sides of other angles.)  Worst\ncase time complexity is important, but the program should also be\nasymptotically good in the average case, since invariably people will\nrun his codes on much larger inputs than he was anticipating.  It is\nalso important that the program run well on small inputs, since the\nother thing people do with his code is run it on batches of very many\nsmall inputs.\n\n<P>\nAny algorithms used\nmust be simple enough that they can implement them quickly, and\nso that they can make changes later if necessary.\nObject oriented design is important - Rajan has found that\nimperative-style pseudo-code is a bad way of communicating with his programmers.\n\n<P>\nOne particular feature of the input is problematic, and not adequately\ntreated by the theory literature.  The inputs Rajan deals with often come\nfrom hierarchical specifications, in which an object at one level of the\nspecification may\nbe repeated several times at a higher level.  One can flatten this\nspecification out to form the usual sort of polygonal input, but at a\ncost of multiplying the size by a large factor, typically from around 5\nfor random logic to around 50 for memory.  It would be helpful\nto develop algorithms that could deal with this sort of hierarchical\nrepresentation directly.\n\n<H2>Implementation</H2>\n\nInitially they were solving the sorts of problems listed above using a\nsweep-line technique\nthat was originally written with the assumption of axis-parallel sides,\nand that had been modified to support 45 degree lines as well.\nHowever they were finding it inflexible and insufficiently robust.\n\n<P>\nInspired by Sugihara's experiments showing that randomized incremental\nVoronoi diagrams could be practical for large inputs,\nRajan then had a programmer working with him write up Mulmuley's randomized\nincremental algorithm for constructing trapezoidal decompositions of\nline segment arrangements.  Among the advantages of explicitly\nconstructing a trapezoidal decomposition are that you can re-use it\nseveral times, and that you can choose your favorite order to traverse it.\n\n<P>The choice of a data representation turned\nout to be important; Rajan first told the programmer to do anything that\nseemed reasonable, but the project bogged down\nbefore he could get it working.  Rajan then tried Guibas and Stolfi's\nquad-edge data structure, which worked much better but perhaps involves\nmore overhead than necessary.  At the time, the extra overhead was unimportant\nbecause this part of the code was still much faster than the other\nthings being done to the input,\nbut now those other things have been improved and he's thinking of\ntrying to speed up the trapezoidal decomposition by using a more\nstreamlined data structure.\n\n<P>\nNowadays, whenever someone comes to Rajan with a geometric problem,\nhe tries to fit it into his trapezoidal decomposition framework.\nFor instance one can test for overlapping objects\n(or, after blowing up each object by a given amount, test for objects\nthat are too close to each other) by simply keeping a count for each\ncell of the number of objects covering that cell, incrementing or decrementing\nthe count as one traverses the decomposition.\n\n<P>\nRajan also talked at some length about his implementation for\ntesting density ground rules.  For the example of such a rule described earlier,\nhe would first reduce the complexity of the problem by\npartitioning the input region into 10 by 10 micron squares, and computing\nthe density within each square.  He then only considers 100 by 100 micron\nsquares aligned with the 10 by 10 grid.  The density in each large\nsquare can then be found by averaging 100 small square values.\nBut by using standard running total techniques one can instead use only\nO(1) operations per large square.\nTo find the density in each small square, he overlays the original input\nwith a grid, using his trapezoidal decomposition\n(he cited some GIS-related work here).\n\n<P>\nA question from the audience asked how he tells, with such large\ninputs (as much as 2 gigabytes) whether his algorithms really are\ncorrect.  The answer is of course testing, on smaller examples, and\nespecially cross-comparison with previous codes for the same problem.\nWhen the two differ, he then has to tediously by hand try to figure out\nwhy.  His co-workers are very conservative about replacing working code,\nand even when he finds discrepancies caused by bugs in the old code it\nis hard to persuade them that the bugs are a problem - they would\nsometimes prefer his new code be consistent with the bugs.  The final\ntest of correctness is whether the VLSI chip works - if it doesn't,\nthat's a costly $100,000 mistake.  If it does, no one cares whether the\nanalysis was perfect.\n\n<H2>Computational Results</H2>\n\nRajan showed some tables of numbers showing that the trapezoidal decomposition\n(which is robust and works for inputs of all angles) is about ten times slower than\nthe more specialized plane sweep method.  However it is also about ten times\nnewer so the comparison is not really fair; also he pointed out that\nthe factor is constant rather than polynomial.  (He also compared both methods\nwith another method for one problem that was polynomially worse; as one would expect,\nthe polynomial won out over the constants quickly.)\n\n<P>\nLater, we discussed these numbers in a conversation with Ken Clarkson\nand Mike Goodrich (but after Rajan had left).  Ken seemed to think that\nthe factor of ten was pretty typical for plane sweep vs. randomized\nincremental techniques.  The main advantage he seems to be gaining from\nthe randomized incremental technique is that Rajan has already written it\nand doesn't need to change it for each new problem he solves.\nBut we wondered whether it wouldn't make sense to replace his code with\ncode that constructs the same trapezoidal decomposition using a plane sweep method.\n(The difference between this and where he started was that the original\ncode did not construct a trapezoidalization, instead it apparently required you\nto fit whatever else you were doing into the plane sweep.)\n\n<P><HR><P>\nPart of\n<A HREF=\"http://www.ics.uci.edu/~eppstein/geom.html\">Geometry in Action</A>,\na collection of applications of computational geometry.<BR>\n<A HREF=\"http://www.ics.uci.edu/~eppstein/\">David Eppstein</A>,\n<A HREF=\"http://www.ics.uci.edu/~theory/\">Theory Group</A>,\n<A HREF=\"http://www.ics.uci.edu/\">ICS</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<P>\n<SMALL>Last update: <!--#flastmod file=\"rajan.html\" -->.</SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}