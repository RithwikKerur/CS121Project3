{"url": "https://www.ics.uci.edu/~smyth/courses/cs274/syllabus.html", "content": "<!DOCTYPE html>\n<html>\n<title>Syllabus for CS 274A | Winter 2019</title>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n\n<!--MATERIAL BELOW TO BE INCLUDED ON ALL SITE PAGES--> \n<link rel=\"stylesheet\" href=\"https://www.ics.uci.edu/~smyth/test/localw3.css\">\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css\"> <!--mini icons--> \n<link rel=\"stylesheet\" href=\"http://www.w3schools.com/lib/w3-theme-blue.css\">  <!-- color theme -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"localstyle.css\">  <!-- my own style -->\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://www.ics.uci.edu/~smyth/courses/class_style.css\">  <!-- Padhraic's class style --->  \n<link rel=\"icon\" type=\"image/png\" href=\"https://news.uci.edu/wp-content/uploads/2014/12/favicon.png\"/> <!-- favicon -->\n\n<body class=\"w3-animate-opacity\">\n  \n<div style=\"margin-left:10%;  margin-right:10%;  margin-top:30px\">  \n  \n\n<!-- -------------------------------------------------------------------------------------------- -->\n \n<h4> <span style=\"font-weight: bold;\"> CS 274A: Syllabus and Schedule, Winter 2019 </span></h4> \n\nNote: the schedule may be adapted/updated during the quarter.\n<br><br>\n \n<ul>\n<li> Week 1: January 7th\n<ul>\n<li>  <b>Probability Review</b>: random variables, conditional and\njoint probabilities, Bayes rule, law of total probability,\nchain rule and factorization. Sets of random variables, the\nmultivariate Gaussian model. Conditional independence and graphical\nmodels.\n</li>\n</ul>\n</li>\n<br>\n\n<li> Week 2: January 14th \n<ul> \n<li>  <b>Learning from Data</b>: Concepts of models and parameters. Definition of the \nlikelihood function and the principle of maximum likelihood parameter estimation. \n</li>\n<li> <b>Maximum Likelihood Learning</b>: Using maximum likelihood methods to learn the \nparameters of Gaussian models, binomial, multivariate and other parametric models.\n</li>\n</ul>\n</li> \n<br>\n \n<li> Week 3: January 21st\n<ul>\n<li> No lecture on Monday (university holiday)\n</li>\n<li>  <b>Sequence Models</b>: Learning from sequential data. Markov models and related approaches.\n</li> \n</ul> </li> \n<br>\n\n<li> Week 4: January 28th\n<ul> \n<li>  <b>Bayesian Learning</b>: Frequentist and Bayesian views of probability. \nGeneral principles of Bayesian estimation: prior densities, posterior densities, MAP, fully Bayesian approaches.  \n</li> \n<li><b>Bayesian Learning</b>: Dirichlet/multinomial and Gaussian examples. Predictive densities, model selection, model averaging. \n</li> \n</ul> </li> \n<br>\n \n\n<li> Week 5: February 4th\n<ul>\n<li> <b>Regression Learning I</b>: Linear models.  Probabilistic perspectives on regression. \nLoss functions. Parameter estimation methods for regression.   \n</li>\n<li>  <b>Regression Learning II</b>: Optimization algorithms, focusing on gradient\nand stochastic gradient methods. Regularization and Bayesian methods.\nConnections between regression and classification.\n</li> \n\n</ul> </li> \n<br>\n\n<li> Week 6: February 11th\n<ul>\n<li>  <b>Midterm Exam</b> during Monday's class (in-class, closed-book)\n</li> \n<li>  <b>Bias-Variance Trade-offs</b>: The bias-variance trade-off for squared error and regression.\n</li>\n\n</ul> </li> \n<br>\n\n<li> Week 7: February 18th\n<ul>\n<li> No lecture on Monday (university holiday)\n</li>\n<li> <b>Classification Learning</b>: Likelihood-based approaches and properties of objective functions. \n Links between logistic regression and neural network models. \n</li> \n</ul> </li> \n<br>\n\n<li> Week 8: February 25th\n<ul>\n<li> <b>Classification Learning</b>: Bayes rule, classification boundaries, discriminant functions,  \noptimal decisions, Bayes error rate, Gaussian classifiers. Generative models.\n</li>\n<li>  <b>Bias-variance</b>: Decomposition of prediction error into bias and variance. Links between regression\nand classification. \n\n</li> \n</ul> </li> \n<br>\n\n\n<li> Week 9: March 4th\n<ul>\n<li> <b>Mixture Models and EM</b>: Mixture models. Examples of mixture models for binary\nand real-valued data.\nThe EM algorithm for learning Gaussian mixtures. \n </li>\n<li> <b>Mixture Models and EM</b>: Properties of the EM algorithm. Relation of K-means clustering to \nGaussian mixture modeling. Mixtures for non-vector data.\n</li> \n</ul> </li> \n<br>\n\n\n<li> Week 10: March 11th\n<ul>\n<li> Monday: Additional topics in unsupervised learning</li>\n<li> Wednesday: <b>Temporal Models</b>: Autoregressive models,  recurrent neural networks. </li> \n</ul> </li> \n<br>\n\n\n<li> Finals Week:  \n<ul>\n<li> <b>Final exam</b>, in class, Friday March 22nd,  8:00am to 10:00am.\nThe time of exam was selected by the registrar, not by the instructor :)\n</li> \n</ul> </li> \n\n</ul>\n\n\n</body>", "encoding": "ascii"}