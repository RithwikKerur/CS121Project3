{"url": "https://www.ics.uci.edu/~eppstein/180a/970417.html", "content": "<HTML>\n<HEAD>\n<TITLE>ICS 180, April 17, 1997</TITLE>\n<META name=\"Owner\" value=\"eppstein\">\n<META name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</HEAD><BODY>\n<IMG SRC=\"icslogo2.gif\" WIDTH=472 HEIGHT=72 ALT=\"\"><P>\n<A HREF=\"index.html\">\n<H1>ICS 180A, Spring 1997:<BR>\nStrategy and board game programming</H1></A>\n\n<H2>Lecture notes for April 17, 1997<BR>\nMinimax and negamax search</H2>\n\n<H3>Game Trees</H3>\n\n<P>For any game, we can define a rooted tree (the \"game tree\") in which the \nnodes correspond to game positions, and the children of a node are the \npositions that can be reached from it in one move.  For instance \ntic-tac-toe:\n\n<P><CENTER><IMG SRC=\"tictactoetree.gif\"></CENTER>\n\n<P>(Actually, the root of this tree should have nine children, but I've \nleft out some symmetric cases. If the same board configuration arises from two different sequences of \nmoves, we create two separate nodes, so this really is a tree.  We'll see \nlater when we talk about hashing how to take advantage of duplicated nodes \nto speed up the search -- we only need to search one copy of a position, \nand use the same search results everywhere else that position appears in \nthe tree. We also assume that the players take turns moving, with no \nmultiple moves or skipped turns; these complications can be dealt with by \ntreating an entire sequence of actions by a single player as forming a \nsingle move. Finally, we'll assume this tree is finite, so that we're not \ndealing with a game that can go on forever or has infinitely many choices \nfor a single move.)\n\n<P>There are three kinds of nodes in the tree:\n<OL>\n<LI><B>Internal nodes at even levels</B> correspond to positions in which \nthe first player is to move.\n<LI><B>Internal nodes at odd levels</B> correspond to positions in which \nthe second player is to move.\n<LI><B>Leaves</B> correspond to positions at which the game has ended. One \nplayer or the other has won, or perhaps the game is drawn.\n</OL>\n\n<H3>Game Tree Evaluation</H3>\n\n<P>Suppose we have an internal node for which all children are leaves, so \nthat the game is certain to end in one more move.\nThen we can assume that the player to move is going to pick the best move.\nIf there is a leaf giving a won position for him, he will move to it and \nwin. If not, but some leaf gives a drawn position for him, he will move to \nit and draw. But, if all leaves give won positions for his opponent, he \nwill lose no matter what happens.\n\n<P>So, we know what the game outcome should be from nodes one level above \nthe leaves. Once we know that, we can apply the same analysis bottom up, to \ndetermine the correct outcome from nodes two levels above the leaves, \nthree levels up, and so on, until we reach the root of the tree.\nAt each node, the position is won for the player on move if he can find a \nchild of that node giving him a won position; it is drawn if he can find a \nchild giving him a draw; and if neither of these holds then it is lost.\nThis gives us an algorithm for playing games perfectly if only we had \nenough computation time, but for any reasonable game we won't have enough \ncomputation time. The trees are too big.\n\n<P>This also tells thus that a \"correct\" evaluation function needs to only \nhave three values, win lose and draw.  The evaluations we use in computer \ngame programs have a wider range of real-number values only because \nthey're inaccurate.  If we represent a first-player win as the value +1, a \ndraw as the value 0, and a second-player win as the value -1, then the \nvalue of each internal node in the game tree is the maximum or minimum of \nits children's values, depending on whether the first or second player is to \nmove respectively.\n\n<H3>Partial Game Trees</H3>\n\nIn practice, our search algorithms will work by only expanding part of the \ngame tree.  We use some kind of <I>stopping rule</I> to decide to stop \nexpanding the tree at certain internal nodes, making them leaves; for \ninstance, we might stop after sequences of eight moves.  Since the game \nwon't have ended at those leaves, we guess at how likely it is for one or \nthe other player to win, using the evaluation functions.  Then, we make \nthe assumption that within the nodes we've expanded, one player will be \ntrying to reach positions with large values of the evaluation function, \nwhile the other player will be trying to reach positions with small values.\n\n<P>If both players really play this way, then we can determine the value of \nthe leaf they will reach by the same min-max procedure outlined above: \ncompute the value of each internal node as either the maximum or minimum of \nits children's values, depending on whether the first or second player is to \nmove respectively.  The path to this leaf is known as the <I>principal \nvariation</I>.  The basic principle of minimax game search is to expand a \npartial game tree, find the principal variation, and make the move forming \nthe first step in this variation.\n\n<H3>Breadth First and Depth First Search; Negamax Code</H3>\n\nAs described above, the computation of game tree values is breadth first \n(we compute the values in the tree bottom-up, a single level in the tree at \na time).  Instead, we can perform a depth-first (post-order) recursive \ntraversal of the tree that evaluates a node by recursively evaluating its \nchildren and keeping track of the values seen so far.  This is much more \nspace-efficient because it doesn't need to store the whole game tree, only \na single path (which would generally be quite short, e.g. eight moves with \nthe example stopping rule above).  As we'll see next time when I discuss \nalpha-beta search, depth-first traversal also has the advantage that you \ncan use the information you've found so far to help decide not to visit \ncertain irrelevant parts of the tree, saving a lot of time.\n\n<P>It's convenient to modify the game tree values slightly, so that we only \nneed maximization operations rather than having to alternate between \nminima and maxima. At odd levels of the tree (nodes in which the second \nplayer is to move), negate the values defined above.  Then at each node,\nthese modified values can be found by computing the maximum of the \nnegations of the node's children's values.  Maybe this will make more \nsense if I write down some source code for game tree search:\n\n<PRE>\n// search game tree to given depth, return evaluation of root node\ndouble negamax(int depth)\n{\n    if (depth &lt;= 0 || game is over) return eval(pos);\n    else {\n        double e = -infty;\n        for (each move m available in pos) {\n            make move m;\n            e = max(e, -negamax(depth - 1));\n            unmake move m;\n        }\n        return e;\n    }\n}\n</PRE>\n\nNote that this only finds the evaluation, but doesn't determine which move \nto actually make.  We only need to find an actual move at the root of the \ntree (although many programs return an entire principal variation).\nTo do this, we slightly modify the search performed at the root:\n\n<PRE>\n// search game tree to given depth, return evaluation of root node\nmove rootsearch(int depth)\n{\n    double e = -infty;\n    move mm;\n    for (each move m available in pos) {\n        make move m;\n        double em = -negamax(depth - 1);\n        if (e &lt; em) {\n            e = em;\n            mm = m;\n        }\n        unmake move m;\n    }\n    return mm;\n}\n</PRE>\n\n<H3>Analysis of negamax: branching factor, depth</H3>\n\nTraditionally one analyzes game tree algorithms by making some simplifying \nassumptions about what the game tree looks like.  We assume that each \ninternal node has the same number of children; this number is known as \nthe <I>branching factor</I>.  We also assume that we search the tree to \nsome fixed <I>depth</I> (as does the algorithm above) and that the game \ndoesn't end early (before this depth is reached).\n\n<P>With these assumptions, it's easy to write down a formula for the amount \nof time the negamax program uses: it's just proportional to the number of \ntree nodes expanded. (It may look like we should multiply by something since \nthere is a loop nested within each call to negamax, but the time spent in \nthis loop can be charged to the recursive calls made in it.)  If the \nbranching factor is b and the depth is d, this number is\n<P><CENTER>1 + b + b^2 + b^3 + ... + b^d = b^d (1 - 1/b^d)/(1 -  1/b).</CENTER>\n<P>The stuff in parentheses at the end of the formula is very close to one, \nso the overall time is very close to b^d.\n\n<P>If our game doesn't meet the strict assumptions above, we can work \nbackwards and define the <I>effective branching factor</I> to be whatever \nvalue of b works to make the formula above describe our program's running \ntime.  Even less formally, we'll use \"branching factor\" to describe the \naverage number of moves available from a \"typical\" position in a game.\n\n<P>What can we say about this formula? First, it's exponential. This means \nthat we won't be able to search too many nodes; if we get a computer twice \nas fast as the old one, we will only be able to increase d by some small \nnumber of levels.  Second, it depends very strongly on the branching factor \nb.  In a game with a small branching factor (like checkers, in which there \nmay often be as few as three moves to search) we can search much deeper \nthan chess (which may have 30 or so moves in a position) or go (hundreds of \nmoves in a position).  So we'd like b to be as small as possible, but \nunfortunately it's more a function of what game we're working on and less a \nfunction of how well we can program. However, the technique I'll talk \nabout next time, alpha-beta pruning, acts to reduce the effective branching \nfactor considerably: if we're lucky, to the square root of its value in \nunpruned game trees, which lets us search to twice the depth we might \nwithout using alpha-beta.\n\n<H3>Iterated Deepening</H3>\n\nOne question remains with the negamax code above: what do we give it for \nits depth argument?  Primitive game programs just set it to some fixed \nnumber, but this will result in a lot of variation in the amount of time \nthe program takes per move. Instead you'd probably like something that \nchooses the search depth based on the amount of time the search will take.\nFortunately, the exponential nature of game search has one advantage: it \nmakes this sort of control easy through a technique known as \"iterated \ndeepening\": start searching very shallowly, then repeatedly increase the \ndepth until you run out of time:\n\n<PRE>\ndepth = 0\nwhile (enough time left for another level of search) {\n    depth++;\n    m = rootsearch(depth);\n}\nmake move m\n</PRE>\n\nIt seems like we're wasting time, since all but the last search is thrown \naway. But the same analysis as before shows that the amount of time wasted \nis very small: the times for the different levels add together like \n1+b+b^2+..., a formula we've already seen to come out to very close to the \nlast term b^d. So, iterated deepening is cheap and provides good \ntime-based control. It's also helpful in other ways: we can use the \nresults of shallower searches to help choose what order to search the moves \nin deeper searches, and as we'll see in alpha-beta searching this ordering \nis critical for fast searching.\n\n<HR>\n<A HREF=\"/~eppstein/\">David Eppstein,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>,\n<!--#flastmod file=\"970415.html\" -->.\n</BODY></HTML>\n", "encoding": "ascii"}