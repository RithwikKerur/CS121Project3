{"url": "https://www.ics.uci.edu/~eppstein/161/960118.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Sorting by divide-and-conquer</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 18, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Three Divide and Conquer Sorting Algorithms</h1>\n\nToday we'll finish heapsort, and describe both mergesort and\nquicksort. Why do we need multiple sorting algorithms? Different\nmethods work better in different applications. \n\n<ul>\n<li>Heapsort uses close to the right number of comparisons but\nneeds to move data around quite a bit. It can be done in a way that\nuses very little extra memory. It's probably good when memory is\ntight, and you are sorting many small items that come stored in an\narray.</li>\n\n<li>Merge sort is good for data that's too big to have in memory at\nonce, because its pattern of storage access is very regular. It\nalso uses even fewer comparisons than heapsort, and is especially\nsuited for data stored as linked lists.</li>\n\n<li>Quicksort also uses few comparisons (somewhat more than the\nother two). Like heapsort it can sort \"in place\" by moving data in\nan array.</li>\n</ul>\n\n<h2>Heapification</h2>\n\nRecall the idea of heapsort: \n\n<pre>\n    heapsort(list L)\n    {\n    make heap H from L\n    make empty list X\n    while H nonempty\n        remove smallest from H and add it to X\n    return X\n    }\n</pre>\n\nRemember that a heap is just a balanced binary tree in which the\nvalue at any node is smaller than the values at its children. We\nwent over most of this last time. The total number of comparisons\nis n log n + however many are needed to make H. The only missing\nstep: how to make a heap? \n\n<p>To start with, we can set up a binary tree of the right size and\nshape, and put the objects into the tree in any old order. This is\nall easy and doesn't require any comparisons. Now we have to switch\nobjects around to get them back in order.</p>\n\n<p>The divide and conquer idea: find natural subproblems, solve\nthem recursively, and combine them to get an overall solution. Here\nthe obvious subproblems are the subtrees. If we solve them\nrecursively, we get something that is close to being a heap, except\nthat perhaps the root doesn't satisfy the heap property. To make\nthe whole thing a heap, we merely have to percolate that value down\nto a lower level in the tree.</p>\n\n<pre>\n    heapify(tree T)\n    {\n        if (T is nonempty) {\n            heapify(left subtree)\n            heapify(right subtree)\n            let x = value at tree root\n            while node containing x doesn't satisfy heap propert\n                switch values of node and its smallest child\n        }\n    }\n</pre>\n\nThe while loop performs two comparisons per iteration, and takes at\nmost log n iterations, so the time for this satisfies a recurrence \n\n<pre>\n    T(n) &lt;= 2 T(n/2) + 2 log n\n</pre>\n\nHow to solve it? \n\n<h2>Divide and conquer recurrences</h2>\n\nIn general, divide and conquer is based on the following idea. The\nwhole problem we want to solve may too big to understand or solve\nat once. We break it up into smaller pieces, solve the pieces\nseparately, and combine the separate pieces together. \n\n<p>We analyze this in some generality: suppose we have a pieces,\neach of size n/b and merging takes time f(n). (In the heapification\nexample a=b=2 and f(n)=O(log n) but it will not always be true that\na=b -- sometimes the pieces will overlap.)</p>\n\n<p>The easiest way to understand what's going on here is to draw a\ntree with nodes corresponding to subproblems (labeled with the size\nof the subproblem)</p>\n\n<pre>\n       n\n    /  |  \\\n    n/b   n/b   n/b\n    /|\\   /|\\   /|\\ \n     .     .     .\n     .     .     .\n     .     .     .\n</pre>\n\nFor simplicity, let's assume n is a power of b, and that the\nrecursion stops when n is 1. Notice that the size of a node depends\nonly on its level: \n\n<pre>\n    size(i) = n/(b^i).\n</pre>\n\nWhat is time taken by a node at level i? \n\n<pre>\n    time(i) = f(n/b^i)\n</pre>\n\nHow many levels can we have before we get down to n=1? For bottom\nlevel, n/b^i=1, so n=b^i and i=(log n)/(log b). How many items at\nlevel i? a^i. So putting these together we have \n\n<pre>\n       (log n)/(log b)\n    T(n) =      sum       a^i f(n/b^i)\n        i=0\n</pre>\n\nThis looks messy, but it's not too bad. There are only a few terms\n(logarithmically many) and often the sum is dominated by the terms\nat one end (f(n)) or the other (n^(log a/log b)). In fact, you will\ngenerally only be a logarithmic factor away from the truth if you\napproximate the solution by the sum of these two, O(f(n) + n^(log\na/log b)). \n\n<p>Let's use this to analyze heapification. By plugging in\nparameters a=b=2, f(n)=log n, we get</p>\n\n<pre>\n         log n\n    T(n) = 2  sum  2^i log(n/2^i)\n          i=0\n</pre>\n\nRewriting the same terms in the opposite order, this turns out to\nequal \n\n<pre>\n         log n\n    T(n) = 2  sum  n/2^i log(2^i)\n          i=0\n\n           log n\n     = 2n   sum  i/2^i\n        i=0\n\n           infty\n     &lt;= 2n   sum  i/2^i\n        i=0\n\n     = 4n\n</pre>\n\nSo heapification takes at most 4n comparisons and heapsort takes at\nmost n log n + 4n. (There's an n log n - 1.44n lower bound so we're\nonly within O(n) of the absolute best possible.) \n\n<p>This was an example of a sorting algorithm where one part used\ndivide and conquer. What about doing the whole algorithm that\nway?</p>\n\n<h2>Merge sort</h2>\n\nAccording to Knuth, merge sort was one of the earliest sorting\nalgorithms, invented by <a href=\"people.html#jvn\">John von\nNeumann</a> in 1945. \n\n<p>Let's look at the combine step first. Suppose you have some data\nthat's close to sorted -- it forms two sorted lists. You want to\nmerge the two sorted lists quickly rather than having to resort to\na general purpose sorting algorithm. This is easy enough:</p>\n\n<pre>\n    merge(L1,L2)\n    {\n    list X = empty\n    while (neither L1 nor L2 empty)\n    {\n        compare first items of L1 &amp; L2\n        remove smaller of the two from its list\n        add to end of X\n    }\n    catenate remaining list to end of X\n    return X\n    }\n</pre>\n\nTime analysis: in the worst case both lists empty at about same\ntime, so everything has to be compared. Each comparison adds one\nitem to X so the worst case is |X|-1 = |L1|+|L2|-1 comparisons. One\ncan do a little better sometimes e.g. if L1 is smaller than most of\nL2. \n\n<p>Once we know how to combine two sorted lists, we can construct a\ndivide and conquer sorting algorithm that simply divides the list\nin two, sorts the two recursively, and merges the results:</p>\n\n<pre>\n    merge sort(L)\n    {\n        if (length(L) &lt; 2) return L\n        else {\n            split L into lists L1 and L2, each of n/2 elements\n            L1 = merge sort(L1)\n            L2 = merge sort(L2)\n            return merge(L1,L2)\n        }\n    }\n</pre>\n\nThis is simpler than heapsort (so easier to program) and works\npretty well. How many comparisons does it use? We can use the\nanalysis of the merge step to write down a recurrence: \n\n<pre>\n    C(n) &lt;= n-1 + 2C(n/2)\n</pre>\n\nAs you saw in homework 1.31, for n = power of 2, the solution to\nthis is n log n - n + 1. For other n, it's similar but more\ncomplicated. To prove this (at least the power of 2 version), you\ncan use the formula above to produce \n\n<pre>\n        log n\n    C(N) &lt;=  sum  2^i (n/2^i - 1)\n         i=0\n\n          log n\n        =  sum  n - 2^i\n           i=0\n\n        = n(log n + 1) - (2n - 1)\n\n        = n log n - n + 1\n</pre>\n\nSo the number of comparisons is even less than heapsort. \n\n<h2>Quicksort</h2>\n\nQuicksort, invented by <a href=\"people.html#hoare\">Tony Hoare</a>,\nfollows a very similar divide and conquer idea: partition into two\nlists and put them back together again It does more work on the\ndivide side, less on the combine side. \n\n<p>Merge sort worked no matter how you split the lists (one obvious\nway is to take first n/2 and last n/2 elements, another is to take\nevery other element). But if you could perform the splits so that\neverything in one list was smaller than everything in the other,\nthis information could be used to make merging much easier: you\ncould merge just by concatenating the lists.</p>\n\n<p>How to split so one list smaller than the other? e.g. for\nalphabetical order, you could split into A-M, N-Z so could use some\nsplit depending on what data looks like, but we want a comparison\nsorting algorithm that works for any data.</p>\n\n<p>Quicksort uses a simple idea: pick one object x from the list,\nand split the rest into those before x and those after x.</p>\n\n<pre>\n    quicksort(L)\n    {\n        if (length(L) &lt; 2) return L\n        else {\n            pick some x in L\n            L1 = { y in L : y &lt; x }\n            L2 = { y in L : y &gt; x }\n            L3 = { y in L : y = x }\n            quicksort(L1)\n            quicksort(L2)\n            return concatenation of L1, L3, and L2\n        }\n    }\n</pre>\n\n(We don't need to sort L3 because everything in it is equal). \n\n<h2>Quicksort analysis</h2>\n\nThe partition step of quicksort takes n-1 comparisons. So we can\nwrite a recurrence for the total number of comparisons done by\nquicksort: \n\n<pre>\n    C(n) = n-1 + C(a) + C(b)\n</pre>\n\nwhere a and b are the sizes of L1 and L2, generally satisfying\na+b=n-1. In the worst case, we might pick x to be the minimum\nelement in L. Then a=0, b=n-1, and the recurrence simplifies to\nC(n)=n-1 + C(n-1) = O(n^2). So this seems like a very bad\nalgorithm. \n\n<p>Why do we call it quicksort? How can we make it less bad?\nRandomization!</p>\n\n<p>Suppose we pick x=a[k] where k is chosen randomly. Then any\nvalue of a is equally likely from 0 to n-1. To do average case\nanalysis, we write out the sum over possible random choices of the\nprobability of that choice times the time for that choice. Here the\nchoices are the values of k, the probabilities are all 1/n, and the\ntimes can be described by formulas involving the time for the\nrecursive calls to the algorithm. So average case analysis of a\nrandomized algorithm gives a randomized recurrence:</p>\n\n<pre>\n       n-1\n    C(n) = sum (1/n)[n - 1 + C(a) + C(n-a-1)]\n       a=0\n</pre>\n\nTo simplify the recurrence, note that if C(a) occurs one place in\nthe sum, the same number will occur as C(n-a-1) in another term --\nwe rearrange the sum to group the two together. We can also take\nthe (n-1) parts out of the sum since the sum of 1/n copies of 1/n\ntimes n-1 is just n-1. \n\n<pre>\n           n-1\n    C(n) = n - 1 + sum (2/n) C(a)\n           a=0\n</pre>\n\nThe book gives two proofs that this is O(n log n). Of these,\ninduction is easier. \n\n<p>One useful idea here: we want to prove f(n) is O(g(n)). The O()\nhides too much information, instead we need to prove f(n) &lt;= a\ng(n) but we don't know what value a should take. We work it out\nwith a left as a variable then use the analysis to see what values\nof a work.</p>\n\n<p>We have C(1) = 0 = a (1 log 1) for all a. Suppose C(i) &lt;= a i\nlog i for some a, all i&lt;n. Then</p>\n\n<pre>\n    C(n) = n-1 + sum(2/n) C(a)\n     &lt;= n-1 + sum(2/n)ai log i\n     = n-1 + 2a/n sum(i=2 to n-1) (i log i)\n     &lt;= n-1 + 2a/n integral(i=2 to n)(i log i)\n     = n-1 + 2a/n (n^2 log n / 2 - n^2/4 - 2 ln 2 + 1)\n     = n-1 + a n log n - an/2 - O(1)\n</pre>\n\nand this will work if n-1 &lt; an/2, and in particular if a=2. So\nwe can conclude that C(n) &lt;= 2 n log n. \n\n<p>Note that this is worse than either merge sort or heap sort, and\nrequires random number generator to avoid being really bad. But\nit's pretty commonly used, and can be tuned in various ways to work\nbetter. (For instance, let x be the median of three randomly chosen\nvalues rather than just one value).</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960118.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}