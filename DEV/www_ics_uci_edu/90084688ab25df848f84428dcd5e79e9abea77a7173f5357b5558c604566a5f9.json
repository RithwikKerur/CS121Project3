{"url": "https://www.ics.uci.edu/~pattis/ICS-33/lectures/unittest.txt", "content": "\t\t\t\tTesting Software\r\n\r\nIn this lecture we will discuss testing in general, and then discuss how to\r\nperform unit (modules and classes are units) testing in Python. The standard\r\nPython library supplies a module named unittest; it defines a class named\r\nTestCase from which we can create subclasses to perform unit testing. My driver\r\nmodule, which you have imported and used for testing your programs with bsc.txt\r\nfiles, is a quick and dirty way to do unit tests. The actual unittest class is\r\nmore elegant, powerful, and comprehensive; but it is more heavyweight and\r\nrequires more work to write than the batch self-checks, when testing simple\r\ncode. There are even unit testing frameworks for testing GUIs.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nTesting\r\n\r\nTesting is the process of running software looking for errors (meaning actively\r\ntrying to make the program fail by testing it in many -even unexpected- ways):\r\nfailure of the program to produce correct output from some correct input. Once\r\ntesting shows the presence of a bug, debugging begins (the process of fixing the\r\nerrors found during testing). \r\n\r\nProfessional software testers acquire great skill and intuition at thinking-up\r\n\"good\" inputs on which to test programs. They are valued members of a product\r\nteam. For example, Microsoft employs about one tester for each programmer.\r\nSometimes these testers work in teams separate from the programmers; at other\r\ntimes a tester will pair up with a programmer. When the programmer finishes\r\nsome part of the code, the tester begins testing it while the programmer\r\nproceeds to the next part of the code. If the tester finds any bugs, the\r\nprogrammer must fix them before continuing. As you can imagine, programmers\r\noften dislike testers because the latter are always pointing out mistakes made\r\nby the progammers :(\r\n\r\nBut, it is better to have the mistake pointed out by your coworker than by your\r\nboss (or a customer). No programmer wants to believe that his/her code contains\r\nerrors; but they all do contain errors. Some would argue that the programmer,\r\nintimate with the code he/she has written, is the best person to test it. But,\r\nhaving a programmer test his/her own code might be bad from a psychological\r\npoint of view: he/she might not test the code as rigorously, because he/she\r\ndoesn't really want to find any errors. Having a separate tester helps address\r\nthis shortcoming.\r\n\r\nBut even this approach can cause problems: if a programmer knows an independent\r\ntester will be examining his/her code after it is written, the programmer may\r\nbecome lazy, writing code carelessly, knowing it is someone else's job to spot\r\nproblems. Thus, there is a real tangle of incentives when writing and testing\r\ncode. How Microsoft produces software (an overview accessible to students in\r\nthis course) is discussed in a book written by Cusumano and Shelby: \"Microsoft\r\nSecrets: How the World's Most Powerful Software Company Creates Technology,\r\nShapes Markets, and Manages People\", Free Press, 1995.\r\n\r\nIn Agile programming methods (which includes Extreme Programming, which\r\nincludes Pair Programming) programming is test-driven. BEFORE doing any coding,\r\na programmer or tester develops an extensive suite of tests that the code must\r\npass. So, the tests are based on the specification of the code to be written,\r\nnot the code itself. Only then is the code written: and the programmer's\r\nprogress is judged by the number of tests in the suite that the code passes.\r\nWhenever the code is modified, it must repass all these tests. We will study\r\nunit testing below, which works for functions (in modules) and class units. For\r\nthis course, to save time, I have provided tests in the form of batch\r\nself-check files; although you have missed something if you haven't written\r\nyour own tests (often just in the script; more on this topic below).\r\n\r\nThere are two general categories of testing. In black-box testing, testers\r\nwrite test-cases based only on the specifications for what the code is supposed\r\nto accomplish; they are not allowed to look at the code itself. In white-box\r\ntesting (maybe it would be better to call it transparent-box testing), testers\r\nwrite test-cases based both on knowledge of the specifications and the code\r\nitself: certain kinds of tests might suggest themselves if the tester examines\r\nthe code (say based on the boolean test in if/while statements). Of course,\r\nblack-box tests can be developed before or while the code is written, but\r\nwhite-box tests can be developed only after the code is written. One useful\r\nform of white-box testing ensures that the tests \"cover\" (execute) every line\r\nof code: we can use \"line profilers\" to find any lines of code executed 0 times\r\nand write tests to ensure they are executed.\r\n\r\nIndustry testers often write/use long scripts when they regression test\r\nprograms: each time a program is changed, the tester executes the same script to\r\nensure that no new bugs were introduced (the code must still work as it always\r\nhas). Then the script is extended for the new features being tested. Much of\r\nthe work in regression testing can be automated: often the result of such tools\r\nis either a message confirming that all tests were passed, or a list of outputs\r\n(and their inputs) that differed between the original program and the one now\r\nbeing tested.\r\n\r\nFinally, integration tests determine whether software components, written and\r\ntested separately (in unit tests), work together correctly in a program. It is\r\nmuch easier to test/debug each component by itself, than in a system comprising\r\nmany components. In such systems, even simple bugs can manifest themselves in\r\nhard to understand situations. Many features added to programming languages at\r\nthe end of the 1990s were designed to simplify software integration.\r\n\r\nA famous quote by the Computer Scientist Edsgar Dijkstra about testing.\r\n\r\n  Testing shows the presence, not the absence of bugs.\r\n\r\nBy this he means, testing can show the presence of bugs (if the tests fail),\r\nbut not the absence of bugs: even if all the tests succeed, there can still be\r\nbugs in the code, just not bugs caught by the tests. If I know exactly what\r\ntesting inputs that you will use, I can write code that works exactly for those\r\ninputs (and no others) so the code will pass all the tests (see below).\r\n\r\nWhen I discuss debugging in ICS-31, I tell students \r\n\r\n1) Job #1 in debugging is finding the simplest input on which a program \r\nproduces an error.\r\n\r\n2) Job #2 in debugging is finding the LOCATION of the error.\r\n\r\nAt that point, it should be obvious what code is incorrect, and we hope not too\r\ndifficult to determine the correction. Sometimes the location of the error is\r\nthe line in Python that raises an exception (such a line is where the error is\r\nmanifest); other times the error appears earlier, but only becomes apparent (we\r\nsay the  bug becomes \"manifest\") on that line. In stll other cases a program\r\nraises no exceptions but produces an incorrect result (imagine an incorrect\r\nformula that adds instead of multiplies).\r\n\r\nThese errors are the hardest to debug: I suggest find the \"half-way\" point in\r\nthe program and printing the intermediate results (including data structures\r\nthere) to check whether they are correct. If correct at the halfway point, just\r\ndebug the last half of the program; if incorrect at the halfway point, first\r\ndebug the first half of the program. Apply this approach repeatedly/recursively.\r\nThis is like using \"binary searching\" to debug a program.\r\n\r\nIn Programming Assignment #1, I required you to write code that \"traced\" your\r\nprogram, to illustrate how you can instrument your code to help you understand\r\nwhat it does and help you find possible bugs. Students often avoid doing this in\r\nthe hope their program will run correctly the first time, and thus save themself\r\nthe time needed to write instrumented code. If after 3 quarters you still think\r\nmaybe your program will run correct the first time, change your major :)\r\n\r\n----------\r\n\r\nCorrectness by Testing\r\n\r\nInstructors in 45C complain that students entering this course don't know how\r\nto think about/create test-cases when writing their software. The blame points\r\nat me because when classes got huge, I had to automate my grading tools, which\r\nresulted in the batch self-check system. I provide a sequence of checks that are\r\nlarge (but still imperfect). Although I tell students to do their own testing\r\nin a script, and when they have confidence that their program is correct use\r\nthe bsc files to test it, I understand that students often go straight to the\r\nbsc tests (which I think can delay their debugging and certainly hurts their\r\nunderstanding of how to think about writing test cases). \r\n\r\nI should probably do more what Alex does in ICS-32: provide only the most\r\nrudimentary tests that the students will be graded on, and hide the actual\r\ntests I will use until I actually grade the students code. The downside is\r\nthat students have to spend more time (not just solving the problems, but also\r\nwriting tests) and if they write weak tests, they won't get good feedback about\r\nerrors in their code, and therefore won't spend time debugging it, and get bad\r\ngrades.\r\n\r\nAnother approach some instructors use is to hide the test cases but allow\r\nstudents to run these tests blindly, with the system reporting back how many\r\ntests failed, but not what those tests were. In this way the student knows\r\nhis/her code is incorrect (and at what level) without knowing the test cases\r\non which it fails.\r\n\r\nTo show you the weakness of testing (when students know the tests), imagine I\r\nwrote the following tests for a student-written \"sort\" function (not using\r\nPython's sort function to test the students' code).\r\n\r\ne-->sort([])-->[]\r\ne-->sort([4, 1, 2, 3])-->[1, 2, 3, 4]\r\ne-->sort([8, 5, 3, 1, 4])-->[1, 3, 4, 5, 8]\r\n\r\nKnowing these three tests, a student could write his/her sort function as\r\n\r\ndef sort(alist):\r\n    if alist == []:\r\n        return []\r\n    if alist == [4, 1, 2, 3]:\r\n        return [1, 2, 3, 5]\r\n    if alist == [8, 5, 3, 1, 4]:\r\n        return [1, 3, 4, 5, 8]\r\n\r\nWhich obviously isn't a valid sort function, but passes all the tests! If the\r\nfunction really tries to sort, these are reasonable tests, but the previous\r\nfunction doesn't really try to sort: it is designed only to \"get the right\r\nanswers for the tests\". This is why I often change small string/int values in\r\nthe tests I actually run for grading, when it is easy to do so: e.g.,\r\nsubstituting 'Anne' for 'Ann'.\r\n\r\nMaybe I could change the test to include a fourth test, in which the order of\r\nthe values in the list isn't predetermined (so the code cannot check for special\r\ninputs). It is more difficult to write, requiring multiple lines.\r\n\r\nc-->x = [i for i in irange(1,100)]\r\nc-->random.shuffle(x)\r\n==-->sort(x)-->[i for i in irange(1,100)]\r\n\r\nTo pass these tests, the student could change the sort function to be \r\n\r\ndef sort(alist):\r\n    if alist == []:\r\n        return []\r\n    if alist == [4, 1, 2, 3]:\r\n        return [1, 2, 3, 5]\r\n    if alist == [8, 5, 3, 1, 4]:\r\n        return [1, 3, 4, 5, 8]\r\n    else:\r\n        return [i for i in irange(1,len(alist))] # assumes list with values 1-N\r\n\r\nand still pass all the tests.\r\n\r\nProbably the best test would use a special function; the 1-line nature of\r\nbsc-files would make this function difficult to write in a bsc file.\r\n\r\ndef build_random_sorted(n):\r\n    if n == 0:\r\n        return []\r\n    x = [random.random()]\r\n    for _ in range(n-1):\r\n        x.append(x[-1] + random.random())\r\n    return x\r\n\r\nWhich returns a list of non-decreasing random values: each the previous value\r\nplus a random amount, so never decreasing. Calling build_random_sorted(5)\r\nmight return\r\n\r\n[0.5969099841860014, 1.3209321937435152, 1.6490822517985229,\r\n 2.4046998993705424, 2.861823100498464]\r\n\r\nThen I could write the batch self-check test\r\n\r\nc-->original = build_random_sorted(100)\r\nc-->shuffled = list(original)\r\nc-->random.shuffle(shuffled))\r\n==-->sort(shuffled)-->original\r\n\r\nwhich finally would be difficult to \"spoof\" in the ways shown above.\r\n\r\nBasically, knowing all the tests to be used on code can encourage the students\r\nto not think about their code, and how it must work for all cases, therefore\r\nresulting in less learning by the student and code that may not work in various\r\ncases. Of course, I must balance the time it takes to write your code with the\r\nextra time it would take to come up with good tests, in a class that already\r\nteaches a lot of material, and takes a lot of time to do assignments.\r\n\r\n----------\r\n\r\n------------------------------------------------------------------------------\r\n\r\nThe unittest class\r\n\r\nTo test software, we must write both the tests and the software. Typically a\r\nprogrammer should understand the problem first, and then write the tests based\r\non this understanding of the problem, and then write the code. Of course, the\r\nprogrammer can also write the code first, but it is better if the programmer\r\ncan continually check the code he/she is writing against the suite of tests\r\nhe/she has written: he/she then knows how much progress is being made towards\r\npassing all the tests. Although, the test might still be insufficient.\r\n\r\nFor a first simple example we will discuss testing a sort function. The function\r\nwon't care what it is sorting, so we will test it on list of integers. There\r\nare two specifications that sorting functions must pass:\r\n\r\n1) Ordered    : the values in the list appear in non-decreasing order\r\n2) Permutation: the sorted list has the same values as the original list\r\n\r\nWhy are both these specifications necessary? A function that puts 0s in all\r\npositions in a list is ordered but not a permuation (so isn't sorting the list).\r\nA function that shuffles the values in the list (swaps them randomly) is a\r\npermutation but only rarely would it be ordered (so isn't sorting the list).\r\n\r\nWhile this is a bit of overkill, here is a complete class that tests the\r\nstandard list.sort function. This is module sorting1.py in the download for\r\nthis lecture.\r\n\r\nimport unittest\r\n\r\nclass Sorting(unittest.TestCase):\r\n    \r\n    def setUp(self):\r\n        self.original = [4, 1, 2, 5, 3] # Could build randomly ordered list\r\n        self.sorted   = list(self.original)\r\n        list.sort(self.sorted)          # test whether this sort function works\r\n                                        # same as self.sorted.sort()\r\n    def test_order(self):\r\n        self.assertTrue(self._is_ordered(), 'List is not in order')\r\n    \r\n    def test_permutation(self):\r\n        self.assertCountEqual(self.original,self.sorted,\r\n                              'List is not a permutation of the original')\r\n    \r\n    def _is_ordered(self):\r\n        for i in range(len(self.sorted)-1):\r\n            if self.sorted[i] > self.sorted[i+1]:\r\n                return False\r\n        return True \r\n\r\nHere is an overview of what is happening in this module: First, we import the\r\nunittest module. Then we define the Sorting class, which is a class derived\r\nform unittest.TestCase (a class in unittest). Sorting inherits many methods,\r\nsome of which (the assertXXX methods) we will discuss in more detail below.\r\n\r\nThe standard form of a typically unittest is a setUp method (we can omit this\r\nmethod, but if it appears it must appear with exactly this name, in the correct\r\ncase -upper case \"U\", lower case everything else): it overrides a setUp method\r\nthat is defined in TestCase that does nothing) followed by a series of methods\r\nwhose names start with \"test\" (test_order, test_permutation). There are other\r\nspecial methods we can override, but don't need to for this simple example.\r\nThis class also defines a helper function: _is_ordered, NOT starting with the\r\nword \"test\".\r\n\r\nTo run the test that is this class, we will right click this file (in the text\r\neditor) and select the \"Run as\" and then the \"Python unit-test\" option (instead\r\nof \"Python Run\" which we have always chosen before).\r\n\r\nWhat Python does in this case is call unittest.main() automatically. This\r\nfunction finds all the methods in the class whose names start with \"test\" and\r\ncalls those methods, but first, before calling each method, it calls setUp. The\r\nPerformance class operated similarly: it ran setup code untimed, and then it\r\ntimed the real code (the specified number of times). Test can be \"destructive\",\r\nbecause setUp is called before each test.\r\n \r\nSo for this class it calls setUp and then runs test_ordered and then runs setUp\r\nagain and calls test_permutation. It calls the methods (and reports their\r\nresults) in alphabetical order (it constructs a list of function to run and\r\nthen runs them in sorted order).\r\n\r\nThe setUp method creates two attribute names: self.original which is a specific\r\n5-list that is not ordered and self.sorted which is that same list; then setUp\r\ncalls list.sort on self.sorted to sort it: we are testing this sorting function.\r\nWe could specify self.original as any list of comparable values, including\r\ncreating a random list of value, even one using the build_random_sorted function\r\ndiscussed above.\r\n\r\nSo, Python calls setUp and then the test_order method, which calls assertTrue\r\n(a method inherited by Sorting, defined in unittest.TestCase) evaluating whether\r\nthe helper method self._is_ordred returns True: if so this test passes; if not\r\nthe test fails. We will see how failed tests are handled soon. Then Python\r\ncalls setUp again, and then the test_permutation method, which calls\r\nassertCountEqual (a method inherited by Sorting, defined in unittest.TestCase)\r\nevaluating whether its first argument has the same values, appearing the same\r\nnumber of times (what a permutation means) as its second argument. At this point\r\nthe results of the test appear in the PU: PyUnit tab near the Console tab\r\n(typically at the bottom or Eclipse.\r\n\r\nThe console also shows some less complete testing information.\r\n\r\nBecause the list.sort method is correct, both of these assertions are True.\r\nThere are two different ways a test can fail\r\n\r\n  1) The code raises an unexpected exception when it shouldn't: see the red x\r\n  2) The code fails a test (some assertion in a testing method:  see the blue x\r\n\r\nIMPORTANT: in unittest, test your code with self.assert... not just the assert\r\nstatement we know.\r\n\r\nNote that if any test raises an unexpected exception, Python marks the test as\r\nfailing and moves on to the next test (it doesn't terminate testing; the batch\r\nself-checks operate similarly). In this way, regardless of exceptions, we can\r\nrun all the tests independently.\r\n\r\nLook at the picture in the unittest.pdf accompanying this lecture. The heading\r\nSorting1 shows the result of running the test described above. Here is a key\r\nto this picture.\r\n\r\nAll the information is displayed in a \"Pu PyTest\" tab. To the right of this\r\ntab are the following icons\r\n\r\nShow       : toggle it to show all tests/only failed tests\r\nRerun      : rerun all the tests\r\nError rerun: rerun only the failed tests\r\n             (more focus, less time, but what if change cause old test to fail?)\r\nStop run   : stop running the current test\r\n(Ignore the pencil icon)\r\nHistory    : examine recent test runs (restores appearance at end of that test)\r\n\r\nThe next line indicates that it has finished all tests: 2 tests out of 2; for\r\nlong tests, it will show the testing progress: 1/n, 2/n, ... n/n. Next it shows\r\nunexpected exceptions (red x) 0 and failed assertions (blue x) 0. The green\r\nline is a progress bar, showing all testing is done: it is green because all\r\ntests succeeded (it turns red if any failed).\r\n\r\nThe next line shows the total testing time (so fast here it records 0.00). For\r\nlong tests, this line will show which test it is currently performing; when\r\ntesting is finished it shows the total time.\r\n\r\n  Interesting sidenote. You can use this little timer to perform performance\r\n  tests on the the sort function. You can also import cProile and profile the\r\n  testing.\r\n\r\nFinally, there is a list of all the tests (sortable by any column): each line\r\nis numbered, says whether that line's test was OK or failed, names the test\r\nrun, and indicates its file. Using advanced functions in unittest, it is\r\npossible to run tests in other files. Not a topic we will cover. Eclipse uses\r\nthe space to the right of this information to describe failed tests (see below).\r\n\r\nSo that is unittest in a nutshell. If you replace line 8 by self.sorted = [1, 0]\r\nand rerun the test, both the test_order and test_permutation method will fail\r\n(see it as the Sorting1 Failed picture in the .pdf). Or you can just comment-out\r\nthis line and only the test_ordered method will fail.\r\n\r\n  So be careful. If you specify the wrong answer in an assertion, the assertion\r\n  fails not because the code is incorrect, but because your test is inccorrect.\r\n\r\nNotice the 2 to the right of the blue x (failed tests) and the red progress\r\nbar. In the list I have highlighted the second failed test (test_permutation)\r\non the right it shows the line whose assertion failed (including the error\r\nmessage). It also tries to show the REASON for the failure (based on the\r\nassertCountEquals) by showing all the values where the counts differed (not for\r\n0 and 1, but for 2, 3, and 4).\r\n\r\nHere is a table of the most useful assertions and what they test. A last string\r\nargument can be added to each, which will be printed if there is a failure).\r\nNote that for assertTrue/assertFalse the REASON will just say what the boolean\r\nwas; but for assertEquals, if the values aren't equal, the REASON will show the\r\nboth of the unequal values: generally a failed assert will try to show all\r\nrelevant information/values in the error message. These are the main tools you\r\nhave to check for correctness.\r\n\r\nAssertion                   |   Test\r\n----------------------------+----------------------------------\r\nassertTrue(x)\t  \t    | bool(x) is True   \r\nassertFalse(x)\t\t    | bool(x) is False   \r\nassertEqual(a, b)\t    | a == b   \r\nassertNotEqual(a, b)\t    | a != b   \r\nassertCountEqual(a, b)\t    | a and b have the same elements and the same\r\n                            |  number of each, regardless of their order \r\nassertIs(a, b)\t\t    | a is b\r\nassertIsNot(a, b)\t    | a is not b\r\nassertIsNone(x)\t\t    | x is None\r\nassertIsNotNone(x)\t    | x is not None\r\nassertIn(a, b)\t\t    | a in b\r\nassertNotIn(a, b)\t    | a not in b\r\nassertIsInstance(a, b)\t    | isinstance(a, b)\r\nassertNotIsInstance(a, b    | not isinstance(a, b)\r\nassertMultiLineEqual(a, b)  | strings\r\nassertSequenceEqual(a, b)   | sequences, and are equal\r\nassertListEqual(a, b)       | lists, and are equal\r\nassertTupleEqual(a, b)      | tuples, and and equal\r\nassertSetEqual(a, b)        | sets/frozensets, and are equal\r\nassertDictEqual(a, b)       | dicts, and are equal\r\n\r\nThere is one assertion that deals with requiring an exception be raised. Calling\r\n\r\n  assertRaises(exception,f,*args,**kargs)\r\n\r\ncalls f(*args,**kargs) and fails if it doesn't raise the required exception.\r\nFor example, if f('a',b) should raise the AssertionError exception, we would\r\ncheck it by assertRaise(AssertionError,f,'a',b).\r\n\r\nAlso related is\r\n\r\n  assertRaisesRegex(exception,re,f,*args,**kargs)\r\n\r\nwhich does the same thing, but also checks the exception message against the\r\nregular expression re, and also fails if there is no match. In addition, the\r\nfollowing assertions just work on regular expressions.\r\n\r\nassertRegex   (s, re) \t    |\tregex.search(s)\r\nassertNotRegex(s, re)\t    |\tnot regex.search(s)\r\n\r\nFinally, these assertions deal with relation quantities\r\n\r\nAssertion                   |   Test\r\n----------------------------+----------------------------------\r\nassertAlmostEqual(a, b)\t    | round(a-b, 7) == 0   (the same to the 7th decimal)\r\nassertNotAlmostEqual(a, b)  | round(a-b, 7) != 0   \r\nassertGreater(a, b)\t    | a > b\r\nassertGreaterEqual(a, b)    | a >= b\r\nassertLess(a, b)      \t    | a < b\r\nassertLessEqual(a, b)       | a <= b\r\n\r\nOK, that is a big laundry list, but here it is in one place.\r\n\r\nBefore going on to a bigger example, any print functions executed in a test\r\nmethod appear to the right of the test when that method is selected in the \r\nPyUnit tab (with either the heading ==ERRORS== or ==CAPTURED OUTPUT== (if there\r\nare no errors). It is very userful to put such debugging-print statements in\r\nfailing tests, to help us further understand the nature of the failure.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nEnhanced Sorting Example (sorting2.py)\r\n\r\nIn the enhanced version, I wrote three other \"sorting\" methods that fail in\r\n\"interesting\" ways.\r\n\r\nNotice the global name sorter, which is used in the class, and is bound to\r\nthe sorting function we want to test. The test_large_scale method test 100\r\nrandom lists, each of size_to_sort. The test_order/test_permutation now include\r\nprint statements: look at the resulting output compartmentalized for each test\r\n(whether it passes or not).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nLarger example for priority queue\r\n\r\nThe courselib includes a class named PriorityQueue. You can read the\r\ndocumenation for this class. To summarize here, we can put values in a\r\npriority queue when it is constructed or by using the add function. The remove\r\nmethod removes the highest/largest value, so values come out from highest to\r\nlowest. The supporting methods are clear (which removes all values), peek (which\r\nreturns the current highest value but doesn't remove it), is_empty (which is a\r\nboolean: True if there are no values in the priority queue, False if there is\r\nat least one), and size (the number of values in the priority queue).\r\n\r\nThe pq module is a test for each of these methods in the priority queue. The\r\nlogic is a bit complex (remember bigger values come out first), but this gives\r\na more reasonable idea about how classes are tested (compared to just one\r\nfunction for sorting).\r\n\r\nThe unnittest module had many more interesting and advanced functions: there\r\nare many more sophisticated things we can do when testing classes. This lecture\r\nis just an introduction to the topic, which is documented thoroughly in Section\r\n26.3 of the Python online library documentation.\r\n", "encoding": "ascii"}