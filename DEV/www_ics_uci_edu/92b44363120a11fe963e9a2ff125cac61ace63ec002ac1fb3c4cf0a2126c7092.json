{"url": "https://www.ics.uci.edu/~pattis/ICS-33/lectures/empiricalaa.txt", "content": "\t\t\tEmpirical Analysis of Algorithms\r\n\r\nIn the two previous lectures we learned about complexity classes and how to\r\nanalyze algorithms (mostly Python statements/functions) to find their complexity\r\nclasses. These analyses were mostly done by just looking at code (aka \"static\r\nanalysis\") and were independent of any technology: i.e., independent of which\r\nPython interpreters we used and the speed of the computers running our code.\r\n\r\nWe also learned that given the complexity class of a runnable Python function,\r\nwe can approximate its running time by T(N) = c*complexity_class(N), where\r\ncomplexity_class(N) is its complexity class: e.g., N, N Log N, N**2, etc. We\r\ncan then run this function on a reasonably-sized problem (with N not too small,\r\nso the discarded lower-order terms are small enough to really ignore) and\r\nmeasure the amount of time it takes (T). Finally, we can solve for the constant\r\nin the time equation: c = T(N)/complexity)class(N) by plugging in the measured\r\nT(N) and complexity_class(N). Then, for large N, we can use the formula\r\n   T(N) = c*complexity_class(N)\r\nwith the computed c, to approximate the amount of time this function requires to\r\nsolve a problem of any large-sized N. Such analysis (by running code) is called\r\n\"dynamic analysis\" or \"empirical analysis\". We also saw that we could\r\napproximate the complexity class by running the code on input sizes that double\r\nand then plot the results, looking for a match  against standard doubling\r\nsignatures.\r\n\r\nIn the first part of this lecture we will examine how to time functions on the\r\ncomputer (rather than using an external timer) and we will write some Python\r\ncode to help automate this task. Given such tools, and the ability to chart the\r\ntime required for various-sized problems, we can also use this data to infer the\r\ncomplexity class of a function without ever seeing its code. Yet we can still\r\ndevelop a formula T(N) to approximate the amount of time this function requires\r\nto solve a problem of any large-size N, without even looking at the code.\r\n\r\nGenerally in this lecture, we will explore using the computer (dynamic/emprical\r\nanalysis) to better help us understand the behavior of algorithms that might be\r\ntoo complex for us to understand by only static analysis (we might not even\r\nhave the algorithm in the form of source code, so we cannot examine it, an only\r\ncan run it).\r\n\r\nIn the second section, we will switch scale and use a Profiling module/tool\r\nnamed cProfile (and its associated module pstats) to run programs and determine\r\nwhich functions are consuming the most time. Once we know this, we will an\r\nattempt to improve the performance of the program by optimizing only those\r\nfunctions: those that are taking significant time.\r\n\r\nFinally, in the third section we will explore how Python uses HASHING in sets,\r\nfrozen sets, and dicts to achieve a constant time complexity class, O(1) for\r\nmany operations. We will close the loop by using dynamic analysis to verify\r\nthis O(1) complexity class. Hashing is more closely studied in ICS-45C and\r\nespecially in ICS-46.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nUnderstanding Sorting and N Log N Algorithms:\r\n\r\nWe previous discussed that the fastest algorithm for sorting a list is in the\r\ncomplexity class O(N Log N). This is true not only for the actual algorithm\r\nused in the Python sort method (operating on lists), but for all possible\r\nalgorithms. In this section, without using this knowledge, we will time the\r\nsorting function and infer its complexity class. In the process we will build\r\na sophisticated timing tool for timing sorting and other algorithms (and use\r\nit a few times later in this lecture).\r\n\r\nHere are Python statements that generate a list of a millon integers, shuffle\r\nthem into a random order, and then sort them.\r\n\r\nalist = [i for i in range(1_000_000)] #Python >= 3.6 allows _ to clarify numbers\r\nrandom.shuffle(alist)\r\nalist.sort()\r\n\r\nLet's first discuss how to time this code. There is a time module in Python that\r\nsupplies access to various system clocks and conversions among various time\r\nformats. I have written a Stopwatch class (in the stopwatch.py module, which is\r\npart of courselib) that makes clocks easier to use for timing code. The four\r\nmain methods for objects constructed from Stopwatch classes are start, stop,\r\nread, and reset; each should be intuitive when operating a physical/software\r\nStopwatch.\r\n\r\nYou can read the complete documentation for this class by following the \"Course\r\nLibrary Reference\" link and then clicking the Stopwatch link. The code itself\r\n(if you are interested in reading it) is accessible in Eclipse by disclosing\r\npython.exe in the \"PyDev Package Explorer\", then disclosing \"System Libs\"\r\nand then \"workspace/courselib\", and finally clicking on stopwatch.py to see\r\nthe code. You can see all the courselib modules using this mechanism.\r\n\r\nHere is a complete script that uses a Stopwatch to time only the alist.sort()\r\npart of the code above. Notice that it imports the gc (garbage collection)\r\nmodule to turn off garbage collection during the timing, so this process will\r\nnot interfere with our measurements. We need to turn it back on afterwards.\r\n\r\n----------\r\n\r\nObjects, Memory, and Garbage (collection)\r\n\r\nWhen we construct an object in Python, we allocate part of the computer's memory\r\nto store the object's attributes/state. The following loop will eventually\r\nconsume all the memory Python can use (it runs for a few seconds on my omputer).\r\nIt tries to store a million factorials in a list: ultimately it raises a\r\nMemoryError exception.\r\n\r\nalist = [0]\r\nfor i in range(1_000_000):\r\n    alist += alist\r\n    print(i)\r\n12 to 12\r\n\r\n--->Update: For Python 3.7 this takes a very long time (~5 minutes) to raise a\r\n--->MemoryError exception. I am trying to understand why, and find code that\r\n--->will raise it faster. It pretty quickly prints the numbers 1-24 and then\r\n--->more slowly prints numbers up to 32. It exhausts all of memory (99%) quickly\r\n--->but seems to continue executing after that, with memory dipping every so\r\n--->often, accompanied by a big increase in disk usage (you can watch the CPU\r\n---> and Memory usage in the Task Manager). I conjecture that when Python runs\r\n--->our of memory, it starts using the hard disk as additional storage, but\r\n--->eventually it gives up on that approach, as memory use skyrockets (doubles).\r\n--->Memory is reclamed only after the process is terminated in Eclipse with the\r\n--->red square.\r\n\r\n\"Garbage\" is objects constructed by Python, which can no longer be referred to.\r\nIf we wrote x = [i for i in range(1_000_000_000)] then x would refer to a\r\nlist object that stored a huge amount of memory. If we then wrote x = 1 the list\r\nobject that x used to refer to would become garbage (because x no longers\r\nrefers to this object and there are no other names that we can use to reach\r\nthis object). It is memory that is unreachableIf we had instead written\r\n\r\nx = [i for i in range(1_000_000_000)]\r\ny = [0, 1, 2, x]\r\nx = 1\r\n\r\nNow, the object x formerly referred to can be referred to by y[3] so we can\r\nreach it from some name and therefore it is NOT GARBAGE.\r\n\r\nGarbage collection is a way for the computer to find/reclaim memory that is\r\ngarbage. Typically Python allocates memory for objects until it finds it has no\r\nmore memory to allocate; then it does garbage collection to try to find more.\r\n\r\nIf it succeeds this process continues until it runs out of memory again, and\r\nthen repeats. If it ever cannot find enough free memory to allocate an object,\r\neven after garbage collection (as in the first example) it raises an exception.\r\nFinally, there are ways to tell Python how much of the computer's memory it can\r\nuse to allocate the objects it constructs. You will learn more about garbage\r\nand garbage collection in ICS-45C (using C++, a language that does not have\r\nautomatic garbage collection) and ICS-46.\r\n\r\nBetter terms would be recyclables (not garbage) and recycling (not garbage\r\ncollection), because memory is never thrown away, but is continually recycled\r\nwhen it can be reused.\r\n\r\n----------\r\n\r\nimport random,gc\r\nfrom stopwatch import Stopwatch\r\n\r\n#setup\r\nalist = [i for i in range(1_000_000)]\r\nrandom.shuffle(alist)\r\ns = Stopwatch()\r\ngc.disable()\r\n\r\n#timing\r\ns.start()\r\nalist.sort()\r\ns.stop()\r\n\r\ngc.enable()\r\n\r\n#results\r\nprint('Time =',s.read())\r\n\r\nWe would like to develop a Performance tool that minimizes what we have to do\r\nto time the code: a tool that also gives us interesting information about \r\nmultiple timings. The tool I developed for this lecture is based on the\r\ntimeit.py module supplied with Python (see secton 27.5 in the standard library\r\ndocumentation). First, we show an example use of the tool, then its code. Here\r\nis the entire script that uses the tool.\r\n\r\nalist = [i for i in range(100_000)]\r\np = Performance(lambda:alist.sort(), lambda:random.shuffle(alist),100,'Sorting')\r\np.evaluate()\r\np.analyze()\r\n\r\nIt first statement creates a list of 100,000 numbers. Then it constructs a\r\nPerformance object with 4 arguments\r\n\r\n(1) A parameterless lambda of the code to execute and time\r\n(2) A parameterless lambda of the setup code to execute (but not time) before\r\n       the lambda in part (1) is called\r\n(3) The number of times to measure the code's execution: how many times to do\r\n       step 2 followed by timing step 1\r\n(4) A short title (printed by the analyze method, which prints the analysis)\r\n\r\nThe actual __init__ function for Performance looks like;\r\n\r\ndef __init__(self,code,setup=lambda:None,times_to_measure=5,title='Generic'):\r\n\r\nSo, in the above call we are timing a call to alist.sort(), which mutates the\r\nlist; before timing each call it sets up (not part of the timing) with a call\r\nto random.shuffle(alist), which mutates the list; it will do 100 timings (of\r\nthe setup -untimed- followed by code -timed); the title when information is\r\nprinted is \"Sorting\".\r\n\r\nCalling p.evaluate() does all the timings and collects the information. It\r\nreturns a 2-list: a 3-tuple of the (minimum time, average time, maximum time),\r\nfollowed by a list of all the (100 in this case) timings. It also saves this\r\ninformation as part of the state of the Performance object, which is used for\r\nanalysis, if we call the analyze function.\r\n\r\nCalling p.analyze() prints the following result on my computer. It consists of\r\nthe title; the number of timings; the average time, the minimum time, the\r\nmaximum time, and the span (a simple approximation to clustering: (max-min)/avg;\r\nand a histogram of the timings (how many fall in the range .0404-.0406 bin,\r\n.0406-.0408, bin, etc.) with an 'A' at the top of the stars indicating the bin\r\nfor the average time. Notice that although the span says the range of values\r\nwas 5.9% of the average, we can see that most of the timings are clustered very\r\nclose to the average (which itself is near the minimum time), although there\r\nare a few timings that are \"much bigger\": .043 vs. average of .041.\r\n\r\nSorting\r\nAnalysis of 100 timings\r\navg = 0.041   min = 0.040  max = 0.043  span = 5.9%\r\n\r\n   Time Ranges    \r\n4.04e-02<>4.06e-02[ 58.4%]|**************************************************\r\n4.06e-02<>4.08e-02[ 20.8%]|*****************A\r\n4.08e-02<>4.11e-02[  6.9%]|*****\r\n4.11e-02<>4.13e-02[  5.0%]|****\r\n4.13e-02<>4.16e-02[  5.0%]|****\r\n4.16e-02<>4.18e-02[  0.0%]|\r\n4.18e-02<>4.20e-02[  1.0%]|\r\n4.20e-02<>4.23e-02[  1.0%]|\r\n4.23e-02<>4.25e-02[  0.0%]|\r\n4.25e-02<>4.28e-02[  1.0%]|\r\n4.28e-02<>4.30e-02[  1.0%]|\r\n\r\nUsing this tool, I ran a series of sorting experiments doubling the length of\r\nthe list to sort each time. Here was the script:\r\n\r\nfrom goody import irange\r\nimport random\r\nfrom performance import Performance\r\n\r\nfor i in irange(0,9)  :\r\n    size = 100_000 * 2**i\r\n    alist = [i for i in range(size)]\r\n    p = Performance(lambda : alist.sort(), lambda : random.shuffle(alist),10,'\\n\\nSorting '+str(size)+' values')\r\n    p.evaluate()\r\n    p.analyze()\r\n\r\nHere is the raw data produced by running this code using Python 3.7 on my new\r\n(in 2019) computer.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nSorting 100000 values\r\nAnalysis of 10 timings\r\navg = 0.01542   min = 0.01505  max = 0.01651  span = 9.5%\r\n\r\n   Time Ranges    \r\n1.50e-02<>1.52e-02[ 40.0%]|**************************************************\r\n1.52e-02<>1.53e-02[ 10.0%]|************\r\n1.53e-02<>1.55e-02[ 30.0%]|*************************************A\r\n1.55e-02<>1.56e-02[  0.0%]|\r\n1.56e-02<>1.58e-02[ 10.0%]|************\r\n1.58e-02<>1.59e-02[  0.0%]|\r\n1.59e-02<>1.61e-02[  0.0%]|\r\n1.61e-02<>1.62e-02[  0.0%]|\r\n1.62e-02<>1.64e-02[  0.0%]|\r\n1.64e-02<>1.65e-02[  0.0%]|\r\n1.65e-02<>1.67e-02[ 10.0%]|************\r\n\r\n\r\nSorting 200000 values\r\nAnalysis of 10 timings\r\navg = 0.03662   min = 0.03462  max = 0.04495  span = 28.2%\r\n\r\n   Time Ranges    \r\n3.46e-02<>3.57e-02[ 60.0%]|**************************************************\r\n3.57e-02<>3.67e-02[ 20.0%]|****************A\r\n3.67e-02<>3.77e-02[  0.0%]|\r\n3.77e-02<>3.88e-02[ 10.0%]|********\r\n3.88e-02<>3.98e-02[  0.0%]|\r\n3.98e-02<>4.08e-02[  0.0%]|\r\n4.08e-02<>4.19e-02[  0.0%]|\r\n4.19e-02<>4.29e-02[  0.0%]|\r\n4.29e-02<>4.39e-02[  0.0%]|\r\n4.39e-02<>4.50e-02[  0.0%]|\r\n4.50e-02<>4.60e-02[ 10.0%]|********\r\n\r\n\r\nSorting 400000 values\r\nAnalysis of 10 timings\r\navg = 0.09267   min = 0.08984  max = 0.09464  span = 5.2%\r\n\r\n   Time Ranges    \r\n8.98e-02<>9.03e-02[ 10.0%]|****************\r\n9.03e-02<>9.08e-02[  0.0%]|\r\n9.08e-02<>9.13e-02[ 10.0%]|****************\r\n9.13e-02<>9.18e-02[  0.0%]|\r\n9.18e-02<>9.22e-02[  0.0%]|\r\n9.22e-02<>9.27e-02[ 30.0%]|**************************************************A\r\n9.27e-02<>9.32e-02[ 10.0%]|****************\r\n9.32e-02<>9.37e-02[ 10.0%]|****************\r\n9.37e-02<>9.42e-02[ 20.0%]|*********************************\r\n9.42e-02<>9.46e-02[  0.0%]|\r\n9.46e-02<>9.51e-02[ 10.0%]|****************\r\n\r\n\r\nSorting 800000 values\r\nAnalysis of 10 timings\r\navg = 0.21788   min = 0.21384  max = 0.22209  span = 3.8%\r\n\r\n   Time Ranges    \r\n2.14e-01<>2.15e-01[ 40.0%]|**************************************************\r\n2.15e-01<>2.15e-01[  0.0%]|\r\n2.15e-01<>2.16e-01[  0.0%]|\r\n2.16e-01<>2.17e-01[  0.0%]|\r\n2.17e-01<>2.18e-01[ 10.0%]|************A\r\n2.18e-01<>2.19e-01[  0.0%]|\r\n2.19e-01<>2.20e-01[ 10.0%]|************\r\n2.20e-01<>2.20e-01[ 10.0%]|************\r\n2.20e-01<>2.21e-01[ 10.0%]|************\r\n2.21e-01<>2.22e-01[ 10.0%]|************\r\n2.22e-01<>2.23e-01[ 10.0%]|************\r\n\r\n\r\nSorting 1600000 values\r\nAnalysis of 10 timings\r\navg = 0.48776   min = 0.48294  max = 0.49364  span = 2.2%\r\n\r\n   Time Ranges    \r\n4.83e-01<>4.84e-01[ 10.0%]|*************************\r\n4.84e-01<>4.85e-01[  0.0%]|\r\n4.85e-01<>4.86e-01[ 20.0%]|**************************************************\r\n4.86e-01<>4.87e-01[ 20.0%]|**************************************************\r\n4.87e-01<>4.88e-01[ 10.0%]|*************************A\r\n4.88e-01<>4.89e-01[ 10.0%]|*************************\r\n4.89e-01<>4.90e-01[ 10.0%]|*************************\r\n4.90e-01<>4.91e-01[ 10.0%]|*************************\r\n4.91e-01<>4.93e-01[  0.0%]|\r\n4.93e-01<>4.94e-01[  0.0%]|\r\n4.94e-01<>4.95e-01[ 10.0%]|*************************\r\n\r\n\r\nSorting 3200000 values\r\nAnalysis of 10 timings\r\navg = 1.08046   min = 1.07263  max = 1.09409  span = 2.0%\r\n\r\n   Time Ranges    \r\n1.07e+00<>1.07e+00[ 10.0%]|****************\r\n1.07e+00<>1.08e+00[ 20.0%]|*********************************\r\n1.08e+00<>1.08e+00[ 30.0%]|**************************************************\r\n1.08e+00<>1.08e+00[ 10.0%]|****************A\r\n1.08e+00<>1.08e+00[ 10.0%]|****************\r\n1.08e+00<>1.09e+00[  0.0%]|\r\n1.09e+00<>1.09e+00[  0.0%]|\r\n1.09e+00<>1.09e+00[  0.0%]|\r\n1.09e+00<>1.09e+00[ 10.0%]|****************\r\n1.09e+00<>1.09e+00[  0.0%]|\r\n1.09e+00<>1.10e+00[ 10.0%]|****************\r\n\r\n\r\nSorting 6400000 values\r\nAnalysis of 10 timings\r\navg = 2.38002   min = 2.35988  max = 2.41731  span = 2.4%\r\n\r\n   Time Ranges    \r\n2.36e+00<>2.37e+00[ 20.0%]|**************************************************\r\n2.37e+00<>2.37e+00[ 20.0%]|**************************************************\r\n2.37e+00<>2.38e+00[  0.0%]|\r\n2.38e+00<>2.38e+00[ 20.0%]|**************************************************A\r\n2.38e+00<>2.39e+00[ 20.0%]|**************************************************\r\n2.39e+00<>2.39e+00[  0.0%]|\r\n2.39e+00<>2.40e+00[ 10.0%]|*************************\r\n2.40e+00<>2.41e+00[  0.0%]|\r\n2.41e+00<>2.41e+00[  0.0%]|\r\n2.41e+00<>2.42e+00[  0.0%]|\r\n2.42e+00<>2.42e+00[ 10.0%]|*************************\r\n\r\n\r\nSorting 12800000 values\r\nAnalysis of 10 timings\r\navg = 5.46846   min = 5.19780  max = 5.88381  span = 12.5%\r\n\r\n   Time Ranges    \r\n5.20e+00<>5.27e+00[ 20.0%]|*********************************\r\n5.27e+00<>5.34e+00[  0.0%]|\r\n5.34e+00<>5.40e+00[ 30.0%]|**************************************************\r\n5.40e+00<>5.47e+00[ 10.0%]|****************A\r\n5.47e+00<>5.54e+00[ 10.0%]|****************\r\n5.54e+00<>5.61e+00[ 10.0%]|****************\r\n5.61e+00<>5.68e+00[  0.0%]|\r\n5.68e+00<>5.75e+00[  0.0%]|\r\n5.75e+00<>5.82e+00[  0.0%]|\r\n5.82e+00<>5.88e+00[ 10.0%]|****************\r\n5.88e+00<>5.95e+00[ 10.0%]|****************\r\n\r\n\r\nSorting 25600000 values\r\nAnalysis of 10 timings\r\navg = 12.00993   min = 11.52789  max = 12.52565  span = 8.3%\r\n\r\n   Time Ranges    \r\n1.15e+01<>1.16e+01[ 10.0%]|****************\r\n1.16e+01<>1.17e+01[  0.0%]|\r\n1.17e+01<>1.18e+01[ 10.0%]|****************\r\n1.18e+01<>1.19e+01[ 30.0%]|**************************************************\r\n1.19e+01<>1.20e+01[ 10.0%]|****************A\r\n1.20e+01<>1.21e+01[ 10.0%]|****************\r\n1.21e+01<>1.22e+01[ 10.0%]|****************\r\n1.22e+01<>1.23e+01[  0.0%]|\r\n1.23e+01<>1.24e+01[  0.0%]|\r\n1.24e+01<>1.25e+01[ 10.0%]|****************\r\n1.25e+01<>1.26e+01[ 10.0%]|****************\r\n\r\n\r\nSorting 51200000 values\r\nAnalysis of 10 timings\r\navg = 26.22217   min = 25.98514  max = 26.48482  span = 1.9%\r\n\r\n   Time Ranges    \r\n2.60e+01<>2.60e+01[ 40.0%]|**************************************************\r\n2.60e+01<>2.61e+01[ 10.0%]|************\r\n2.61e+01<>2.61e+01[  0.0%]|\r\n2.61e+01<>2.62e+01[  0.0%]|\r\n2.62e+01<>2.62e+01[  0.0%]|A\r\n2.62e+01<>2.63e+01[ 10.0%]|************\r\n2.63e+01<>2.63e+01[  0.0%]|\r\n2.63e+01<>2.64e+01[  0.0%]|\r\n2.64e+01<>2.64e+01[  0.0%]|\r\n2.64e+01<>2.65e+01[ 30.0%]|*************************************\r\n2.65e+01<>2.65e+01[ 10.0%]|************\r\n\r\n------------------------------------------------------------------------------\r\n\r\nWe can summarize this data as follows:\r\n\r\n        N  |   Time | Ratio | Predicted | %Error\r\n-----------+--------+-------+-----------+--------\r\n   100,000 |  0.015 |       |   0.025\t|    64\r\n   200,000 |  0.037 |  2.5  |\t0.052\t|    41\r\n   400,000 |  0.093 |  2.5  |\t0.110\t|    18\r\n   800,000 |  0.218 |  2.3  |\t0.232\t|     6\r\n 1,600,000 |  0.488 |  2.2  |\t0.488\t|     0  (predictions based on this run)\r\n 3,200,000 |  1.080 |  2.3  |\t1.023\t|     5\r\n 6,400,000 |  2.380 |  2.2  |\t2.141\t|    10 \r\n12,800,000 |  5.468 |  2.3  |\t4.472   |    18\r\n25,600,000 | 12.010 |  2.2  |   9.323   |    22\r\n51,200,000 | 26.222 |  2.2  |  19.405   |    26\r\n\r\n----------\r\nFor comparison, here is the same summary information from my previous computer\r\n(new in 2012) running an earlier version of Python (I did not record which).\r\nNote that the times are about twice as long, but the ratios are about the same.\r\nJust what we would expect from a slower technology running an algorithm in a\r\ncomplexity class that hasn't changed.\r\n\r\n        N  |   Time | Ratio | Predicted | %Error\r\n-----------+--------+-------+-----------+--------\r\n   100,000 |  0.037 |       |   0.048\t|    29\r\n   200,000 |  0.080 |  2.2  |\t0.101\t|    26\r\n   400,000 |  0.178 |  2.2  |\t0.213\t|    20\r\n   800,000 |  0.416 |  2.3  |\t0.450\t|     8\r\n 1,600,000 |  0.945 |  2.3  |\t0.945\t|     0  (predictions based on this run)\r\n 3,200,000 |  2.145 |  2.3  |\t1.982\t|     8\r\n 6,400,000 |  4.853 |  2.3  |\t4.150\t|    15 \r\n12,800,000 | 10.925 |  2.3  |\t8.660   |    21\r\n25,600,000 | 24.578 |  2.2  |  18.055   |    27\r\n51,200,000 | 56.953 |  2.3  |  37.576   |    34\r\n----------\r\n\r\nI sorted lists from 100 thousand to 51.2 million values, doubling the length\r\nevery time, whoses sizes are listed in the first column. The average times\r\n(from 10 experiments each) are listed in the second column. I computed the\r\nratio of T(2N)/T(N) for each N (after the first) and the ratio was always bigger\r\nthan 2 by a small amount tht generally gets smaller. This indicates that the\r\ncomplexity class is slightly higher than O(N). As we discussed, it is actually\r\nO(N Log N), and this is the signagure for O(N Log N): slightly bigger than 2.\r\n\r\nUsing O(N Log N) as the complexity class and using N = 1,600,000 I solved for\r\nthe constant in the forumla T(N) = c * N Log N and got 1.48E-08 (for logarithms\r\nbase 2: I have to choose a base, but I can use any), so we can approximate the\r\ntime taken to sort as T(N) = 1.48*10^-8 * N Log N. Given this approximation,\r\nthe next columns shows the times predicted for that size N, and the percent\r\nerror between the predicted and real time (which grows as N gets farther away\r\n-in both directions- from 1,600,000). The errors are not bad: even a 100% error\r\nmeans that we have still predicted the time within a factor of of 2, and here\r\nthe worst error was about 64%, when N was smallest.\r\n\r\nHere is the actual code for the Performance class. You will see that although\r\nthe constructor specifies times_to_measure, we can omit/override this value\r\nwhen calling evaluate() by passing the number of times to test the code.\r\nLikewise with the title and the analyze method (which also allows specification\r\nof the number of bins to use in the histogram of times created).\r\n\r\nimport gc\r\nfrom stopwatch import Stopwatch\r\nfrom goody import frange\r\n\r\n\r\nclass Performance:\r\n    def __init__(self,code,setup=lambda:None,times_to_measure=5,title='Generic'):\r\n        self._code             = code\r\n        self._setup            = setup\r\n        self._times            = times_to_measure\r\n        self._evaluate_results = None\r\n        self._title            = title\r\n    \r\n    def evaluate(self,times=None):\r\n        results = []\r\n        s = Stopwatch()\r\n        times = times if times != None else self._times\r\n        for _ in range(times):\r\n            self._setup()\r\n            s.reset()\r\n            gc.disable()\r\n            s.start()\r\n            self._code()\r\n            s.stop()\r\n            gc.enable()\r\n            results.append(s.read())\r\n        self._evaluate_results = [(min(results),sum(results)/times,max(results))] + [results]\r\n        return self._evaluate_results\r\n    \r\n    def analyze(self,bins=10,title=None):\r\n        if self._evaluate_results == None:\r\n            print('No results from calling evaluate() to analyze')\r\n            return\r\n        \r\n        def print_histogram(bins_dict):\r\n            count = sum(bins_dict.values())\r\n            max_for_scale = max(bins_dict.values())\r\n                \r\n            for k,v in sorted(bins_dict.items()):\r\n                pc = int(v/max_for_scale*50)\r\n                extra = 'A' if k[0] <= avg < k[1] else ''\r\n                print('{bl:.2e}<>{bh:.2e}[{count: 5.1f}%]|{stars}'.format(bl=k[0],bh=k[1],count=v/count*100,stars='*'*pc+extra))\r\n\r\n        (mini,avg,maxi),times = self._evaluate_results\r\n        incr = (maxi-mini)/bins\r\n        hist = {(f,f+incr):0 for f in frange(mini,maxi,incr)}\r\n        for t in times:\r\n            for (min_t,max_t) in hist:\r\n                if min_t<= t < max_t:\r\n                    hist[(min_t,max_t)] += 1\r\n\r\n        print(title if title != None else self._title)\r\n        print('Analysis of',len(times),'timings')\r\n        print('avg = {avg:.3f}   min = {min:.3f}  max = {max:.3f}  span = {span:.1f}%'.\r\n                format(min=mini,avg=avg,max=maxi,span=(maxi-mini)/avg*100))\r\n        print('\\n   Time Ranges    ')   \r\n        print_histogram(hist)    \r\n\r\nThis class is in the performance.py module in the empirical project folder: a\r\ndownload that accompanies this lecture. So, you can run your own experiments by\r\nimporting this module wherever is code that you want to time.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHeights of Random Binary Search Trees: A Dynamic Analysis\r\n\r\nLet's empirically examine the heights of binary search trees constructed at\r\nrandom: values are added into binary search trees in random orders. We know that\r\nthe maximum/worst-case height for a binary search tree with size N is N-1; the\r\nminimum/best case is a height of about (Log2 N)-1. We will write code below to\r\nperform a specified number of experiments, each building a random binary search\r\ntree of a specified size: for each experiment we will collect the height of the\r\ntree produced and ultimately plot a histogram of all the heights.\r\n\r\nTo run these experiments, we need to access the TN class and the height, add,\r\nand add_all functions, which are all written in the tree project folder\r\n(examined when we discussed trees). I copied that code into the randomtrees.py\r\nmodule, but we could have imported it.\r\n\r\nHere is the code to prompt the user for the experiment and compute a histogram\r\nof all the different tree heights.\r\n\r\nimport prompt,random,math\r\nfrom goody import irange\r\nfrom collections import defaultdict\r\n\r\nexperiments = prompt.for_int('Enter # of experiments to perform')\r\nsize        = prompt.for_int('Enter size of tree for each experiment')\r\n\r\nhist  = defaultdict(int)\r\nalist = [i for i in range(size)]\r\n\r\nfor exp in range(experiments):\r\n    if exp % (experiments//100) == 0:\r\n        print('Progress: {p:d}%'.format(p =int(exp/(experiments//100))))\r\n    random.shuffle(alist)\r\n    hist[ height(add_all(None,alist)) ] += 1\r\n\r\nprint_histogram('Binary Search Trees of size '+str(size),hist)\r\nprint('\\nminimum possible height =',math.ceil(math.log2(size)-1),'  maximum possible height =',size-1)\r\n\r\nFor 10,000 experiments run on binary search trees of size 1,000, this code\r\nprinted the following results (after computing for a few minutes). For a 1,000\r\nnode tree, the minimum possible height is 9 and the maxium possible height is\r\n999. The heights recorded here are all between about 1.5 times the minimum and\r\nabout 3 times the minimum (which is true for much larger random binary search\r\ntrees as well; see the next analysis).\r\n\r\nBinary Search Trees of size 1000\r\nAnalysis of 10,000 experiments\r\n\r\navg = 21.0  min = 16  max = 31\r\n\r\n  16[  0.0%]|\r\n  17[  1.0%]|**\r\n  18[  5.4%]|************\r\n  19[ 14.3%]|*********************************\r\n  20[ 21.2%]|*************************************************\r\n  21[ 21.5%]|**************************************************A\r\n  22[ 16.1%]|*************************************\r\n  23[  9.8%]|**********************\r\n  24[  5.7%]|*************\r\n  25[  2.9%]|******\r\n  26[  1.3%]|**\r\n  27[  0.5%]|*\r\n  28[  0.1%]|\r\n  29[  0.1%]|\r\n  30[  0.0%]|\r\n  31[  0.0%]|\r\n\r\nminimum possible height = 9   maximum possible height = 999\r\n\r\nNote because the 16, 30, 31 bins are printed, they were not 0 (there were\r\nrandomly constructed trees with those heights), although there were so few trees\r\nof these heights that their percentages (to one decimal place) were 0. There\r\nwere no trees at heights less than 16 or greater than 31 (or they too would have\r\nbeen printed).\r\n\r\nThe print_histogram method called in the code above is shown below\r\n\r\ndef print_histogram(title,bins_dict):\r\n    print(title)\r\n    \r\n    count = sum(bins_dict.values())\r\n    min_bin = min(bins_dict.keys())\r\n    max_bin = max(bins_dict.keys())\r\n    max_for_scale = max(bins_dict.values())\r\n    print('Analysis of {count:,} experiments'.format(count=count))\r\n    \r\n    w_sum = 0\r\n    for i in bins_dict:\r\n        w_sum += i*bins_dict[i]\r\n    avg = w_sum/count\r\n    \r\n    print('\\navg = {avg:.1f}  min = {min}  max = {max}\\n'.format(avg=avg,min=min_bin,max=max_bin))\r\n    for i in irange(min_bin,max_bin):\r\n        pc = int(bins_dict[i]/max_for_scale*50)\r\n        extra = 'A' if int(avg+.5) == i else ''\r\n        print('{bin:4}[{count: 5.1f}%]|{stars}'.format(bin=i,count=bins_dict[i]/count*100,stars='*'*pc+extra))\r\n\r\nThe results below are for 10,000 experiments run on binary search trees of size\r\n100,000. This code took about 5 hours to run. Notice too that almost all the\r\ntrees are between 2 and 3 times the minimum possible tree; none are near the\r\nmaximum of 99,9999.\r\n\r\nBinary Search Trees of size 100000\r\nAnalysis of 10,000 experiments\r\n\r\navg = 39.6  min = 34  max = 53\r\n\r\n  34[  0.1%]|\r\n  35[  0.7%]|*\r\n  36[  4.2%]|**********\r\n  37[ 11.1%]|****************************\r\n  38[ 16.9%]|******************************************\r\n  39[ 19.7%]|**************************************************\r\n  40[ 17.1%]|*******************************************A\r\n  41[ 12.5%]|*******************************\r\n  42[  8.0%]|********************\r\n  43[  4.9%]|************\r\n  44[  2.4%]|******\r\n  45[  1.4%]|***\r\n  46[  0.7%]|*\r\n  47[  0.3%]|\r\n  48[  0.2%]|\r\n  49[  0.1%]|\r\n  50[  0.1%]|\r\n  51[  0.0%]|\r\n  52[  0.0%]|\r\n  53[  0.0%]|\r\n\r\nminimum possible height = 16   maximum possible height = 99999\r\n\r\nAll this code is in the randomtrees.py module in the empirical project folder: a\r\ndownload that accompanies this lecture. So, you can run your own experiments.\r\n\r\nIf you know a lot about math and probability, you compute the height of a\r\nrandom binary search tree, and it will closely agree with this empirical result.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nProfiling Programs: Performance Summary of all Functions at the Program Level\r\n\r\nProfilers are modules that run other modules and collect information about\r\ntheir execution: typically information about how many times their functions are\r\ncalled, and the time spent inside each function: both the individual time and\r\nthe cumulative time (which also includes the amount of time spent inside the\r\nfunctions they call). For example, in\r\n\r\n  def f(...):\r\n    ...code 1, no function calls\r\n    g(...)\r\n    ...code 2, no function calls\r\n\r\nThe INDIVIDUAL time for f includes the amount of time spent in code 1 and\r\ncode 2; the CUMULATIVE time for f also includes the amount of time spent in\r\nfunction g; of course g will have its own individual and cumulative times too.\r\n\r\nSuch information is useful for focusing our attention on the functions taking a\r\nsignificant amount of time, so that we can optimize/rewrite them in the hope\r\nof significantly improving the speed of our entire program (and not wasting our\r\ntime optimizing/rewriting functions that do not significantly affect the running\r\ntime of the entire program).\r\n\r\nAlthough the programs that we wrote this quarter were sophisticated, they ran\r\non only a small amount of data and executed quickly. The ideal program to\r\nprofile is one that uses a lot of data and takes a long time to run. In my\r\nICS-31 class, students wrote a program that performs \"shotgun assembly\" on DNA\r\nstrands. The program isn't huge (only about 50 lines of code), but in the\r\nlargest problem I have students solve, the input is 1,824 DNA fragments that\r\nare each 50-100 bases long, so the input is hundreds of thousands of bases. My\r\nsolution program took almost about 1.5 minutes to run, before printing the\r\nresulting 10,000 base DNA strand that it built from all these overlapping\r\nDNA fragments. \r\n\r\nThis program is a module that defines functions followed by a script that calls\r\nthese functions. To run it using the profiler module, we need to move the\r\nstatements in the script into their own function (which I generically called\r\nperform_task). Then, we add the following import at the top, and add the\r\nfollowing function call at the bottom to profile the function/program.\r\n\r\nimport cProfile\r\n\r\n...all the functions in the module, plus perform_task (the script in a function)\r\n\r\ncProfile.run('perform_task()')\r\n\r\nWhen run, the DNA assembly program performs its job and produces the correct\r\noutput (and in the process, prints information into the console). Then the\r\nprofiler prints the following information in the console, which shows in the top\r\nline that it profiled 234 million function calls over 107 seconds: so overall\r\nPython called 2.19 million functions/second.\r\n\r\nThe data shown here (and all the data shown below later) is always sorted by\r\none of the three headings. This data is sorted by the ASCII values in the\r\nstrings produced by filename:lineno (function). The columns (no matter how\r\nthey are sorted) represent\r\n\r\nncalls : the number of times the function was called\r\n\r\ntottime: the total time spent in just that function, NOT INCLUDING the time\r\n           spent in the other functions that it calls (although some built-in\r\n           functions it calls cannot be timed separately)\r\n         This as a bad name; I'd prefer individual time; but it is total time.\r\n\r\ncumtime: the cumulative time spent in that function, INCLUDING the time spent\r\n           in the other functions that it calls\r\n\r\n  So, as illustrated above, if function f performed some computation and called\r\n  function g, the tottime for f would NOT include the time spent in g, but the\r\n  cumtime would include this time. So it should always be the case that tottime\r\n  <= cummtime.\r\n\r\nExamine the information shown below, but we will look at parts of it more\r\nselectively soon. Note that the sum of all the tottime data is the running time,\r\nbut many cumtime data have the same value as the total running time (or close):\r\nthe <module>, exec, perform_task, and assemble all show a cumulative time equal\r\nto the running time, because cumtime for them is counted in the functions they\r\ncall.\r\n\r\n         233,525,220 function calls in 106.705 seconds\r\n\r\n   Ordered by: standard name\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000  106.705  106.705 <string>:1(<module>)\r\n        1    0.000    0.000    0.000    0.000 codecs.py:164(__init__)\r\n        1    0.000    0.000    0.000    0.000 codecs.py:238(__init__)\r\n        6    0.000    0.000    0.000    0.000 cp1252.py:18(encode)\r\n       14    0.000    0.000    0.000    0.000 cp1252.py:22(decode)\r\n  4671030    3.541    0.000    4.028    0.000 goody.py:17(irange)\r\n     3627    1.311    0.000    1.768    0.000 listlib.py:16(remove)\r\n        2    0.000    0.000    0.000    0.000 locale.py:555(getpreferredencoding)\r\n  4671030   39.505    0.000  103.272    0.000 profilednamaker.py:10(max_overlap)\r\n        1    0.000    0.000    0.001    0.001 profilednamaker.py:19(read_fragments)\r\n        1    0.001    0.001    0.001    0.001 profilednamaker.py:20(<listcomp>)\r\n     1814    1.650    0.001  104.921    0.058 profilednamaker.py:23(choose)\r\n        1    0.010    0.010  106.699  106.699 profilednamaker.py:33(assemble)\r\n  1658018    0.152    0.000    0.152    0.000 profilednamaker.py:42(<lambda>)\r\n  1656204    0.155    0.000    0.155    0.000 profilednamaker.py:43(<lambda>)\r\n        1    0.004    0.004  106.705  106.705 profilednamaker.py:49(perform_task)\r\n        7    0.000    0.000    0.000    0.000 profilednamaker.py:55(<lambda>)\r\n        7    0.000    0.000    0.000    0.000 profilednamaker.py:56(<lambda>)\r\n194186759   58.607    0.000   58.607    0.000 profilednamaker.py:6(overlaps)\r\n        2    0.000    0.000    0.000    0.000 {built-in method _getdefaultlocale}\r\n       14    0.000    0.000    0.000    0.000 {built-in method charmap_decode}\r\n        6    0.000    0.000    0.000    0.000 {built-in method charmap_encode}\r\n        1    0.000    0.000  106.705  106.705 {built-in method exec}\r\n 22001998    1.140    0.000    1.140    0.000 {built-in method len}\r\n  4671030    0.630    0.000    0.630    0.000 {built-in method min}\r\n        2    0.000    0.000    0.000    0.000 {built-in method open}\r\n        3    0.000    0.000    0.000    0.000 {built-in method print}\r\n     1813    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\r\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\r\n     1824    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\r\n        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\r\n\r\nBy calling cProfile.run('perform_task()','profile') we direct the run function\r\nto not print its results on the console, but instead to write them into the file\r\nnamed 'profile' (or any other file name we want to use). Then we can use the\r\npstats module, described below, to read this data file and print it in\r\nsimplified (more easy to read and use) forms.\r\n\r\nHere is a script that uses pstats to show just the top 10 lines of the data\r\nabove, when sorted by ncalls, cumtime, and tottime.\r\n\r\nimport pstats\r\n# Get data from stored file named 'profile'\r\np = pstats.Stats('profile')\r\n\r\n# uncomment the line below to print all the the information above\r\n#   strip_dirs removes directory information, but leaves file names\r\n#   no argument to print_stats() prints all statistics\r\n#p.strip_dirs().sort_stats(-1).print_stats()\r\n\r\n# An argument to print_stats() prints that many lines, in decreasing order\r\np.strip_dirs().sort_stats('calls').print_stats(10)\r\np.strip_dirs().sort_stats('cumulative').print_stats(10)\r\np.strip_dirs().sort_stats('time').print_stats(10)\r\n\r\nThe three results this script prints are \r\n\r\n         233,5252,20 function calls in 106.705 seconds\r\n\r\n   Ordered by: call count\r\n   List reduced from 31 to 10 due to restriction <10>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n194186759   58.607    0.000   58.607    0.000 profilednamaker.py:6(overlaps)\r\n 22001998    1.140    0.000    1.140    0.000 {built-in method len}\r\n  4671030    3.541    0.000    4.028    0.000 goody.py:17(irange)\r\n  4671030    0.630    0.000    0.630    0.000 {built-in method min}\r\n  4671030   39.505    0.000  103.272    0.000 profilednamaker.py:10(max_overlap)\r\n  1658018    0.152    0.000    0.152    0.000 profilednamaker.py:42(<lambda>)\r\n  1656204    0.155    0.000    0.155    0.000 profilednamaker.py:43(<lambda>)\r\n     3627    1.311    0.000    1.768    0.000 listlib.py:16(remove)\r\n     1824    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\r\n     1814    1.650    0.001  104.921    0.058 profilednamaker.py:23(choose)\r\n\r\n\r\n         233,525,220 function calls in 106.705 seconds\r\n\r\n   Ordered by: cumulative time\r\n   List reduced from 31 to 10 due to restriction <10>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000  106.705  106.705 {built-in method exec}\r\n        1    0.000    0.000  106.705  106.705 <string>:1(<module>)\r\n        1    0.004    0.004  106.705  106.705 profilednamaker.py:49(perform_task)\r\n        1    0.010    0.010  106.699  106.699 profilednamaker.py:33(assemble)\r\n     1814    1.650    0.001  104.921    0.058 profilednamaker.py:23(choose)\r\n  4671030   39.505    0.000  103.272    0.000 profilednamaker.py:10(max_overlap)\r\n194186759   58.607    0.000   58.607    0.000 profilednamaker.py:6(overlaps)\r\n  4671030    3.541    0.000    4.028    0.000 goody.py:17(irange)\r\n     3627    1.311    0.000    1.768    0.000 listlib.py:16(remove)\r\n 22001998    1.140    0.000    1.140    0.000 {built-in method len}\r\n\r\n\r\n         233,525,220 function calls in 106.705 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 31 to 10 due to restriction <10>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n194186759   58.607    0.000   58.607    0.000 profilednamaker.py:6(overlaps)\r\n  4671030   39.505    0.000  103.272    0.000 profilednamaker.py:10(max_overlap)\r\n  4671030    3.541    0.000    4.028    0.000 goody.py:17(irange)\r\n     1814    1.650    0.001  104.921    0.058 profilednamaker.py:23(choose)\r\n     3627    1.311    0.000    1.768    0.000 listlib.py:16(remove)\r\n 22001998    1.140    0.000    1.140    0.000 {built-in method len}\r\n  4671030    0.630    0.000    0.630    0.000 {built-in method min}\r\n  1656204    0.155    0.000    0.155    0.000 profilednamaker.py:43(<lambda>)\r\n  1658018    0.152    0.000    0.152    0.000 profilednamaker.py:42(<lambda>)\r\n        1    0.010    0.010  106.699  106.699 profilednamaker.py:33(assemble)\r\n\r\nNote it says \"Ordered by: internal time\" although it is ordered by \"tottime\";\r\nthe work \"internal\" is similar to how I described \"individual\" time above.\r\n\r\nAs we can see directly from the information above, the most tottime is spent in\r\nthe overlaps function. It is very simple, so when I tried to write it another\r\nway, I couldn't get any time improvement. So then I moved on to the max_overlap\r\nfunction. The max_overlap function calls overlaps for each possible overlap\r\n(based on lengths of the stands to match), so since the ratio of calls\r\n(overlap calls/max_overlap calls) is about 42/1, the average possible strand\r\noverlap is 42. So, it is possible to infer information about the code from this\r\nempirical data.\r\n\r\nBy numcalls I realized from the place it was called (in choose) that I could\r\nsimplify max_overlaps to not compute the maximum overlap, but just find (and\r\nimmediately return) any overlap that exceeds a minimum specified in the choose\r\nmethod. By changing this code, the above profile turned into the following one.\r\n\r\n         196057553 function calls in 87.748 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 31 to 10 due to restriction <10>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n152048062   46.566    0.000   46.566    0.000 profilednamaker.py:6(overlaps)\r\n  4671030   31.241    0.000   84.226    0.000 profilednamaker.py:10(exceeds_overlap)\r\n  4671030    4.615    0.000    5.287    0.000 goody.py:17(irange)\r\n     1814    1.713    0.001   85.939    0.047 profilednamaker.py:23(choose)\r\n 26673028    1.334    0.000    1.334    0.000 {built-in method len}\r\n     3627    1.316    0.000    1.795    0.000 listlib.py:16(remove)\r\n  4671030    0.636    0.000    0.636    0.000 {built-in method min}\r\n  1656204    0.158    0.000    0.158    0.000 profilednamaker.py:43(<lambda>)\r\n  1658018    0.153    0.000    0.153    0.000 profilednamaker.py:42(<lambda>)\r\n        1    0.009    0.009   87.743   87.743 profilednamaker.py:33(assemble)\r\n\r\nI was able to decrease the run time by about 20 seconds (almost 20%). Although\r\nexceeds_overlap (I changed the name from max_overlap) is called the same number\r\nof times as before, it calls overlaps about 25% fewer times, saving 12 seconds;\r\nand because it is called fewer times, exceed_max saves another 8 seconds over\r\nmax_overlap, which together accounts for the full 20 seconds.\r\n\r\nIn a large system with thousands of functions, it is a big win to use the\r\nprofiler to focus our attention on the important functions: the ones that take\r\na significant amount of time, and therefore the ones likely to cause major\r\ndecrease in the runtime if improved. A rule of thumb is 20% of the code accounts\r\nfor 80% of the execution time (some say 10%/90%, but the idea is the same). We\r\nneed to be able to focus on which small amount of code the program spends most\r\nof its time in. In the code above, if by hard work I could make the bottom 28\r\nfunctions run instantaneously and there would be at most a 6 second (7%)\r\nspeedup: why bother? Better to have that code written as clearly as possible,\r\nsince its execution accounts for so little time.\r\n\r\nFinally, the complete specifications for cProfile and pstats are avilable in\r\nthe Python library documentation, under section 27. Debugging and Profiling,\r\nand under 27.4: The Python Profilers.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nHashing: How Sets/Dicts are Faster than Lists for operations like \"in\"\r\n\r\nWhen we examined the complexity classes of various operations/methods on the\r\nlist, set, and dict data-types/classes, we found that sets/dicts had many\r\nmethods that were O(1). We will briefly explain how this is accomplished, but a\r\nfuller discussion will have to wait until ICS-46. First we will examine how\r\nhashing works and then analyze the complexity class of operations using hashing.\r\n\r\nPython defines a hash function that takes any object as an argument and produces\r\na \"small\" integer (sometimes positive, sometimes negative) whose magnitude is\r\n32 or 64 bits (depending on which Python you use). How this value is computed\r\nwon't concern us now. But we are guaranteed (1) there is a fast way to compute\r\nit, (2) within the same program, an object with unchanged state always computes\r\nthe same result for hashing, so immutable objects always compute the same result\r\nfor their hash function. It is possible, but not likely, that two DIFFERENT\r\nobjects will hash to the same value.\r\n\r\nEach class in Python implements a __hash__ method, which is called by the hash\r\nfunction in Python's builtins module. Small integers are their own hash: hash(1)\r\nis 1; hash(1000000) is 1000000; but hash(1000000000000000000000000) is\r\n1486940387. Even small strings have interesting hash values: hash('a') is\r\n1186423063; hash('b') is 1561756564; hash('pattis') is -1650297348 (is your\r\nname hashed positive or negative)? (BUT SEE IMPORTANT NOTE BELOW).\r\n\r\n-----\r\nNote that when we define our own classes, if we want them to be used in sets or\r\nas the keys in dictionaries (things that are hashed), we must define a __hash__\r\nmethod for that class (with only self as its parameter), and this method must\r\nreturn an int. If we do not provide a __hash__ method and try to use objects\r\nfrom that class in sets or as the keys of dictionaries, Python will raise a\r\nTypeError with the message: \"unhashable type\".\r\n\r\nTechnically, classes that contain mutator methods should NOT be hashable, but\r\nit is OK to define __hash__ for a class of mutable objects: but if we do so, we\r\nmust NEVER mutate an object while it is in a set or the key of dictionary:\r\notherwise the object might be \"lost\" because it is in the wrong bucket (see\r\nbelow). If we want to mutate such an object, we should remove it from the set\r\nor dict, mutate it, and then put it back in: all these operations are O(1).\r\n\r\nAgain hashing and hash algorithms are covered in much more detail in ICS-46.\r\n-----\r\n\r\nLet's investigate how to use hashing and lists to implement pset (pseudo set)\r\nwith a quick way to add/remove values, and check if a values is in the set: all\r\nare in complexity class O(1).\r\n\r\nWe define the class pset (and its __init__) as follows. Notice the _bins is\r\na list of lists (starting with just 1 inner list, which is empty). The __str__\r\nmethod prints these bins.\r\n\r\nObjects in this class use _len to cache the number of values in their sets:\r\nincrementing it when adding values and decrementing when removing values. The\r\nlast parameter specifies the load factor threshold, which we will discuss when\r\nwe examine the add method. Notice that the first parameter, iterable, is\r\nanything we can iterate over, adding each value to the set to initialize it.\r\n\r\nclass pset:\r\n    def __init__(self,iterable=[],load_factor_threshold=1):\r\n        self._bins = [[]]\r\n        self._len  = 0      # cache, so don't have to recompute from bins\r\n        self._lft  = load_factor_threshold\r\n        for v in iterable:\r\n            self.add(v)     # See add method below, using hashing into _bins\r\n\r\n    def __str__(self):\r\n        return str(self._bins)\r\n    \r\nRecall that _bins will store a list of lists (which we call a hash table). Each\r\ninner list is called a bin. Hash tables grow as we add values to them (just how\r\nthey grow is the secret of the O(1) performance). Before discussing the add\r\nmethod, let's observe what _bins looks like as values are added.\r\n\r\nThe load factor of a hash table is the number of values in the table divided by\r\nthe number of bins (inner lists). As we add values to the bins, this number\r\nincreases. Whenever this value exceeds the load factor threshold the number of\r\nbins is doubled, and all the values that were in the original bins are put back\r\ninto the new bins (we will see why their positions often change). Such an\r\noperation is called rehashing. By increasing the number of bins, it will lower\r\nthe load factor below the threshold (by increasing its denominator). Generally,\r\nwhen we double the number of bins, the average size of a bins is cut in half.\r\n\r\nIn the example below, we will assume the load factor threshold is the default\r\nvalue, 1.\r\n\r\n0) start  : [[]]\r\n1) add 'a': [['a']]\r\n2) add 'b': [['b'], ['a']]\r\n3) add 'c': [['b'], [], [], ['a', 'c']]\r\n4) add 'd': [['b'], [], ['d'], ['a', 'c']]\r\n5) add 'e': [[], [], [], ['c'], ['b'], ['e'], ['d'], ['a']]\r\n6) add 'f': [['f'], [], [], ['c'], ['b'], ['e'], ['d'], ['a']]\r\n7) add 'g': [['f'], [], [], ['c'], ['b'], ['e'], ['d'], ['a', 'g']]\r\n8) add 'h': [['f'], [], ['h'], ['c'], ['b'], ['e'], ['d'], ['a', 'g']]\r\n\r\nRecall Load Factor = # values in the table/# of bins in the table\r\n\r\n0) At the start, 1 bin with no values so the load factor is 0.\r\n1) We add 'a' to the first bin in the _bins list; the load factor 1\r\n2) We add 'b' to the first bin in the _bins list; the load factor is 2, which\r\n      exceeds the threshold so all the values are rehashed as shown, and\r\n      the load factor is now 2/2.\r\n3) We add 'c' to a bin in the _bins list; the load factor is 3/2, which exceeds\r\n      the threshold so all the values are rehashed (notice that 'a' and\r\n      'c' are both in the same bin; this is called a collision and often\r\n      happens when there are many values in hash tables)\r\n4) We add 'd' to the third bin in the _bins list; the load factor is 4/4.\r\n5) We add 'e' to one bin in the _bins list; the load factor is 5/4, which\r\n      exceeds the threshold so all the values are rehashed (notice all the\r\n      values are in their own bins now), and the load factor is now 5/8.\r\n6) We add 'f' to the first bin in the _bins list; the load factor is 6/8.\r\n7) We add 'g' to the last bin in the _bins list; the load factor is 7/8.\r\n8) We add 'h' to the third bin in the _bins list; the load factor is 8/8.\r\n(adding yet another value will double the number of bins)\r\n\r\nSo, hashing (and rehashing) values put them in bins. Notice that some\r\nbins are empty and some store more than 1 value, but most store 1 value.\r\nThis is because the load factor threshold is about 1.\r\n\r\nNow lets look at the _bin helper method, which finds the bin for the value:\r\nif the value is in the pset, it must be in that bin (although this calculation\r\nchanges if the length of _bins changes, which is why rehashing is necessary); if\r\nthe value is to be added to a pset, that is the bin it belongs in.\r\n\r\n    def _bin_of(self,v):\r\n        return abs(hash(v))%len(self._bins)\r\n\r\nIt hashes the value, computes its absolute value, and then computes its\r\nremainder when divided by the number of bins; so the index it produces is\r\nalways between 0 and len(self._bins)-1, is always a legal index for the _bins\r\nlist. This is the bin the value belongs in WHEN THE HASH TABLE IS THAT LENGTH:\r\nif its length changes, the denominator in the calculation above also changes, so\r\nthe bin that it belongs in probably changes too. \r\n\r\nThe code for add is as follows:\r\n\r\n    def add(self,v):\r\n        index = self._bin_of(v)     # hash v and compute its bin\r\n        if v in self._bins[index]:  # No work to do: already in the pset\r\n            return                  #  bins tend to be small so \"in\" is fast\r\n        \r\n        self._len += 1              # Cache len: now containing one more value\r\n        self._bins[index].append(v) # Store it in the right bin\r\n\r\n        # if exceed LoadFactorThreshold, rehash all values in a bigger table\r\n        if self._len/len(self._bins) > self._lft:\r\n            self._rehash()\r\n\r\nIt first computes the bin in which the value v must be in (if it is in the hash\r\ntable) and then checks if it is there; if so it returns immediately because\r\npsets have just one copy of a value. Otherwise, it increments _len and appends\r\nthe value v into the bin in which it belongs. But if the newly added value\r\nmakes the load factor exceeed the threshold all the values are rehashed in the\r\nfollowing helper method. The _rehash helper method is only called from add\r\n\r\n   def _rehash(self):\r\n        old       = self._bins\r\n        #double the number of bins (to drop the load_factor_threshold)\r\n        #rehash old values: len(self._bins) has changed\r\n        self._bins = [[] for _ in range(2*len(old))]\r\n        self._len = 0\r\n        for bins in old:\r\n            for v in bins:\r\n                self.add(v)\r\n  \r\nThis method remembers the old bins, resets the _bins and _len, and then adds\r\neach value v from the old bins into the new bins; its bin number might change\r\nbecause the % function calculated in _bin. By doubling the number of bins, there\r\nwill be no calls to _rehash while all these values are added.\r\n\r\nChecking whether a value v is in a pset is simple: it just checks whether v is\r\nin the bin/list that hashing says it belongs in. What will be stored in that\r\nbin: an empty list or a small list (maybe storing v, maybe not storing it). So\r\ncalling in on it will be fast.\r\n\r\n    def __contains__(self,v):\r\n        return v in self._bins[self._bin_of(v)]\r\n\r\nLikewise, removal goes to the bin the value v would be in IF it were in the\r\npset, and if it is there it is removed and the cached length is decremented; if\r\nnot in this bin, no others need to be checked no changes are made to the pset.\r\n\r\n    def remove (self,v):\r\n        alist = self._bins[self._bin_of(v)]  # Share list in hash table\r\n        if v in alist:\r\n            alist.remove(v)\r\n        else:\r\n            raise KeyError(str(v))\r\n\r\nActually, removal can sometimes shrink the number of bins and rehash all the\r\nvalues. We leave that feature out of this discussion, but you are welcome to\r\nextend the remove code to ensure as psests get smaller, the number of bins gets\r\nsmaller too.\r\n\r\nFinally, we can show the trivial __len__ function, returning the cached values\r\n(incremented in add and decrmented in remove):\r\n\r\n    def __len__(self):\r\n        return self._len  # cached\r\n\r\nSo, why are the add, contains, and remove methods O(1)?\r\n\r\nBecause the hash function does a good job of randomizing in which bins values\r\nare stored, and the load factor is kept around 1 (meaning there are about as\r\nmany bins as values: as more and more values are added, the length of the list\r\nof bins grows), the amount of time it takes to add/examine/remove a value from a\r\nits bin in hash table (as used in pset) is constant. That is, if there are N\r\nvalues in the pset, there are at lease N bins, and the average number of values\r\nin a bin is close to 1.\r\n\r\nIt takes a constant amount of work (independent from the number of values in the\r\nhash table) to hash a value to find its bin, and since each bin has about 1\r\nvalue in it, it takes a constant amount of time to check or update that bin.\r\n\r\nNow, some bins are empty and some can have more than one value (but very few\r\nhave a lot of values). I added an analyze method to pset so that it can show\r\nstatistics about the number of bins and their lengths.\r\n\r\nIf we call the following function as experiment(1_000_000), it generates 1\r\nmillion random 26 letter strings and puts them in a hash table, whose size\r\ngrows from 1 to 2**20 (which is a bit over a million)\r\n\r\n    def build_set(n,m=26):\r\n        s = pset()\r\n        word = list('abcdefghijklmnopqrstuvwxyz'[:m])\r\n        for i in range(n):\r\n            random.shuffle(word)\r\n            s.add(''.join(word))\r\n        return s\r\n    \r\n    def experiment(n,m=26):\r\n        s = build_set(n,m)\r\n        s.analyze()\r\n \r\nWe can call this function to analyze the distribution of values in bins. Here\r\nis one result produced by calling experiment witht the argument 1 million\r\n(whose output text is reduced a bit to fit nicely on one page).\r\n\r\nbins with  0 values = 403,619 totalling       0 values; cumulative =         0\r\nbins with  1 values = 385,426 totalling 385,426 values; cumulative =   385,426\r\nbins with  2 values = 184,243 totalling 368,486 values; cumulative =   753,912\r\nbins with  3 values =  58,602 totalling 175,806 values; cumulative =   929,718\r\nbins with  4 values =  13,708 totalling  54,832 values; cumulative =   984,550\r\nbins with  5 values =   2,492 totalling  12,460 values; cumulative =   997,010\r\nbins with  6 values =     427 totalling   2,562 values; cumulative =   999,572\r\nbins with  7 values =      47 totalling     329 values; cumulative =   999,901\r\nbins with  8 values =      10 totalling      80 values; cumulative =   999,981\r\nbins with  9 values =       1 totalling       9 values; cumulative =   999,990\r\nbins with 10 values =       1 totalling      10 values; cumulative = 1,000,000\r\n\r\nAs you can see, most bins store no values! But many other bins store just a few\r\nvalues; in fact the bins storing 1-5 values account for over 99% of the values\r\nin the hash table. So for over 99% of the values in the hash table, it takes\r\nat most 5 comparisons to examine/update these bin, and 5 is a constant. If we\r\nput 2 million values into the pset, the bin profile above would be similar,\r\nagain with 99% of the values findable with at most 5 comparisons. Here is the\r\ninformation for a 2 million value pset.\r\n\r\nbins with  0 values = 807,118 totalling       0 values; cumulative =         0\r\nbins with  1 values = 771,329 totalling 771,329 values; cumulative =   771,329\r\nbins with  2 values = 368,773 totaling  737,546 values; cumulative = 1,508,875\r\nbins with  3 values = 115,937 totalling 347,811 values; cumulative = 1,856,686\r\nbins with  4 values =  27,777 totalling 111,108 values; cumulative = 1,967,794\r\nbins with  5 values =   5,245 totalling  26,225 values; cumulative = 1,994,019\r\nbins with  6 values =     851 totalling   5,106 values; cumulative = 1,999,125\r\nbins with  7 values =     104 totalling     728 values; cumulative = 1,999,853\r\nbins with  8 values =      15 totalling     120 values; cumulative = 1,999,973\r\nbins with  9 values =       3 totalling      27 values; cumulative = 2,000,000\r\n\r\nNow we can close the circle started in this lecture by using Performance to\r\nempirically analyze whether all our conjectures about the performance of hash\r\ntables are correct. We will construct psets with different numbers of values,\r\ndoubling each time. We test each pset by adding N values and then performing N\r\nlookups. If each of these operations is truly O(1) and we do N of each, the\r\ncomplexity class of doing both it O(N), so doubling N should double the time.\r\n\r\nThe data shows this behavior exactly, with much less error than our sorting\r\nanalysis.\r\n\r\n     N  |   Time  |  Ratio  |  Predicted  |  %Error\r\n--------+---------+---------+-------------+----------\r\n  1,000\t|  0.030  |         |   0.030          0\r\n  2,000 |  0.060  |    2.0  |   0.060     |    0\r\n  4,000 |  0.120  |    2.0  |   0.120     |    0\r\n  8,000 |  0.240  |    2.0  |   0.241     |    0\r\n 16,000 |  0.481  |    2.0  |   0.481     |    0 (predictions based on this run)\r\n 32,000 |  0.962  |    2.0  |   0.962     |    0\r\n 64,000 |  1.927  |    2.0  |   1.924     |    0\r\n128,000 |  3.873  |    2.0  |   3.848     |    1\r\n256,000 |  7.735  |    2.0  |   7.696     |    1\r\n\r\nAll this code is in the hashing.py module in the empirical project folder: a\r\ndownload that accompanies this lecture. So, you can run your own experiments.\r\n\r\n------------------------------\r\nIMPORTANT:\r\n\r\nAt present Python always computes exactly the same value when hashing a string\r\nwhile a program is running; but when the program stops and a new program is run,\r\nit gives a different value for the same string (but always the same one for\r\nthat run of the progma). That makes things much harder to explain, because I\r\ncannot use hashing examples and write their output here). Python's hashing\r\nfunction uses a random number in the hash function, but one that is the same\r\nfor all hashing while a program runs. This is good for exposing errors in code\r\nthat uses hashing, but not so good for being able to show examples of hashing.\r\n\r\nThis is also why different runs of exactly the same program with exactly the\r\nsame data may produce different iteration orders for set (although Python 3.7\r\nimposes extra constraints on iteration order, which may or may not go away in\r\nfuture Pythons, so don't count on them).\r\n------------------------------\r\n\r\nHere are the methods that implement iteration for psets. The order that the\r\nvalues in the pset are produced is: all those value (in order) in the list in\r\nbin 0, all those values (in order) in the list in bin 1, etc.\r\n\r\n    def __iter__(self):\r\n        for b in self._bins:\r\n            for v in b:\r\n                yield v\r\n\r\nRecall that the values moved around when rehashed. So that is why there is no\r\nsimple order in the sets/dicts we iterate over.\r\n\r\nFinally, we have discussed that set and dictionary keys cannot be mutable. Now\r\nwe can get some insight why. If we put a value in its bin, but then change its\r\nstate (mutate it), the hash function would compute a different result and\r\n_bin_of would probably want it in a different bin. And if it is in the wrong\r\nbin, looking for it, or trying to remove it, or trying to add it (with no\r\nduplicates) will not work correctly.\r\n\r\n------------------------------------------------------------------------------\r\nDATA: Sorting first (on my old computer); Hash Table Next\r\n\r\nSorting Data: actual data\r\n\r\nSorting 100000 values\r\nAnalysis of 10 timings\r\navg = 0.037   min = 0.036  max = 0.038  span = 3.8%\r\n\r\n   Time Ranges    \r\n3.63e-02<>3.64e-02[ 10.0%]|*************************\r\n3.64e-02<>3.66e-02[ 20.0%]|**************************************************\r\n3.66e-02<>3.67e-02[  0.0%]|\r\n3.67e-02<>3.68e-02[ 10.0%]|*************************\r\n3.68e-02<>3.70e-02[ 20.0%]|**************************************************A\r\n3.70e-02<>3.71e-02[ 20.0%]|**************************************************\r\n3.71e-02<>3.72e-02[  0.0%]|\r\n3.72e-02<>3.74e-02[  0.0%]|\r\n3.74e-02<>3.75e-02[ 10.0%]|*************************\r\n3.75e-02<>3.77e-02[  0.0%]|\r\n3.77e-02<>3.78e-02[ 10.0%]|*************************\r\n\r\n\r\nSorting 200000 values\r\nAnalysis of 10 timings\r\navg = 0.080   min = 0.079  max = 0.081  span = 2.3%\r\n\r\n   Time Ranges    \r\n7.94e-02<>7.96e-02[ 10.0%]|*************************\r\n7.96e-02<>7.98e-02[ 20.0%]|**************************************************\r\n7.98e-02<>8.00e-02[  0.0%]|\r\n8.00e-02<>8.02e-02[ 10.0%]|*************************\r\n8.02e-02<>8.03e-02[ 10.0%]|*************************A\r\n8.03e-02<>8.05e-02[ 10.0%]|*************************\r\n8.05e-02<>8.07e-02[  0.0%]|\r\n8.07e-02<>8.09e-02[ 20.0%]|**************************************************\r\n8.09e-02<>8.11e-02[ 10.0%]|*************************\r\n8.11e-02<>8.12e-02[  0.0%]|\r\n8.12e-02<>8.14e-02[ 10.0%]|*************************\r\n\r\n\r\nSorting 400000 values\r\nAnalysis of 10 timings\r\navg = 0.178   min = 0.175  max = 0.181  span = 3.0%\r\n\r\n   Time Ranges    \r\n1.75e-01<>1.76e-01[ 20.0%]|*********************************\r\n1.76e-01<>1.76e-01[  0.0%]|\r\n1.76e-01<>1.77e-01[  0.0%]|\r\n1.77e-01<>1.77e-01[ 10.0%]|****************\r\n1.77e-01<>1.78e-01[ 30.0%]|**************************************************\r\n1.78e-01<>1.79e-01[  0.0%]|A\r\n1.79e-01<>1.79e-01[ 10.0%]|****************\r\n1.79e-01<>1.80e-01[ 10.0%]|****************\r\n1.80e-01<>1.80e-01[ 10.0%]|****************\r\n1.80e-01<>1.81e-01[  0.0%]|\r\n1.81e-01<>1.81e-01[ 10.0%]|****************\r\n\r\n\r\nSorting 800000 values\r\nAnalysis of 10 timings\r\navg = 0.416   min = 0.406  max = 0.457  span = 12.1%\r\n\r\n   Time Ranges    \r\n4.06e-01<>4.12e-01[ 60.0%]|**************************************************\r\n4.12e-01<>4.17e-01[ 10.0%]|********A\r\n4.17e-01<>4.22e-01[ 20.0%]|****************\r\n4.22e-01<>4.27e-01[  0.0%]|\r\n4.27e-01<>4.32e-01[  0.0%]|\r\n4.32e-01<>4.37e-01[  0.0%]|\r\n4.37e-01<>4.42e-01[  0.0%]|\r\n4.42e-01<>4.47e-01[  0.0%]|\r\n4.47e-01<>4.52e-01[  0.0%]|\r\n4.52e-01<>4.57e-01[  0.0%]|\r\n4.57e-01<>4.62e-01[ 10.0%]|********\r\n\r\n\r\nSorting 1600000 values\r\nAnalysis of 10 timings\r\navg = 0.945   min = 0.940  max = 0.952  span = 1.3%\r\n\r\n   Time Ranges    \r\n9.40e-01<>9.41e-01[ 30.0%]|**************************************************\r\n9.41e-01<>9.42e-01[ 10.0%]|****************\r\n9.42e-01<>9.43e-01[  0.0%]|\r\n9.43e-01<>9.44e-01[  0.0%]|\r\n9.44e-01<>9.46e-01[ 10.0%]|****************A\r\n9.46e-01<>9.47e-01[ 10.0%]|****************\r\n9.47e-01<>9.48e-01[  0.0%]|\r\n9.48e-01<>9.49e-01[ 10.0%]|****************\r\n9.49e-01<>9.50e-01[ 20.0%]|*********************************\r\n9.50e-01<>9.52e-01[  0.0%]|\r\n9.52e-01<>9.53e-01[ 10.0%]|****************\r\n\r\n\r\nSorting 3200000 values\r\nAnalysis of 10 timings\r\navg = 2.145   min = 2.124  max = 2.177  span = 2.5%\r\n\r\n   Time Ranges    \r\n2.12e+00<>2.13e+00[ 20.0%]|*********************************\r\n2.13e+00<>2.13e+00[ 10.0%]|****************\r\n2.13e+00<>2.14e+00[  0.0%]|\r\n2.14e+00<>2.15e+00[ 30.0%]|**************************************************A\r\n2.15e+00<>2.15e+00[ 10.0%]|****************\r\n2.15e+00<>2.16e+00[ 10.0%]|****************\r\n2.16e+00<>2.16e+00[ 10.0%]|****************\r\n2.16e+00<>2.17e+00[  0.0%]|\r\n2.17e+00<>2.17e+00[  0.0%]|\r\n2.17e+00<>2.18e+00[  0.0%]|\r\n2.18e+00<>2.18e+00[ 10.0%]|****************\r\n\r\n\r\nSorting 6400000 values\r\nAnalysis of 10 timings\r\navg = 4.853   min = 4.833  max = 4.885  span = 1.1%\r\n\r\n   Time Ranges    \r\n4.83e+00<>4.84e+00[ 30.0%]|**************************************************\r\n4.84e+00<>4.84e+00[ 10.0%]|****************\r\n4.84e+00<>4.85e+00[ 10.0%]|****************\r\n4.85e+00<>4.85e+00[  0.0%]|A\r\n4.85e+00<>4.86e+00[  0.0%]|\r\n4.86e+00<>4.86e+00[ 30.0%]|**************************************************\r\n4.86e+00<>4.87e+00[  0.0%]|\r\n4.87e+00<>4.87e+00[ 10.0%]|****************\r\n4.87e+00<>4.88e+00[  0.0%]|\r\n4.88e+00<>4.88e+00[  0.0%]|\r\n4.88e+00<>4.89e+00[ 10.0%]|****************\r\n\r\n\r\nSorting 12800000 values\r\nAnalysis of 10 timings\r\navg = 10.925   min = 10.819  max = 11.348  span = 4.8%\r\n\r\n   Time Ranges    \r\n1.08e+01<>1.09e+01[ 40.0%]|**************************************************\r\n1.09e+01<>1.09e+01[ 40.0%]|**************************************************A\r\n1.09e+01<>1.10e+01[ 10.0%]|************\r\n1.10e+01<>1.10e+01[  0.0%]|\r\n1.10e+01<>1.11e+01[  0.0%]|\r\n1.11e+01<>1.11e+01[  0.0%]|\r\n1.11e+01<>1.12e+01[  0.0%]|\r\n1.12e+01<>1.12e+01[  0.0%]|\r\n1.12e+01<>1.13e+01[  0.0%]|\r\n1.13e+01<>1.13e+01[  0.0%]|\r\n1.13e+01<>1.14e+01[ 10.0%]|************\r\n\r\n\r\nSorting 25600000 values\r\nAnalysis of 10 timings\r\navg = 24.578   min = 24.388  max = 25.426  span = 4.2%\r\n\r\n   Time Ranges    \r\n2.44e+01<>2.45e+01[ 40.0%]|****************************************\r\n2.45e+01<>2.46e+01[ 50.0%]|**************************************************A\r\n2.46e+01<>2.47e+01[  0.0%]|\r\n2.47e+01<>2.48e+01[  0.0%]|\r\n2.48e+01<>2.49e+01[  0.0%]|\r\n2.49e+01<>2.50e+01[  0.0%]|\r\n2.50e+01<>2.51e+01[  0.0%]|\r\n2.51e+01<>2.52e+01[  0.0%]|\r\n2.52e+01<>2.53e+01[  0.0%]|\r\n2.53e+01<>2.54e+01[  0.0%]|\r\n2.54e+01<>2.55e+01[ 10.0%]|**********\r\n\r\n\r\nSorting 51200000 values\r\nAnalysis of 10 timings\r\navg = 56.953   min = 56.651  max = 57.477  span = 1.5%\r\n\r\n   Time Ranges    \r\n5.67e+01<>5.67e+01[ 30.0%]|**************************************************\r\n5.67e+01<>5.68e+01[ 20.0%]|*********************************\r\n5.68e+01<>5.69e+01[ 10.0%]|****************\r\n5.69e+01<>5.70e+01[ 10.0%]|****************A\r\n5.70e+01<>5.71e+01[  0.0%]|\r\n5.71e+01<>5.71e+01[  0.0%]|\r\n5.71e+01<>5.72e+01[  0.0%]|\r\n5.72e+01<>5.73e+01[ 10.0%]|****************\r\n5.73e+01<>5.74e+01[ 10.0%]|****************\r\n5.74e+01<>5.75e+01[  0.0%]|\r\n5.75e+01<>5.76e+01[ 10.0%]|****************\r\n\r\n\r\n------------------------------------------------------------------------------\r\n\r\nNote when building a hash tabler of size = N, the code looks up every value\r\nin the table, so for size = 2N it does twice as many look ups. So we should\r\nreally compute the amount of time/look up for each size, which stays relatively\r\nconstant, because both the time and N double.\r\n\r\nSets via Hash Table: Size = 1000\r\nAnalysis of 20 timings\r\navg = 0.028   min = 0.028  max = 0.030  span = 7.5%\r\n\r\n   Time Ranges    \r\n2.77e-02<>2.79e-02[ 25.0%]|*****************************************\r\n2.79e-02<>2.81e-02[ 15.0%]|*************************\r\n2.81e-02<>2.83e-02[ 30.0%]|**************************************************A\r\n2.83e-02<>2.85e-02[ 15.0%]|*************************\r\n2.85e-02<>2.87e-02[  5.0%]|********\r\n2.87e-02<>2.89e-02[  5.0%]|********\r\n2.89e-02<>2.91e-02[  0.0%]|\r\n2.91e-02<>2.93e-02[  0.0%]|\r\n2.93e-02<>2.95e-02[  0.0%]|\r\n2.95e-02<>2.98e-02[  0.0%]|\r\n2.98e-02<>3.00e-02[  5.0%]|********\r\n\r\nSets via Hash Table: Size = 2000\r\nAnalysis of 20 timings\r\navg = 0.056   min = 0.055  max = 0.057  span = 2.8%\r\n\r\n   Time Ranges    \r\n5.54e-02<>5.55e-02[ 10.0%]|*********************************\r\n5.55e-02<>5.57e-02[ 15.0%]|**************************************************\r\n5.57e-02<>5.59e-02[ 15.0%]|**************************************************\r\n5.59e-02<>5.60e-02[ 10.0%]|*********************************\r\n5.60e-02<>5.62e-02[ 15.0%]|**************************************************A\r\n5.62e-02<>5.63e-02[ 10.0%]|*********************************\r\n5.63e-02<>5.65e-02[ 10.0%]|*********************************\r\n5.65e-02<>5.66e-02[ 10.0%]|*********************************\r\n5.66e-02<>5.68e-02[  0.0%]|\r\n5.68e-02<>5.70e-02[  0.0%]|\r\n5.70e-02<>5.71e-02[  5.0%]|****************\r\n\r\nSets via Hash Table: Size = 4000\r\nAnalysis of 20 timings\r\navg = 0.117   min = 0.111  max = 0.161  span = 42.2%\r\n\r\n   Time Ranges    \r\n1.11e-01<>1.16e-01[ 80.0%]|**************************************************\r\n1.16e-01<>1.21e-01[ 10.0%]|******A\r\n1.21e-01<>1.26e-01[  0.0%]|\r\n1.26e-01<>1.31e-01[  5.0%]|***\r\n1.31e-01<>1.36e-01[  0.0%]|\r\n1.36e-01<>1.41e-01[  0.0%]|\r\n1.41e-01<>1.46e-01[  0.0%]|\r\n1.46e-01<>1.51e-01[  0.0%]|\r\n1.51e-01<>1.56e-01[  0.0%]|\r\n1.56e-01<>1.61e-01[  0.0%]|\r\n1.61e-01<>1.66e-01[  5.0%]|***\r\n\r\nSets via Hash Table: Size =8000\r\nAnalysis of 20 timings\r\navg = 0.229   min = 0.223  max = 0.247  span = 10.7%\r\n\r\n   Time Ranges    \r\n2.23e-01<>2.25e-01[ 20.0%]|******************\r\n2.25e-01<>2.28e-01[  5.0%]|****\r\n2.28e-01<>2.30e-01[ 55.0%]|**************************************************A\r\n2.30e-01<>2.33e-01[ 10.0%]|*********\r\n2.33e-01<>2.35e-01[  5.0%]|****\r\n2.35e-01<>2.37e-01[  0.0%]|\r\n2.37e-01<>2.40e-01[  0.0%]|\r\n2.40e-01<>2.42e-01[  0.0%]|\r\n2.42e-01<>2.45e-01[  0.0%]|\r\n2.45e-01<>2.47e-01[  0.0%]|\r\n2.47e-01<>2.50e-01[  5.0%]|****\r\n\r\nSets via Hash Table: Size = 16000\r\nAnalysis of 20 timings\r\navg = 0.452   min = 0.447  max = 0.461  span = 3.1%\r\n\r\n   Time Ranges    \r\n4.47e-01<>4.49e-01[ 20.0%]|****************************************\r\n4.49e-01<>4.50e-01[ 20.0%]|****************************************\r\n4.50e-01<>4.51e-01[ 25.0%]|**************************************************\r\n4.51e-01<>4.53e-01[ 10.0%]|********************A\r\n4.53e-01<>4.54e-01[  5.0%]|**********\r\n4.54e-01<>4.56e-01[  5.0%]|**********\r\n4.56e-01<>4.57e-01[  0.0%]|\r\n4.57e-01<>4.59e-01[  5.0%]|**********\r\n4.59e-01<>4.60e-01[  0.0%]|\r\n4.60e-01<>4.61e-01[  5.0%]|**********\r\n4.61e-01<>4.63e-01[  5.0%]|**********\r\n\r\nSets via Hash Table: Size = 32000\r\nAnalysis of 20 timings\r\navg = 0.908   min = 0.897  max = 0.918  span = 2.3%\r\n\r\n   Time Ranges    \r\n8.97e-01<>8.99e-01[ 20.0%]|**************************************************\r\n8.99e-01<>9.01e-01[  5.0%]|************\r\n9.01e-01<>9.03e-01[  5.0%]|************\r\n9.03e-01<>9.05e-01[  5.0%]|************\r\n9.05e-01<>9.08e-01[ 15.0%]|*************************************\r\n9.08e-01<>9.10e-01[ 10.0%]|*************************A\r\n9.10e-01<>9.12e-01[  0.0%]|\r\n9.12e-01<>9.14e-01[ 15.0%]|*************************************\r\n9.14e-01<>9.16e-01[ 10.0%]|*************************\r\n9.16e-01<>9.18e-01[ 10.0%]|*************************\r\n9.18e-01<>9.20e-01[  5.0%]|************\r\n\r\nSets via Hash Table: Size = 64000\r\nAnalysis of 20 timings\r\navg = 1.811   min = 1.796  max = 1.837  span = 2.2%\r\n\r\n   Time Ranges    \r\n1.80e+00<>1.80e+00[ 20.0%]|**************************************************\r\n1.80e+00<>1.80e+00[ 20.0%]|**************************************************\r\n1.80e+00<>1.81e+00[ 15.0%]|*************************************\r\n1.81e+00<>1.81e+00[  5.0%]|************A\r\n1.81e+00<>1.82e+00[ 10.0%]|*************************\r\n1.82e+00<>1.82e+00[ 10.0%]|*************************\r\n1.82e+00<>1.82e+00[  0.0%]|\r\n1.82e+00<>1.83e+00[ 10.0%]|*************************\r\n1.83e+00<>1.83e+00[  5.0%]|************\r\n1.83e+00<>1.84e+00[  0.0%]|\r\n1.84e+00<>1.84e+00[  5.0%]|************\r\n\r\nSets via Hash Table: Size = 128000\r\nAnalysis of 20 timings\r\navg = 3.612   min = 3.597  max = 3.637  span = 1.1%\r\n\r\n   Time Ranges    \r\n3.60e+00<>3.60e+00[ 10.0%]|****************\r\n3.60e+00<>3.60e+00[  0.0%]|\r\n3.60e+00<>3.61e+00[ 30.0%]|**************************************************\r\n3.61e+00<>3.61e+00[ 15.0%]|*************************A\r\n3.61e+00<>3.62e+00[ 20.0%]|*********************************\r\n3.62e+00<>3.62e+00[ 10.0%]|****************\r\n3.62e+00<>3.62e+00[ 10.0%]|****************\r\n3.62e+00<>3.63e+00[  0.0%]|\r\n3.63e+00<>3.63e+00[  0.0%]|\r\n3.63e+00<>3.64e+00[  0.0%]|\r\n3.64e+00<>3.64e+00[  5.0%]|********\r\n\r\nSets via Hash Table: Size = 256000\r\nAnalysis of 20 timings\r\navg = 7.211   min = 7.186  max = 7.238  span = 0.7%\r\n\r\n   Time Ranges    \r\n7.19e+00<>7.19e+00[ 10.0%]|****************\r\n7.19e+00<>7.20e+00[  0.0%]|\r\n7.20e+00<>7.20e+00[  0.0%]|\r\n7.20e+00<>7.21e+00[ 20.0%]|*********************************\r\n7.21e+00<>7.21e+00[ 30.0%]|**************************************************A\r\n7.21e+00<>7.22e+00[ 20.0%]|*********************************\r\n7.22e+00<>7.22e+00[  5.0%]|********\r\n7.22e+00<>7.23e+00[  5.0%]|********\r\n7.23e+00<>7.23e+00[  0.0%]|\r\n7.23e+00<>7.24e+00[  5.0%]|********\r\n7.24e+00<>7.24e+00[  5.0%]|********\r\n", "encoding": "ascii"}