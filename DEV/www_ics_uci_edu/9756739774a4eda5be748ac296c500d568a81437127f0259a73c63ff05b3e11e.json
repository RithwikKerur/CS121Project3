{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/memory.txt", "content": "\t\tFundamentals of Memory and Memory Management\r\n\r\n\r\nMemory Hierarchies: Speed/Size/Cost Tradeoffs\r\n\r\nTo start, we will discuss hierarchies in memory, including tradeoffs between\r\ntheir speeds, sizes, and costs. First, we will discuss the following prefixes\r\nfor sizes/speeds, which every computer scientist should be familiar with.\r\n\r\n    kilo  x 10^3\t      milli  x 10^-3\r\n    mega  x 10^6\t      micro  x 10^-6\r\n    giga  x 10^9\t      nano   x 10^-9\r\n    tera  x 10^12\t      pico   x 10^-12\r\n    peta  x 10^15\t      femto  x 10^-15\r\n    exa   x 10^18\t      atto   x 10^-18\r\n\r\nSo, 1 gigabyte is 10^9 bytes. The following might become relevant for your life\r\n\r\n    zetta x 10^21\t      zepto  x 10^-21\r\n    yotta x 10^24\t      yoxto  x 10^-24\r\n\r\nThere are three standard locations for memory: on the CPU chip (also known as\r\ncache memory), in special memory chips (main memory), and external memory (disk\r\ndrives, DVDs, USB, etc.). Each is slower than the previous, but its size is\r\nbigger and its cost/byte is lower. CPU chips act as a cache for main memory;\r\nand main memory often acts as a cache for external memory (some external memory\r\ndevices, like disk drives, also have their own dedicated caches).\r\n\r\nA book by Van Loan/Fan (Insight Through Computing) made some of these numbers\r\nconcrete.\r\n\r\n  1 megabyte: A 500 page novel; 1 minute of MP3 music\r\n  1 gigabyte: The human genome; 20 minutes of a DVD\r\n  1 terabyte: A University library; photos of all US Airline passengers (1 day)\r\n  1 petabyte: The amount of text in the Library of Congress; he says that\r\n              printing it would take 50x10^6 trees\r\n\r\n------------------------------------------------------------------------------\r\n\r\nExternal Memory/Disk Drives:\r\n\r\nNote that disk drives/DVDs are modeled by a spinning platter that has\r\nconcentric circles that each store data; there is a \"read head\" that can move\r\nbetween concentric circles. To read a specific word of data, the read head seeks\r\nthe correct circle and waits for the data to rotate into the correct position\r\nunder it. A typical rotation speed is 7,200 rpm. This is roughly 100\r\nrevolutions/second, meaning that it takes about 10 milliseconds to complete one\r\nfull rotation; this is called the \"rotational delay\". The read head takes about\r\nthe same amount of time to move into position over the correct circle (called\r\n\"seek time\"). Note that a processor executing 1 billion operations per second\r\ncan execute 10 million instructions in 10 milliseconds, while the read head\r\nseeks and the platter rotates to the needed position, so it can do a lot of\r\nwork between setting up for the read and actually receiving the data.\r\n\r\nWe will look at algorithms that measure their efficiency not by the number of\r\ninstructions executed, but by the number of times they must seek data on an\r\nexternal memory device, because the time waiting for data will dominate the\r\ntime spent in the number of instructions executed.\r\n\r\nWe often talk about the terms \"latency\" and  \"bandwidth\" when discussing\r\nmemory access (and transmitting information over networks as well). Latency is\r\nthe time it takes from a request for data until the first information arrives.\r\nBandwidth is the throughput (data rate) starting when the first data begins to\r\narrive. The latency for a disk drive/DVD can be large (see the 10 millisecond\r\nnumbers above), because it involves moving something physical: the read head\r\nand the platter; but once the read head/platter are in the right position, with\r\nthe right data is under it, the quickly spinning platter (storing data densely)\r\ncan transfer all the data on the circle very quickly. Historically, data\r\ntransfer rates on standard hard drives are ~70 megabytes per second.\r\n\r\nBecause external memory typically has high latency (a long time, measured in\r\nmachine instructions that can execute before the first piece of data arrives)\r\nand high bandwidth (after that, lots of data can be delivered quickly), we\r\ntypically use every memory request to transfer a BLOCK of memory, not a single\r\nword of memory. The expectation is that the subsequent information will be\r\nneeded soon (certainly the case when reading a sequential file).\r\n\r\nSo, when reading information from a file stored on a disk drive, instead of\r\njust reading one character at a time, many characters (tens of thousands to\r\nmillions) are read and cached in main memory (or the disk drive's dedicated\r\ncache; these caches can transfer data to memory at the rate of 3 gigabytes per\r\nsecond -much faster than information can be read as the disk rotates).\r\n\r\nSo, for many subsequent character reads, the information is retrieved directly\r\nfrom memory and the computer doesn't have to perform any more disk operations.\r\nSuch a cache has a special name, a \"buffer\". When the buffer is exhausted (all\r\ncharacters read from it), the next character read initiates another block\r\ntransfer. As we saw, it might take 10 millisecond to move the read head and\r\nwait for the data to rotate on the platter under the read head, but it will\r\ntake little time to read tens of thousands to millions more characters. Another\r\nway to look at this: to get one character takes 10 milliseconds, but to get\r\n100,000 characters requires only 10 + 1 = 11 milliseconds (at 100 megabytes/sec:\r\na round number that is a bit more,than the 70 quoted above as the transfer\r\nrate). So, the amortized cost of reading one of the 100,000 characters is\r\n11/100,000 milliseconds/character or about .11 microseconds/character (an\r\noverall rate of almost 10 million characters/second). If we read 1,000,000\r\ncharacters into the buffer, the rate would be 20/1,000,000 or about\r\n.02 microseconds/character, an overall rate of 50 million characters/second).\r\n\r\nThis analysis is a bit like what we did for putting N values in an ArrayQueue,\r\nwhich doubles its size. Some adds are very quick, but a few require much more\r\ntime, when the array size doubles. But if you take a look at lots of adds, the\r\naverage cost is very cheap. Likewise, when reading blocks of characters, reading\r\nevery new block takes lots of time, but most characters will then be in the\r\nmemory buffer for quick reading.\r\n\r\nHere are some current sizes and relative speeds for CPU Chip, main, and\r\nexternal memory (these are very approximate).\r\n\r\n  CPU Chip ~1-10 Mb  10-100 times faster than main memory\r\n  Main     ~1-10 Gb  100K - 1M times faster than external memory (latency)\r\n  External ~1- ? Tb \r\n\r\nAs a rule of thumb, each step up increases the size by a factor of 1,000 and\r\ndecreases the speed and cost/byte by a similar factor.\r\n\r\nWhen we have analyzed algorithms earlier in this course, we have assumed that\r\nall data is in main memory. In fact, often most of the data is on the CPU chip\r\nmemory, and its performance is often an important practical consideration for\r\ndetermining the time an algorithm will take. With effective caching, often 9 of\r\n10 memory access will occur in the CPU Chip memory, not the main memory. This\r\ncan speed up the execution by a factor of 10-100. We will briefly discuss the\r\ninterplay between CPU chip and main memory below, using CPU chip memory to cache\r\ninformation from main memory.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nData Access patterns and the CPU Cache:\r\n\r\nAccess patterns for data often exhibits two kinds of locality.\r\n\r\n Temporal locality: if data is being accessed now, it is likely to be accessed\r\n          soon in the future; for example, a loop index (or more generally, a\r\n          cursor) is accessed frequently during the execution of a loop (its\r\n          value is initialized, checked, and updated in each loop iteration).\r\n\r\n Spatial  locality: if data is being accessed now, data near it is likely to be\r\n          accessed soon in the future; for example, if we are accessing an\r\n          ARRAY at position i, it is likely we will access it at position i+1\r\n          in the near future (when scanning an array, not doing a binary search\r\n          in an array). This effect is similar, but not quite as pronounced in\r\n          linked lists/trees whose nodes were allocated at a similar time (and\r\n          therefore initially in memory locations that are close by).\r\n\r\nScanning all the values in an array, using an index variable, exhibits both\r\ntemporal (the index variables) and spatial (the array elements) locality. If\r\nwe can rewrite our program (or write it in machine code) so that it can \r\ncompletely fit in CPU chip memory, it can run much faster than a slightly\r\nbigger amount of code that cannot fit in CPU chip memory.\r\n\r\nThe following algorithm is implemented in hardware. It is used whenever the\r\nCPU needs to access data. Here we use the term cache for the CPU Chip memory.\r\nThe cache starts out empty.\r\n\r\n 1) If the data is already in the cache, use it\r\n\r\n 2) If the data is not already in the cache\r\n      a) Retrieve it (and other data near it: some block of memory)\r\n         As with external memory, there is high latency to get the data, but\r\n         there is high bandwidth to transfer a block of data from main memory\r\n         to the cache, so it accesses/transfers blocks of data\r\n      b) If the cache is not full, add the new block of data to it\r\n      c) If the cache is full, determine which block of data to remove and\r\n           add the new block of data to replace it\r\n\r\nIn the future, all memory addresses in the cache can be accessed quickly; an\r\naddress outside the cache must go through the process above.\r\n\r\nWe need a policy dictating which old data block to remove from a filled\r\ncache. Three standard and well-studied policies are Random, First in first out\r\n(FIFO), and Least Recently Used (LRU). Any policy must be fairly simple,\r\notherwise it could not be directly implemented in hardware (because of the\r\nspeeds needed, cache replacement algorithms must be implemented in hardware).\r\n\r\nThe idea is to leave inside the cache any data that is expected to be accessed\r\nsoon in the future. Random does not bring anything relevant into the decision,\r\nbut it is simple/cheap to implement (no extra storage). FIFO seems a reasonable\r\nstrategy: if something was brought in a long time ago, it is less likely to be\r\nused compared to something that was brought in more recently (it requires only\r\na simple queue to keep track of which block to replace next). But, LRU gets\r\nmore to the heart of temporal locality: if something has been used recently, it\r\nis more likely to be used in the near future (regardless of when it was\r\ninitially used, which is what FIFO monitors). While LRU is harder to implement\r\nin hardware (it uses something like a priority queue), it can be implemented\r\nthere and it is a better predictor of what to remove and what to leave in the\r\ncache.\r\n\r\nBecause the time needed to locate and transfer a block of data is large (while\r\nwaiting for the data to arrive, the computer could execute many instructions),\r\nchoosing a replacement policy like LRU that is less efficient (takes more time\r\nto determine what block to remove) but better (determines more accurately which\r\nblock won't be used in the future) is likely to provide better performance\r\noverall.\r\n\r\nThe concept of \"prefetching\" works for Main->CPU or External->Main memory.\r\nIf as programmers we know that some data is expected to be used in the near\r\nfuture (but not needed yet) we can prefetch it (touch it so that it loads into\r\nthe cache). Then, while the computer is doing other things, before the data is\r\nactually needed, it will be brought from the slower to the faster memory. \r\n\r\nThe maintenance of caches is an important part of chip design. Cache design\r\nbecomes even more interesting when multiple cores/processors access the same\r\nmemory. For example, if 4 CPUs each cache some main memory that they share,\r\nand one CPU changes the value (in its cache) the other CPUs that are caching\r\nthat same memory have to be updated (and main memory as well, eventually).\r\n\r\nWhen new cache mechanisms are proposed, they are often evaluated by using\r\npreviously collected \"memory traces\" showing which memory locations actual\r\n\"important\" programs generate, and determining how well the caching mechanism\r\nworks in these cases. Such memory traces can constitute billions, trillions\r\n(even more) of memory references of a running program; recall machines can\r\nexecute billions of operations per second.\r\n\r\nLikewise, using the concept of \"virtual memory\" we can consider main memory to\r\nact as a cache for external memory. Using virtual memory, we can solve huge\r\nproblems by just pretending that the computer's memory is as large as its\r\ndisk-drive's memory (terabytes not gigabytes). Then we use the main memory\r\nas a cache for external memory (just as we discussed above using a CPU\r\nChip's memory as a cache for main memory, including replacement policies).\r\n\r\nUsing virtual memory we can \"easily\" solve problems that do not fit into a\r\ncomputer's main memory, but unless the data structures and algorithms\r\nprocessing the data structures exhibit strong temporal/spatial locality, the\r\nrun time can be enormously larger (thousands to millions of time longer, as\r\ndata is shuttled between main and externall memory). In the next two lectures\r\nwe will discuss data structures and algorithms for fast searching and sorting,\r\nwhen using huge amounts of external memory.\r\n\r\nGordon Bell (a famous computer designer) has written a book called \"Total\r\nRecall: How the E-Memory Revolution will change Everything\" (Dutton, 2009). In\r\nthe book he posits that in the future, everyone can have everything that they\r\never see and hear (e.g., every conversation that they have), every web-page\r\nthat they look at, etc. stored in memory and indexed for retrieval. Here is a\r\nquote from page  9, early in the book.\r\n\r\n  In fact, digital storage capacity is increasing faster than our\r\n  ability to pull information back out. Once upon a time, you had\r\n  to be extremely judicious and stingy about which pieces of data\r\n  you hung on to. You had to be thrifty with your electronic pieces\r\n  of information, or bits, as we call them. But starting around 2000\r\n  it became trivial and cheap to sock away tremendous piles of data.\r\n  The hard part is no longer deciding what to hold on to, but how to\r\n  efficiently organize it, sort it, access it, and find patterns and\r\n  meaning in it. This is a primary challenge for the engeineers\r\n  developing the software that will fully unleash the power of Total\r\n  Recall.\r\n\r\nBasically, Moore's Law (http://www.intel.com/technology/mooreslaw/) postulated\r\nby Gordon Moore (Intel) says that the number of transistors in a given area\r\nwill double every 18 months. This typically translated into computer speed\r\ndoubling as well, but not now. It requires too much power to speed up\r\ncomputers. So instead, we use the extra transistors to create more cache memory\r\nand more cores (CPUs) on a single chip. They all run at a \"slow\" speed, but if\r\nprogrammed correctly, to work together, they can accomplish as much as a faster\r\nchip. How to coordinate cores is still a problem (some say the biggest practical\r\nproblem facing computing today).\r\n\r\nExternal memory is still growing at a slightly faster pace than predicted by\r\nMoore's law; typically every couple of years you can buy twice the amount of\r\nexternal memory for about the same price (with no speed degradation, but also\r\nnot a lot of speed improvements). Solid state external memory is gaining (cost\r\nand performance) on hard disk drives.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nStacks and Heaps:\r\n\r\nMost computer programming languages use memory in two special ways: as a stack\r\nand as a heap (NOT the same kind of binary heaps used for efficient priority\r\nqueues; here the same name is used for something very different). Main memory\r\nis really just a giant array of words. A 32 bit word stores an int or a\r\nreference; it can also be divided into four, 8-bit bytes, where each byte can\r\nstore a single ASCII character.\r\n\r\nThink of all available memory (once the program has been stored in memory) as\r\nbeing divided between stacks and heaps, with the stack on the left growing \r\ntowards the right, and the heap on the right growing towards the left\r\n\r\n Memory\r\n+---------+----------------------+\r\n| program | Stack ->      <- Heap|\r\n+---------+----------------------+\r\n\r\nWe have seen that stacks are used for method calls -including recursive method\r\ncalls- to store parameters and locals variables; stacks are also used to\r\nevaluate arithmetic expressions. Stacks grow and shrink with no \"holes\":\r\neach method call increases the stack size (adds to it) by N locations (storing\r\nN parameters and local variables) and each method return decreases the stack\r\n(removes from it) by the same N locations.\r\n\r\nWe use heaps for objects constructed by \"new\". Heaps can have a holes. For\r\nexample, if we store a Set in an array, initially we allocate an array of a\r\ncertain size to store the Set; later we might double the length of tha array,\r\nallocating another array whose size is twice as big as the first (coming from\r\nheap space to the left of the original array). Now the original array is\r\ngarbage (there will be no references from the program pointing to it) creating\r\na hole in the heap space (that can be reused if we delete [] it: also see the\r\nsection on garbage collection below). Thus, it is more difficult for\r\nprogramming languages to manage (allocate and reuse garbage) heap space.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nBasics of (Heap) Memory Management:\r\n\r\nWhether programmers do their own memory management (as in the C and C++\r\nlanguages, where they explicitly must \"delete\" memory they no longer need)\r\nor whether an Automatic Garbage Collector (really, a \"recycler\") does it for\r\nus, we can discuss various needs and strategies for recycling memory. First, a\r\nfree block of memory is a contiguous number of free memory locations.\r\n\r\nTypically each memory block allocated in the heap has a few words reserved for\r\nmemory management information. A minimal amount would allows us to store\r\nthe address and size of each free block of memory and a reference to the next\r\nfree block of memory (keeping all the free blocks in a linear linked list).\r\nInitially we would have a list with one huge block of free memory. When a block\r\nof memory is freed (either  explicitly because we delete it, or because an\r\nautomatic garbage collector -in Languages like Python and Java- finds it) we\r\ncan add it to the linked list of free memory blocks.\r\n\r\nIf we need to allocate a block of memory, before going to the remaining memory\r\nin the heap (or after going there and not finding enough memory), we can check\r\nwhether we can reallocate a block of memory from this linked list of free\r\nmemory blocks that were previously allocated but garbage collected. We will\r\ndiscuss four strategies below, using the concept of \"fragmentation\". Memory is\r\nfragmented if there are many small blocks (as opposed to a few large free\r\nblocks). If memory is fragmented, it is likely to take longer to search for a\r\nfree block of the necessary size.\r\n\r\nHere are four policies that decide which memory block to use from the linked\r\nlist of free memory blocks.\r\n\r\n  1) First-fit: Search the linked list starting at the begining and stop at the\r\n       first memory block with enough space.\r\n\r\n  2) Next-fit: Search the linked list starting wherever the last reclaimed\r\n       block came from, and stop at the first block with enough space (if we\r\n       run off the end of the list, start at the front: e.g., circular list).\r\n\r\n  3) Best-fit: Search the entire linked list and find the smallest block with\r\n       enough space (or keep the the list sorted by size, or use a hash-like\r\n       stucture with all blocks 1-2 words, 3-4 words, 5-8 words, 9-16 words,\r\n       17-32 words, etc. linked together: bin 30 is holding memory objects\r\n       whose size is a gigabyte: 2^30).\r\n\r\n  4) Largest: Use the largest free memory block (sometimes called \"worst fit\",\r\n       but not a pejorative).\r\n\r\nAfter allocating a memory block of the needed size, the remaining memory in\r\nthat block goes back on the the linked list of free memory blocks (with a\r\nsmaller size). So, regardless of the policy, if we need 100 words of memory to\r\nallocate for an object, and we decide to use a block that stores 300 words of\r\nmemory, we allocate the 100 and put the remaining 200 back into free memory.\r\n\r\n  1) First-fit: initially fast, but can create lots of small memory blocks at\r\n     at the front of the linked list, slowing down searching.\r\n\r\n  2) Next-fit: improves on first-fit by spreading fragmentation throughout the\r\n     linked list (not always at the front).\r\n\r\n  3) Best-fit wastes little extra space, but tends to create very small memory\r\n     blocks, possibly unallocatable (because they are too small to be useful)\r\n     at the front of the linked list. Must search a lot.\r\n\r\n  4) Largest: can be fast (we can use a priority queue where the largest memory\r\n     block has the highest priority), and puts the largest memory blocks (thus\r\n     more easily allocatable in the future) put back in the priority queue.\r\n\r\nComputer Scientists have created lots of models for managing recycled memory\r\nand collected lots of data (in the form of memory-use traces) to simulate and\r\nevaluate all sorts of memory recycling policies.\r\n\r\nNote that when a block memory is deleted, it is a good idea (but it takes a bit\r\nof time) to discover whether it is adjacent to another free block, and if so,\r\ncombining the two blocks into one bigger block.\r\n\r\nTechnically, once you delete a block you should never access any of its\r\ninformation. Some delete operations will overwrite all the data with  a special\r\nbit pattern (like all 0s) to ensure if you try to access that data in the future\r\nyou will see a strange result.  Others don't take the time to do that because\r\ngood programmers should never access that information :) I believe the PC's\r\ndelete stores the strange bit patterns but the Mac's does not. When student\r\naccess information in deleted blocks on the PC, they often crash, but on the\r\nMac (so long as the block has not be recycled) the code will work. This causes\r\nproblems when we grade Mac-written solutions on the PC: they often fail on the\r\nPC but run correctly (although the programmer is doing something wrong) on the\r\nMac.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nGarbage Collection:\r\n\r\nWhen programmers manage free memory, their code is often prone to error (even\r\ngood/experienced programmers), creating memory leaks: memory that the program\r\nis not using (and no longer has access to) but is also not on a free list for\r\nfuture use: truly garbage that cannot be recycled. Other times programmers\r\ndelete memory (which might be recycle) while other parts of the program are\r\nstill using it. Smart pointers in C++ are used to minimize these kinds of\r\nmistakes.\r\n\r\nSometimes such programs must be stopped and restarted because they run out of\r\nmemory. There are some mission critical programs that do not allow the use of\r\n\"new\" (after setting up initial data structures) because of the possible memory\r\nleaks. During the first Gulf War, a memory leak was found in an anti-missle\r\nweapon which would not function well after operating for days (ir was designed\r\nto be used in a \"fast European war\" and not expected to have to work\r\ncontinuously for days at a time). Until the software was fixed, the operators\r\nwere instructed to shut down and restart the software every few days (of\r\ncourse, when to shut it down was problematic, as the system was inoperable\r\nduring the minutes required for a shutdown and restart). As I said above, it\r\nwas designed to operate in Europe, where antimissile batteries would go on\r\nalert for just hours at a time, therefore testing it under these conditions\r\nfailed to show any problems with running it 24/7 (as was needed during the\r\nfirst Gulf War).\r\n\r\nBy using an automatic Garbage Collector (GC) we avoid explicit deallocation;\r\nour code calls \"new\" when it needs memory but NOT \"delete\" (typically the code\r\njust makes some variable refer to a different object; then the original object\r\nit referred to -if no other variables refer to it- become garbage/reycleable).\r\n\r\nSuch systems can find all the memory blocks not currently used by a program\r\nand put them all on the linked list of free memory blocks. Note that languages\r\nlike Lisp had automatic garbage collection as early as the 1960s. Note too that\r\nfor a program that doesn't exhaust memory, automatic garbage collection (which\r\ndoesn't occur in such a program) can be faster than manual garbage collection,\r\nsince manual garbage collection requires doing some work on disposing of some\r\nmemory, while automatic garbage collection does no work on disposal, but\r\npossibly more work on recycling garbage when there is no more free memory in\r\nthe heap; if an application can run without garbage collection, it can take\r\nless time.\r\n\r\nSimple garbage collection can be accomplished by storing reference counts to\r\ndata, so we know that when a count goes to 0 it can be recycled. Although, a\r\ncircular structure can have each of its nodes with a non-zero reference count\r\nyet no variable refers to any part of the circular structure.\r\n\r\nMark and Sweep garbage collectors are fairly standard and simple to understand,\r\nbut there are many different algorithms for this universally useful task. We\r\nwill discuss some briefly.\r\n\r\nIn the Mark phase, the GC first finds all the pointers in a program: initially,\r\nthese are all pointer variables stored in the stack, representing pointers to\r\nobjects in the heap (from global variables, and parameter, and local variables\r\nin executing function/methods). The GC follows these references to the objects\r\nthat they refer to and marks these objects as \"live\" (often there is a bit in\r\nan extra word associated with each free block of memory to mark whether or not\r\nit is live).\r\n\r\nFrom these live objects, the GC follows their pointer instance variables to\r\nthe objects that they refer to in the heap, and marks these objects live as\r\nwell. The GC continues this process (which is like searching a graph of\r\nobjects -which point to other objects- for \"reachability\") until it has marked\r\nevery object object live that can be reached from parameter/local variables\r\nactive in the code. This is like the \"reachable\" computation from Programming\r\nAssignment #1. There are some very clever algorithms that use the extra space\r\nin these live objects to store the data structures needed to reach all the live\r\nobjects, so we don't need much extra memory during garbage collection (because\r\nat that time we don't have much extra memory!).\r\n\r\nIn the sweep phase, the GC sweeps through the heap memory and puts on the\r\nlinked list of free memory blocks all those memory blocks that it enoucounters\r\nthat are NOT marked as live. If possible, it will compact two adjacent free\r\nmemory blocks into one larger one. More sophisticated \"compacting\" GC\r\nalgorithms can also change live pointers so that their data occurs on the far\r\nright of the heap space, with all free heap space to the left (as per the\r\ndiagram above).\r\n\r\nFinally, note that in Java (or any automatic GC language) when we are storing\r\npointer data in an array (say a Set) and we perform a \"clear\" operation,\r\ntypically we set used to 0 AND store nullptr in every previously used location\r\nin the array. This ensures that objects with pointers to them currently stored\r\nin the array can be garbage collected (if there are no other references to\r\nthem).\r\n\r\nWhy do we have to set everything to nullptr and not just set used to 0? Even\r\nthough WE know know that no array positions store useful data when used is 0,\r\nthe garbage collector thinks every pointer in an array is live. If we left those\r\npointers stored in the array, the garbage collector would consider all\r\npointers in the array as live when doing the Mark phase, and not garbage\r\ncollect those object. Of course the code works correctly either way, but if we\r\ndon't set object reference to nullptr, we may eventually run out of space. So\r\nclear becomes O(N) instead of O(1) if we must remove pointers.\r\n\r\nGarbage collectors have problems as well: they run at unpredictable times\r\nand take an unpredictable amount of time to run (although we can determine\r\nlimits on their run time).  So, there are some mission critical real-time\r\nprograms that do not allow the use of \"new\" because of this unpredictability.\r\nFor example, in real-time applications (such as software flying an airplane),\r\nwe would like to ensure that garbage collection does not take place during a\r\ncritical phase (like landing). So, some real-time software prohibits the use of\r\nheap memory.\r\n\r\nThere are ways around this unpredictability. We can run an \"incremental\" GC at\r\nthe same time as the program to minimize the frequency and length of pauses in\r\nactual code. Say, every 100 milliseconds the GC runs for a few milliseconds,\r\ndoing some of it work. The result is that the program executes a few percent\r\nslower (typically not a big problem on fast CPUs), but garbage collection runs\r\nmore predictably. When it is required, much of its work has been accomplished.\r\n\r\nIn fact, with multi-core CPUs, we can always run a GC on one of the cores to\r\nminimize the impact of automatical garbage collection.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nA Class Doing its own Memory Management\r\n\r\nA class can do its own memory management. Imagine a linked list of LN for\r\nsome application (say in a hash table implementing a Map). When we erase an\r\nitem, instead of deleting its LN, we could put it on a free list: allocated LN\r\nobjects that are no longer needed. Then, when we need to allocate a new LN, we\r\nfirst look on that free list; if it contains at least one LN, we use it. If it\r\ndoesn't we call \"new\" to get the LN we need. The destructor for such a class\r\nwould ultimately delete all the LNs it was using and on its free list.\r\n\r\nThe main problem with this approach is that if we have multiple data structures\r\nand each has its own free list, a data structure needing a new LN can find only\r\nthose it is controlling; it cannot easily use LNs on the free list controlled\r\nby another data structure. By using \"delete\" instead, all free storage ends up\r\ncontrolled by the same mechanism, so any sized data structure can be allocated.\r\n\r\n\r\nFinal Words:\r\n\r\nOne lecture on the material described above and below is not enough to get a\r\ntruly intuitive feeling for the information. The course ICS 51 (Introductory\r\nComputer Organization) and courses on programming languages and operating\r\nsstems covers these topics in much more depth. Read about these terms on the\r\ninternet as well (e.g., Wikipedia). Use the names provided here.\r\n", "encoding": "ascii"}