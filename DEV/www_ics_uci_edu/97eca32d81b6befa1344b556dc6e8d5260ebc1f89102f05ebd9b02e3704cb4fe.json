{"url": "https://www.ics.uci.edu/~dan/class/267P/homework/hw1.html", "content": "<HTML>\n<span style=\"color:#000000; font:16px Arial, Helvetica, sans-serif;\">\n\n<center>\n<H3>CompSci 267P Homework #1</H3>\n</center>\n<OL>\n<LI> [Sayood p.38#1]\n    Suppose X is a random variable that takes on values\n    from an M-letter alphabet.\n    Show that 0 <u>&lt;</u> H(X) <u>&lt;</u> lg M.\n<BR> &nbsp;\n\n<LI> [Sayood p.38#3]\n    Given an alphabet {<I>a,b,c,d</I>}, find the first-order entropy\n    in the following cases:\n   <BR> (<I>a</I>) P(<I>a</I>)=P(<I>b</I>)=P(<I>c</I>)=P(<I>d</I>) = 1/4\n   <BR> (<I>b</I>) P(<I>a</I>)= 1/2, P(<I>b</I>)= 1/4, P(<I>c</I>)=P(<I>d</I>)= 1/8\n   <BR> (<I>c</I>) P(<I>a</I>)=0.505, P(<I>b</I>)=1/4, P(<I>c</I>)=1/8, P(<I>d</I>)=.12\n<BR> &nbsp;\n\n<LI> [Sayood p.39#6abcd]\nConduct an experiment to see how well a model can describe a source.\n<BR>\n(a) Write a program that randomly selects letters from the 26-letter\nalphabet and forms four-letter words.  Form 100 such words and see\nhow many of these words make sense.  Do this several times and\ndetermine the approximate expected number of sensible words.\n<BR>\n<BR>\n(b) File\n<a href=\"/~dan/class/267P/datasets/text/4letter.words\">http://www.ics.uci.edu/~dan/class/267P/datasets/text/4letter.words</a>&nbsp; &nbsp;\ncontains a list of four-letter words. Using this file, and remembering\nto fold upper- and lower-case letters, obtain a probability model for\nthe alphabet.\n<BR>\nRepeat part (a) generating words using the probability model.\n(You may use the random number generator located in file\n<a href=\"/~dan/class/267P/programs/random.c\">http://www.ics.uci.edu/~dan/class/267P/programs/random.c</a>&nbsp; &nbsp;)&nbsp;\nCompare your results with part (a).\n<BR>\n<BR>\n(c) Repeat part (b) using a single-letter context.\n<BR>\n<BR>\n(d) Repeat part (b) using a two-letter context.\n<BR> &nbsp;\n\n<LI> (a) Find the entropy of a source with 6 symbols\n     having probabilities .5, .2, .1, .1, .05, and .05.\n<BR> &nbsp;\n<BR> (b) An information source has 128 equally probable symbols.\n     How long is a message from the source whose entropy is 56 bits?\n<BR> &nbsp;\n<BR> (c) What is the entropy of a message of 32 symbols\n     from a source with three symbols if\n     <UL>\n     <LI> each symbol is equally likely\n     <LI> the symbol probabilities are .6, .3, and .1.\n     </UL>\n</OL>\n</span>\n</HTML>\n", "encoding": "ascii"}