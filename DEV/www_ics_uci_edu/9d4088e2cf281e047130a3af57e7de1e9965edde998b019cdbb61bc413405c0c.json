{"url": "https://www.ics.uci.edu/~smyth/courses/cs274/homeworks/hw4_materials/logistic_regression_problem2_incomplete.python", "content": "## CS 274 2019, Homework 4, Skeleton Python Code for Logistic Regression\n## (Code originally written in Winter 2017 by Eric Nalisnick, UC Irvine)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy\n\n### Helper Functions ###\n\ndef logistic_fn(z):\n    ### TO DO ###\n\ndef load_data_pairs(type_str):\n    return pd.read_csv(\"mnist_3s_and_7s/\"+type_str+\"_x.csv\").values, pd.read_csv(\"mnist_3s_and_7s/\"+type_str+\"_y.csv\").values\n\ndef run_log_reg_model(x, beta):\n    ### TO DO ###\n\ndef calc_log_likelihood(x, y, beta):\n    theta_hats = run_log_reg_model(x, beta)\n    ### Return an average, not a sum!\n    ### TO DO ###\n\ndef calc_accuracy(x, y, beta):\n    theta_hats = run_log_reg_model(x, beta)\n    ### TO DO ###\n\n\n### Model Training ###\n\ndef train_logistic_regression_model(x, y, beta, learning_rate, batch_size, max_epoch):\n    beta = copy.deepcopy(beta)\n    n_batches = x.shape[0]/batch_size\n    train_progress = []\n\n    for epoch_idx in xrange(max_epoch):\n        for batch_idx in xrange(n_batches):\n            \n            ### TO DO ###\n            \n            # perform updates\n            beta += learning_rate * beta_grad\n        \n        train_progress.append(calc_log_likelihood(x, y, beta))\n        print \"Epoch %d.  Train Log Likelihood: %f\" %(epoch_idx, train_progress[-1])\n        \n    return beta, train_progress\n\n\nif __name__ == \"__main__\":\n\n    ### Load the data\n    train_x, train_y = load_data_pairs(\"train\")\n    valid_x, valid_y = load_data_pairs(\"valid\")\n    test_x, test_y = load_data_pairs(\"test\")\n\n    # add a one for the bias term                                                                                                                                                 \n    train_x = np.hstack([train_x, np.ones((train_x.shape[0],1))])\n    valid_x = np.hstack([valid_x, np.ones((valid_x.shape[0],1))])\n    test_x = np.hstack([test_x, np.ones((test_x.shape[0],1))])\n\n    ### Initialize model parameters\n    beta = np.random.normal(scale=.001, size=(train_x.shape[1],1))\n    \n    ### Set training parameters\n    learning_rates = [1e-3, 1e-2, 1e-1]\n    batch_sizes = [train_x.shape[0]]\n    max_epochs = 250\n    \n    ### Iterate over training parameters, testing all combinations\n    valid_ll = []\n    valid_acc = []\n    all_params = []\n    all_train_logs = []\n\n    for lr in learning_rates:\n        for bs in batch_sizes:\n            ### train model\n            final_params, train_progress = train_logistic_regression_model(train_x, train_y, beta, lr, bs, max_epochs)\n            all_params.append(final_params)\n            all_train_logs.append((train_progress, \"Learning rate: %f, Batch size: %d\" %(lr, bs)))\n    \n            ### evaluate model on validation data\n            valid_ll.append( calc_log_likelihood(valid_x, valid_y, final_params) )\n            valid_acc.append( calc_accuracy(valid_x, valid_y, final_params) )\n\n    ### Get best model\n    best_model_idx = np.argmax(valid_acc)\n    best_params = all_params[best_model_idx]\n    test_ll = calc_log_likelihood(test_x, test_y, best_params) \n    test_acc = calc_accuracy(test_x, test_y, best_params) \n    print \"Validation Accuracies: \"+str(valid_acc)\n    print \"Test Accuracy: %f\" %(test_acc)\n\n    ### Plot \n    plt.figure()\n    epochs = range(max_epochs)\n    for idx, log in enumerate(all_train_logs):\n        plt.plot(epochs, log[0], '--', linewidth=3, label=\"Training, \"+log[1])\n        plt.plot(epochs, max_epochs*[valid_ll[idx]], '-', linewidth=5, label=\"Validation, \"+log[1])\n    plt.plot(epochs,  max_epochs*[test_ll], '*', ms=8, label=\"Testing, \"+all_train_logs[best_model_idx][1])\n\n    plt.xlabel(r\"Epoch ($t$)\")\n    plt.ylabel(\"Log Likelihood\")\n    plt.ylim([-.8, 0.])\n    plt.title(\"MNIST Results for Various Logistic Regression Models\")\n    plt.legend(loc=4)\n    plt.show()\n", "encoding": "ascii"}