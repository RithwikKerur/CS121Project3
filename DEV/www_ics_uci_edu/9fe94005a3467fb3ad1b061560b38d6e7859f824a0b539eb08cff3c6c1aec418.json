{"url": "https://www.ics.uci.edu/~dan/class/267P/datasets/calgary/paper6", "content": ".EQ\ndelim $$\ndefine <- ?< \"\\h'-0.5m'\" up 10 \"\\(em\" down 10 ?\ndefine gtorder ?\"\\z>\\d\\~\\u\"?\ndefine EXIST ?\"\\z\\-\\d\\z\\-\\r\\-\\d\\v'0.2m'\\(br\\v'-0.2m'\"?\ndefine ALL ?\"\\o'V-'\"?\ndefine 0M '0~...~M-1'\ndefine LH 'lo~...~hi'\ndefine RR 'bold R'\ndefine HH 'bold H'\ndefine KK 'bold K'\ndefine or '\"\\fBor\\fI\"~'\ndefine and '\"\\fBand\\fI\"~'\ndefine if '\"\\fBif\\fI\"~'\ndefine then '\"\\fBthen\\fI\"~'\ndefine else '\"\\fBelse\\fI\"~'\ndefine repeat '\"\\fBrepeat\\fI\"~'\ndefine until '\"\\fBuntil\\fI\"~'\ndefine while '\"\\fBwhile\\fI\"~'\ndefine do '\"\\fBdo\\fI\"~'\ndefine case '\"\\fBcase\\fI\"~'\ndefine end '\"\\fBend\\fI\"~'\ndefine begin '\"\\fBbegin\\fI\"~'\ndefine elseif '\"\\fBelseif\\fI\"~' \ndefine for '\"\\fBfor\\fI\"~'\ndefine From '\"\\fBfrom\\fI\"~'\ndefine To '\"\\fBto\\fI\"~'\ndefine exit '\"\\fBexit\\fI\"~'\n.EN\n.ls 1\t\n.ce\nCOMPACT HASH TABLES USING BIDIRECTIONAL LINEAR PROBING\n.sp 3\n.ce\nJohn G. Cleary\n.ce\nThe University of Calgary, Alberta, Canada.\n.sp 3\n.sp 20\n\\u1\\dAuthors Present Address: Man-Machine Systems Group, Department of\nComputer Science, The University of Calgary, 2500 University Drive NW\nCalgary, Canada T2N 1N4.\n.sp\n\\u2\\dThis research was supported by\nthe Natural Sciences and Engineering Research Council of Canada.\n.sp 2\n.ls 2\n.bp\nIndex Terms --  Searching, hash storage, open addressing, \nbidirectional linear probing,\naddress calculation, information retrieval, scatter storage, \nperformance analysis, memory compaction.\n.bp\n.pp\nAbstract -- An algorithm is developed which reduces the memory \nrequirements of hash tables.\nThis  is achieved by storing only\na  part of each key along with a few extra bits needed to ensure that\nall keys are stored unambiguously.  The fraction of each key stored\ndecreases as the size of the hash table increases.  Significant reductions\nin total memory usage can be achieved especially when the key size is not\nmuch larger than the size of a memory index and when only a small amount\nof data is stored with each key.\nThe algorithm is  based on \nbidirectional linear probing.\nSearch and insertion times are shown by simulation \nto be similar to those\nfor ordinary bidirectional linear probing.\n.bp\n.sh \"1 Introduction\"\n.pp\nThe retrieval of a single item from among many others is a common problem\nin computer science.  I am particularly concerned here with the case where \nthe item is retrieved on the basis of a single label\nor key attached to each entry and where the keys are not ordered in any\nparticular way.\nThere is a well known solution\nto this problem in the form of hash tables.\nKnuth [8], Knott [7] and Maurer and Lewis [11] provide good introductions to \nthis subject.\n.pp\nAn efficient version of hashing called\n.ul\nbidirectional linear probing \n(BLP),\nwas developed by Amble and Knuth [1].\nAs it forms the basis of what follows it is described in more detail in the\nfollowing section.  Section 3 shows how it can be modified so as to \nsignificantly reduce its memory requirements.  This is done by storing only\na small part of each key -- a few extra bits are needed to ensure \nthat different keys, that look the same after truncation, are correctly\ndistinguished.\n.pp\nThe execution time of this compact hashing algorithm is considered in\nSection 4.  It is shown by simulation to be \nsimilar to  ordinary BLP\nfor both successful searches and insertion.  It is significantly\nbetter for unsuccessful searches.  \n.pp\nA hashing scheme similar to compact hashing in that not all of the key is\nstored has been proposed by Andreae [2] (Chapter 1).  However, his technique \nhas a small but finite probability of retrieving an incorrect key.\nAlthough compact hashing\nis not based on this earlier technique it provided the impetus to\nseek the current solution.\n.pp\nIn hashing algorithms using an overflow area and a linked list of synonyms\nor by variations of this using buckets (see Maurer and Lewis [11]) only the\nremainder of each key need be stored.  This has been known since at least\n1965 (Feldman and Low [6] and Knuth [8] sec. 6.4, exercise 13, p543).  \nHowever, each entry (including the original hash location) requires a pointer\nto the next overflow record.  This pointer will about the same size as the\nreduction in the key size.  So, there is no net memory saving  over\nopen addressing techniques such as BLP.\n.pp\nAmongst the possible applications of compact hashing is the storage\nof trees and TRIES without the use of pointers but still preserving\na $log N$ retrieval time. \nIt is hoped to report on this application in more detail later.\n.pp\nPascal versions of the algorithms described below are available\nfrom the author.\n.sh \"2 Bidirectional linear probing.\"\n.pp\nI will now introduce the open addressing technique which forms the basis\nof compact hashing.\nThe \n.ul\nhash table\nin which the keys will be stored is an array $T[ 0M ]$ .  I will\nbe concerned only with the the keys themselves as the \nitems associated with each key do not \nsignificantly affect the algorithms.  In order to compute the location\nfor each key I will use two functions: $t$ which randomises the original\nkeys, and $h$ which computes a value in the range $0M$.  \n.pp\nLet $KK$ be the set of all possible keys and $HH$ be the set of all possible\ntransformed keys.  Then $t: KK -> HH$ is an invertible function.\nThis function is introduced\nto ensure that the keys stored are  random and so, as a consequence,\nthe hashing\nprocedure has a satisfactory\naverage performance.  In what follows these transformed\nkeys will be used rather than the original keys.  For example, it is the \ntransformed keys that are stored in $T$.  (-1 is used to indicate an unoccupied\nlocation in $T$.)\n.pp\n$h: HH ->\"{\" 0M \"}\"$ and has the \nproperty that for\n$H sub 1 ~, H sub 2 ~ \"\\(mo\" HH$\n$H sub 1 ~<=~ H sub 2~~ \"\\fBiff\\fP\"~~h(H sub 1 ) ~<=~ h(H sub 2 )$.  \nAs a consequence the keys will be mapped \ninto the hash table in the same order as the values of their transformed\nkeys.  \nThis ordering is essential to the compaction attained later.\nSuitable functions $t$ and $h$ have been extensively discussed \n(Carter and Wegman, [3]; Knott [7]; Lum, [9]; Lum, Yuen and Dodd, [10]).\nThese authors show that there are functions which almost always make\nthe distribution of transformed keys random.  I will not consider any\nparticular functions for $t$ although some examples of $h$ will be introduced\nlater.\n.pp\nTo retrieve a key, $K$, from the hash table the transformed key and the \nhash location are first computed.  If the (transformed) key stored at the\nhash location is greater than $t(K)$ then the table is searched upward \nuntil one of three things happen.  Either an empty location will be found,\n$T[j]=-1$, or the sought key will be found, $T[j]=t(K)$, or a key greater\nthan the sought key will be found, $T[j]>t(K)$.  If the first key examined\nis less than $t(K)$ then an analogous search is done down the hash table.\nThe search is successful if the sought key is found, that is\nif the last location examined is equal to $t(K)$, and is unsuccessful\notherwise.  (See Amble and Knuth [1] for the details of this algorithm).\n.pp\nFor a given set of keys there are many ways that they can be arranged in $T$\nso that the search algorithm above will  still work correctly.\nThere is thus\nfreedom, when designing an algorithm to insert new keys, to choose different \nstrategies for positioning the keys.\nThere are two conditions that must be satisfied when a new key is inserted.\nOne is that all keys in the memory must remain in ascending order\nand the other is that there must be no empty locations between the original hash\nlocation of any key and its actual storage position.  These imply that all\nkeys sharing the same initial hash location must form a single unbroken group.\n.pp\nWithin these constraints one would like to insert a new key so as to minimise \nlater retrieval times and the time to do the insertion itself.  Intuitively\nkeys which share the same initial hash location should be centered around that\ninitial address.  There are two ways of inserting keys which cause little\ndisturbance to the memory.  One is to find the position where the key should\nbe placed according to its ordering and then to create a gap for it by\nmoving \n.ul\nup \nall entries from this position up to the next empty location.  The second way is\nsymmetric to this and creates a gap by moving entries \n.ul\ndown \none location.\nThe insertion algorithm given by  Amble and Knuth [1] chooses which of these\ntwo moves to make using a strategy which is  guaranteed to minimise the number\nof locations in $T$ which are examined during later successful or unsuccessful\nsearches, although it is not guaranteed to minimise the insertion time itself.\n.pp\nOne  consequence of this insertion strategy is that sometimes it is necessary\nto move entries below 0 and above $M$ in the array $T$.  One solution to this\nwould be to make the array circular and move entries from 0 to $M-1$ and\nvice versa.  However, following Amble and Knuth [1], I will instead extend\nthe array $T$ and other arrays to be defined later at their top and bottom.\nThis gives 'breathing room' for the array to expand.  An extra 20 entries\nat the top and bottom were found to be quite sufficient for all\nthe simulation runs reported in Section 4.  Accordingly I will define\n$lo ~=~-20$ and $hi~=~M+19$ and define the array $T$ over the range\n$lo$ to $hi$.\n.sh \"3 Compact Hashing Using Bidirectional Linear Probing\"\n.pp\nI will now show that the memory required to store the keys in BLP can be\nsignificantly reduced.  First consider the case when\nthe number of possible keys in $KK$ is less than $M$, then every possible key\ncan be assigned its own location in $T$ without possibility of collision.\nIn this case $T$ degenerates to an ordinary indexed array and the keys need\nnever be stored.  At worst a single bit might be needed to say whether\na particular key has been stored or not.  This raises the question of whether\nit is necessary to hold the entire key in memory if the key space $KK$ is slightly\nlarger than $M$.  For example if $KK$ were, say, four times larger than $M$\nthen it might be possible to hold only two bits of the key rather than the entire\nkey.  The reasoning here is that the main function of the stored keys is to\nensure that entries which collide at the same location can be correctly\nseparated.\nProvided $h$ is suitably chosen at most four keys can be mapped to a \nsingle location.  The two bits might then be sufficient to store four\ndifferent values for these four keys.  It is in fact \npossible to realise this\nreduction in stored key size although a fixed amount of extra information \nis needed\nat each location in order to correctly handle collisions.\n.pp\nSo that I can talk about the part of the key which is in excess of the\naddress space I will now introduce a \n.ul\nremainder function\n$r$.  $r$ maps from the transformed keys $HH$ to a set of remainders \n$RR~==~\"{\"0,~1,~2,~...,~Rm-1\"}\"$.  \nIt  is these remainders that will be stored in lieu\nof the transformed keys.  \nThe essential property\nof $r$ is that $r(H)$ and $h(H)$ together are sufficient to uniquely \ndetermine $H$.  \n.pp\n.ne 9\nFormally,\n.sp\n\t$RR ~~==~~ \"{\"0,~1,~2,~...,~Rm-1\"}\"$\n.sp\n\t$r: HH -> RR$\n.sp\nand\t$h( H sub 1 )~=~h( H sub 2 )~and~r( H sub 1 )~=~r( H sub 2 )\n~~ \"\\fBiff\\fP\" ~~ H sub 1 ~~=~~ H sub 2$ .\n.sp\nFor a given function $h$ there are usually many possible functions $r$.\nOne particularly simple pair of functions, referred to by Maurer and Lewis [10]\nas the \n.ul\ndivision method, \nis $h(H)~~=~~ left floor^ H/Rm right floor$ and\n$r(H)~~=~~ H~ \"\\fBmod\\fP\"~Rm$ . \n.sp\nWhen $r$ is defined as above and $Rm$ is between $2 sup d$ and $2 sup d+1$ \nthe number of bits needed to \nspecify a remainder is the number of bits in the key less $d$.\n.pp\nConsider a new array\n$R [ LH ]$ into which the remainders will be stored.   \nIn what follows $R$ will be kept in place of $T$ but it will be useful to\ntalk about $T$ as if it were still there.  $R$ and the additional arrays to\nbe introduced shortly specify just the information in $T$, albeit\nmore compactly.  Each value $R [i]$ will hold the value $r(T[i])$ with the\nexception that when $T[i]$ is $-1$ (marking an empty location) then $R[i]$\nis also set to $-1$.  If\nthere have been no collisions then each $R[i]$ paired with the value $i$\nunambiguously gives the transformed key that would have been stored in $T[i]$.\nHowever, if there have been collisions it is not possible\nto tell if a value of $R[i]$ is at its home location or if it has been moved\nfrom, say, $i-1$ and corresponds to a key, $H$, where $r(H)~=~ R[i]$ and $h(H)~=~i-1$.\nIf there were some way to locate for each $R[i]$ where it was originally \nhashed then the original keys could all be unambiguously determined.\nThis can be done by maintaining two extra arrays of bits, the virgin array $V$,\nand the change array $C$.\n.pp\nThe virgin array\n$V[ LH ]$ marks those \nlocations which have never been hashed to.  That is, $V[i]$ has a value of $1$\nstored if any of the stored keys in the hash table has $i$ as its hash\naddress, and $0$ otherwise.  $V$ is maintained by initialising it to $0$\nand thereafter setting $V[h(H)] <-~1$ whenever a key $H$ is inserted in the\nmemory.  The virginity of a location is unaffected by the move operations\nduring insertion.\nThe $V$ array is similar to the array of pass bits recommended in [1].\n.pp\nTo understand the change array $C[ LH ]$ it is necessary to look more closely\nat the distribution of values of $R[i]$.  These remainders can be grouped \naccording to whether or not they share the same original hash address.\nAlso recall that the hash table, as in BLP, is ordered, so,\nall the remainders in a particular group will occur at \nconsecutive locations. \nThe change bits $C[i]$ are used to delimit the \nboundaries of these groups.  This is done by marking the first remainder\n(the one stored at the lowest address) of each group with a $1$.  All other \nmembers of a group have $C[i]=0$.  To simplify the search and insertion\nalgorithms it is also convenient to set $C[i]$ to 1 for all locations\nwhich are empty ($R[i]=-1$).\nThus we have the formal definitions of the\nvalues of $V$ and $C$ in terms of the now notional array $T$ (the array\n$A$ is described later):\n.bp\n.nf\n.ls 1\n.ta 0.5i +0.75i +0.9i\n\t\t\\(lt\\|$r(T[i])$\t$T[i] != -1$\n\t$R[i]~~==~~$\t\\(lk\\|\n\t\t\\(lb\\|$-1$ \t$T[i]=-1$\n.sp\n\t\t\\(lt\\|$1\tEXIST~ j~h(T[j])=i$\n\t$V[i]~~==~~$\t\\(lk\\|\n\t\t\\(lb\\|$0$\totherwise\n.sp\n\t\t\\(lt\\|$1\tT[i] != T[i-1]~ roman or ~T[i]=-1$\n\t$C[i]~~==~~$\t\\(lk\\|\n\t\t\\(lb\\|$0$\totherwise\n.sp 2\n\t\t\\(lt\\|$a(i)\t-Na <= a(i) <= Na$\n\t$A[i]~~==~~$\t\\(lk\\|\n\t\t\\(lb\\|$inf$\totherwise\n.sp\n\twhere\n.sp\n\t\t$Na ~>=~ 0$\n.br\n\t\t$a(i)~==~ sum from j=lo to i |C[j]=1~\"and\"~R[j] != -1|~-~\nsum from j=lo to i V[j]$\n.fi\n.ls 2\n.ta 0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i \n.rh \"Searching.\nFor every group of remainders there will somewhere be a $V$ bit equal to $1$ \nand a $C$\nbit  at a non-empty location equal to $1$.  That is,\nfor every $V$ bit which is $1$ there is a corresponding $C$ bit \nwhich is also $1$.\n.FC \"Fig. 1.\"\nThis correspondence is indicated in \nFig. 1 by the dotted lines.  When searching for a key $H$ in the table\nthe location $h(H)$ is examined.  If the $V$ bit is $0$ then the search \ncan stop\nimmediately.  Otherwise a search is made for the corresponding $C$ bit \nwhich is $1$.  To do this a search is made down (or up) the hash table until\nan empty location is found.  The number of $V$ bits which are $1$\nfrom $h(H)$ to this empty\nlocation are counted.  The correct $C$ bit is then found by counting back\nup (or down) the array from the empty location\nfor the same number of $C$ bits which are $1$.  Details of this algorithm\nfollow.\n.ls 1\n.sp \n.nf\n.ta 1.5c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c\n.sp\n.ne 2\nStep 1:\t{initialise variables}\n\t$H <-~ t(K);~~j <-~ h(H);~~rem <-~ r(H);~i <-~ j;~~count <-~ 0;$\n\t{check virgin bit}\n\t$if~ V[j]=0~then$ {search unsuccessful} $exit ;$\n.sp\n.ne 3\nStep 2:\t{find first empty location down the table}\n\t$while ~R[i] != -1~do~~begin~~count <-~count - V[i];~i <-~ i-1 ~end ;$\n.sp\n.ne 4\nStep 3:\t{search back to find uppermost member of relevant group}\n\t$while count < 0 ~do~begin~ i <-~i+1;~count <-~count +C[i];~end ;$\n\t{$i$ now points at the first(lowest) member of the group associated}\n\t{with the original location $j$}\n.sp\n.ne 6\nStep 4:\t{search group associated with $j$}\n\t$while R[i+1] <= rem ~and C[i+1]=0~do i <-~i+1 ;$\n\t{check last location to see if key found}\n\t$if R[i]=rem~ mark then$ {search successful}\n\t$lineup            else$ {search unsuccessful} ;\n.sp 2\n.ls 2\n.fi\n.pp\nAn example search is illustrated in Fig. 1 for the key 75.\nFor this example $h$ is computed by dividing by 10 and rounding down, \n$r$ is computed by taking the remainder modulo 10.  \n.br\nStep 1: The initial hash location\nfor 75 is 7 and its remainder is 5.  The $V$ bit at location 7 is 1 so the \nsearch continues.\n.br\nStep 2:\nThe first empty location found by searching down the table is at location 3.\nThere are three $V$ bits with a value of 1 between 7 and 3 at locations \n4, 6 and 7.\n.br\nStep 3:\nCounting back from location 3 three $C$ bits are 1 at locations 4, 5 and 8.\nSo the $C$ bit at location 8 corresponds to the $V$ bit at the \noriginal hash location 7.\n.br\nStep 4:\nThe group of remainders which share the same initial location 7 can then be \nfound in locations 8 and 9.  Thus the remainder 5 at location 8 can be\nunambiguously associated with the original key 75 and so it can be\nconcluded that the information associated with the key 75 is present \nat location 8 in the memory.\n.pp\nIt still remains to specify the update\nalgorithm and to address some issues of efficiency.  To this end a third\narray will be added.\n.rh \"Speeding up search.\"\nIt was found during the simulations reported in Section 4 \nthat the most time consuming element of this search\nis step 2 when the table is scanned for an empty location.  The essential\nrole played by the empty locations here is to provide a synchronisation\nbetween the 1 bits in the $V$ and $C$ arrays. \nThis lengthy search could be eliminated by maintaining two additional arrays,\n$#C[ LH ]$ and $#V[ LH ]$, which count from the start of memory the number of \n$C$ and $V$ bits which are 1.  That is:\n.br\n\t$#C[i] ~==~ sum from j=lo to i |C[j]=1~and~R[j] != -1 |$\n.br\nand\t$#V[i] ~==~ sum from j=lo to i V[j]$ .\n.br\n.pp\nIn order to find the $C$ bit corresponding to some $V[i]=1$ then all that \nis necessary is to compute the difference $count <-~#C[i]-#V[i]$.  \nIf $count$ is zero then the remainder stored at $i$ was originally\nhashed there and has not been moved.  If $count$ is positive then it is \nnecessary to scan down the memory until $'count'$ $C$ bits equal to 1 have been \nfound.  If $count$ is negative then it is necessary to scan up the memory\nuntil $'-count'$ $C$ bits which are 1 have been found.  Fig. 2 shows some\nexamples of the various situations which can arise.\n.FC \"Fig. 2.\"\n.pp\nIn fact, it is not necessary to store $#C$ and $#V$ explicitly, it is \nsufficient merely to store the differences $#C[i]-#V[i]$.  To do this the\n.ul\nAt home\narray, $A[ LH ]$, will be used.\n.pp\nAt this point it might seem that all earlier gains have been lost because\nin the most extreme case $#C[i]-#V[i]~=~M$.  To store a value of $A$\nwill require as many bits as a memory index -- precisely the gain made by\nstoring remainders rather than keys!\\   However, all is not lost.  The values \nof $A$ tend to cluster closely about 0.  Simulation\nshows that a hash memory which is 95% full has 99% of the $A$ values\nin the range -15 to +15.  Therefore the following strategy can be\nadopted.  Assign a fixed number of bits for storing each value of $A$, say\n5 bits.  Use these bits to represent the 31 values -15 to +15 and a 32nd\nvalue for $inf$.  Then anywhere that $#C[i]-#V[i]~<~-15~\"or\"~>+15$ assign $inf$\nto $A[i]$ otherwise assign the true difference.\n.pp\nWhen searching for a key a scan can now be done down (or up) the memory\nuntil a location $i$ where $A[i] != inf$ is found.  (At worst this will occur\nat the first unoccupied location where $A[i]$ will be zero.)\\  From there\na count can be made up (or down) the memory for the appropriate number of\n$C$ bits which are 1.\n.pp\nIn the detailed algorithm given below some differences from the simpler search\ncan be noted.\nIn step 3, $count$ can be both\npositive and negative.  Therefore code is included to scan both up and down\nthe memory as appropriate.  At the end of step 3, $i$ can be pointing at any\nmember of the group associated with the original hash location.  (Above\n$i$ was always left pointing at the lowest member of the \ngroup.)\\    Therefore code is included for scanning both up and down the\nmembers of the group.  In order to prevent redundant checking of locations\nby this code a flag $direction$ is used.  It can take on three values\ndepending on the direction of the memory scan: $\"up\"$, $\"down\"$, and $here$\n(no further searching need be done).\n.ls 1\n.sp \n.nf\n.ta 1.5c +1.45c +1.45c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c\n.sp\n.ne 2\n{Search using at-home count}\nStep 1:\t{initialise variables}\n\t$H <-~ t(K);~~j <-~ h(H);~~rem <-~ r(H);~~i <-~ j;~~count <-~ 0;$\n\t{check virgin bit}\n\t$if~ V[j]=0~then$ {search unsuccessful} $exit ;$\n.sp\n.ne 5\nStep 2:\t{find first well defined $A$ value down the memory}\n\t$while ~A[i] = inf~do~begin~count <-~count - V[i];~i <-~i-1 ~end ;$\n\t$count <-~count +A[i];$\n.sp\n.ne 16\nStep 3:\t{Search either up or down until a member of sought group is found}\n\t{Also ensure $direction$ is set for Step 4.}\n\t$if count < 0 ~then$\n\t\t$direction <-~\"up\";$\n\t\t$repeat i <-~i+1;~count <-~count +C[i]~ until count = 0 ;$\n\t\t$if R[i] ~>=~ rem ~then direction <-~here;$\n\t$else if count > 0 ~then$\n\t\t$direction <-~\"down\";$\n\t\t$repeat ~count <-~count -C[i];~i <-~i-1~ until count = 0 ;$\n\t\t$if R[i] ~<=~ rem ~then direction <-~here;$\n\t$else${$count = 0$}\n\t\t$if R[i] > rem ~then direction <-~\"down\"$\n\t\t$else if R[i] < rem ~then direction <-~\"up\"$\n\t\t$else direction <-~here ;$\n.sp\n.ne 16\nStep 4:\t{search group associated with $j$}\n\t$case direction~ \"\\fBof\\fP\"$\n\t$here:\t;${do nothing}\n\t$\"down\":\trepeat\tif C[i] = 1 ~then direction <-~here$\n\t\t\t$else$\n\t\t\t$begin$\n\t\t\t\t$i <-~i-1;$\n\t\t\t\t$if R[i] ~<=~ rem ~then direction <-~here;$\n\t\t\t$end$\n\t\t$until direction = here ;$\n\t$\"up\":\trepeat\tif C[i+1] = 1 ~then direction <-~here$\n\t\t\t$else$\n\t\t\t$begin$\n\t\t\t\t$i <-~i+1;$\n\t\t\t\t$if R[i] ~>=~ rem ~then direction <-~here;$\n\t\t\t$end$\n\t\t$until direction = here ;$\n\t$end ;$\n.sp\n.ne 4\nStep 5:\t{check last location to see if key found}\n\t$if R[i]=rem~ mark then$ {search successful}\n\t$lineup            else$ {search unsuccessful} ;\n.sp 2\n.ls 2\n.fi\n.FC \"Fig. 3.\"\n.pp\nFig. 3, gives an example of this searching algorithm.\nThe same memory and key (75) as in Fig. 1 are used.  For the\npurposes of the example each $A$ value is allocated one bit.  This allows \nonly two values 0 and $inf$.  The search proceeds as follows:\n.br\nStep 1: The initial hash location\nfor 75 is 7 and its remainder is 5.  The $V$ bit at location 7 is 1 so the \nsearch continues.\n.br\nStep 2: \nThe first $A$ value not equal to $inf$ found by searching down the table \nis at location 6.\nThere is one $V$ bit with a value of 1 between 7 and 6, at location 7.\n$count$ is then set to $A[6]+1~=~1$.  So on the next step one $C$\nbit will be sought.\n.br\nStep 3:\nCounting back up from 6 the first $C$ bit equal to 1 is at location 8.\nSo the $C$ bit at location 8 corresponds to the $V$ bit at the \noriginal hash location 7.\n.br\nStep 4:\nThe group of remainders which share the same initial location 7 can then be \nfound in locations 8 and 9.  The remainder 5 at location 8 can thus be\nunambiguously associated with the original key 75 and it can be\nconcluded that the information associated with the key 75 is present \nat location 8 in the memory.\n.rh \"Insertion.\"\nInsertion of a new key into the memory requires three distinct steps:\nfirst locating whereabouts in the memory the key is to be placed;\nsecond deciding how the memory is to be rearranged to make room for the new\nkey; and third moving the remainders whilst correctly preserving the\n$A$ and $C$ values.  (The $V$ bits remain fixed during the move.)\\  \nThe initial search can be done as explained above with the small addition that\nthe correct insertion point must still be located when the key is not present.\nThe second and third steps follow the algorithm in Amble and Knuth [1]\nwith the addition that the values of the $A$ array must be re-calculated\nover the shifted memory locations and the $C$ but not the $V$ bits must\nbe moved with the keys.  \nDetails of this can be found in an earlier draft of this paper, [4].\n.sh \"4 Performance\"\n.pp\nNow I consider how long these algorithms will take to run.  The measure of \nrun time that I will use is the number of \n.ul\nprobes\nthat each algorithm makes, that is, the number of times locations in the \nhash table are examined or updated.  \nCPU time measures were taken as well and correlate well with the empirical \ncounts of probes given below.\n.FC \"Table I\"\n.FC \"Table II\"\n.rh \"Searching.\"  \nTables I and II list the results of simulations\nfor successful and unsuccessful searches respectively.  Results are tabulated\nfor ordinary BLP and for compact hashing with  \ndifferent memory loadings and different sizes for\nthe $A$ field.  If the number of keys stored\nin the memory is $N$ then the memory loading is measured by \n$alpha ~==~N/M$, the fraction of locations in the memory which are full. \nValues of\nNa were chosen to correspond to $A$ field lengths of 1, 2, 3,\n4, and 5 bits, that is for Na equal to 0, 1, 3, 7, and 15 respectively,\nand also for the case where no $A$ field was used.\nIncreasing the size of the $A$ field beyond 5 bits had no effect at\nthe memory loadings investigated.  So Na equal to 15 is effectively the\nsame as an unbounded size for the $A$ values.  \n.pp\nThe insertion procedure is \nguaranteed to be optimum only for BLP, not for compact hashing.  If none\nof the values in $A$ is $inf$ then the sequence of locations examined by\ncompact\nhashing is the same as for BLP and so the strategy will still be optimum.\n(This is easily seen by noting that in compact hashing\n$A[h(t(K))]$ determines the direction\nof the search depending on whether it is positive or negative.  During the \nsubsequent search no\nlocations past the sought key will be probed.  This is exactly the same\nprobing behaviour as in BLP.)\\ \nHowever, if no $A$ array is being used or if some values of $A$ are $inf$\nthen extra locations need to be probed to find an empty location or one which\nis not equal to $inf$.\n.pp\nAs expected the figures in Table I show that for Na at 15 and using optimum\ninsertion the probes for a successful search are almost the same as for BLP.\n(The small differences are accounted for by statistical fluctuations\nin the simulation results.)\\   \n.pp \nAs Na is decreased the number of probes needed for searching increases.\nThis\nreflects the greater distances that must be traversed to find a value of \n$A$ not equal to $inf$.  It is notable however that even a single bit allocated\nto the $A$ fields dramatically improves the performance.  Even at a\nmemory density of 0.95 some 25% of non-empty locations have $A$ values of 0.\n.pp\nThe pattern for unsuccessful searches is broadly the same as sketched above\nfor successful searches except that in general unsuccessful searches\nare quicker than successful ones.  This is a result of the $V$ bits\nwhich allow many unsuccessful searches to be stopped after a single probe. \nFor example even at the maximum possible memory density of 1 some 36% of\n$V$ bits are zero.  This results in compact hashing being faster than\nthe reported values for ordinary BLP.  \nHowever, unsuccessful BLP searches could be\nimproved to a similar degree by incorporating $V$ bits.\n.FC \"Table III\"\n.rh \"Insertion.\"\nThe probes to insert a new key can be broken down into three components,\nthose needed to locate the position where the key is to be inserted,\nthose to decide the direction of movement \nand those to effect the movement of the memory.\nThe first of these will be slightly larger than\na successful search and so the results of Table I have not been repeated.\nThe second two are independent of Na as they are dependent only on\nthe lengths of blocks of adjacent non-empty locations.  The values\nfor these Na independent components are listed in Table III.\nIn most cases\nthis Na independent component is much larger than the search component.\nThe exception occurs \nwhere no $A$ values are being used, when the two components\nare comparable.\n.pp\nCleary [5] examines a random insertion strategy for ordinary BLP\nwhere blocks of entries in the hash table are moved in a randomly chosen\ndirection\nto accomodate a\nnew entry rather than in the optimum way described by \nAmble and Knuth [1].\nIt is shown that this strategy can\nimprove insertion times by a factor of 4 at the expense of small degradations\n(at most 15%) in retrieval times.  These\nresults are shown by simulation to extend to compact hashing.  \nIndeed for small values of\nNa the optimum and random strategies show no significant differences in\nretrieval times.\n.rh \"Analytic approximation.\"\nWhile analytic results are not available for the number of probes \nneeded for retrieval or insertion an\napproximation can be developed for some of the cases.  It is shown by\nAmble and Knuth [1] and Knuth [8] (problem 6.4-47) that the average\nlength of a block of consecutive non-empty locations when using\nthe optimum insertion strategy is approximately\n$(1- alpha ) sup -2 ~-~1$.  \nLet this block length be $L$.  \n.pp\nConsider the case of a successful search when no $A$ field is used.\nA successful scan of a block from an arbitrary\nposition to the end takes on average $L/2~+~1/2$ probes.  \nDuring the initial scan down the memory in the simulations the initial check of the\n$V$ bit and the final empty location examined were each counted as a single probe.\nThis gives a total of $L/2~+~5/2$ probes for the initial scan down. (This is not\nexact because there will be a correlation between the position \nof a key's home location within a block \nand the number of keys hashing to that home location).\nThe scan back up a block will take $L/2~+1/2$ probes (exact for a successful search).\nThis gives $(1- alpha ) sup -2 +2$ for the expected\nnumber of probes during a successful search.  These values are listed in Table I\nand are consistently low by about 10%.\n.pp\nFor an unsuccessful search with no $A$ field the initial scan down the \nmemory will take $L/2~+5/2$ probes as above (again this will not be exact because\nthe probability of a $V$ bit being one will be correlated with the \nsize of a block and its\nposition within the block).\nAn unsuccessful scan of a block takes $L/2~+~1/2$ probes.  (This assumes\nthe keys in the block are distributed uniformly.  \nThis gives the following probabilities that the search will stop at a \nparticular location in the block: the first location, $1/2L$; locations 2 \nthrough $L$, $1/L$; the empty $(L+1)$st location, $1/~2L$.\nThis will not be true for compact hashing because the probability of stopping at a key\nwhich shares its home location with a large number of other keys will be smaller than\nfor one which shares it with few others.)\\ \\ Summing these two terms gives $L~+~7/2$\nprobes.\nGiven that the keys are distributed randomly there is a probability of \n$e sup {- alpha}$ that a given $V$ bit will be zero.  So the expected number \nof probes overall for an unsuccessful search is \n$e sup {- alpha}~+~(1-e sup {- alpha}) cdot ((1- alpha ) sup -2 + 5/2)$.\nThese values are listed in Table II and are consistently low by about 5%.\n.pp\nConsidering only the insertion component which is independent of Na then\nit is possible to derive an expression for the number of probes.\nThere is an initial\nscan to move the memory down and insert the new key which will scan about half \nthe block ($L/2~+~5/2$ probes) \nand a subsequent scan back up of the entire block ($L~+~1$ probes).  \nEmpirically the probability\nthat the entire block will subsequently be moved back up is a half which gives\nan expected $1/2(L~+~1)$ probes.\nSumming these three contributions gives $2(1- alpha ) sup -2 ~+~2$\nas the expected number of probes for an insertion (excluding the search time).\nValues for this expression are tabulated  in Table III, they are in good \nagreement with the empirical values.\n.sh \"Acknowledgements\"\n.pp\nI would like to thank Ian Witten for careful checking of a draft version.\nAlso John Andreae for discussions which showed that something like compact\nhashing might be possible.\n.sh \"References\"\n.ls 1\n.LB \"[6]    \"\n.sp\n.NI \"[1]  \"\n[1]\\ \\ O.\\ Amble and D.\\ E.\\ Knuth, \"Ordered Hash Tables,\"\n.ul\nComputer Journal,\nvol. 17, pp135-142, 1974.\n.sp\n.NI \"[1]  \"\n[2]\\ \\ J.\\ H.\\ Andreae,\n.ul\nThinking with the teachable machine.\nLondon: Academic Press, 1977.\n.sp\n.NI \"[1]  \"\n[3]\\ \\ J.\\ L.\\ Carter and M.\\ N.\\ Wegman, \"Universal classes of hash \nfunctions,\"\n.ul\nJ. Computer System Sci.,\nvol. 18, pp143-154, 1979.\n.sp\n.NI \"[2]  \"\n[4]\\ \\ J.\\ G.\\ Cleary, \"Compact hash tables,\"\nResearch Report, 82/100/19,\nDepartment of Computer Science, University of Calgary, July 1982.\n.sp\n.NI \"[3]  \"\n[5]\\ \\ J.\\ G.\\ Cleary, \"Random insertion for bidirectional linear probing\ncan be better than optimum,\" \nResearch Report, 82/105/24,\nDepartment of Computer Science, University of Calgary, September 1982.\n.sp\n.NI \"[5]  \"\n[6]\\ \\ J. A. Feldman and J. R. Low, \"Comment on Brent's Scatter Storage \nAlgorithm,\"\n.ul\nCACM,\nvol. 16, p703, 1973.\n.sp\n.NI \"[7]  \"\n[7]\\ \\ G. D. Knott, \"Hashing functions,\"\n.ul\nThe Computer Journal,\nvol. 18, pp265-278, 1975.\n.sp\n.NI \"[7]  \"\n[8]\\ \\ D.\\ E.\\ Knuth, \n.ul\nThe art of computer programming:Sorting and searching.\nVol III.\nReading, Massachusetts: Addison Wesley, 1973.\n.sp\n.NI \"[8]  \"\n[9]\\ \\ V.\\ Y.\\ Lum, \"General performance analysis of key-to-address \ntransformation methods using an abstract file concept,\"\n.ul\nCACM,\nvol. 16, pp603-612, 1973.\n.sp\n.NI \"[12]  \"\n[10]\\ \\ V.\\ Y.\\ Lum,\\ P.\\ S.\\ T.\\ Yuen and M.\\ Dodd, \"Key-to-address\ntransformation techniques,\"\n.ul\nCACM,\nvol. 14, pp228-239, 1971.\n.sp\n.NI \"[13]  \"\n[11]\\ \\ W. D. Maurer and T. G. Lewis, \"Hash table methods,\"\n.ul\nComp. Surveys,\nvol. 7, pp5-19, 1975.\n.ls 2\n.in 0\n.bp 0\n\\&\\ \n.RF\n.ta 0.5i +0.75i +0.75i +0.75i +0.75i +0.75i\n.nf\n\n\t$i\tT[i]\tR[i]\tV[i]\tC[i]$\n\t\\l'3.5i'\n.br\n\n\t12\t\\0\\ -1\t\\ -1\t0\t1\n.br\n\t11\t101\t\\01\t0\t1\n.br\n\t10\t\\087\t\\07\t1\t1\n.br\n\t\\09\t\\076\t\\06\t0\t0\n.br\n\t\\08\t\\075\t\\05\t1\t1\n.br\n\t\\07\t\\067\t\\07\t1\t0   \n.br\n\t\\06\t\\066\t\\06\t1\t0\n.br\n\t\\05\t\\065\t\\05\t0\t1\n.br\n\t\\04\t\\041\t\\01\t1\t1\n.br\n\t\\03\t\\0\\ -1\t\\ -1\t0\t1\n.br\n\t\\02\t\\019\t\\09\t0\t0\n.br\n\t\\01\t\\018\t\\08\t1\t0\n.br\n\t\\00\t\\016\t\\06\t0\t1\n.br\n\t\t\t\t\t     Step 1 Step 2 Step 3 Step 4\n.br\n\n\t$h(H)~=~ left floor^ H/10 right floor$\n.br\n\n\t$r(H)~=~ H~ roman mod ~10$\n.br\n\n.FG \"\"\n\n.bp 0\n\\&\\ \n.RF\n.nf\n.ta 0.5i +0.75i +0.75i +0.75i +0.75i\n\t$count~=~A[i]~=~#C[i]-#V[i]$\n.sp\n\t$count = 0$\t\t\t$count = 0$\n\t$C$\t$V$\t\t$C$\t$V$\n\t0\\|\\(rt\t1\t\t0\\|\\(rt\t1\n\t0\\|\\(rk\t0\t\t0\\|\\(rk\t1$<-~i$\n\t1\\|\\(rb\t1$<-~i$\t\t1\\|\\(rb\t0\n.sp\n\t$count =1>0$\t\t$count = 2 > 0$\n\t$C$\t$V$\t\t$C$\t$V$\n\t0\t1$<-~i$\t\t0\t1$<-~i$\n\t1\t0\t\t1\t0\n\t0\\|\\(rt\t1\t\t1\t1\n\t0\\|\\(rk\t0\t\t0\\|\\(rt\t0\n\t1\\|\\(rb\t0\t\t0\\|\\(rk\t0\n\t\t\t\t1\\|\\(rb\t0\n.sp\n\t$count =-1<0$\n\t$C$\t$V$\n\t0\\|\\(rt\t0\t\t\t\\|\\(rt\n\t0\\|\\(rk\t0\t\t\t\\|\\(rk\\ \\ Group of entries which hash to \n\t1\\|\\(rb\t0\t\t\t\\|\\(rb\\ \\ location i\n\t0\t0\n\t1\t1$<-~i$\t\t\t\\ \\ \\ Corresponding $C$ and $V$ bits\n.FG \"\"\n.bp 0\n\\&\\ \n.RF\n.ta 0.5i +0.5i +0.5i +0.5i +0.5i +0.9i +0.6i +0.4i\n$i\tR[i]\tV[i]\tC[i]\t#V[i]\t#C[i]~~#C[i]-#V[i]\tA[i]$\n.br\n\\l'4.5i'\n.br\n12\t\\ -1\t0\t1\t6\t6\t\\00\t0\n.br\n11\t\\01\t0\t1\t6\t6\t\\00\t0\n.br\n10\t\\07\t1\t1\t6\t5\t\\ -1\t$inf$\n.br\n\\09\t\\06\t0\t0\t5\t4\t\\ -1\t$inf$\n.br\n\\08\t\\05\t1\t1\t5\t4\t\\ -1\t$inf$\n.br\n\\07\t\\07\t1\t0\t4\t3\t\\ -1\t$inf$\n.br\n\\06\t\\06\t1\t0\t3\t3\t\\00\t0\n.br\n\\05\t\\05\t0\t1\t2\t3\t\\01\t$inf$\n.br\n\\04\t\\01\t1\t1\t2\t2\t\\00\t0\n.br\n\\03\t\\ -1\t0\t1\t1\t1\t\\00\t0\n.br\n\\02\t\\09\t0\t0\t1\t1\t\\00\t0\n.br\n\\01\t\\08\t1\t0\t1\t1\t\\00\t0\n.br\n\\00\t\\06\t0\t1\t0\t1\t\\01\t$inf$\n.br\n\t\t\t\t\t\t\t\tStep 1 Step 2 Step 3 Step 4\n.sp \nNote: \tOnly one bit has been allowed for the values of $A$. \n.br\n\tSo the only two possible values are 0 and $inf$.\n\n.FG \"\"\n.bp 0\n\\&\\ \n.RF\n.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i\n.ce\nSuccessful Search\n\t\\l'4i'\n\n\t$alpha$\t\\0.25\t\\0.5\t\\0.75\t\\0.8\t\\0.85\t\\0.9\t\\0.95\n\t\\l'4i'\n\t\n\t$BLP sup 1$\t\\0\\01.1\t\\0\\01.3\t\\0\\01.7\t\\0\\02.0\t\\0\\02.3\t\\0\\02.9\t\\0\\04.2\n\t\n\tNa\n\t15\t\\0\\01.1\t\\0\\01.3\t\\0\\01.7\t\\0\\01.9\t\\0\\02.2\t\\0\\02.8\t\\0\\04.6\n\t\\07\t\\0\\01.1\t\\0\\01.3\t\\0\\01.7\t\\0\\01.9\t\\0\\02.2\t\\0\\02.8\t\\0\\09.7\n\t\\03\t\\0\\01.1\t\\0\\01.3\t\\0\\01.7\t\\0\\01.9\t\\0\\02.4\t\\0\\04.2\t\\025\n\t\\01\t\\0\\01.1\t\\0\\01.3\t\\0\\02.0\t\\0\\02.5\t\\0\\04.1\t\\0\\08.8\t\\045\n\t\\00\t\\0\\01.1\t\\0\\01.5\t\\0\\03.3\t\\0\\04.9\t\\0\\07.9\t\\015\t\\061\n\t\\0-\t\\0\\04.2\t\\0\\07.1\t\\020\t\\030\t\\049\t110\t370\n\t\\0*\t\\0\\03.77\t\\0\\06.00\t\\018.0\t\\027.0\t\\046.4\t102\t402\n\n\t\t$\\& sup 1~$Taken from Amble and Knuth [1].\n\t\t- No $A$ field used.\n\t\t* Analytic approximation to line above.\n.FG \"\"\n.bp 0\n\\&\n.RF\n.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i\n.ce\nUnsuccessful Search\n\t\\l'4i'\n\n\t$alpha$\t\\0.25\t\\0.5\t\\0.75\t\\0.8\t\\0.85\t\\0.9\t\\0.95\n\t\\l'4i'\n\n\t$BLP sup 1$\t\\0\\01.3\t\\0\\01.5\t\\0\\02.1\t\\0\\02.3\t\\0\\02.6\t\\0\\03.1\t\\0\\04.4\n\n\tNa\n\t15\t\\0\\01.2\t\\0\\01.4\t\\0\\01.8\t\\0\\01.9\t\\0\\02.1\t\\0\\02.4\t\\0\\03.5\n\t\\07\t\\0\\01.2\t\\0\\01.4\t\\0\\01.8\t\\0\\01.9\t\\0\\02.1\t\\0\\02.4\t\\0\\09.7\n\t\\03\t\\0\\01.2\t\\0\\01.4\t\\0\\01.8\t\\0\\01.9\t\\0\\02.2\t\\0\\03.3\t\\015\n\t\\01\t\\0\\01.2\t\\0\\01.4\t\\0\\01.9\t\\0\\02.2\t\\0\\03.2\t\\0\\06.0\t\\028\n\t\\00\t\\0\\01.2\t\\0\\01.5\t\\0\\02.6\t\\0\\03.4\t\\0\\05.3\t\\0\\09.9\t\\036\n\t\\0-\t\\0\\01.7\t\\0\\03.4\t\\011\t\\016\t\\028\t\\064\t220\n\t\\0*\t\\0\\01.72\t\\0\\03.16\t\\010.2\t\\015.6\t\\027.3\t\\061.2\t247\n\n\t\t$\\& sup 1~$Taken from Amble and Knuth [1].\n\t\t- No $A$ field used.\n\t\t* Analytic approximation to line above.\n.FG \"\"\n.bp 0\n\\&\n.RF\n.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i\n\t\\l'4i'\n\n\t$alpha$\t\\0.25\t\\0.5\t\\0.75\t\\0.8\t\\0.85\t\\0.9\t\\0.95\n\t\\l'4i'\n\n\t\t\\0\\04.3\t\\0\\08.8\t\\032\t\\049\t\\086\t200\t700\n\t*\t\\0\\04.56\t\\0\\09.00\t\\033.0\t\\051.0\t\\089.9\\\n\t201\t801\n\n\t* Analytic approximation to line above\n.FG \"\"\n.bp 0\n\\&\n.ce \nList of Figures\n.sp 2\nFig. 1. Example of compact hash memory and search for key.\n.sp 2\nFig. 2. Examples showing different values of $#C[i]-#V[i]$.\n.sp 2\nFig. 3. Example of calculation and use of array $A$.\n.sp 2\n.ce\nList of Tables\n.sp 2\nTable I. Average number of probes during a successful search.\n.sp 2\nTable II. Average number of probes during an unsuccessful search.\n.sp 2\nTable III. Average number of probes to move block of memory.\n.sp 2\n", "encoding": "ascii"}