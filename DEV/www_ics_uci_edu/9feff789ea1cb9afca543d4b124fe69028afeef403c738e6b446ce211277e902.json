{"url": "https://www.ics.uci.edu/~dan/pubs/DC-Sec5.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 5</TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_5\">\n<H2> 5.  OTHER ADAPTIVE METHODS</H2> </a>\n\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec678.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tTwo more adaptive data compression methods, algorithm BSTW and\nLempel-Ziv coding, are discussed in this section.  Like the adaptive\nHuffman coding techniques, these methods do not require a first pass to\nanalyze the characteristics of the source.  Thus, they provide coding\nand transmission in real time.  However, these\nschemes diverge from the fundamental Huffman coding approach\nto a greater degree than the methods discussed in\n<a href=\"DC-Sec4.html#Sec_4\">Section 4</a>.\nAlgorithm BSTW is a defined-word scheme which attempts to\nexploit locality.  Lempel-Ziv coding is a <EM>free-parse</EM> method;\nthat is, the words of the source alphabet are defined dynamically,\nas the encoding is performed.  Lempel-Ziv coding is the basis for\nthe UNIX utility <EM>compress</EM>.  Algorithm BSTW is a variable-variable\nscheme, while Lempel-Ziv coding is variable-block.\n\n<a name=\"Sec_5.1\">\n<H3> 5.1  Lempel-Ziv Codes</H3> </a>\n\nLempel-Ziv coding represents a departure from the classic view\nof a code as a mapping from a fixed set of source messages\n(letters, symbols or words) to a fixed set of codewords.\nWe coin the term <EM>free-parse</EM> to characterize this type of\ncode, in which the set of source messages and the codewords\nto which they are mapped are defined as the algorithm executes.\nWhile all adaptive methods create a set of codewords dynamically,\ndefined-word schemes have a fixed set of source messages, defined\nby context (eg., in text file processing the source messages might\nbe single letters; in Pascal source file processing the source\nmessages might be tokens).  Lempel-Ziv coding defines the set\nof source messages as it parses the ensemble.\n<P>\nThe Lempel-Ziv algorithm consists of a rule for parsing strings\nof symbols from a finite alphabet into substrings, or words,\nwhose lengths do not exceed a prescribed integer <VAR>L</VAR>(1);\nand a coding scheme which maps these substrings sequentially\ninto uniquely decipherable codewords of fixed length <VAR>L</VAR>(2)\n[Ziv and Lempel 1977].  The strings are selected so that they\nhave very nearly equal probability of occurrence.  As a result,\nfrequently-occurring symbols are grouped into longer strings\nwhile infrequent symbols appear in short strings.\nThis strategy is effective at exploiting redundancy\ndue to symbol frequency, character repetition, and high-usage\npatterns.  Figure 5.1 shows a small Lempel-Ziv code table.\nLow-frequency letters such as Z are assigned individually\nto fixed-length codewords \n(in this case, 12 bit binary numbers represented in base ten\nfor readability).\nFrequently-occurring symbols, such as blank \n(represented by _) and zero, appear\nin long strings.  Effective compression is achieved when a long\nstring is replaced by a single 12-bit code.\n\n<PRE>\nSymbol string     Code\n\n     A               1\n     T               2\n     AN              3\n     TH              4\n     THE             5\n     AND             6\n     AD              7\n     _               8\n     __              9\n     ___            10\n     0              11\n     00             12\n     000            13\n     0000           14\n     Z              15\n\n     ###          4095\n\nFigure 5.1 -- A Lempel-Ziv code table.\n</PRE>\n\nThe Lempel-Ziv method is an incremental parsing strategy in which\nthe coding process is interlaced with a learning process for\nvarying source characteristics [Ziv and Lempel 1977].  In Figure 5.1,\nrun-length encoding of zeros and blanks is being learned.  \n<P>\nThe Lempel-Ziv algorithm parses the source ensemble into a collection of \nsegments of gradually increasing length.  At each encoding step,\nthe longest prefix of the remaining source ensemble which matches\nan existing table entry (<VAR>alpha</VAR>) is parsed off, along with the \ncharacter (<VAR>c</VAR>) following this prefix in the ensemble.  The \nnew source message, <VAR>alpha</VAR> <VAR>c</VAR>, is added\nto the code table.  The new table entry is coded as (<VAR>i,c</VAR>) where\n<VAR>i</VAR> is the codeword for the existing table entry and <VAR>c</VAR> is the appended\ncharacter.  For example, the ensemble 010100010 is parsed into\n{ 0, 1, 01, 00, 010 } and is coded as { (0,0), (0,1), (1,1), \n(1,0), (3,0) }.  The table built for the message ensemble\n<VAR>EXAMPLE</VAR> is shown in Figure 5.2.  The coded ensemble\nhas the form: { (0,<VAR>a</VAR>), (1,<VAR>space</VAR>), (0,<VAR>b</VAR>), (3,<VAR>b</VAR>), (0,<VAR>space</VAR>),\n(0,<VAR>c</VAR>), (6,<VAR>c</VAR>), (6,<VAR>space</VAR>), (0,<VAR>d</VAR>), (9,<VAR>d</VAR>), (10,<VAR>space</VAR>), (0,<VAR>e</VAR>), \n(12,<VAR>e</VAR>), (13,<VAR>e</VAR>),\n(5,<VAR>f</VAR>), (0,<VAR>f</VAR>), (16,<VAR>f</VAR>), (17,<VAR>f</VAR>), (0,<VAR>g</VAR>), (19,<VAR>g</VAR>), (20,<VAR>g</VAR>), \n(20) }.  The string table is represented in a\nmore efficient manner than in Figure 5.1; the string is\nrepresented by its prefix codeword followed by the extension \ncharacter, so that the table entries have fixed length.  The \nLempel-Ziv strategy is simple, but\ngreedy.  It simply parses off the longest recognized string each time\nrather than searching for the best way to parse the ensemble.  \n\n<table align=center>\n<TR><TD>\n<TABLE>\n<tr><th>Message</th><th>Codeword</th>\n<tr><td></td><td></td>\n<tr><td><VAR>a</VAR></td><td>1</td>\n<tr><td>1<VAR>space</VAR></td><td>2</td>\n<tr><td><VAR>b</VAR></td><td>3</td>\n<tr><td>3<VAR>b</VAR></td><td>4</td>\n<tr><td><VAR>space</VAR></td><td>5</td>\n<tr><td><VAR>c</VAR></td><td>6</td>\n<tr><td>6<VAR>c</VAR></td><td>7</td>\n<tr><td>6<VAR>space</VAR></td><td>8</td>\n<tr><td><VAR>d</VAR></td><td>9</td>\n<tr><td>9<VAR>d</VAR></td><td>10</td>\n<tr><td>10<VAR>space</VAR></td><td>11</td>\n</TABLE></TD><TD>\n<TABLE>\n<tr><th>Message</th><th>Codeword</th>\n<tr><td></td><td></td>\n<tr><td><VAR>e</VAR></td><td>12</td>\n<tr><td>12<VAR>e</VAR></td><td>13</td>\n<tr><td>13<VAR>e</VAR></td><td>14</td>\n<tr><td>5<VAR>f</VAR></td><td>15</td>\n<tr><td><VAR>f</VAR></td><td>16</td>\n<tr><td>16<VAR>f</VAR></td><td>17</td>\n<tr><td>17<VAR>f</VAR></td><td>18</td>\n<tr><td><VAR>g</VAR></td><td>19</td>\n<tr><td>19<VAR>g</VAR></td><td>20</td>\n<tr><td>20<VAR>g</VAR></td><td>21</td>\n<tr><td>&nbsp;</td><td></td>\n</TABLE></TD>\n</table>\n<center>\n<PRE>\nFigure 5.2 -- Lempel-Ziv table for the message ensemble <VAR>EXAMPLE</VAR>\n              (code length=173).\n</PRE>\n</center>\n\nThe Lempel-Ziv method specifies fixed-length codewords.  The\nsize of the table and the maximum source message length are\ndetermined by the length of the codewords.  It should\nbe clear from the definition of the algorithm that Lempel-Ziv\ncodes tend to be quite inefficient during the initial portion\nof the message ensemble.  For example, even if we assume 3-bit\ncodewords for characters <VAR>a</VAR> through <VAR>g</VAR> and <VAR>space</VAR> and 5-bit\ncodewords for table indices, the Lempel-Ziv algorithm transmits\n173 bits for ensemble <VAR>EXAMPLE</VAR>.  This compares poorly with\nthe other methods discussed in this survey.  The ensemble must \nbe sufficiently long \nfor the procedure to build up enough symbol frequency experience \nto achieve good compression over the full ensemble.  \n<P>\nIf the \ncodeword length is not sufficiently\nlarge, Lempel-Ziv codes may also rise slowly to reasonable efficiency,\nmaintain good performance briefly, and fail to make any gains\nonce the table is full and messages can no longer be added.\nIf the ensemble's characteristics vary over time, the method\nmay be \"stuck with\" the behavior it has learned and may\nbe unable to continue to adapt. \n<P>\nLempel-Ziv coding is asymptotically optimal, meaning that the redundancy\napproaches zero as the length of the source ensemble\ntends to infinity.  However, for particular finite\nsequences, the compression achieved may be far from optimal\n[Storer and Szymanski 1982].  When the method begins, each source\nsymbol is coded individually.  In the case of 6- or 8-bit source\nsymbols and 12-bit codewords, the method yields as much as\n50% expansion during initial encoding.  This initial inefficiency\ncan be mitigated somewhat by initializing the string table\nto contain all of the source characters.  Implementation\nissues are particularly important in Lempel-Ziv methods.  A\nstraightforward implementation takes <VAR>O</VAR>(<VAR>n</VAR>^2) time to process\na string of <VAR>n</VAR> symbols; for each encoding operation, the existing\ntable must be scanned for the longest message occurring as a\nprefix of the remaining ensemble.  Rodeh et al. address the\nissue of computational complexity by defining a linear implementation\nof Lempel-Ziv coding based on suffix trees [Rodeh et al. 1981].\nThe Rodeh et al. scheme is asymptotically optimal, but an input must\nbe very long in order to allow efficient compression, and the\nmemory requirements of the scheme are large, <VAR>O</VAR>(<VAR>n</VAR>) where <VAR>n</VAR>\nis the length of the source ensemble.  It should also be mentioned\nthat the method of Rodeh et al. constructs a variable-variable code;\nthe pair (<VAR>i,c</VAR>) is coded using a representation of the integers,\nsuch as the Elias codes, for <VAR>i</VAR> and for <VAR>c</VAR> (a letter <VAR>c</VAR> can always\nbe coded as the <VAR>k</VAR>th member of the source alphabet for some <VAR>k</VAR>).\n<P>\nThe other major implementation consideration involves the way\nin which the string table is stored and accessed.  Welch\nsuggests that the table be indexed by the codewords (integers 1 ...\n2^<VAR>L</VAR> where <VAR>L</VAR> is the maximum codeword length) and that the\ntable entries be fixed-length codeword-extension character \npairs [Welch 1984].  Hashing is proposed to assist in encoding.\nDecoding becomes a recursive operation, in which the codeword\nyields the final character of the substring and another codeword.\nThe decoder must continue to consult the table until the \nretrieved codeword is 0.  Unfortunately, this strategy peels\noff extension characters in reverse order and some type of\nstack operation must be used to reorder the source.\n<P>\nStorer and Szymanski present a general model for data compression\nwhich encompasses Lempel-Ziv coding [Storer and Szymanski 1982].\nTheir broad theoretical work compares classes of <VAR>macro\nschemes</VAR>, where macro schemes include all methods which factor\nout duplicate occurrences of data and replace them by \nreferences either to the source ensemble or to a code table.  They also\ncontribute a linear-time Lempel-Ziv-like algorithm with better\nperformance than the standard Lempel-Ziv method.\n<P>\nRissanen \nextends the Lempel-Ziv incremental parsing approach [Rissanen 1983].  \nAbandoning\nthe requirement that the substrings partition the ensemble,\nthe Rissanen method gathers \"contexts\" in which each symbol\nof the string occurs.  The contexts are substrings of the\npreviously encoded string (as in Lempel-Ziv), have varying size, and are\nin general overlapping.\nThe Rissanen method hinges upon the identification of a\ndesign parameter capturing the concept of \"relevant\"\ncontexts.  The problem of finding the best parameter is\nundecidable, and Rissanen suggests estimating the parameter\nexperimentally. \n<P>\nAs mentioned earlier, Lempel-Ziv coding is the basis for the UNIX\nutility <EM>compress</EM> and is one of the methods commonly used in\nfile archival programs.  The archival system PKARC uses Welch's \nimplementation, as does <EM>compress</EM>.  The compression provided\nby <EM>compress</EM> is generally much better than that achieved by \n<EM>compact</EM> (the UNIX utility based on algorithm FGK), and takes\nless time to compute [UNIX 1984].  Typical compression values attained\nby <EM>compress</EM> are in the range of 50-60%.\n\n<a name=\"Sec_5.2\">\n<H3> 5.2  Algorithm BSTW</H3> </a>\n\n\tThe most recent of the algorithms surveyed here is due\nto Bentley, Sleator, Tarjan and Wei [Bentley et al. 1986].\nThis method, algorithm BSTW, possesses the advantage that it\nrequires only one pass over the data to be transmitted yet\nhas performance which compares well to that of the static\ntwo-pass method along the dimension of number of bits per word\ntransmitted.  This number of bits is never much\nlarger than the number of bits transmitted by static Huffman \ncoding (in fact, is usually quite close), and can be significantly \nbetter.  Algorithm BSTW incorporates the additional benefit of taking\nadvantage of locality of reference, the tendency for words to\noccur frequently for short periods of time then fall into long \nperiods of disuse.  The algorithm uses a self-organizing list as\nan auxiliary data structure and employs shorter encodings for words\nnear the front of this list.  There are many strategies for\nmaintaining self-organizing lists (see [Hester and Hirschberg 1985]);\nalgorithm BSTW uses move-to-front.\n<P>\n\tA simple example serves to outline the method of\nalgorithm BSTW.  As in other adaptive schemes, sender and receiver\nmaintain identical representations of the code; in this case message\nlists which are updated at each transmission, using the move-to-front\nheuristic.  These lists are initially empty.  When message <VAR>a</VAR>(<VAR>t</VAR>) is \ntransmitted, if <VAR>a</VAR>(<VAR>t</VAR>) is on the sender's list, he transmits its current\nposition.  He then updates his list by moving <VAR>a</VAR>(<VAR>t</VAR>) to position 1 and\nshifting each of the other messages down one position.  The receiver\nsimilarly alters his word list.  If <VAR>a</VAR>(<VAR>t</VAR>) is being transmitted for the\nfirst time, then <VAR>k</VAR>+1 is the \"position\" transmitted, where <VAR>k</VAR> is the\nnumber of distinct messages transmitted so far.  Some representation\nof the message itself\nmust be transmitted as well, but just this first time.  Again,\n<VAR>a</VAR>(<VAR>t</VAR>) is moved to position one by both sender and receiver\nsubsequent to its transmission.\nFor the ensemble \"<kbd>abcadeabfd</kbd>\", the transmission would be\n<kbd> 1  a  2  b  3  c  3  4  d  5  e  3  5  6  f  5</kbd>.\n(for ease of presentation, list positions are represented in base ten).\n<P>\n\tAs the example shows, algorithm BSTW transmits each source\nmessage once; the rest of its transmission consists of encodings of \nlist positions.\nTherefore, an essential feature of algorithm BSTW is a reasonable\nscheme for representation of the integers.  The methods discussed\nby Bentley et al. are the Elias codes presented in\n<a href=\"DC-Sec3.html#Sec_3.3\">Section 3.3</a>.\nThe simple scheme, code <VAR>gamma</VAR>, involves prefixing the binary representation of \nthe integer <VAR>i</VAR> with floor[ lg <VAR>i</VAR> ] zeros.  This yields a prefix\ncode with the length of the codeword for <VAR>i</VAR> equal to\n2 floor[ lg <VAR>i</VAR> ] + 1.  Greater compression can be gained\nthrough use of the more sophisticated scheme, <VAR>delta</VAR>, which encodes an \ninteger <VAR>i</VAR> in 1 + floor[ lg <VAR>i</VAR> ] + 2 floor[ (lg (1 + \nfloor[ lg <VAR>i</VAR> ] ) ] bits.\n<P>\n\tA message ensemble on which algorithm BSTW is particularly \nefficient, described by Bentley et al., is\nformed by repeating each of <VAR>n</VAR> messages <VAR>n</VAR> times, for example\n1^<VAR>n</VAR> 2^<VAR>n</VAR> 3^<VAR>n</VAR> ... <VAR>n</VAR>^<VAR>n</VAR>.\nDisregarding overhead, a static Huffman code \nuses <VAR>n</VAR>^2 lg <VAR>n</VAR> bits (or lg <VAR>n</VAR> bits per message), while algorithm \nBSTW uses <VAR>n</VAR>^2 + 2 SUM{ <VAR>i</VAR>=1 to <VAR>n</VAR>} floor[ lg <VAR>i</VAR> ] \n(which is less than or equal to <VAR>n</VAR>^2 + 2 <VAR>n</VAR> lg <VAR>n</VAR>, or <VAR>O</VAR>(1) bits per message).  \nThe overhead for algorithm BSTW consists of just the <VAR>n</VAR> lg <VAR>n</VAR> bits\nneeded to transmit each source letter once.  As discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>,\nthe overhead for static Huffman coding includes an additional 2<VAR>n</VAR> bits.\nThe locality present in ensemble <VAR>EXAMPLE</VAR> is similar to that in the\nabove example.  The transmission effected by algorithm BSTW is:\n<VAR>    1 a 1 2 space 3 b 1 1 2 4 c 1 1 1 2 5 d 1 1\n 1 1 2 6 e 1 1 1 1 1 2 7 f 1 1 1 1 1 1 8 g\n 1 1 1 1 1 1 1</VAR>.  Using 3 bits for each source letter (<VAR>a</VAR> through\n<VAR>g</VAR> and <VAR>space</VAR>) and the Elias code <VAR>delta</VAR> for list positions,\nthe number of bits used is 81, which is a great improvement over all\nof the other methods discussed (only 69% of the length used by static\nHuffman coding).  This could be improved further by the use\nof Fibonacci codes for list positions.\n<P>\nIn [Bentley et al. 1986] a proof is given that with the simple scheme for encoding \nintegers, the performance of algorithm BSTW is bounded above by \n2 <VAR>S</VAR> + 1, where <VAR>S</VAR> is the cost of the static Huffman coding scheme.  \nUsing the more sophisticated integer encoding scheme, the bound is \n1 + <VAR>S</VAR> + 2 lg(1 + <VAR>S</VAR>).  A key idea in the proofs given by\nBentley et al. is the fact that, using the move-to-front \nheuristic, the integer transmitted for a message <VAR>a</VAR>(<VAR>t</VAR>) will be one \nmore than the number of different words transmitted since the last \noccurrence of <VAR>a</VAR>(<VAR>t</VAR>).  Bentley et al. also prove that\nalgorithm BSTW is asymptotically optimal.\n<P>\nAn implementation of algorithm BSTW is described in great detail \nin [Bentley et al. 1986].  In this implementation, encoding an integer consists of\na table lookup; the codewords for the integers from 1 to <VAR>n</VAR>+1 are\nstored in an array indexed from 1 to <VAR>n</VAR>+1.  A binary\ntrie is used to store the inverse mapping, from codewords to integers.\nDecoding an Elias codeword to find the corresponding integer \ninvolves following a path in the trie.  Two interlinked data structures,\na binary trie and a binary tree, are used to maintain the word list.\nThe trie is based on the binary encodings of the source words.\nMapping a source message <VAR>a</VAR>(<VAR>i</VAR>) to its list position <VAR>p</VAR> involves\nfollowing a path in the trie, following a link to the tree, and then computing\nthe symmetric order position of the tree node.  Finding the source\nmessage <VAR>a</VAR>(<VAR>i</VAR>) in position <VAR>p</VAR> is accomplished by finding the symmetric\norder position <VAR>p</VAR> in the tree and returning the word stored there.\nUsing this implementation, the work done by sender and receiver is\n<VAR>O</VAR>(<VAR>length</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) + <VAR>length</VAR>(<VAR>w</VAR>)) where <VAR>a</VAR>(<VAR>i</VAR>) is the message being transmitted\nand <VAR>w</VAR> the codeword representing <VAR>a</VAR>(<VAR>i</VAR>)'s position in the list.\nIf the source alphabet consists of single characters, then the\ncomplexity of algorithm BSTW is just <VAR>O</VAR>(<VAR>length</VAR>(<VAR>w</VAR>)).\n<P>\nThe move-to-front scheme of Bentley et al. was independently developed\nby Elias in his paper on <EM>interval encoding</EM> and <EM>recency rank \nencoding</EM> [Elias 1987].  Recency rank encoding is equivalent to \nalgorithm BSTW.  The name emphasizes the fact, mentioned above, that\nthe codeword for a source message represents the number of distinct\nmessages which have occurred since its most recent occurrence.\nInterval encoding represents a source message by the total number\nof messages which have occurred since its last occurrence (equivalently,\nthe length of the interval since the last previous occurrence of the\ncurrent message).  It is obvious that the length of the interval \nsince the last occurrence of a message <VAR>a</VAR>(<VAR>t</VAR>) is at least as great\nas its recency rank, so that recency rank encoding never uses more,\nand generally uses fewer, symbols per message than interval encoding.\nThe advantage to interval encoding is that it has a very simple \nimplementation and can encode and decode selections from a very\nlarge alphabet (a million letters, for example) at a microsecond\nrate [Elias 1987].  The use of interval encoding might be justified\nin a data transmission setting, where speed is the essential factor.\n<P>\nRyabko also comments that the work of Bentley et al. coincides with\nmany of the results in a paper in which he considers data compression\nby means of a \"book stack\" (the books represent the source messages\nand as a \"book\" occurs it is taken from the stack and placed on top)\n[Ryabko 1987].  Horspool and Cormack have considered move-to-front,\nas well as several other list organization heuristics, in connection\nwith data compression [Horspool and Cormack 1987].\n\n<P>\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec678.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}