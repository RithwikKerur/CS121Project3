{"url": "https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt", "content": "\t\t\tComplexity of Python Operations\r\n\r\n\r\nIn this lecture we will learn the complexity classes of various operations on\r\nPython data types. Then we wil learn how to combine these complexity classes to\r\ncompute the complexity class of all the code in a function, and therefore the\r\ncomplexity class of the function. This is called \"static\" analysis, because we\r\ndo not need to run any code to perform it (contrasted with Dynamic or Emperical\r\nAnalysis, when we do run code and take measurements of its execution).\r\n\r\n------------------------------------------------------------------------------\r\n\r\nPython Complexity Classes\r\n\r\nIn ICS-46 we will write low-level implementations of all of Python's data types\r\nand see/understand WHY these complexity classes apply. For now we just need to\r\ntry to absorb (not memorize) this information, with some -but minimal- \r\njustification.\r\n\r\nBinding a value to any name (copying a refernce) is O(1). Simple operators on\r\nintegers (whose values are small: e.g., under 12 digits) like + or == are also\r\nO(1). You should assume small integers in problems unless explicitly told\r\notherwise.\r\n\r\nIn all these examples, N = len(data-structure). The operations are organized by\r\nincreasing complexity class\r\n\r\nLists:\r\n                               Complexity\r\nOperation     | Example      | Class         | Notes\r\n--------------+--------------+---------------+-------------------------------\r\nIndex         | l[i]         | O(1)\t     |\r\nStore         | l[i] = 0     | O(1)\t     |\r\nLength        | len(l)       | O(1)\t     |\r\nAppend        | l.append(5)  | O(1)\t     | mostly: ICS-46 covers details\r\nPop\t      | l.pop()      | O(1)\t     | same as l.pop(-1), popping at end\r\nClear         | l.clear()    | O(1)\t     | similar to l = []\r\n\r\nSlice         | l[a:b]       | O(b-a)\t     | l[1:5]:O(l)/l[:]:O(len(l)-0)=O(N)\r\nExtend        | l.extend(...)| O(len(...))   | depends only on len of extension\r\nConstruction  | list(...)    | O(len(...))   | depends on length of ... iterable\r\n\r\ncheck ==, !=  | l1 == l2     | O(N)          |\r\nInsert        | l[a:b] = ... | O(N)\t     | \r\nDelete        | del l[i]     | O(N)\t     | depends on i; O(N) in worst case\r\nContainment   | x in/not in l| O(N)\t     | linearly searches list \r\nCopy          | l.copy()     | O(N)\t     | Same as l[:] which is O(N)\r\nRemove        | l.remove(...)| O(N)\t     | \r\nPop\t      | l.pop(i)     | O(N)\t     | O(N-i): l.pop(0):O(N) (see above)\r\nExtreme value | min(l)/max(l)| O(N)\t     | linearly searches list for value\r\nReverse\t      | l.reverse()  | O(N)\t     |\r\nIteration     | for v in l:  | O(N)          | Worst: no return/break in loop\r\n\r\nSort          | l.sort()     | O(N Log N)    | key/reverse mostly doesn't change\r\nMultiply      | k*l          | O(k N)        | 5*l is O(N): len(l)*l is O(N**2)\r\n\r\nTuples support all operations that do not mutate the data structure (and they\r\nhave the same complexity classes).\r\n\r\n\r\nSets:\r\n                               Complexity\r\nOperation     | Example      | Class         | Notes\r\n--------------+--------------+---------------+-------------------------------\r\nLength        | len(s)       | O(1)\t     |\r\nAdd           | s.add(5)     | O(1)\t     |\r\nContainment   | x in/not in s| O(1)\t     | compare to list/tuple - O(N)\r\nRemove        | s.remove(..) | O(1)\t     | compare to list/tuple - O(N)\r\nDiscard       | s.discard(..)| O(1)\t     | \r\nPop           | s.pop()      | O(1)\t     | popped value \"randomly\" selected\r\nClear         | s.clear()    | O(1)\t     | similar to s = set()\r\n\r\nConstruction  | set(...)     | O(len(...))   | depends on length of ... iterable\r\ncheck ==, !=  | s != t       | O(len(s))     | same as len(t); False in O(1) if\r\n      \t      \t     \t       \t\t       the lengths are different\r\n<=/<          | s <= t       | O(len(s))     | issubset\r\n>=/>          | s >= t       | O(len(t))     | issuperset s <= t == t >= s\r\nUnion         | s | t        | O(len(s)+len(t))\r\nIntersection  | s & t        | O(len(s)+len(t))\r\nDifference    | s - t        | O(len(s)+len(t))\r\nSymmetric Diff| s ^ t        | O(len(s)+len(t))\r\n\r\nIteration     | for v in s:  | O(N)          | Worst: no return/break in loop\r\nCopy          | s.copy()     | O(N)\t     |\r\n\r\nSets have many more operations that are O(1) compared with lists and tuples.\r\nNot needing to keep values in a specific order in a set (while lists/tuples\r\nrequire an order) allows for faster implementations of some set operations.\r\n\r\nFrozen sets support all operations that do not mutate the data structure (and\r\nthey have the same  complexity classes).\r\n\r\n\r\nDictionaries: dict and defaultdict\r\n                               Complexity\r\nOperation     | Example      | Class         | Notes\r\n--------------+--------------+---------------+-------------------------------\r\nIndex         | d[k]         | O(1)\t     |\r\nStore         | d[k] = v     | O(1)\t     |\r\nLength        | len(d)       | O(1)\t     |\r\nDelete        | del d[k]     | O(1)\t     |\r\nget/setdefault| d.get(k)     | O(1)\t     |\r\nPop           | d.pop(k)     | O(1)\t     | \r\nPop item      | d.popitem()  | O(1)\t     | popped item \"randomly\" selected\r\nClear         | d.clear()    | O(1)\t     | similar to s = {} or = dict()\r\nView          | d.keys()     | O(1)\t     | same for d.values()\r\n\r\nConstruction  | dict(...)    | O(len(...))   | depends # (key,value) 2-tuples\r\n\r\nIteration     | for k in d:  | O(N)          | all forms: keys, values, items\r\n\t      \t      \t       \t\t     | Worst: no return/break in loop\r\nSo, most dict operations are O(1).\r\n\r\ndefaultdicts support all operations that dicts support, with the same\r\ncomplexity classes (because it inherits all those operations); this assumes that\r\ncalling the constructor when a values isn't found in the defaultdict is O(1) -\r\nwhich is true for int(), list(), set(), ... (the things we commonly use)\r\n\r\nNote that for i in range(...) is O(len(...)); so for i in range(1,10) is O(1).\r\nIf len(alist) is N, then\r\n\r\n  for i in range(len(alist)):\r\n\r\nis O(N) because it loops N times. Of course even \r\n\r\n  for i in range (len(alist)//2):\r\n\r\nis O(N) because it loops N/2 times, and dropping the constant 1/2 makes\r\nit O(N): the work doubles when the list length doubles. By this reasoning,\r\n\r\n  for i in range (len(alist)//1000000):\r\n\r\nis O(N) because it loops N/1000000 times, and dropping the constant 1000000\r\nmakes  it O(N): the work doubles when the list length doubles. Remember, we\r\nare interested in what happens as N -> Infinity.\r\n\r\nFinally, when comparing two lists for equality, the complexity class above\r\nshows as O(N), but in reality we would need to multiply this complexity class by\r\nO==(...) where O==(...) is the complexity class for checking whether two values\r\nin the list are ==. If they are ints, O==(...) would be O(1); if they are\r\nstrings, O==(...) in the worst case it would be O(len(string)). This issue\r\napplies any time an == check is done. We mostly will assume == checking on\r\nvalues in lists is O(1): e.g., checking ints and small/fixed-length strings.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComposing Complexity Classes: Sequential and Nested Statements\r\n\r\nIn this section we will learn how to combine complexity class information about\r\nsimple operations into complexity class information about complex operations\r\n(composed from simple operations). The goal is to be able to analyze all the\r\nstatements in a functon/method to determine the complexity class of executing\r\nthe function/method. As with computing complexity classes themselves, these\r\nrules are simple and easy to apply once you understand how to use them.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nLaw of Addition for big-O notation\r\n\r\n O(f(n)) + O(g(n)) is O( f(n) + g(n) )\r\n\r\nThat is, we when adding complexity classes we bring the two complexity classes\r\ninside the O(...). Ultimately, O( f(n) + g(n) ) results in the bigger of the two\r\ncomplexity class (because we alwasy drop the lower-complexity added term). So,\r\n\r\nO(N) + O(Log N)  =  O(N + Log N)  =  O(N)\r\n\r\nbecause N is the faster growing term: lim (N->infinity) Log N/N = 0.\r\n\r\nThis rule helps us understand how to compute the complexity class of doing any\r\nSEQUENCE of operations: executing a statement that is O(f(n)) followed by\r\nexecuting a statement that is O(g(n)). Executing both statements SEQUENTIALLY\r\nis O(f(n)) + O(g(n)) which is O( f(n) + g(n) ) by the rule above.\r\n\r\nFor example, if some function call f(...) is O(N) and another function call\r\ng(...) is O(N Log N), then doing the sequence\r\n\r\n   f(...)\r\n   g(...)\r\n\r\nis O(N) + O(N Log N) = O(N + N Log N) =  O(N Log N). Of course, executing the\r\nsequence (calling f twice)\r\n\r\n  f(...)\r\n  f(...)\r\n\r\nis O(N) + O(N) which is O(N + N) which is O(2N) which is O(N) because we discard\r\nmultiplicative constants in big-O notation.\r\n\r\nNote that an if statment sequentially evaluates test AND THEN one of the blocks.\r\n\r\n  if test:    \t assume complexity class of computing test is O(T)\r\n     block 1     assume complexity class of executing block 1 is O(B1)\r\n  else:\r\n     block 2     assume complexity class of executing block 2 is O(B2)\r\n\r\nThe complexity class for the if is O(T) + max(O(B1),O(B2)). The test is always\r\nevaluated, and one of the blocks is always executed afterward (so, a sequence\r\nof evaulating a test followed by executing a block). In the worst case, the if\r\nwill execute the block with the largest complexity class. So, given\r\n\r\n  if test:    \t complexity class is O(N)\r\n     block 1     complexity class is O(N**2)\r\n  else:\r\n     block 2     complexity class is O(N)\r\n\r\nThe complexity class for the if is O(N) + max (O(N**2),O(N))) = O(N) + O(N**2)\r\n= O(N + N**2) = O(N**2).\r\n\r\nIf the test had complexity class O(N**3), then the complexity class for the if\r\nis O(N**3) + max (O(N**2),O(N))) = O(N**3) + O(N**2) = O(N**3 + N**2) = O(N**3).\r\n\r\nIn fact, the complexity class for an if can also be written as\r\nO(T) + O(B1) + O(B2): for the if above example O(N) + O(N**2) + O(N) = O(N**2).\r\nWhy? Because we always throw away the lower-order terms, whic is like taking\r\nthe max of the terms. I prefer writing O(T) + max(O(B1),O(B2)) because it looks\r\nlike what is happening: the test is always evaluated, and one of the blocks.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nLaw of Multiplcation for big-O notation\r\n\r\n O(f(n)) * O(g(n)) is O( f(n) * g(n) )\r\n\r\nIf we repeat an O(f(N)) process O(N) times, the resulting complexity class is\r\nO(N)*O(f(N)) = O( N*f(N) ). An example of this is, if some function call f(...)\r\nis O(N**2), then executing that call N times (in the following loop)\r\n\r\n  for i in range(N):\r\n    f(...)\r\n\r\nis O(N)*O(N**2) = O(N*N**2) = O(N**3)\r\n\r\nThis rule helps us understand how to compute the complexity class of doing some \r\nstatement INSIDE A BLOCK controlled by a statement that is REPEATING it. We\r\nmultiply the complexity class of the number of repetitions by the complexity\r\nclass of the statement (sequence; using the summing rule) being repeated.\r\n\r\nCompound statements can be analyzed by composing the complexity classes of\r\ntheir constituent statements. For sequential statements (including if tests and\r\ntheir block bodies) the complexity classes are added; for statements repeated\r\nin a loop the complexity classes are multiplied.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nOne Function Specification/3 Implementations and their Analysis\r\n\r\nLet's use the data and tools discussed above to analyze (determine the\r\ncomplexity classes) of three different functions that each compute the same\r\nresult: whether or not a list contains only unique values (no duplicates). We\r\nwill assume in all three examples that len(alist) is N and that we can compare\r\nthe list elements in O(1): e.g., they are small ints or strs.\r\n\r\n1) Algorithm 1: A list is unique if each value in the list does not occur in any\r\nlater indexes: alist[i+1:] is a list slice containing all values after the one\r\nat index i.\r\n\r\ndef is_unique1 (alist : [int]) -> bool:\r\n    for i in range(len(alist)):\t\tO(N) - for every index; see * below\r\n        if alist[i] in alist[i+1:]:\tO(N) - index+add+slice+in: O(1)+O(1)+O(N)+O(N) = O(N)\r\n            return False\t\tO(1) - never executed in worst case; ignore\r\n    return True\t\t\t\tO(1) - always executed in worst case; use\r\n\r\n*Note that creating a range object requires 3 sequential operations: computing\r\nthe arguments, passing the arguments to __init__, and executing the body of\r\n__init__. The latter two are both O(1), and computing len(alist) is also O(1),\r\nso the complexity of range(len(alist)) is O(1)+O(1)+O(1) = O(1).\r\n\r\nThe complexity class for executing the entire function is O(N) * O(N) + O(1)\r\n= O(N**2). So we know from the previous lecture that if we double the length of\r\nalist, this function takes 4 times as long to execute.\r\n\r\n-----\r\nMany students want to write this as O(N) * ( O(N) + O(1) ) + O(1) because the\r\nif statement's complexity is O(N) + O(1): complexity of test + complexity of\r\nblock when test is True (there is no block when test is False). But in the\r\nWORST CASE, the return is NEVER EXECUTED (the loop keeps executing) so it\r\nshould not appear in the formula. Although, even if it appears in this formula,\r\nthe formula still computes the same complexity class (because O(N) + O(1) is\r\nstill O(N)): O(N**2).\r\n\r\nSo, in the worst case, we never return False and keep executing the loop, so\r\nthis O(1) does not appear in the formula. Also, in the worst case the list\r\nslice is aliset[1:] which is O(N-1) = O(N), although when i is len(alist) the\r\nslice contains 0 values: is empty. The average list slice taken in the if has\r\nN/2 values, which is still O(N).\r\n-----\r\n\r\nWe can also write this function purely using loops (no slicing), but the\r\ncomplexity class is the same.\r\n\r\ndef is_unique1 (alist : [int]) -> bool:\r\n    for i in range(len(alist)):\t\tO(N) - for every index\r\n        for j in range(i+1,len(alist)): O(N) - N-i indexes; O(N) in worst case\r\n            if alist[i] == a[j]:\tO(1) - index+index+==: O(1)+O(1)+O(1) = O(1)\r\n                return False\t\tO(1) - never executed in worst case; ignore\r\n    return True\t\t\t\tO(1) - always executed in worst case; use\r\n\r\nThe complexity class for executing the entire function is O(N)*O(N)*O(1) + O(1)\r\n= O(N**2). So we know from the previous lecture that if we double the length of\r\nalist, this function takes 4 times as long to execute.\r\n\r\n------\r\n2) Algorithm 2: A list is unique if when we sort its values, no ADJACENT values\r\nare equal. If there were duplicate values, sorting the list would put these\r\nduplicate values right next to each other (adjacent). Here we copy the list so\r\nas to not mutate (change the order of) the parameter's list by sorting it\r\n(functions generally shouldn't mutate their arguments unless that is the purpose\r\nof the function): it turns out that copying the list does not increase the\r\ncomplexity class of the method, because the O(N) used for copying is not the\r\nlargest added term computing the complexity class of this function's body.\r\n\r\ndef is_unique2 (alist : [int]) -> bool:\r\n    copy = list(alist)\t\t\tO(N)\r\n    copy.sort()\t\t\t\tO(N Log N) - for fast Python sorting\r\n    for i in range(len(alist)-1):\tO(N) - really N-1, but that is O(N); len and - are both O(1)\r\n        if copy[i] == copy[i+1]:\tO(1): +, 2 [i], and  == on ints: all O(1)\r\n            return False\t\tO(1) - never executed in worst case\r\n    return True\t   \t\t\tO(1) - always executed in worst case\r\n\r\nThe complexity class for executing the entire function is given by the sum\r\nO(N) + O(N Log N) + O(N)*O(1) + O(1) = O(N + N Log N + O(N*1) + 1) =\r\nO(N + N Log N + N + 1) = O(N Log N + 2N + 1) = O(N Log N). So the complexity\r\nclass for this algorithm/function is lower than the first algorithm, the\r\nis_unique1 function. For large N unique2 will eventually run faster. Because\r\nwe don't know the constants, we don't know which is faster for small N.\r\n\r\nNotice that the complexity class for sorting is dominant in this code: it does\r\nmost of the work. If we double the length of alist, this function takes a bit\r\nmore than twice the amount of time. In N Log N: N doubles and Log N gets a tiny\r\nbit bigger (i.e., Log 2N = 1 + Log N; e.g., Log 2000 = 1 + Log 1000 = 11, so\r\ncompared to 1000 Log 1000, doubling N is 2000 Log 2000, which is just 2.2 times\r\nbigger, or 10% bigger than just doubling).\r\n\r\nLooked at another way if T(N) = c*(N Log N), then T(2N) = c*(2N Log 2N) =\r\nc*2N(Log N + 1) = c*2N Log N + c*2N = 2*T(N) + c*2N. Or, computing the doubling signature\r\n\r\n  T(2N)     c*2N Log N + c*2N     c*2N Log N       c*2N              2\r\n-------- = ------------------- = ------------ + ----------- = 2 + -------\r\n  T(N)         c N Log N           c N Log N     c N Log N         Log N\r\n\r\nSo, the ratio is 2 + a bit (and that bit gets smaller -very slowly- as N\r\nincreases): for N >= 10**3 it is <= 2.2; for N >= 10**6 it is <= 2.1; for N >=\r\n10**9 it it < 2.07. So, it is a bit worse than doubling each time, but much\r\nbetter than O(N**2) which is quadrupling each time.\r\n\r\nIn fact, we could also simplify\r\n\r\n    copy = list(alist)\t\t\tO(N)\r\n    copy.sort()\t\t\t\tO(N Log N) - for fast Python sorting\r\n\r\nto just\r\n\r\n    copy = sorted(alist)                O(N Log N) - for fast Python sorting\r\n\r\nbecause sorted will create a list of all the values in its iterable argument,\r\nand return it after mutating (sorting) it. So we don't have to explicitly\r\ncreate such a copy in our code.\r\n\r\nThis change will speed up the code, but it won't change the complexity analysis\r\nbecause O(N + N Log N) = O (N Log N). Speeding up code is always good, but\r\nfinding an algorithm in a better complexity class (as we did going from\r\nis_unique1 to is_unique2) is much btter\r\n\r\nFinally, is_unique2 works only if all the values in the list are comparable\r\n(using the < relational operator needed for sorting): it would fail if the list\r\ncontained both integers and strings. Whereas, using either version of\r\nis_unique1, requires only comparing values with ==: 3 == 'xyz' is False; it\r\ndoes not raise an exception.\r\n\r\n------\r\n3) Algorithm 3: A list is unique if when we turn it into a set, its length is\r\nunchanged: if duplicate values were added to the set, its length would be\r\nsmaller than the length of the list by exactly the number of duplicates in the\r\nlist added to the set.\r\n\r\ndef is_unique3 (alist : [int]) -> bool:\r\n    aset = set(alist)\t\t\tO(N): construct set from alist values\r\n    return len(aset) == len(alist)\tO(1): 2 len (each O(1)) and == ints O(1)\r\n\r\nThe complexity class for executing the entire function is O(N) + O(1) =\r\nO(N + 1) = O(N). So the complexity class for this algortihm/function is lower\r\nthan both the first and second algorithms/functions. If we double the length of\r\nalist, this function takes just twice the amount of time. We could write the\r\nbody of this function more simply as: return len(set(alist)) == len(alist),\r\nwhere evaluating set(alist) takes O(N) and then computing the two len's and\r\ncomparing them for equality are all O(1): O(N)+O(1)+O(1)+O(1) = O(N).\r\n\r\nUnlike is_unique2, it can work for lists containing both integers and strings.\r\nBut, is_unique3 works only if all the values in the list are hashable/immutable\r\n(a requirement for storing values in a set). So, it would not work for a list of\r\nlists.\r\n\r\nSo the bottom line here is that there might be many algorithms/functions to\r\nsolve some problem. If the function bodies are small, we can analyze them\r\nstatically (looking at the code, not needing to run it) to determine their\r\ncomplexity classes. For large problem sizes, the algorithm/function with the\r\nsmallest complexity class will ultimately be best, running in the least amount\r\nof time.\r\n\r\nBut, for small problem sizes, complexity classes don't determine which is best:\r\nfor small problem we need to take into account the CONSTANTS and lower order\r\nterms that we ignored when computing complexity classes). We can run the\r\nfunctions (dynamic analysis, aka empirical analysis) to test which is fastest\r\non small problem sizes.\r\n\r\nAnd finally, sometimes we must put additional constraints on data passed to\r\nsome implementations: is_unique2 require that its list store values comparable\r\nby < (for sorting it); is_unique3 requires that its list store values that\r\nhashable/immutable.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nUsing a Class (implementable 3 ways) Example:\r\n\r\nWe will now look at the solution of a few problems (combining operations on a\r\npriority queue: pq) and how the complexity class of the result is affected by\r\nthree different classes/implementations of priority queues.\r\n\r\nIn a priority queue, we can add values to and remove values from the data\r\nstructure. A correctly working priority queue always removes the maximum value\r\nremaining in the priority queue (the one with the highest priority). Think of a\r\nline/queue outside of a Hollywood nightclub, such that whenever space opens up\r\ninside, the most famous person in line gets to go in (the \"highest priority\"\r\nperson), no matter how long less famous people have been standing in line\r\n(contrast this with first come/first serve, which is a regular -non priority-\r\nqueue; in a regaular queue, whoever is first in the line -has been standing in\r\nline longest- is admitted next).\r\n\r\nFor the problems below, all we need to know is the complexity class of the\r\n\"add\" and \"remove\" operations.\r\n\r\n                      add           remove\r\n\t         +-------------+-------------+\r\nImplementation 1 |    O(1)     |    O(N)     |\r\n\t         +-------------+-------------+\r\nImplementation 2 |    O(N)     |    O(1)     |\r\n\t         +-------------+-------------+\r\nImplementation 3 |  O(Log N)   |  O(Log N)   |\r\n\t         +-------------+-------------+\r\n\r\nImplementation 1 adds the new value into the pq by appending the value at the\r\nrear of a list or the front of a linked list: both are O(1); it removes the\r\nhighest priority value by scanning through the list or linked list to find the\r\nhighest value, which is O(N), and then removing that value, also O(N) in the\r\nworst case  (removing at the front of a list; at the rear of a linked list).\r\n\r\nImplementation 2 adds the new value into the pq by scanning the list or linked\r\nlist for the right spot to put it and putting it there, which is O(N). Lists\r\nstore their highest priority at the rear (linked lists at the front); it\r\nremoves the highest priority value from the rear for lists (or the front for\r\nlinked lists), which is O(1).\r\n\r\nSo Implementations 1 and 2 swap the complexity classes in their add/remove\r\nmethod. Implementation 1 doesn't keep the values in order: so easy to add but\r\nhard to find/remove the maximum (must scan). Implementation 2 keeps the values\r\nin order: so hard to add (need to scan to find where it goes) but easy to\r\nfind/remove the maximum (at one end; the fast one).\r\n\r\nImplementation 3, which is discussed in ICS-46, uses a binary heap tree (not a\r\nbinary search tree) to implement both operations with \"middle\" complexity\r\nO(Log N): this complexity class greater than O(1) but less than O(N). Because\r\nLog N grows so slowly, O(Log N) is actually closer to O(1) than O(N) even though\r\nO(1) doesn't grow at all: Log N grows that slowly.\r\n\r\nProblem 1: Suppose we wanted to use the priority queue to sort N values: we\r\nadd N values in the pq and then remove all N values (first the highest, next\r\nthe second highest, ...). Here is the complexity of these combined operations\r\nfor each implementation.\r\n\r\nImplementation 1: N*O(1) + N*O(N)         = O(N)   + O(N**2)    = O(N**2)\r\nImplementation 2: N*O(N) + N*O(1)         = O(N**2) + O(N)      = O(N**2)\r\nImplementation 3: N*O(Log N) + N*O(Log N) = O(NLogN) + O(NLogN) = O(NLogN)\r\n\r\nNote N*O(...) is the same as O(N)*O(...) which is the same as O(N * ...)\r\n\r\nHere, Implementation 3 has the lowest complexity class for the combined\r\noperations. Implementations 1 and 2 each do one operation quickly but the other\r\nslowly: both are done O(N) times. The slowest operation determines the\r\ncomplexity class, and both are equally slow. The complexity class O(Log N) is\r\nbetween O(1) and O(N); surprisingly, it is actually \"closer\" to O(1) than O(N),\r\neven though it does grow -because it grows so slowly; yes, O(1) doesn't grow at\r\nall, but O(Log N) grows very slowly: the known Universe has about 10**90\r\nparticles of matter, and Log 10**90 = Log (10**3)**30 = 300, which isn't very\r\nbig compared to 10**90 (like 86 orders of magnitude less).\r\n\r\nProblem 2: Suppose we wanted to use the priority queue to find the 10 biggest\r\n(of N) values: we would enqueue N values and then dequeue 10 values. Here is\r\nthe complexity of these combined operations for each implementation..\r\n\r\nImplementation 1: N*O(1) + 10*O(N)         = O(N)   + O(N)      = O(N)\r\nImplementation 2: N*O(N) + 10*O(1)         = O(N**2) + O(1)     = O(N**2)\r\nImplementation 3: N*O(Log N) + 10*O(Log N) = O(NLogN) + O(LogN) = O(NLogN)\r\n\r\nHere, Implementation 1 has the lowest complexity for the combined operations.\r\nThat makes sense, as the operation done N times (add) is very simple (add to\r\nthe end of a list/the front of a linked list, where each add is O(1)) and the\r\noperation done a constant number of times (10, independent of N) is the\r\nexpensive operation (remove, which is O(N)). It even beats the complexity of\r\nImplementation 3. So, as N gets bigger, implementation 1 will eventually become\r\nfaster than the other two for the \"find the 10 biggest\" task.\r\n\r\nSo, the bottom line here is that sometimes there is NOT a \"best all the time\" \r\nimplementation for a data structure. We need to know what problem we are\r\nsolving (the complexity classes of all the operations in various\r\nimplementations and HOW OFTEN we must do these operations) to choose the most\r\nefficient implementation for solving the problem.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nProblems:\r\n\r\nTBA\r\n", "encoding": "ascii"}