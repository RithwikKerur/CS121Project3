{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/sorting1.txt", "content": "\t\tSorting: Analysis of O(N^2) Sorts (and Stability)\r\n\r\n\r\nSorting is one of the most studied problems in Computer Science. Hundreds of\r\ndifferent sorting algorithms have been developed, each with relative advantages\r\nand disadvantages based on the data being truly random or partially ordered, and\r\nother features, some of which we will discuss below.\r\n  \r\nWe will start our discussion of sorting by covering general characteristics\r\n(applied to each algorithm we later examine), simple O(N^2) algorithms, more\r\ncomplicated O(N Log2 N) algorithms, non-trivial lower bounds for \"comparison\"\r\nbased sorting, and finally sorting methods that do not use comparisons: their\r\ncomplexity classes, and how to interpret them.\r\n\r\nFirst, we will often examine the following sorting characteristics for the\r\nalgorithms discussed below.\r\n\r\n1) The complexity class of the algorithm (normally worst-case, but\r\n   sometimes average-case and sometimes even best case too)\r\n\r\n2) The amount of extra storage needed to run it. If just O(1) extra storage is\r\n   needed (not proportional to N, the size of the array being sorted), we call\r\n   the sorting method \"In Place\". In fact, we will call it \"in place\" if it\r\n   uses O(Log2 N) space as well, because this number is << N for large N.\r\n\r\n3) The number of comparsions and data movements needed to sort. For example,\r\n   it we are sorting a huge amount of information on an external memory that\r\n   is non-random access (say a hard disk) the cost of a comparison might be\r\n   a small fraction of the moving data, so we'd prefer an algorithm that does\r\n   more of the former and less of the later. This won't change its complexity\r\n   class, but it can have a large impact on its actual performance. We will\r\n   discuss sorting huge amounts of data on external storage the last week of\r\n   course.\r\n\r\n4) Is the algorithm stable: do equal values in the array keep their same\r\n   relative order in the sorted array as in the original array. Stability is\r\n   sometimes useful (e.g., when sorting data based on multiple keys), but there\r\n   is often a price to pay for it (increased execution time).\r\n\r\n-----------\r\n\r\nIllustrating and Using Stability:\r\n\r\nStability is useful, for example, in the following situation. Imagine we have\r\nan array of objects that store a student's name and grade. We want to sort the\r\narray by grade (first all A students, then all B students, etc.), but with all\r\nstudents who have the same grade listed in alphabetical order. With a stable\r\nsorting algorithm we can do this easily as two sorts: one on each key.\r\n\r\nFirst, assume the original array contains the following pairs of data in each\r\nobject (Name-Grade).\r\n\r\n  Bob-A   Mary-C   Pat-B   Fred-B   Gail-A   Irving-C   Betty-B   Rich-F\r\n\r\n1) Sort on the minor/secondary key (name) first; we don't care whether or not\r\nthe sort is stable (and in this case, none of the names are equal); so the\r\nresult is\r\n\r\n  Betty-B   Bob-A   Fred-B   Gail-A   Irving-C   Mary-C   Pat-B   Rich-F\r\n\r\n2) Sort on the primary/major key (grade) using a stable sort. So, for\r\nexample, since Betty (grade B) is to the left of Fred (grade B) who is to\r\nthe left of Pat (grade B), for data with those equal keys of B (when sorting by\r\ngrade, with a stable sort), this order will be maintained in the newly sorted\r\narray: Betty to the left of Fred, Fred to the left of Pat, and all those names\r\nwill move together based on all having a grade of B (after all with an A grade;\r\nbefore all with a C grade).\r\n\r\n  Bob-A   Gail-A   Betty-B   Fred-B   Pat-B   Irving-C   Mary-C   Rich-F\r\n\r\nThus, the information is finally sorted by grade, with all those students with\r\nthe same grade (sub)sorted by name (which was done in the first sort).\r\n\r\nAnother way to accomplish this same ordering is by sorting once, but with a\r\nmore complicated operator< (instead of sorting twice, with two simple\r\noperator<). Use an operator< such that if the grades are different, the better\r\n(lower in the alphabet) one is smaller; but if they are the same, the one with\r\nthe smaller (earlier in the dictionary) name is smaller. So when comparing\r\nBetty-B and Bob-A, the grades are different so Bob-A comes first; when\r\ncomparing Betty-B and Fred-B, the grades are the same but Betty's name is\r\nsmaller (comes before in the dictionary ordering) Fred's name. Here is the\r\noperator< assuming std::string fields .name and .grade in a class called\r\nStudent\r\n\r\n  bool operator< (const Student& a, const Student& b) {\r\n    if (a.grade < b.grade)\r\n      return true;\r\n    else if (a.grade > b.grade)\r\n      return false;\r\n    else /* if a.grade == b.grade */\r\n      return a.name < b.name;\r\n\r\n    //or, \r\n    //return (a.grade != b.grade ? a.grade < b.grade : a.name < b.name);\r\n    }\r\n  }\r\n\r\n-----\r\nNot available in C++ yet; but there is a Java version.\r\n\r\nIn the spirit of empirical investigation, I have written a small driver that\r\nserves as a testbed for sorting application. It is available off the\r\nPrograms link for the course (Sorting). It allows us to time various sorting\r\nalgorithms on various sized-data with various orderings (including random).\r\n-----\r\n\r\n-----------\r\n\r\nSimple to Code O(N^2) Sorts\r\n\r\nIn Selection Sort, the left part of the array is correct (sorted with the final\r\nvalues there) and the right is unknown. Each iteration around the outer loop\r\nensures the sorted part expands by one index (on the left of the boundary\r\nbetween sorted/unsorted) and the unsorted part shrinks by one index (on the\r\nright of that boundary). The algorithm scans forwards from the 1st unsorted\r\nindex to the end of the array to find the smallest remaining value in the\r\narray, then it swaps that value with the one in the first unsorted index.\r\n\r\n  1) Worst is O(N^2), best is O(N^2), and average is O(N^2)\r\n  2) In-place O(1): needs a few extra local variables in the method\r\n  3) O(N^2) comparisons; O(N) swaps\r\n  4) Unstable (swapping can move values many array indexes, over other values)\r\n\r\n  template<class T>\r\n  void selection_sort(T a[],int length) {\r\n    for (int index_to_update=0; index_to_update<length; ++index_to_update) {\r\n      int index_of_min = index_to_update;\r\n      for (int i = index_to_update+1; i<length; ++i)\r\n        if ( a[i] < a[index_of_min] )\r\n          index_of_min = i;\r\n      std::swap(a[index_to_update], a[index_of_min]);\r\n    }\r\n  }\r\n\r\nWhen index_to_update is 3, we have\r\n\r\n                      0   1   2   3   4   5   6   7   8   9\r\n                    +---+---+---+---+---+---+---+---+---+---+\r\n                    |   |   |   |   |   |   |   |   |   |   |\r\n                    +---+---+---+---+---+---+---+---+---+---+\r\n                                ^\r\n<- Sorted, and required to be   | Unsorted ->\r\n   in their correct indexes\r\n\r\nmeaning the values at indexes 0-2 are sorted (with the 3 smallest array values,\r\nin order) and the values at indexes 3-9 are unsorted; this loop scans all of\r\nthe unsorted values to find the smallest one, and immediately after the end of\r\nthis loop, the code swaps it with the value in the first unsorted index (3).\r\nSo, the value at array index 3 will store the next biggest value and the\r\ndividing line will moved one to the right and be between 3 and 4.\r\n\r\nNote the body of the inner for-loop is executed N-1 times (N = length) times\r\nwhen index_to_update is 0; N-2 times when index_to_update is 1; N-3 times when\r\nindex_to_update is 2; ... 0 times when index_to_udate is N-1. So, the total\r\nnumber of times it executes is the sum: 0+1+2+...+(N-1) = N(N-1)/2 by a\r\nformula we studied previously.\r\n\r\nNote that the body of the inner loop does one comparison and at most one data\r\nmovement (moving an int: i to index_of_min); each time the inner loop is\r\nfinished, the body of the outer loop finally moves/swaps two data values in the\r\narray.\r\n\r\nSome students might want to rewrite the swapping by embedding it in an if\r\nstatement, to \"avoid doing extra work\":\r\n\r\n  if (index_to_update != index_of_min)\r\n    std::swap(a[index_to_update], a[index_of_min]);\r\n\r\nadding extra code to avoid swapping a value with itself (in such a case, the\r\nswap code will execute correctly, but ultimately makes no changes in the array).\r\nThe problem with this code is that to save doing a swap that is SOMETIMES\r\nunneeded, we must ALWAYS do a comparison of indexes in the if.\r\n\r\nSuppose that the comparison takes 1 computer operation and the swap takes 3;\r\nalso suppose that when sorting 1,000 values, 95% of the time index_to_update is\r\nnot equal to i. Then, the original code takes 3,000 instructions (always\r\nswapping for 1,000 values). The conditional code takes 1,000 instructions to\r\ntest whether to swap, and swaps 950 times (so takes 1,000+3*950 = 3,850\r\ncomputer instructions, compared to the 3,000 done by the \"always swap\" way).\r\nSo, the extra code isn't really an \"improvement\". In contrast, when sorting an\r\narray that is already sorted, the updated code takes only 1,000 instructions\r\nbecause it never swaps. So the preconditioning of the data makes a difference.\r\n\r\nHow about stability? If you sort the following tiny array (currently sorted by\r\nname) by grade\r\n\r\n  Betty-B  Fred-B  Gail-A\r\n\r\nThe first swap will be Betty-B and Gail-A (the smallest), which inverts the\r\norder of the \"equal\" keys Betty and Frd. The final result will be\r\n\r\n  Gail-A  Fred-B  Betty-B\r\n\r\nwhich still has inverted the order of Betty-B and Fred-B, so selection sort is\r\nunstable. This algorithm moves data too radically in the array. Generally,\r\nswapping the value at index_to_move (on the left) with the one at index_of_min\r\n(anwhere to the right) might make the new value at index_to_move move be to the\r\nleft of other values that are equal to it (between index_to_move and\r\nindex_of_min).\r\n\r\nThis algorithm works equally well for arrays and linked lists (with slight\r\nchanges in code). It runs in about the same amount of time no matter what the\r\nordering in the original array. Also note that it is an offline algorithm: it\r\nrequires all the data be present in the array before it can start: it looks at\r\nall the data to determine what belongs at index 0.\r\n\r\n----------\r\n\r\nIn Insertion Sort, again the left part of the array is sorted (although it\r\ndoesn't have its final values until the last iteration), but and the right is\r\nunknown. Each iteration around the outer loop ensures the sorted part expands\r\nby one index (on the left of the boundary between sorted and unsorted) and the\r\nunsorted part shrinks by one index (on the right of the boundary). The\r\nalgorithm moves/swaps the value in the 1st unsorted index backwards, until it\r\nis >= the value before it. So, only data in the sorted part changes.\r\n\r\nNote that unlike selection sort, the left part does not immediately have its\r\nvalues in their final, correct place: the left part contains some subset of the\r\narray values, but that subset is always sorted. Eventually it contains all the\r\narray values, sorted.\r\n\r\n  1) Worst is O(N^2), best is O(N), average is O(N^2)\r\n  2) In-place O(1): needs a few extra local variables in the method\r\n  3) O(N^2) comparisons; O(N^2) swaps\r\n  4) Stable\r\n\r\n  template<class T>\r\n  insertion_sort(T a[], int length) {\r\n    for (int index_to_move=0; index_to_move<length; index_to_move++)\r\n      for (int i=index_to_move-1; i>=0; --i)\r\n          if ( a[i] > a[i+1] )\r\n            std::swap(a[i], a[i+1]);\r\n          else\r\n            break;\r\n  }\r\n\r\nWhen index_to_move is 3, we have\r\n\r\n                      0   1   2   3   4   5   6   7   8   9\r\n                    +---+---+---+---+---+---+---+---+---+---+\r\n                    |   |   |   |   |   |   |   |   |   |   |\r\n                    +---+---+---+---+---+---+---+---+---+---+\r\n                                ^\r\n<- Sorted, but not required to  | Unsorted ->\r\n   be in their correct indexes\r\n\r\nmeaning the values at indexes 0-2 are sorted (although, unlike Selection Sort\r\nthey might not yet contain the 3 smallest array values!) and indexes 3-9 are\r\nunsorted; this loop swaps the value at index 3 backwards until it is in index 0\r\nor >= the value to its left, so at the end of this loop, the array indexes 0-4\r\nwill be in order (although they might not yet contain the 4 smallest array\r\nvalues!) and the dividing line will be between 3 and 4.\r\n\r\nNote the body of the inner for-loop is executed at most 0 times when\r\nindex_to_move is 0; at most 1 time when index_to_move is 1; ... at most N-1\r\ntimes when index_to_move is N-1 (when N = length). So, the most number of times\r\nit executes is the sum 0+1++2+...+(N-1) = N(N-1)/2  by a formula we studied\r\npreviously.\r\n\r\nNote that the body of the inner loop does one comparison and at most one swap\r\nof a pair of data values each time it executes.\r\n\r\nIn the best case (where the entire array is completely sorted), each value in\r\nindex_to_move will already be bigger than the one before it, so the inner loop\r\nwill immedately break, requiring just a total of N comparisons (one for each\r\niteration in the outer loop) and no data movement in the best case.\r\n\r\nIf some values in the array are equal, the one on the right will move left, but\r\nstop to the right of any equal values (see the break, controlled by the \">\"\r\noperator). There is no \"severe\" swapping; only adjacent values are swapped.\r\nThis means that the Insertion sorting  method is stable.\r\n\r\nSo, this method in the worst case does the same number of comparisons as in\r\nSelection sort, but is likely to do many more swaps than Selection sort, and\r\ntherefore it has a higher constant. But if we know the array is sorted (or very\r\nclose to being sorted: where no values are far away from where they belong)\r\nthis method is O(N), whereas selection sort is always O(N^2) -worst, best, and\r\naverage case.\r\n\r\nThis means if we know something about the data (like it is almost sorted) it\r\nmeans that we might prefer this algorithm over the previous one (in fact, if\r\nthe data is almost sorted, this algorithm beats the O(N Log N) algorithms.\r\n\r\nThere is a better variant of this algorithm, which is a bit more complicated to\r\nwrite, but has better performance: it does half the data movements by caching\r\nthe value to move and translating swaps into single data movements.\r\n\r\n  1) Worst is O(N^2), best is O(N), average is O(N^2)\r\n  2) In-place: needs a few extra local variables in the method\r\n  3) O(N^2) comparisons; O(N^2) swaps\r\n      (although here it moves just once piece of data in the inner loop; by not\r\n       swapping, less work is done; so at worst it still does O(N^2) data\r\n       movements)\r\n  4) Stable\r\n\r\n  template<class T>\r\n  insertion_sort(T a[], length) {\r\n    for (int index_to_move=0; index_to_move<length; index_to_move++) {\r\n      to_move = a[index_to_move];\r\n      int i = index_to_move-1;        //i must be usable after/outside the loop\r\n      for (/*See above*/; i>=0; --i)\r\n          if ( a[i] > to_move )\r\n            a[i+1] = a[i];            //Single assignment, not swapping\r\n          else\r\n            break;\r\n      a[i+1] = to_move    \r\n    }\r\n  }\r\n\r\nThis version caches the value a[index_to_move] and then shifts to the right all\r\nvalue > that to_move; finally, it stores to_move in the correct place. In both\r\nversions we can start index_to_move at 1, because inserting the value at index\r\n0 requires no comparisions or movement: it is already inserted in the correct\r\nplace. If the length is 1, then, the outer loop immediately terminates because\r\nall 1 length arrays are sorted.\r\n\r\nInsertion sort also works for doubly linked lists, but not simply for linear\r\nlinked lists (note the inner for loop is incrementing backwards); but, if we\r\nremove each value from the first list and insert it into a second list (so that\r\nthe second list is always sorted), this algorithm works for simple linked lists\r\n(although a sorted list takes O(N^2) comparisons while the a list sorted in\r\nreverse order takes only O(N)).\r\n\r\nFinally, note that it is an online algorithm: it doesn't require that all the\r\ndata be present in the array before it can start sorting: as each new value is\r\n\"added in the right part of the array\", the algorithm can move it backward to\r\nits correct position in the left part of the array. When the final piece of\r\ndata appears, the array to the left can be completely sorted, and sorting the\r\nentire array (with the new piece of data) requires O(N) operations.\r\n\r\n\r\nIs there any situation where Selection Sort is better than Insertion Sort? Note\r\nthat Selection Sort does O(N) swaps in the worst case, while Insertion Sort\r\ndoes O(N^2). So, if we have an array filled with very large records, but\r\ncomparing records uses just a small amount of time (so comparing is much faster\r\nthan swapping) then Selection Sort may operate faster than Insertion Sort.\r\n\r\nAlso, suppose that the sorting is done in one process, which passes its result\r\nalong to another process. Selection Sort computes the smallest value in the\r\narray after the first pass over the data (so it could be sent to the next\r\nprocess at that time); the same for the 2nd, 3rd, etc. smallest values. But\r\nInsertion Sort is not guaranteed to know the smallest value until its last pass\r\nover the data: the smallest value might be in the last array index.\r\n\r\nIf you run the program associated with this lecture that collects empirical\r\ndata, you will find that Insertion Sort is typically faster than Selection\r\nSort (runs in about 60% of the time). That should make sense, as Selection Sort\r\nalways does N*(N-1)/2 comparisons, but on average Insertion Sort will do only\r\nhalf as many (assuming on average we Insert each value backward through about\r\nhalf of the previous indexes). \r\n\r\n------------------------------------------------------------------------------\r\n\r\nI'm not a big fan of animations, but you might want to check out the sorting\r\nanimations at http://www.sorting-algorithms.com/. I think these animations\r\nare better on the O(N^2) algorithms, which are pretty easy to visualize without\r\na computer anyway. Recently I have also been made aware of various folk dancing\r\ntroups dancing sorting algorithms: google \"dancing sorting algorithms\" or \r\nsomething similar.\r\n", "encoding": "ascii"}