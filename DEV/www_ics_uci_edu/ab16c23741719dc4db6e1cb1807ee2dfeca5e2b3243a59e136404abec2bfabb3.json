{"url": "https://www.ics.uci.edu/~eppstein/180a/970415.html", "content": "<HTML>\n<HEAD>\n<TITLE>\nICS 180, April 15, 1997</TITLE>\n<META name=\"Owner\" value=\"eppstein\">\n<META name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</HEAD><BODY>\n<IMG SRC=\"icslogo2.gif\" WIDTH=472 HEIGHT=72 ALT=\"\"><P>\n<A HREF=\"index.html\">\n<H1>ICS 180A, Spring 1997:<BR>\nStrategy and board game programming</H1></A>\n\n<H2>Lecture notes for April 15, 1997<BR>\nTuning the evaluation function</H2>\n\n<P>\nLast time I talked about the different types of functions that evaluate \nfeatures in a position, and how to combine them into an evaluation \nfunction by adding the values of many such functions.  But, where do the \nnumbers come from?\n\n<P>\nE.g., in Othello, you might have say four functions:\n<PRE>\nf(pos) = material (# of my pieces - # of opponent pieces)\ng(pos) = corners (# I control - # opponent controls)\nh(pos) = mobility (# moves I have available)\n</PRE>\nYou want to form an evaluation function by combining these (probably with \nsome other terms): eval = a f + b g + c h.  For instance, you might try \neval = -1*f + 10*g + 1*h.  But where do these numbers come from?  What \ncombination of numbers gives the best performance?\n\n<P>\nThere are various methods for finding numbers by hand:\n\n<UL>\n<P><LI><B>Normalize</B>. Since you only care about the ordering of \nevaluations, and (usually) not the actual evaluation values, you can \nmultiply everything by the same constant without changing the results.\nWhat this means is that you can choose some particular value\n(say the material value of a pawn) and force it to be one, so that all the \nother values are expressed in terms of how many pawns that value is worth.\nThe net effect is that you have one fewer parameter that needs setting.\n\n<P><LI><B>Deduce constraints</B>. Sometimes it is possible to choose some \nof the parameters by considering what you want the machine to do in \ncertain types of positions. For instance, in chess, it is usually bad to \ntrade a rook for a bishop or knight, even if you also end up winning a \nsingle pawn, but good if you win two pawns, so the material values should satisfy \nR&gt;B+P (to prevent the single-pawn trade) and R&lt;B+2P (to encourage \nthe double-pawn trade). The more of these inequalities you have, the \nsmaller the set of weights that satisfy them.\nThis can sometimes help get to a reasonable \nstarting approximation for the evaluation weights, but you probably still \nneed to do some adjustment afterwards.\n\n<P><LI><B>Hand tweaking</B>. Most commonly used. Simply play your program enough \ntimes to get an idea of its strengths and weaknesses, guess which \nparameters would improve those the best, and pick new values for the \nparameters. Produces a reasonable answer \nquickly. Requires that you understand the game well enough to play \nreasonable games against the computer and analyze what it does wrong\n(i.e. best when the computer is stupid and you are intelligent).\n</UL>\n\n... and without human intervention (much of this should be review from 171, \nfor those students who've taken 171 already; you probably won't have time to do \nmuch more than hand-tweaking):\n\n<UL>\n<P><LI><B>Hill-climbing</B> is like hand-tweaking: make \na small change to the weights, test out performance of that change, keep \nthe change only if performance improves, repeat many times.\nTends to be slow, get stuck in \"local optima\" in which eval weights are bad \nbut any change makes them even worse.\n\n<P><LI><B>Simulated annealing</B>. Like hill-climbing, makes small changes \nto eval and keeps changes that improve performance. But if change doesn't \nimprove performance, sometimes (randomly, with a certain probability) \naccepts the change anyway, in an attempt to escape local optima. Need to \nspecify these probabilities; they should start high and gradually become \nlower.  Even slower than hill-climbing but eventually can get good values.\n\n<P><LI><B>Genetic algorithms</B>. Hill-climbing and simulated annealing \nmaintain one good set of weights, which they change gradually.  Instead, \ngenetic algorithms maintain a collection of several different good sets of \nweights, add new sets to the collection by combining pairs of existing ones \n(take some weights from one and some from another, with a little mutation \nas well), and keep the size of the collection down by killing off sets of \nweights with bad performance.\n\n<P><LI><B>Neural networks</B>. Really, this is more a type of evaluation \nfunction than a method for choosing weights: a neuron is a function of the \nform threshhold(weighted sum of inputs), and one can form networks in which the \nneurons in the first layer take as inputs some basic features of the \nposition (e.g. the individual bits of a bitboard representation) and \nsuccessive layers take as inputs the neurons from the previous layer.\nSo a one-layer network with only one-input neurons is the same as the \nfirst-order evaluation functions we talked about last time, but it's \nstraightforward to build much more complicated neural networks, and \nit's not hard to use such a thing as the evaluation function (just \nrecompute the outputs of neurons with changed inputs).  The question is as \nbefore, how to set the weights?  Along with the other methods above, there \nare some that have been developed specifically for neural networks, such \nas \"temporal difference learning\". The basic idea is to decide when the \nnetwork makes a bad decision, and determine for each weight separately whether \nchanging it up or down would lead to a better decision, so it's a lot like \nhill-climbing.  One advantage of neural nets is they need even less human \nintelligence than the other automatic learning methods: you don't even \nreally need to understand the game well enough to program a decent \nevaluation function. However, in the time available to us (a few weeks), \nyou'll get good results faster by being more intelligent yourselves and \nleaving less of the work to the machines.\n</UL>\n\nAll of these methods require some method of automatically evaluating the \nperformance of a program.\n\n<UL>\n<P><LI>We can run the program on a large suite of test positions (say, \ntaken from high-quality human games) and see if it gets the right answers. \n\n<P><LI>We can play the program against some known opponent (say another \nprogram) and see how often it wins. Or, we can play the program against itself, or against other versions of \nitself; e.g. in hill-climbing the modified program can play against the \nunmodified one. Both of these have the disadvantage that, unless there is \nsome randomness in the system, both programs will play exactly the same \neach time, so you only get to see the results of one game which may not be \nrepresentative of overall play.  One possible way around this would be to \nstart playing several different games at positions taken from some test \nsuite.\n\n<P><LI>We can compare the results of the evaluation function with the \nresults of combining evaluation and search. If the eval is good, they \nshould be similar, but is vice versa true?\n</UL>\n\nWhat has actually been done in automatically learning evaluation weights?\nA good source for this is Jay Scott's\n\"<A HREF=\"http://satirist.org/learn-game/\">machine learning in\ngames</A>\" web page. He lists two experiments that I think are\nparticularly interesting:\n\n<UL>\n<P><LI>John Stanback (well-known as a commercial chess programmer) tried \nusing a genetic algorithm to set the weights in the evaluation function of \nhis program Zarkov.  He only ran 2000-3000 games, which I think is way too \nfew, and got material values that were ok but still worse than hand-tuned \nvalues. I think the lesson is that genetic algorithms do work, but need \neither a lot of generations or a good initial set of weights.\n\n<P><LI>Risto Miikkulainen, genetic algorithms researcher at U. Texas, gave \na talk last year on some experiments he'd done with Othello. He used a \ngenetic algorithm to tune the weights in a neural net evaluation function.\nPerformance evaluation of a net was done by play against a fixed opponent.\nIf the fixed opponent played randomly, the neural net learned an evaluation\nin the form of piece-square \ntables (pieces in corners are good, pieces next to corners are bad, etc) \nafter which it won all the time and stopped learning.\nAgainst an opponent combining piece-square tables with a short search,\nit eventually (after weeks of computer \ntime) learned a better mobility-based strategy.  But if its opponent was \nalready a sophisticated mobility-based program, it lost all the time and \nnever started learning.  I think the lesson is to play against programs of \nsimilar strength, for instance to compare different evaluations in the \ngenetic algorithm's current collection by playing them against each other, \nor alternatively to play against several fixed opponents of different \nstrengths.\n</UL>\n\n<HR>\n<A HREF=\"/~eppstein/\">David Eppstein,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>,\n<!--#flastmod file=\"970415.html\" -->.\n</BODY></HTML>\n", "encoding": "ascii"}