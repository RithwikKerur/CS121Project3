{"url": "https://www.ics.uci.edu/~majumder/VC/211HW3/vlfeat/doc/overview/gmm.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n   <html xmlns=\"http://www.w3.org/1999/xhtml\">\n <head>\n  <!-- IE Standards Mode -->\n  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"></meta>\n\n  <!-- Favicon -->\n  <link href=\"../images/vl_blue.ico\" type=\"image/x-icon\" rel=\"icon\"></link>\n  <link href=\"../images/vl_blue.ico\" type=\"image/x-icon\" rel=\"shortcut icon\"></link>\n\n  <!-- Page title -->\n  <title>VLFeat - Tutorials > Gaussian Mixture Models</title>\n\n  <!-- Stylesheets -->\n  <link href=\"../vlfeat.css\" type=\"text/css\" rel=\"stylesheet\"></link>\n  <link href=\"../pygmentize.css\" type=\"text/css\" rel=\"stylesheet\"></link>\n  <style xml:space=\"preserve\">\n    /* fixes a conflict between Pygmentize and MathJax */\n    .MathJax .mo, .MathJax .mi {color: inherit ! important}\n  </style>\n  \n\n  <!-- Scripts-->\n  \n\n  <!-- MathJax -->\n  <script xml:space=\"preserve\" type=\"text/x-mathjax-config\">\n    MathJax.Hub.Config({\n    tex2jax: {\n      inlineMath: [ ['$','$'], ['\\\\(','\\\\)'] ],\n      processEscapes: true,\n    },\n    TeX: {\n      Macros: {\n        balpha: '\\\\boldsymbol{\\\\alpha}',\n        bc: '\\\\mathbf{c}',\n        be: '\\\\mathbf{e}',\n        bg: '\\\\mathbf{g}',\n        bq: '\\\\mathbf{q}',\n        bu: '\\\\mathbf{u}',\n        bv: '\\\\mathbf{v}',\n        bw: '\\\\mathbf{w}',\n        bx: '\\\\mathbf{x}',\n        by: '\\\\mathbf{y}',\n        bz: '\\\\mathbf{z}',\n        bsigma: '\\\\mathbf{\\\\sigma}',\n        sign: '\\\\operatorname{sign}',\n        diag: '\\\\operatorname{diag}',\n        real: '\\\\mathbb{R}',\n      },\n      equationNumbers: { autoNumber: 'AMS' }\n      }\n    });\n  </script>\n  <script src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" xml:space=\"preserve\" type=\"text/javascript\"></script>\n\n  <!-- Google Custom Search -->\n  <script xml:space=\"preserve\">\n    (function() {\n    var cx = '003215582122030917471:oq23albfeam';\n    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;\n    gcse.src = (document.location.protocol == 'https' ? 'https:' : 'http:') +\n    '//www.google.com/cse/cse.js?cx=' + cx;\n    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);\n    })();\n  </script>\n\n  <!-- Google Analytics -->\n  <script xml:space=\"preserve\" type=\"text/javascript\">\n    var _gaq = _gaq || [];\n    _gaq.push(['_setAccount', 'UA-4936091-2']);\n    _gaq.push(['_trackPageview']);\n    (function() {\n    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n    })();\n  </script>\n </head>\n\n <!-- Body Start -->\n <body>\n  <div id=\"header-section\">\n    <div id=\"header\">\n      <!-- Google CSE Search Box -->\n      <div class=\"searchbox\">\n        <gcse:searchbox-only autoCompleteMaxCompletions=\"5\" autoCompleteMatchType=\"any\" resultsUrl=\"http://www.vlfeat.org/search.html\"></gcse:searchbox-only>\n      </div>\n      <h1 id=\"id-16\"><a shape=\"rect\" href=\"../index.html\" class=\"plain\"><span id=\"vlfeat\">VLFeat</span><span id=\"dotorg\">.org</span></a></h1>\n    </div>\n    <div id=\"sidebar\"> <!-- Navigation Start -->\n      <ul>\n<li><a href=\"../index.html\">Home</a>\n<ul>\n<li><a href=\"../about.html\">About</a>\n</li>\n<li><a href=\"../license.html\">License</a>\n</li>\n</ul></li>\n<li><a href=\"../download.html\">Download</a>\n<ul>\n<li><a href=\"../install-matlab.html\">Using from MATLAB</a>\n</li>\n<li><a href=\"../install-octave.html\">Using from Octave</a>\n</li>\n<li><a href=\"../install-shell.html\">Using from the command line</a>\n</li>\n<li><a href=\"../install-c.html\">Using from C</a>\n<ul>\n<li><a href=\"../xcode.html\">Xcode</a>\n</li>\n<li><a href=\"../vsexpress.html\">Visual C++</a>\n</li>\n<li><a href=\"../gcc.html\">g++</a>\n</li>\n</ul></li>\n<li><a href=\"../compiling.html\">Compiling</a>\n<ul>\n<li><a href=\"../compiling-unix.html\">Compiling on UNIX-like platforms</a>\n</li>\n<li><a href=\"../compiling-windows.html\">Compiling on Windows</a>\n</li>\n</ul></li>\n</ul></li>\n<li class='active'><a href=\"tut.html\">Tutorials</a>\n<ul>\n<li><a href=\"frame.html\">Local feature frames</a>\n</li>\n<li><a href=\"covdet.html\">Covariant feature detectors</a>\n</li>\n<li><a href=\"hog.html\">HOG features</a>\n</li>\n<li><a href=\"sift.html\">SIFT detector and descriptor</a>\n</li>\n<li><a href=\"dsift.html\">Dense SIFT</a>\n</li>\n<li><a href=\"liop.html\">LIOP local descriptor</a>\n</li>\n<li><a href=\"mser.html\">MSER feature detector</a>\n</li>\n<li><a href=\"imdisttf.html\">Distance transform</a>\n</li>\n<li><a href=\"encodings.html\">Fisher Vector and VLAD</a>\n</li>\n<li class='active' class='activeLeaf'><a href=\"gmm.html\">Gaussian Mixture Models</a>\n</li>\n<li><a href=\"kmeans.html\">K-means clustering</a>\n</li>\n<li><a href=\"aib.html\">Agglomerative Infromation Bottleneck</a>\n</li>\n<li><a href=\"quickshift.html\">Quick shift superpixels</a>\n</li>\n<li><a href=\"slic.html\">SLIC superpixels</a>\n</li>\n<li><a href=\"svm.html#tut.svm\">Support Vector Machines (SVMs)</a>\n</li>\n<li><a href=\"kdtree.html\">KD-trees and forests</a>\n</li>\n<li><a href=\"plots-rank.html\">Plotting AP and ROC curves</a>\n</li>\n<li><a href=\"utils.html\">Miscellaneous utilities</a>\n</li>\n<li><a href=\"ikm.html\">Integer K-means</a>\n</li>\n<li><a href=\"hikm.html\">Hierarchical integer k-means</a>\n</li>\n</ul></li>\n<li><a href=\"../applications/apps.html\">Applications</a>\n</li>\n<li><a href=\"../doc.html\">Documentation</a>\n<ul>\n<li><a href=\"../matlab/matlab.html\">MATLAB API</a>\n</li>\n<li><a href=\"../api/index.html\">C API</a>\n</li>\n<li><a href=\"../man/man.html\">Man pages</a>\n<ul>\n<li><a href=\"../man/mser.html\">mser</a>\n</li>\n<li><a href=\"../man/sift.html\">sift</a>\n</li>\n<li><a href=\"../man/vlfeat.html\">vlfeat</a>\n</li>\n</ul></li>\n</ul></li>\n</ul>\n\n    </div> <!-- sidebar -->\n  </div>\n  <div id=\"headbanner-section\">\n    <div id=\"headbanner\">\n      <span class='page'><a href=\"tut.html\">Tutorials</a></span><span class='separator'>></span><span class='page'><a href=\"gmm.html\">Gaussian Mixture Models</a></span>\n    </div>\n  </div>\n  <div id=\"content-section\">\n    <div id=\"content-wrapper\">\n      <div id=\"content\">\n        \n    \n\n<div class='toc'>\n<h3>Table of Contents</h3><ul><li class=\"level1\"><a href=\"#tut.gmm.introduction\">Learning a GMM with expectation maximization</a></li>\n<ul><li class=\"level2\"><a href=\"#tut.gmm.cov\">Diagonal covariance restriction</a></li>\n</ul>\n<li class=\"level1\"><a href=\"#tut.gmm.initialization\">Initializing a GMM model before running EM</a></li>\n</ul>\n</div><!-- Table of contents -->\n\n\n<p>This tutorial shows how to estiamte <a shape=\"rect\" href=\"../api/gmm.html\">Gaussian\nmixture model</a> using the VlFeat implementation of\nthe <em>Expectation Maximization</em> (EM) algorithm.</p>\n\n<p>A GMM is a collection of $K$ Gaussian distribution. Each\ndistribution is called a <em>mode</em> of the GMM and represents a\ncluster of data points.  In computer vision applications, GMM are\noften used to model <em>dictionaries of visual words</em>. One\nimportant application is the computation\nof <a shape=\"rect\" href=\"encodings.html#tut.fisher\">Fisher vectors encodings</a>.</p>\n\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n<h1 id=\"tut.gmm.introduction\">Learning a GMM with expectation maximization</h1>\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n\n<p>Consider a dataset containing 1000 randomly sampled 2D points:</p>\n\n<div class=\"highlight\"><pre><span class=\"n\">numPoints</span> <span class=\"p\">=</span> <span class=\"mi\">1000</span> <span class=\"p\">;</span>\n<span class=\"n\">dimension</span> <span class=\"p\">=</span> <span class=\"mi\">2</span> <span class=\"p\">;</span>\n<span class=\"n\">data</span> <span class=\"p\">=</span> <span class=\"nb\">rand</span><span class=\"p\">(</span><span class=\"n\">dimension</span><span class=\"p\">,</span><span class=\"n\">N</span><span class=\"p\">)</span> <span class=\"p\">;</span>\n</pre></div>\n\n\n<p>The goal is to fit a GMM to this data. This can be obtained by\nrunning the <code/><a href=../matlab/vl_gmm.html>vl_gmm</a></code> function, implementing\nthe <a shape=\"rect\" href=\"../api/gmm-fundamentals.html#gmm-em\">EM algorithm</a>.</p>\n\n<div class=\"highlight\"><pre><span class=\"n\">numClusters</span> <span class=\"p\">=</span> <span class=\"mi\">30</span> <span class=\"p\">;</span>\n<span class=\"p\">[</span><span class=\"n\">means</span><span class=\"p\">,</span> <span class=\"n\">covariances</span><span class=\"p\">,</span> <span class=\"n\">priors</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"n\">vl_gmm</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">numClusters</span><span class=\"p\">)</span> <span class=\"p\">;</span>\n</pre></div>\n\n\n<p>Here <code/>means</code>, <code/>covariances</code>\nand <code/>priors</code> are respectively the means $\\mu_k$, diagonal\ncovariance matrices $\\Sigma_k$, and prior probabilities $\\pi_k$ of\nthe <code/>numClusters</code> Gaussian modes.</p>\n\n<p>These modes can be visualized on the 2D plane by plotting ellipses\ncorresponding to the equation:\n\\[\n   \\{ \\bx: (\\bx-\\mu_k)^\\top \\Sigma_k^{-1} (\\bx-\\mu_k) = 1 \\}\n\\]\nfor each of the modes. To this end, we can use\nthe <code/><a href=../matlab/vl_plotframe.html>vl_plotframe</a></code>:</p>\n\n<div class=\"highlight\"><pre><span class=\"n\">figure</span> <span class=\"p\">;</span>\n<span class=\"n\">hold</span> <span class=\"n\">on</span> <span class=\"p\">;</span>\n<span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,:),</span><span class=\"n\">data</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,:),</span><span class=\"s\">&#39;r.&#39;</span><span class=\"p\">)</span> <span class=\"p\">;</span>\n<span class=\"k\">for</span> <span class=\"nb\">i</span><span class=\"p\">=</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">numClusters</span>\n    <span class=\"n\">vl_plotframe</span><span class=\"p\">([</span><span class=\"n\">means</span><span class=\"p\">(:,</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">&#39;</span> <span class=\"n\">sigmas</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"nb\">i</span><span class=\"p\">)</span> <span class=\"mi\">0</span> <span class=\"n\">sigmas</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"nb\">i</span><span class=\"p\">)]);</span>\n<span class=\"k\">end</span>\n</pre></div>\n\n\n<p>This results in the figure:</p>\n\n<div class=\"figure\">\n  <image src=\"../demo/gmm_2d_rand.jpg\"></image>\n  <div class=\"caption\">GMM fittting 2D random points.</div>\n</div>\n\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n<h2 id=\"tut.gmm.cov\">Diagonal covariance restriction</h2>\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n\n<p>Note that the ellipses in the previous example are axis\nalligned. This is a restriction of the <code/><a href=../matlab/vl_gmm.html>vl_gmm</a></code>\nimplementation that imposes covariance matrices to be diagonal.</p>\n\n<p>This is suitable for most computer vision applications, where\nestimating a full covariance matrix would be prohebitive due to the\nrelative high dimensionality of the data. For example, when clustering\nSIFT features, the data has dimension 128, and each full covariance\nmatrix would contain more than 8k parameters.</p>\n\n<p>For this reason, it is sometimes desirable to globally decorrelated\nthe data before learning a GMM mode. This can be obtained by\npre-multiplying the data by the inverse of a square root of its\ncovariance.</p>\n\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n<h1 id=\"tut.gmm.initialization\">Initializing a GMM model before running EM</h1>\n<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->\n\n<p>The EM algorithm is a local optimization method, and hence\nparticularly sensitive to the initialization of the model. The\nsimplest way to initiate the GMM is to pick <code/>numClusters</code>\ndata points at random as mode means, initialize the individual\ncovariances as the covariance of the data, and assign equa prior\nprobabilities to the modes. This is the default initialization\nmethod used by <code/><a href=../matlab/vl_gmm.html>vl_gmm</a></code>.</p>\n\n<p>Alternatively, a user can specifiy manually the initial paramters\nof the GMM model by using the <code/>custom</code> initalization\nmethod. To do so, set\nthe <code/>'Initialization'</code> option  to <code/>'Custom'</code> and\nalso the options <code/>'InitMeans'</code>, <code/>'InitCovariances'</code> and\n<code/>'IniPriors'</code> to the desired values.</p>\n\n<p>A common approach to obtain an initial value for these parameters\nis to run KMeans first, as demonstrated in the following code\nsnippet:</p>\n\n<div class=\"highlight\"><pre><span class=\"n\">numClusters</span> <span class=\"p\">=</span> <span class=\"mi\">30</span><span class=\"p\">;</span>\n<span class=\"n\">numData</span> <span class=\"p\">=</span> <span class=\"mi\">1000</span><span class=\"p\">;</span>\n<span class=\"n\">dimension</span> <span class=\"p\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n<span class=\"n\">data</span> <span class=\"p\">=</span> <span class=\"nb\">rand</span><span class=\"p\">(</span><span class=\"n\">dimension</span><span class=\"p\">,</span><span class=\"n\">numData</span><span class=\"p\">);</span>\n\n<span class=\"c\">% Run KMeans to pre-cluster the data</span>\n<span class=\"p\">[</span><span class=\"n\">initMeans</span><span class=\"p\">,</span> <span class=\"n\">assignments</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"n\">vl_kmeans</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">numClusters</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;Algorithm&#39;</span><span class=\"p\">,</span><span class=\"s\">&#39;Lloyd&#39;</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;MaxNumIterations&#39;</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">);</span>\n\n<span class=\"n\">initCovariances</span> <span class=\"p\">=</span> <span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"n\">dimension</span><span class=\"p\">,</span><span class=\"n\">numClusters</span><span class=\"p\">);</span>\n<span class=\"n\">initPriors</span> <span class=\"p\">=</span> <span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">numClusters</span><span class=\"p\">);</span>\n\n<span class=\"c\">% Find the initial means, covariances and priors</span>\n<span class=\"k\">for</span> <span class=\"nb\">i</span><span class=\"p\">=</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">numClusters</span>\n    <span class=\"n\">data_k</span> <span class=\"p\">=</span> <span class=\"n\">data</span><span class=\"p\">(:,</span><span class=\"n\">assignments</span><span class=\"o\">==</span><span class=\"nb\">i</span><span class=\"p\">);</span>\n    <span class=\"n\">initPriors</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span> <span class=\"p\">=</span> <span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">data_k</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">numClusters</span><span class=\"p\">;</span>\n\n    <span class=\"k\">if</span> <span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">data_k</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"o\">||</span> <span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">data_k</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span>\n        <span class=\"n\">initCovariances</span><span class=\"p\">(:,</span><span class=\"nb\">i</span><span class=\"p\">)</span> <span class=\"p\">=</span> <span class=\"nb\">diag</span><span class=\"p\">(</span><span class=\"n\">cov</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">&#39;</span><span class=\"p\">));</span>\n    <span class=\"k\">else</span>\n        <span class=\"n\">initCovariances</span><span class=\"p\">(:,</span><span class=\"nb\">i</span><span class=\"p\">)</span> <span class=\"p\">=</span> <span class=\"nb\">diag</span><span class=\"p\">(</span><span class=\"n\">cov</span><span class=\"p\">(</span><span class=\"n\">data_k</span><span class=\"o\">&#39;</span><span class=\"p\">));</span>\n    <span class=\"k\">end</span>\n<span class=\"k\">end</span>\n\n<span class=\"c\">% Run EM starting from the given parameters</span>\n<span class=\"p\">[</span><span class=\"n\">means</span><span class=\"p\">,</span><span class=\"n\">covariances</span><span class=\"p\">,</span><span class=\"n\">priors</span><span class=\"p\">,</span><span class=\"n\">ll</span><span class=\"p\">,</span><span class=\"n\">posteriors</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"n\">vl_gmm</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">numClusters</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;initialization&#39;</span><span class=\"p\">,</span><span class=\"s\">&#39;custom&#39;</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;InitMeans&#39;</span><span class=\"p\">,</span><span class=\"n\">initMeans</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;InitCovariances&#39;</span><span class=\"p\">,</span><span class=\"n\">initCovariances</span><span class=\"p\">,</span> <span class=\"c\">...</span>\n    <span class=\"s\">&#39;InitPriors&#39;</span><span class=\"p\">,</span><span class=\"n\">initPriors</span><span class=\"p\">);</span>\n</pre></div>\n\n\n<p>The demo scripts <code/>vl_demo_gmm_2d</code>\nand <code/><a href=../matlab/demo/vl_demo_gmm_3d.html>vl_demo_gmm_3d</a></code> also produce cute colorized figures\nsuch as these: </p>\n\n<div class=\"figure\">\n  <image src=\"../demo/gmm_2d_shell.jpg\"></image>\n  <div class=\"caption\">The figure shows how the estimated gaussian\n  mixture looks like with and without the kmeans initialization.</div>\n</div>\n\n\n  \n      </div>\n      <div class=\"clear\">&nbsp;</div>\n    </div>\n  </div> <!-- content-section -->\n  <div id=\"footer-section\">\n    <div id=\"footer\">\n      &copy; 2007-13 The authors of VLFeat\n    </div> <!-- footer -->\n  </div> <!-- footer section -->\n </body>\n <!-- Body ends -->\n</html>\n ", "encoding": "ascii"}