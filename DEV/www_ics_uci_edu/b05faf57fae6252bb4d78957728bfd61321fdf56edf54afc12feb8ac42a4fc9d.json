{"url": "https://www.ics.uci.edu/~eppstein/161/960111.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Fibonacci Numbers</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 11, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Sequential and Binary Search</h1>\n\nExample: looking up a topic in Baase. Suppose after Tuesday's\napplication of matrix multiplication to Fibonacci numbers, that you\nwanted to know what she says about matrix multiplication. \n\n<p>You could look it up in the index but it will give you many\ndifferent pages to look at, some of which are only somewhat\nrelevant. Or you could read through the table of contents until you\nfind relevant looking titles (6.2 and 7.3).</p>\n\n<h2>Sequential search</h2>\n\nThe second method (reading through the table of contents) is an\nexample of <i>sequential search</i>. Similar sorts of problems show\nup all the time in programming (e.g. operating systems have to look\nup file names in a directory, and the Unix system usually does it\nwith sequential search). \n\n<p>Very abstractly:</p>\n\n<pre>\n    sequential search(list L,item x)\n    {\n    for (each item y in the list)\n        if (y matches x)\n        return y\n    return no match\n    }\n</pre>\n\nThis has many variants -- do you stop once you've found one match\nor do you keep going until you've found all of them? do you\nrepresent the list using pointers, linked lists, or what? how do\nyou indicate that there was no match? \n\n<p>So we want to analyse this...</p>\n\n<p>To really understand the running time, we have to know how quick\nthe \"y matches x\" part is -- everything else is straightforward.\nThe way I've written it in pseudocode, that part still needs to be\nfilled in. But we can still analyse the algorithm! We just measure\nthe time in terms of the number of comparisons.</p>\n\n<p>Examples: does 8 appear in the list of the first 10 Fib.\nnumbers? Does 9? Note for 9, algorithm has to go through whole\nlist.</p>\n\n<p>So the time seems to depend on both L and x. We want to be able\nto predict the time easily without running the algorithm, so\nsaying</p>\n\n<pre>\n    comparisons(x,L) = position of x in L\n</pre>\n\nis true but not very informative. \n\n<h2>Methods of analysis</h2>\n\nTo be able to predict the time without having to look at the\ndetails of the input, we measure it as a function of the length of\nthe input. Here x has basically constant length (depending on what\nan item is) and the length of L is just the number of items in it. \n\n<p>So given a list L with n items, how many comparisons does the\nalgorithm take? Answer: it depends.</p>\n\n<p>We want an answer that doesn't depend. There are various ways of\ngetting one, by combining the times for different inputs with the\nsame length.</p>\n\n<ul>\n<li style=\"list-style: none\"><a name=\"worst\"></a></li>\n\n<li><b>Worst case analysis</b> -- what is the most comparisons we\ncould ever see no matter how perverse the input is? \n\n<pre>\n    time_wc(n) =        max         time(I)\n         (input I of size n) \n</pre>\n\n<a name=\"best\"></a></li>\n\n<li><b>Best case analysis</b> -- what is the fewest comparisons the\nalgorithm could take if the input is well behaved? \n\n<pre>\n    time_wc(n) =        min         time(I)\n         (input I of size n) \n</pre>\n\n<a name=\"average\"></a></li>\n\n<li><b>Average case analysis</b> -- how much time would the\nalgorithm take on \"typical\" input? \n\n<p>We assume that each input I of size n has a probability P[I] of\nbeing the actual input and use these proabilities to find a\nweighted average:</p>\n\n<pre>\n    time_avg(n) = sum P[I] time(I)\n</pre>\n</li>\n</ul>\n\nThese distinctions didn't make sense with Fibonacci numbers because\nthe time there was always a function of n, but here they can give\ndifferent answers (we'll see with sequential search). \n\n<p>Average case is probably the most important in general, but is\nproblematic in terms of what is a typical input? You have to make\nsome assumption about the probabilities, and your analysis will\nonly be as accurate as the validity of your assumptions. Also note\nthat it's possible to have an algorithm for which no input takes\nthe \"average\" time -- e.g. if it takes either 1 step or 100 steps,\nthe average may be around 50 even though no input actually takes 50\nsteps.</p>\n\n<p>Worst case is what we usually do, it's easier than average case\nanalysis and it's useful because you can guarantee that the\nalgorithm will not ever take longer than its worst case bound. It's\nalso true that the average case is at most the worst case, no\nmatter what probabilities you choose, so you can use worst case\nanalysis to get some information about the average case without\nhaving to make assumptions about what a \"typical\" input looks\nlike.</p>\n\n<p>Best case is fun but not very useful.</p>\n\n<h2>Analysis of sequential search</h2>\n\nThe best case for sequential search is that it does one comparison,\nand matches X right away. \n\n<p>In the worst case, sequential search does n comparisons, and\neither matches the last item in the list or doesn't match\nanything.</p>\n\n<p>The average case is harder to do. We know that the number of\ncomparisons is the position of x in the list. But what is typical\nposition of x?</p>\n\n<p>One reasonable assumption: If x is in the list, it's equally\nlikely to be anywhere in it. so P[pos] = 1/n.</p>\n\n<pre>\n    average number of comparisons\n\n     n   1\n     =  sum  - . i\n    i=1  n\n\n    1  n\n     =  - sum  i\n    n i=1\n\n     = (n+1)/2.\n</pre>\n\nBut if x is not in the list, the number of comparisons is always n.\n\n\n<p>So finding something takes half as long as not finding it, on\naverage, with this definition of \"typical\".</p>\n\n<p>We can define a stronger version of \"typical\": suppose for any\nlist, any permutation of the list is equally likely. Then we can\naverage over all possible permutations:</p>\n\n<pre>\n    average number of comparisons\n\n     n!  1\n     =  sum  -  . (position of x in permutation i)\n    i=1  n!\n\n     n  1\n     =  sum - . p . (number of permutations with x in position p)\n    p=1 n!\n\n     n  1\n     =  sum - . p . (n-1)!\n    p=1 n!\n\n     n  1\n     =  sum - . p\n    p=1 n\n\n     = (n+1)/2.\n</pre>\n\nSo this assumption ends up giving the same analysis. \n\n<p>A second point to be made about average case analysis: sometimes\nit makes sense to analyse different cases separately. The analysis\nabove assumes x is always in the list; if x is not in the list, you\nalways get n comparisons. You could make up a probability p that x\nis in or out of the list and combine the two numbers above to get a\ntotal average number comparisons equal to pn + (1-p)(n+1)/2 but it\nmakes more sense to just report both numbers separately.</p>\n\n<h2>Randomized algorithms</h2>\n\nSometimes it's useful to pay a little bit to reduce the uncertainty\nin the world -- e.g. insurance, you know you'll pay a fixed amount\ninstead of either paying nothing (if you stay healthy) or a lot (if\nyou get appendicitis). \n\n<p>the same concept applies to computer programs -- if the worst\ncase is much larger than the average case, we might prefer to have\na slightly more complicated program that reduces the worst case as\nlong as it doesn't increase the average case too much. For instance\nif you're programming the computer controlling a car, and you want\nto tell if you're in a crash and should activate the air bags, you\ndon't want to be running some algorithm that usually takes half a\nsecond but maybe sometimes takes as much as five minutes.</p>\n\n<p>Random numbers are very useful in this respect. they're also\nuseful in making \"average case\" analysis apply even when the input\nitself is not random at all, or when we don't know a good\ndefinition for a \"typical\" input. The idea is to \"scramble\" the\ninput so that it looks typical. We say that an algorithm is <i>\nrandomized</i> if it uses random numbers. An algorithm that is not\nrandomized is called <i>deterministic</i>.</p>\n\n<p><a name=\"expected\">The \"expected time\" analysis of a random\nalgorithm is measured in terms of time(input,sequence of random\nnumbers). For some particular input I, the expected time of the\nalgorithm is just the average over different sequences of random\nnumbers:</a></p>\n\n<pre>\n        sum    Prob(R) . time(I,R)\n    (random sequence R)\n</pre>\n\nThe expected time of the algorithm on (worst case) inputs of length\nn is then computed by combining this formula with the previous\nformula for worst case analysis: \n\n<pre>\n        max             sum        Prob(R) . time(I,R)\n    (input I of size n) (random sequence R)\n</pre>\n\nThis looks complicated, but isn't usually much harder than average\ncase analysis. Here it is for sequential search. \n\n<p>We want to scramble (x,L) so that position of x in L is random.\nIdea: pick a random permutation of L then do the sequential\nsearch.</p>\n\n<pre>\n    randomized search(list L,item x)\n    {\n    randomly permute L\n    for (each item y in L)\n        if (y matches x)\n        return y\n    return no match\n    }\n</pre>\n\nThis slows down the algorithm somewhat (because you have to take\ntime to do the permutation) but may speed up the searching part. If\nyou're just searching for a number in a list of numbers, this would\nbe a pretty bad method, because the time for doing the random\npermutation would probably be more than the worst case for the\noriginal deterministic sequential search algorithm. However if\ncomparisons are very slow, much slower than the other steps in the\nalgorithm, the total number of comparisons will dominate the\noverall time and this algorithm could be an improvement. \n\n<p>Let's plug this algorithm into our formula for expected\ntimes:</p>\n\n<pre>\n    time = max(x,L) sum(permutation p) probability(p) time(x,p(L))\n</pre>\n\nNote there are n! permutations. of those, there are (n-1)! such\nthat x is in some given position i. \n\n<pre>\n    time = max    sum  sum                 prob(perm) . time(x,L')\n        (x,L)  i    (perm w/x at pos i)\n\n     = max    sum  #(perms w/x at pos i) 1/n! . i\n        (x,L)  i   \n\n     = max    sum  (n-1)!/n! . i\n        (x,L)  i   \n\n     = max(x,L) sum(i) i/n\n\n     = (n+1)/2\n</pre>\n\nso the number of comparisons is exactly the same as the average\ncase but now it doesn't matter what the list is. \n\n<p>We'll see that same idea of using random permutation to avoid\nthe worst case later, in the quicksort and quickselect algorithms.\nFor both of these algorithms, the use of randomization decreases\nthe running time enormously, from O(n^2) to O(n log n) or O(n).</p>\n\n<p>It is also sometimes possible to make stronger forms of analysis\nabout random algorithms than just their expected time, for instance\nwe could compute the variance of the running time, or prove\nstatements such as that with very high probability, an algorithm\nuses time close to its expectation. This is important if one wants\nto be sure that the slow possibilities are very rare, but is\nusually much more complicated, so we won't do much of that sort of\nanalysis in this class.</p>\n\n<h2>Binary search</h2>\n\nLet's go back to the original example -- finding matrix\nmultiplication in Baase. I talked about looking it up in the table\nof contents (by sequential search) but also about looking it up in\nthe index. \n\n<p>The index of Baase and most other books has the useful property\nthat it's alphabetized, so we can be smarter about our search. For\ninstance, we could stop the sequential search whenever we found a\ny&gt;x, and this would speed up the time for x not in L. But we can\nbe much better, and this is basically what people do in\nalphabetized lists.</p>\n\n<pre>\n    binary search(x,L)\n    {\n    let n = length of L, i=n/2.\n    if (n = 0) return no match\n    else if (L[i] matches x) return L[i]\n    else if (L[i] &gt; x) binary search(x,L[1..i-1])\n    else binary search(x,L[i+1..n])\n    }\n</pre>\n\nRecursion is not really necessary: \n\n<pre>\n    alternate search(x,L)\n    {\n    let n = length of L\n    let a = 1, b = n\n    while (L[i = (a+b)/2] doesn't match)\n        if (L[i] &gt; x) b = i-1\n        else a = i+1        \n        if a&gt;b return no match\n    return L[i]\n    }\n</pre>\n\nAnalysis: T(n) = O(1) + T(n/2) = O(log n) \n\n<p>More precisely in the worst case, T(n) = 2 + T(ceiling((n-1)/2))\nwhich solves to approximately 2 log n (logarithm to base 2).</p>\n\n<p>So binary search is fast, but in order to use it we need to\nsomehow get the list to be in sorted order -- this problem is known\nas sorting, and we'll see it in much detail next week.</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960111.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}