{"url": "https://www.ics.uci.edu/~dchudova/278/Report.html#ClassificationResults", "content": "<!doctype html public \"-//w3c//dtd html 4.0 transitional//en\">\n<html>\n<head>\n   <meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1251\">\n   <meta name=\"Generator\" content=\"Microsoft Word 97\">\n   <meta name=\"Template\" content=\"F:\\MSOFFICE\\OFFICE\\html.dot\">\n   <meta name=\"GENERATOR\" content=\"Mozilla/4.7 [en] (WinNT; U) [Netscape]\">\n   <meta name=\"Author\" content=\"Dasha Chudova\">\n   <title>ICS 278 Report DChudova</title>\n</head>\n<body text=\"#000000\" bgcolor=\"#FFFFFF\" link=\"#0000FF\" vlink=\"#800080\" alink=\"#FF0000\">\n\n<center>\n<h1>\nClassification and discovery of local patterns in transactional data</h1></center>\n\n<center><b><font face=\"Arial\"><font size=+1><a href=\"http://www.ics.uci.edu/~dchudova\">Darya\nChudova</a></font></font></b>\n<p>Project Report for <a href=\"http://www.ics.uci.edu/~smyth/courses/ics278/\">ICS\n278</a>, Fall 1999\n<br>Instructor: <a href=\"http://www.ics.uci.edu/~smyth\">Prof. Padhraic\nSmyth</a></center>\n\n<p><b><font face=\"Arial\"><font size=+1>Data Description</font></font></b>\n<p>I worked with transactional data that were supplied by one of the commercial\nfirms and were thus considered confidential. The data reflect the purchases\nmade during two years at a large chain of retail department stores. The\ninformation available for each transaction includes:\n<ul>\n<li>\nCustomer ID</li>\n\n<li>\nDate of purchase</li>\n\n<li>\nList of product IDs purchased</li>\n\n<li>\nPrice and quantity of each item</li>\n</ul>\nThere is a total of ~500,000 transaction records residing in the Informix\ndatabase that correspond to ~190,000 different customers. Additionally,\nthe hierarchy of product IDs is available so that each product can be mapped\nto a particular department, style, color etc. Attributes of the data are\nmostly categorical.\n<p><b><font face=\"Arial\"><font size=+1>Task Definition</font></font></b>\n<p>I used the information on how frequently the customers made purchases\nin each department or how much money they spent at each department as attributes\nin my classification task. For any given customer I defined purchasing\nfrequency profile and spending profile, which is described in more detail\nbelow. I used C5.0 decision tree inducer to\n<ul>\n<li>\ndo classification and</li>\n\n<li>\nproduce a set of local rules.</li>\n</ul>\nFinally, I analyzed the quality of the obtained set of rules by their general\nproperties, such as length, confidence, support, quality, and generalization\nability.\n<p><b><font face=\"Arial\"><font size=+1>Algorithm Specification</font></font></b>\n<p>I used decision tree induction algorithm from the MLC++ library available\nat UCI. The original paper Induction of Decision Trees by Quinlan describing\nID3 algorithm dates back to 1985 and since that time decision tree algorithms\nare widely used in practice. There has been a wide range of refinements\nmade for the tree induction algorithms. A more recent and comprehensive\nwork that is considered classical in decision trees is Classification and\nRegression Trees (CART) by Breiman, Friedman and Olshen (1994). MLC++ library\ncontains implementations of the C4.5 and C5.0 decision tree algorithms\nthat both use sophisticated pruning mechanisms.\n<p>The high level outline of the algorithm on the continuous attributes\nis as follows:\n<p><i><font face=\"Baltica\">Learn Decision Tree (Tree &amp;T){</font></i>\n<blockquote><i><font face=\"Baltica\">Grow Tree (&amp;T);</font></i>\n<br><i><font face=\"Baltica\">Prune Tree (&amp;T);</font></i></blockquote>\n<i><font face=\"Baltica\">}</font></i>\n<p><i><font face=\"Baltica\">Grow Tree (Tree &amp;T){</font></i>\n<blockquote><i><font face=\"Baltica\">While (Not All Leafs of T are Pure){</font></i></blockquote>\n\n<ul>\n<ul>\n<li>\n<i><font face=\"Baltica\">Look at all possible attribute-value pairs</font></i></li>\n\n<li>\n<i><font face=\"Baltica\">(A<sup>best</sup>, V<sup>best</sup>)=Greedily choose\nand record the attribute-value pair that reduces impurityor entropy most</font></i></li>\n\n<li>\n<i><font face=\"Baltica\">Split the data set according to the selected best\nattribute-value\npair all data points whose value in the A<sup>best</sup> is less than V<sup>best</sup>\nwill be moved down to the left subtree and all the rest to the right subtree.</font></i></li>\n</ul>\n<i><font face=\"Baltica\">}</font></i></ul>\n<i><font face=\"Baltica\">}</font></i>\n<p>In the Grow Tree procedure the purity of the leaf node could be judged\nby for example the entropy or the GINI index. The tree is grown till all\ntraining patterns are correctly classified. Once the tree is grown and\nthe classification labels are assigned to its leaves it should be pruned\nand this is exactly the purpose of the Prune Tree routine. The idea of\npruning is that we might sacrifice some of the correctly classified patterns\nin the training data for the simplicity of the tree, which will make it\nmore biased. A lot of heuristics have been proposed to do the pruning,\nthe most complicated in my opinion being the one proposed by Breiman et\nal is based on the idea of the internal cross validation.\n<p>The main advantage of the decision trees that immediately makes it the\nnumber one technique in such spheres as, for example, medical diagnostics\nand credit card applications evaluation is the interpretability of the\ncreated model. Namely, it is easy to read the rules of the type if the\nFOREIGN_STUDENT=YES <b>and</b> YEARS_IN_COUNTRY &lt; 2 then CREDIT_CARD=NO\nfrom the tree. In the example above I used attributes FOREIGN_STUDENT (takes\n2 values YES and NO) and YEARS_IN_COUNTRY (integer) and CREDICT_CARD is\na classification label taking 2 values YES and NO. It becomes even more\nimportant for a doctor in the ER to know <b><u>how</u></b> the system came\nto a certain conclusion but not the final conclusion on its own. Now take\nfor example neural networks that are known to be able to approximate arbitrary\nfunction with any prespecified accuracy. Note that decision trees wont\nhave such ability since they are only capable of creating piece wise constant\ndecision boundaries. Neural networks in general will be much more accurate\nbut absolutely uninterpretable which tremendously limits their applicability.\nIn my project Id like to be able to come up with an interpretable model\nand this clearly makes decision trees the number one choice.\n<p>Finally, even if it might not be possible to learn a tree with a satisfying\nglobal accuracy one could look at the local regions in the original state\nspace returned by a decision tree where classification can be done with\nsatisfactory accuracy. I considered such an example in my original proposal.\n<p>Decision trees also have some limitations that one should be aware of.\nThe trees are usually grown so that the most informative attributes are\nselected first and the growth continues till the tree that makes no classification\nerrors. The attributes and their splitting values are selected greedily\nand there is no guarantee that the true best attribute and its splitting\nvalue will be selected. Secondly, the complexity of learning is slightly\nhigher than linear in the parameters of the data (namely, N*(log N) where\nN is the number of data points), so it might be problematic to scale it\nup for large data sets.\n<p><b><font face=\"Arial\"><font size=+1>Data Preprocessing</font></font></b>\n<p><b><i><font face=\"Arial\">Defining Customer Profile</font></i></b>\n<p>I considered two sets of input data for classification and rule finding.\nIn both cases, one training pattern corresponds to aggregated information\nabout all transactions of a given customer.\n<blockquote>The first data set contains each customers <b>purchasing frequency\nprofiles</b>.&nbsp; For this data set, attribute <i>i</i>, <i>i</i> =1,..,\n<i>N<sub>dep</sub></i>&nbsp;\nof training pattern <i>p</i>, <i>p</i> = 1, .., <i>N<sub>cust</sub></i>\n,&nbsp; <i>f<sub>ip</sub></i> was calculated as a ratio of number of items\npurchased by customer <i>p</i> in department\n<i>i</i> to the total number\nof items purchased by customer\n<i>p</i>.</blockquote>\n\n<blockquote>The second data set contains customers <b>spending profiles</b>.\nFor this data set, attribute <i>i</i>, <i>i</i> =1,.., <i>N<sub>dep</sub></i>&nbsp;\nof training pattern <i>p</i>, <i>p</i> = 1, .., <i>N<sub>cust</sub></i>\n,&nbsp; <i>P<sub>ip</sub></i> was calculated as a ratio of amount spent\nby customer <i>p</i> in department <i>i</i> to the total amount spent by\ncustomer\n<i>p</i>.</blockquote>\nGenerally speaking, the profiles are defined as relative frequency of purchasing\nin each department and relative spending in each department respectively.\nThe total number of departments in both data sets <i>N<sub>dep </sub></i>was\nequal to <b>55</b>.\n<p>I worked with a subset of all transactions in the database, that corresponded\nto a total of 50000 transactions. Profile based data sets contained <i>N<sub>cust</sub></i>\n=&nbsp; <b>8150</b> patterns.\n<p><b><i><font face=\"Arial\">Frequency Profile Vs. Spending Profile</font></i></b>\n<p>In order to understand possible differences in the predictive power\nof each data set, I studied the correlation between corresponding columns\nin both sets (correlation between frequency of making a purchase in the\ndepartment and relative spending in the department). The results summarized\nin a chart below not surprisingly show very high correlation and so the\nclassification accuracy based on two data sets was expected to be very\nsimilar. Note, that correlation with a value of 0 here denotes departments\nwith constant frequency / spending across all customers (actually, zero\nfrequency and spending). This might be a poor representation as results\nmay appear deceiving at the first sight...\n<center>\n<p><img SRC=\"Correlation2.JPG\" height=420 width=560>\n<p><b>Fig.1 Frequency / Spending profiles correlation</b></center>\n\n<p><b><font face=\"Arial\"><font size=+1>Data Analysis Results</font></font></b>\n<p>I used the data sets described above to generate different splits into\ntraining / test sets, and to perform 10-fold cross validation for decision\ntree induction algorithm. I was interested in the following cross validated\nparameters: average classification error, standard deviation of the CV\nerror, average number of false positives (infrequent customers classified\nas being frequent) and false negatives (frequent customers classified as\ninfrequent). The last two parameters were of interest since the ratio of\nthe number of positive examples to the number of negative examples in the\noriginal data set was 0.16. When doing cross validation, I preserved this\nratio within each test / train set.\n<br><a NAME=\"ClassificationResults\"></a>\n<p><b><i><font face=\"Arial\">Classification Results</font></i></b>\n<p>The experiments show very good performance in terms of the overall accuracy.\nAn important factor here is not only the value of the error itself, but\nalso a low standard deviation of error across different folds. This shows\nthe stability of the classification quality on different train / test splits.\nIn terms of estimating the accuracy of the classifier I think it would\nbe better to look at the rate of false positives / false negatives rather\nthan the overall error because of the unbalanced training / test sets.\nThe highest fraction of error appears to be the false positive rate (~12%),\nwhich stands for infrequent customers incorrectly classified as frequent.\nNote, that missing a frequent customer in this classification is a relatively\nrear event (~3.2%), which might be more important in marketing applications,\nwhere there is a need to identify a set of potentially frequent customers\nwithout missing any. Frequency profile showed a slightly better performance\nacross all folds, which could be attributed to the presence of additional\n(higher order) information in these data, not captured by the correlation\nanalysis above.\n<br><a NAME=\"CVResults\"></a>\n<br>&nbsp;\n<br>&nbsp;\n<table BORDER=2 CELLSPACING=2 CELLPADDING=7 WIDTH=\"608\" BGCOLOR=\"#FFFFCC\" BORDERCOLOR=\"#808000\" >\n<tr>\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#800000\"></td>\n\n<td VALIGN=TOP WIDTH=\"15%\" BGCOLOR=\"#800000\"><b><i><font color=\"#FFFFFF\"><font size=+0>CV\nError</font></font></i></b></td>\n\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#800000\"><b><i><font color=\"#FFFFFF\"><font size=+0>Standard\nDeviation of CV Error</font></font></i></b></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#800000\"><b><i><font color=\"#FFFFFF\"><font size=+0>CV\nFalse Positive</font></font></i></b></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#800000\"><b><i><font color=\"#FFFFFF\"><font size=+0>CV\nFalse Negative</font></font></i></b></td>\n</tr>\n\n<tr BGCOLOR=\"#FFFFCC\">\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#FFFFCC\"><b><i><font size=+0>Frequency\nProfile</font></i></b></td>\n\n<td VALIGN=TOP WIDTH=\"15%\" BGCOLOR=\"#FFFFCC\"><font size=+0>4.34%</font></td>\n\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#FFFFCC\"><font size=+0>0.77%</font></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#FFFFCC\"><font size=+0>12.07%</font></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#FFFFCC\"><font size=+0>3.17%</font></td>\n</tr>\n\n<tr>\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#FFFFCC\"><b><i><font size=+0>Spending\nProfile</font></i></b></td>\n\n<td VALIGN=TOP WIDTH=\"15%\" BGCOLOR=\"#FFFFCC\"><font size=+0>4.71%</font></td>\n\n<td VALIGN=TOP WIDTH=\"27%\" BGCOLOR=\"#FFFFCC\"><font size=+0>0.66%</font></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#FFFFCC\"><font size=+0>12.71%</font></td>\n\n<td VALIGN=TOP WIDTH=\"16%\" BGCOLOR=\"#FFFFCC\"><font size=+0>3.53%</font></td>\n</tr>\n</table>\n\n<p><b>Table 1. Cross validated error of decision tree classifier</b>\n<br><a NAME=\"LocalRules\"></a>\n<p><b><i><font face=\"Arial\">Local Rules</font></i></b>\n<p>On average, the algorithm produced 45 rules when trained to classify\na set of over 7000 training patterns. Out of those 45 rules, 13 identify\ninfrequent customers. Total number of extracted rules was fairly low and\nin my opinion this means that the differences between frequent and infrequent\ncustomers can be found on a generic enough level. Along with the low CV\nclassification error and low variation of the error across the folds, it\nallows to conclude that there is a significant amount of predictive power\nin the input data.\n<p>I analyzed frequency of occurrence of each department in the set of\nrules, i.e. the number of times each department was used as a part of a\nrule. The most frequent departments in this experiment appeared to be \"LADIES\nSHIRTS\" (occurs in 15 rules) and \"SHOES\" (occurs in 10 rules). It is interesting\nto note, that \"SHOES\" department occurred as \"has purchases in the department\"\n/ \"doesn't have purchases in the department\" binary test in 9 out of 10\nrules. \"LADIES SHIRTS\" occurred as a binary test in 11 out of 15 rules.&nbsp;<a NAME=\"DeptOccurence\"></a>\n<center>\n<p><img SRC=\"Frequency.JPG\" height=350 width=570 align=ABSBOTTOM>\n<br><b>Fig. 2 Occurrence of departments in classification rules</b></center>\n\n<p><br>\n<br>\n<br>\n<p>Another interesting parameter of produced rules is their length. The\nshorter rules can be viewed as more generic, and the rules produced based\non Customer Spending Profile appear not to involve more than 3-4 departments.\nThis opens a way to researching methods to identify \"sufficient portions\"\nof customer profile, that could provide enough information to classify\na customer into a particular category. It may happen that a narrow subset\nof customer characteristics would be sufficient for a successful classification.\n<br><a NAME=\"RuleLength\"></a>\n<center>\n<p><img SRC=\"RuleLength.JPG\" height=350 width=570>\n<br><b>Fig.3 Rule Length (Customer Spending profile)</b></center>\n\n<p>The last aspect of the rules produced by C5.0 that I'd like to cover\nhere is the confidence level for each rule. 24 rules out of 45 had a confidence\nlevel greater than 0.95. Only a couple of rules (they appeared to be the\nones with just one department as a left-hand side) had a confidence level\nof less than 0.8.\n<br><a NAME=\"RuleConfidence\"></a>\n<center>\n<p><img SRC=\"RuleConfidence.JPG\" height=350 width=570>\n<br><b>Fig.4 Rule Confidence levels</b>\n<hr WIDTH=\"100%\">\n<br><a NAME=\"Extensions\"></a>\n<br><b><font face=\"Arial\"><font size=+1>Possible extensions of work</font></font></b></center>\n\n<p>The work done in the frameworks of the project may be viewed as a basis\nfor further research, especially in the area of extracting local patterns\nfrom transactional data. This project was solely devoted to discrimination\nbetween frequent and infrequent customers. There might be many more tasks\nof practical interest posed with respect to this database, for example\nwhich items are most often purchased together and so on.\n<p>There are also some generic open questions about the quality of extracted\nrules. These questions include:\n<ul>\n<li>\nStability of rules across different sets of customers in the training data</li>\n\n<li>\nStability of rules over time. Let's say the profile of one customer was\ngenerated at the moment of time T1, and profile of the other one at T2,\nand suppose both customers have different length of transaction history\nwith the store. Can the same set of rules be applied to their profiles\nto get an accurate answer? The dynamics of the profiles wasn't taken into\naccount in my experiments, rather they were assumed static.</li>\n\n<li>\nLevel of support for each rule. C5.0 produces only a probability identifying\nthe confidence of each rule. It would be beneficial to be able to analyze\nsupport as well.</li>\n</ul>\n\n</body>\n</html>\n", "encoding": "ascii"}