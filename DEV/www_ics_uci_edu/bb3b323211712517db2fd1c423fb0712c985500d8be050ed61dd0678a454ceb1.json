{"url": "https://www.ics.uci.edu/~thornton/ics46/Notes/SkipLists/", "content": "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\r\n<!DOCTYPE html PUBLIC\r\n \"-//W3C//DTD XHTML 1.1//EN\"\r\n \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\r\n\r\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\">\r\n\r\n<head>\r\n<meta http-equiv=\"content-type\" content=\"text/html; charset=iso-8859-1\" />\r\n<link rel=\"stylesheet\" href=\"../../course.css\" type=\"text/css\" />\r\n\r\n<title>ICS 46 Spring 2018, Notes and Examples: Skip Lists</title>\r\n\r\n</head>\r\n\r\n<body>\r\n\r\n<div class=\"navbar\">\r\n\r\n<p>\r\nICS 46 Spring 2018 |\r\n<a href=\"../../index.html\">News</a> |\r\n<a href=\"../../CourseReference.html\">Course Reference</a> |\r\n<a href=\"../../Schedule.html\">Schedule</a> |\r\n<a href=\"../../ProjectGuide\">Project Guide</a> |\r\n<a href=\"../../Notes\">Notes and Examples</a> |\r\n<a href=\"http://www.ics.uci.edu/~thornton/\">About Alex</a>\r\n</p>\r\n\r\n<hr />\r\n\r\n</div>\r\n\r\n<div class=\"header\">\r\n\r\n<p>ICS 46 Spring 2018<br />\r\n   Notes and Examples: Skip Lists</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Recalling binary search</p>\r\n\r\n<p>If you have an array of elements that you know are sorted, and you want to check whether a particular value is in that array, there is a well-known algortihm called <i>binary search</i> that can do the job surprisingly quickly.  Consider this array of integers sorted in ascending order:</p>\r\n\r\n<table class=\"normal\">\r\n  <tr class=\"top\">\r\n    <td>0</td>\r\n    <td>1</td>\r\n    <td>2</td>\r\n    <td>3</td>\r\n    <td>4</td>\r\n    <td>5</td>\r\n    <td>6</td>\r\n    <td>7</td>\r\n    <td>8</td>\r\n  </tr>\r\n  <tr>\r\n    <td>11</td>\r\n    <td>13</td>\r\n    <td>20</td>\r\n    <td>34</td>\r\n    <td>37</td>\r\n    <td>44</td>\r\n    <td>59</td>\r\n    <td>67</td>\r\n    <td>71</td>\r\n  </tr>\r\n</table>\r\n\r\n<p>Suppose we were searching for the integer 34.  Binary search would have us solve the searching problem by first looking at the middle element of the array.  If it's the one we want, we're done.  If not, we instead see how it compares to what we're searching for.  The middle element is 37, because there are four elements before it and four elements after it.  While it isn't the element we're looking for, we do know two important things that will help us dramatically as we continue our search:</p>\r\n\r\n<ul>\r\n  <li>34 is smaller than 37</li>\r\n  <li>The array is sorted, which means that 34 could only be in a cell with a smaller index than the one we're looking at; the others can be eliminated from consideration entirely</li>\r\n</ul>\r\n\r\n<p>The algorithm proceeds similarly, looking at the middle of the remaining elements and eliminating (slightly more than) half the elements from subsequent consideration.  After roughly log<sub><small>2</small></sub><i>n</i> steps in an <i>n</i>-element array, we'll have narrowed our search down to a single element, and no others will need to be looked at.  So we would say that a binary search would take <i>O</i>(log <i>n</i>) time to complete.</p>\r\n\r\n<p>When first confronted with a logarithmic search algorithm like this, it sounds like the silver bullet you may have been looking for.  But, of course, we can't always use binary search, for the simple reason that the cost of sorting the array &mdash; if it isn't already sorted &mdash; would be quite high (&Omega;(n) time, since you might have to move every element to its correct position, and quite likely worse than this).  And if we're making many modifications to the array, the cost of maintaining that sort as we go along would also be prohibitive, because inserting or removing elements early in an array requires shifting a large number of them one way or the other.  However, if there are many more searches than there are modifications (e.g., you're loading the data from disk once and searching it repeatedly in memory), this can be a simple and effective technique; like many algorithms, the important thing is to understand the costs and the benefits and decide when one clearly outweighs the other.</p>\r\n\r\n<p class=\"subtitle\">Why binary search is problematic on a linked list</p>\r\n\r\n<p>While it is possible to run this same algorithm on a linked list, it no longer confers the same benefits.  Consider if you had a singly-linked list with <i>n</i> nodes, each containing an integer, and suppose that the integers are sorted in ascending order as you iterate through the list.  On the face of it, that sounds like a case where binary search might be very useful indeed.</p>\r\n\r\n<p>But the very first step in a binary search &mdash; look at the middle element and compare it to what you're looking for &mdash; turns out to be a doozy.  Regardless of which linked list variation you have, there will be no pointer directly to the middle node, so you'll be reduced to iterating through <i>n</i>/2 nodes, one by one, to obtain that pointer.  In a list with <i>n</i> nodes, this will take &Theta;(<i>n</i>) time.  <i>And that's only the first step!</i></p>\r\n\r\n<p>So, generally, binary search on a linked list is pointless; the key to the logarithmic performance of binary search is that you can divide the search space in half in constant time, by looking only at the middle element, but a linked list doesn't offer you that ability, because it gives you no immediate access to that middle element.  (You might argue that you could maintain a pointer to the middle node in the list, but that would only delay the inevitable; the second step in a binary search is to look at the middle of the remaining elements, and you won't have a pointer there.  And if you then argued that you could simply maintain an array of pointers to all the nodes, then you've got a data structure that is, in a sense, the worst of both worlds, with both the benefits and costs of arrays and linked lists, particularly with respect to inserting new nodes into the list later or removing existing ones.)</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Tolerating imperfection</p>\r\n\r\n<p>We've seen previously how we can sometimes achieve some really favorable and interesting results when we're willing to tolerate a certain level of imperfection.  Even if it takes an unreasonably long time to calculate a perfect answer, it's often true in algorithm design that we can spend a lot less time to calculate an imperfect &mdash; but \"good enough\" &mdash; answer instead.  We've seen a couple of examples of that in practice already in this course:</p>\r\n\r\n<ul>\r\n  <li>Even though it was too expensive to keep binary search trees in perfect balance, AVL trees are balanced enough (i.e., they still have logarithmic height, even in the worst case), but compromise on perfection in favor of much more efficient algorithms for insertion and removal.</li>\r\n  <li>When we try to add an element to a <b>std::vector</b> and it's full, the reallocation that's done deliberately gives up a lot of extra space (temporarily) &mdash; by multiplying its capacity, rather than adding to it &mdash; but does so specifically so that the cost of the reallocations amortizes evenly across the subsequent cheap adds that follow it.  It would be nice if we could use less memory at a time, but we then would lose that amortization benefit.</li>\r\n</ul>\r\n\r\n<p>Another kind of compromise that we can make is to use <i>randomization</i> in the behavior of an algorithm.  Randomization generally means that we give up on any guarantee that it will perform within a certain bound &mdash; since it's possible, albeit very unlikely, to flip a coin 1,000 times and have it come up heads every time &mdash; we can (with care) leave ourselves an extremely high probability that the performance will be acceptable, even with the algorithm remaining quite simple.  <i>Skip lists</i>, which we'll look at next, make exactly this trade-off.</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Skip lists</p>\r\n\r\n<p>A <i>skip list</i> is a collection of linked lists that together provide what you could think of, conceptually, as a linked list that can be searched much more efficiently.  They give up some extra memory &mdash; by duplicating some of their nodes one or more times &mdash; so that they can be searched in a manner more like binary search.  A skip list's behavior isn't as \"perfect\" as binary search, but we'll see that it has a very high probability of being in the same ballpark; in practice, their performance rivals that of balanced binary search trees.</p>\r\n\r\n<p class=\"subtitle\">Structural details</p>\r\n\r\n<p>A skip list is an ordered collection of linked lists, one on top of another.  The linked lists store a set of unique <i>keys</i>, and those keys are required to have order, i.e., it's necessary to be able to determine, for any two keys, which is smaller.  We say that each of these linked lists is a <i>level</i>, and we'll refer to these levels by their index, with <i>level 0</i> being the bottommost, <i>level 1</i> being the one directly above it, and so on.  Over time, the number of levels can vary, but there will always be at least one, and there will almost always be more than that.</p>\r\n\r\n<p>Level 0 is a fairly standard singly-linked list with head.  If the skip list contains <i>n</i> keys, level 0 will have a total of <i>n</i> + 2 nodes: one containing each key, and two additional ones containing the conceptually special keys -&infin; and +&infin;, which are considered, respectively, to be smaller and larger than any possible key.  The keys on level 0 are stored in ascending order, so the first node will contain -&infin;, the last node will contain +&infin;, and the other <i>n</i> nodes will contain all of the keys in the skip list in ascending order.</p>\r\n\r\n<p>Subsequent levels above level 0 will also contain -&infin; and +&infin;, along with a subset of the keys on the level below.  (All keys in level <i>i</i> are present on level <i>i</i> &minus; 1, though not all keys on level <i>i</i> &minus; 1 are present on level <i>i</i>.)  When the same key is present on both level <i>i</i> and <i>i</i> &minus; 1, the node on level <i>i</i> points to the node on <i>i</i> &minus; 1 containing the same key; this mechanism is used to link the levels together, so it becomes possible to traverse the structure both across and down, which, as we'll see, is the key to skip list's solid performance characteristics.</p>\r\n\r\n<p>Below is an example of a skip list, with each node consisting of a key and two pointers: one to the node after it on the same level, and another to the corresponding node on the level below it.</p>\r\n\r\n<p class=\"center\"><img src=\"SkipList_Structure.png\" alt=\"Skip list structure\" /></p>\r\n\r\n<p class=\"subtitle\">Searching a skip list</p>\r\n\r\n<p>At first, the arrangement of a skip list might seem like a lot of overhead with little benefit; why are there multiple copies of nodes containing the same key?  The reason becomes clearer after we see how to search one.  A search algorithm might look something like this:</p>\r\n\r\n<blockquote><pre>\r\nstart at the head node of the top level\r\n\r\nloop:\r\n    if the current node's key is the one we're looking for:\r\n        found it!\r\n    else if the next node's key is larger than the key we're looking for:\r\n        move down one level (terminating the search if we're already on the bottom level)\r\n    else:\r\n        move forward to the next node on this level\r\n</pre></blockquote>\r\n\r\n<p>That algorithm, run against the skip list pictured above, would traverse the following path to search for the key 37.</p>\r\n\r\n<p class=\"center\"><img src=\"SkipList_Search.png\" alt=\"Skip list searching\" /></p>\r\n\r\n<ul>\r\n  <li>On level 4, the first node following -&infin; is 44, which is larger than our search key of 37, so we move one level down.</li>\r\n  <li>On level 3, the first node following -&infin; is 29, which is smaller than our search key of 37, so we move forward.  The next node is 44, which is larger than our search key of 37, so move one level down.</li>\r\n  <li>On level 2, the first node following 29 is 44, which is larger than our search key of 37, so we move one level down.</li>\r\n  <li>On level 1, the first node following 29 is 37, which is equal to our search key of 37, so we move forward, then find what we're looking for; our search is done.</li>\r\n</ul>\r\n\r\n<p>Note that the reason for the -&infin; and +&infin; nodes on each level is to keep the algorithm simple, so that there are no special cases (e.g., checking for <b>nullptr</b>, handling the first node differently from the others).  They can be avoided in practice, but at the cost of making the algorithm slightly more complex &mdash; and note that the complexity has not only a cost in terms of understandability, but also potentially a performance cost, as well.</p>\r\n\r\n<p>You could implement this algorithm iteratively or recursively.  The potential benefit is the same either way: using the higher levels of the list to quickly get close to the key we're looking for, then using the lower levels to refine the search and obtain a final result.  This algorithm will be an improvement over a regular linked list search if the higher levels have many fewer nodes on them than the lower levels do, because it will allow us to skip large numbers of nodes on the bottom level without ever looking at them.</p>\r\n\r\n<p>But how do we ensure that the higher levels have fewer nodes &mdash; and a reasonably good spread of keys &mdash; than the lower ones?  As with binary search trees, skip lists don't just fall out of the sky already arranged; we need an algorithm for inserting keys into them.</p>\r\n\r\n<p class=\"subtitle\">An insertion algorithm</p>\r\n\r\n<p>Conceptually, inserting a key into a skip list is quite simple, even though the algorithm's implementation can be a bit hairy.  Rather than specifically attempting to arrange the keys on each level in a particular way, we instead use randomness to decide which keys appear on higher levels and which don't.</p>\r\n\r\n<ul>\r\n  <li>Begin by doing a search, partly because our keys are required to be unique (so we need to find if the key we want is already there) and partly so we can find the appropriate insertion point in level 0 where our new key would need to be inserted.  Either way, we would run the same basic search algorithm.</li>\r\n  <li>Every time we insert a key, we always insert it into a new node on level 0.  Level 0 is always complete (i.e., it always contains all of the keys, plus -&infin; and +&infin;).</li>\r\n  <li>After inserting the key on level 0, we \"flip a coin,\" which is to say that we choose a random bit, equally likely to be 0 or 1.  If the random bit is 1, we also insert the key in a new node on level 1; if the random bit is 0, we're done with the insertion.</li>\r\n  <li>If we did insert the node on level 1, we do the same thing again: flip a coin and decide whether it should also be inserted on level 2.  If so, do it; if not, we're done.</li>\r\n  <li>This continues until our random bit comes back 0, at which point we're done with the insertion.</li>\r\n</ul>\r\n\r\n<p>Levels are added to the skip list as necessary.  Initially, the skip list has one level; as our coin flips take us to levels we've not yet seen, we create them on the fly.</p>\r\n\r\n<p>Assuming we're generating random bits properly (i.e., they really do have a 50% chance of being 0 or 1), this algorithm's result is that a given key has a 100% chance of appearing on level 0, a 50% chance of appearing on level 1, a 25% chance of appearing on level 2, and so on.  Generally, the probability that a key will appear on level <i>i</i> is 1/2<sup><small><i>i</i></small></sup>.</p>\r\n\r\n<p>Thought differently, if there are <i>n</i> keys on level 0, you'd expect  about <i>n</i>/2 of them to appear on level 1 (though it would not be guaranteed), about <i>n</i>/4 of them to appear on level 2, and so on.  This is where skip lists derive their power; because so few keys are on the highest levels, you'll quickly eliminate large swaths of the search space by looking at just a few nodes; in very general terms, moving from one level down to the next should be of similar benefit to the early steps in a binary search of an array.</p>\r\n\r\n<p class=\"subtitle\">A removal algorithm</p>\r\n\r\n<p>Removing a key from a skip list would begin with the same search algorithm we've seen previously, with the goal of finding the node at the highest level that contains the key we want to remove.  At that point, we would \"point around\" that node (and delete it), then move down and do the same with each subsequent level until we reach level 0.</p>\r\n\r\n<p>One nice performance optimization would be to remove levels that we no longer need.  When we remove the last node from a level (aside from the -&infin; and +&infin; nodes), we can safely remove that level from the skip list altogether.</p>\r\n\r\n<p>As always, the devil is in the details, but the concept is a simple one.</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Analysis of skip lists</p>\r\n\r\n<p>To analyze the performance characteristics of a skip list, there are a few things we should pay attention to, which we'll take in turn.</p>\r\n\r\n<p class=\"subtitle\">Memory usage</p>\r\n\r\n<p>The first thing we should consider is how much memory is needed.  It's clear that skip lists are attempting a fairly classic tradeoff in computing: time vs. memory, in this case spending extra memory (to store nodes containing the same keys on multiple levels) to save time in searching.  So how much memory do skip lists use to make that trade?</p>\r\n\r\n<p>Generally, the answer to this question has a lot to do with the randomness of the random bits we're generating to do our \"coin flips.\"  If we assume a good random engine &mdash; so each random bit independently has a 50% chance of being a 0 or a 1 &mdash; then it's reasonable for us to expect the following to be true as <i>n</i> grows large.</p>\r\n\r\n<ul>\r\n  <li>Level 0 will contain <i>n</i> nodes (we'll leave the -&infin; and +&infin; nodes out, because they don't dramatically affect the analysis)</li>\r\n  <li>Level 1 will contain <i>n</i>/2 nodes</li>\r\n  <li>Level 2 will contain <i>n</i>/4 nodes</li>\r\n  <li>And so on...</li>\r\n</ul>\r\n\r\n<p>In total, then, here's how many nodes we'd expect there to be in all the lists combined:</p>\r\n\r\n<blockquote><pre>\r\n<i>n</i> + <i>n</i>/2 + <i>n</i>/4 + ...\r\n</pre></blockquote>\r\n\r\n<p>This is something called a <i>geometric series</i> and it has a well-known result of 2<i>n</i>.  So we would expect that there would be a total of &Theta;(<i>n</i>) nodes across all the levels, so a total of &Theta;(<i>n</i>) memory would be needed.  Note that this is the same asymptotic notation as the amount of memory required by just one linked list; all we've done is multiplied our memory requirement by a constant factor.</p>\r\n\r\n<p>Of course, if the quality of our random bit sequence is bad, all bets are off; it's vital that we use a good random engine for this, and that we use it properly (seeding it from a legitimate source of entropy, then using the same engine to generate a sequence of results).</p>\r\n\r\n<p class=\"subtitle\">Height of the skip list</p>\r\n\r\n<p>We say that the <i>height</i> of a skip list is the number of levels it has.  Note that we add new levels only when we need them, and we remove levels when they become empty, so it's safe to say that a skip list's height is a function of the number of keys it stores at any given time.  But what is that function?</p>\r\n\r\n<p>There isn't a specific function that is always correct; it depends very much on the coin flips we do, which determine how many levels each key appears on.  So we can't say for sure, but we can say some useful things about it.</p>\r\n\r\n<p>First of all, your intuition should be telling you something right off the bat.  In the expected case where all of the nodes appear on level 0, half of them appear on level 1, a quarter of them appear on level 2, and so on, what level would you be on by the time there's only one node left?  The answer here is log<sub><small>2</small></sub><i>n</i>, which is roughly the height we'd expect.</p>\r\n\r\n<p>However, there's uncertainty here.  We're generating random bits, and even if we use a good random engine, there still exists the possibility that we'll see uncharacteristically long sequence of 1's once in a while.  What are the odds that the height is much worse than what we expect?  A couple of facts will help us to estimate the probability.</p>\r\n\r\n<ul>\r\n  <li>The probability that an individual key is present on level <i>i</i> is the same as the probability of flipping a coin and having it come up heads <i>i</i> times in a row: 1/2<sup><small><i>i</i></small></sup>.</li>\r\n  <li>The probability that <i>any</i> of the <i>n</i> keys is present on level <i>i</i> &mdash; which is also the probabiliy that the height of the skip list is at least <i>i</i> &mdash; is harder to calculate.  But one thing's for sure: it's no more than the <i>sum</i> of the probability that each individual key is on that level, i.e., it's no more than <i>n</i>/2<sup><small>i</small></sup>.  (In practice, it's probably a lot less than that, but certainly no more.)</li>\r\n</ul>\r\n\r\n<p>We'll define P<sub><small>i</sub></small> as the probability that the height of the skip list is at least <i>i</i>.  Given the facts above, we can come to the following conclusion:</p>\r\n\r\n<blockquote><pre>\r\n<i>P</i><sub><small>3 log <i>n</i></sub></small> &le; <i>n</i> / 2<sup><small>3 log n</small></sup>\r\n       &le; <i>n</i> / <i>n</i><sup><small>3</small></sup>\r\n       &le; 1 / <i>n</i><sup><small>2</small></sup>\r\n</pre></blockquote>\r\n\r\n<p>To put that result into perspective, if we have 1,000,000 keys in our skip list, the odds that the height is even three times worse than what we expect is no more than 1 in 1,000,000,000,000.  And, even then, we'd still be talking about a height of &Theta;(log <i>n</i>).</p>\r\n\r\n<p>So, what are the odds that the height is <i>n</i>?  From our analysis, we have:</p>\r\n\r\n<blockquote><pre>\r\n    <i>P</i><sub><small><i>n</i></sub></small> &le; <i>n</i> / 2<sup><small>n</small></sup>\r\n</pre></blockquote>\r\n\r\n<p>If we have just 10,000 keys, what are the odds that the height of our skip list will be 10,000?  The answer is ridiculously small: 10,000 / 2<sup><small>10,000</small></sup> &asymp; 10,000 / 10<sup><small>3,010</small></sup> &asymp; 1 / 10<sup><small>3,006</small></sup>.  (To get an idea of just how small that number is, it is estimated that there are 10<sup><small>80</small></sup> atoms in the universe.)</p>\r\n\r\n<p>So, with extremely high probability, the height of the skip list will be near our expectation.  As <i>n</i> grows large enough for us to care, it's reasonable to presume that the height will be logarithmic in practice.</p>\r\n\r\n<p class=\"subtitle\">Search time</p>\r\n\r\n<p>To keep the analysis simple here &mdash; some calculus and some probability theory would be necessary to be more precise &mdash; there are two facts we can focus on.</p>\r\n\r\n<ul>\r\n  <li>We'd expect the topmost level of the skip list to roughly divide the entire list in half.</li>\r\n  <li>We'd expect the next level to roughly divide the entire list in quarters.</li>\r\n  <li>And so on...</li>\r\n</ul>\r\n\r\n<p>In other words, we would expect the number of keys we'd visit in a search to be approximately log<sub><small>2</small></sub><i>n</i>, for the same reason that a binary search will require us to look at about log<sub><small>2</small></sub><i>n</i> elements.</p>\r\n\r\n<p>Additionally, we'll have to traverse downward from the topmost level to the bottommost.  We expect there to be approximately log<sub><small>2</small></sub><i>n</i> levels, as well.</p>\r\n\r\n<p>So, in total, we'd expect to visit about 2 log<sub><small>2</small></sub><i>n</i> nodes during a search.  This would take <i>O</i>(log <i>n</i>) time, generally, which is a very good result.</p>\r\n\r\n<p>It's certainly true that the searches could take substantially longer if, for example, the height of the skip list was abnormally large, or if the number of nodes on the higher levels was abnormally large; but, as we saw in the previous section, the probability of that is ridiculously small.  Skip lists, in practice, perform very well and are a good tool to have in our toolbox.</p>\r\n\r\n</div>\r\n\r\n</body>\r\n</html>\r\n", "encoding": "ascii"}