{"url": "https://www.ics.uci.edu/~dechter/publications/r42a.html", "content": "<html>\r\n    <head>\r\n      <title>\r\n        Dr. Rina Dechter @ UCI\r\n      </title>\r\n      <LINK REL=\"Stylesheet\" HREF=\"/~dechter/basic.css\">\t\t\r\n    </HEAD>\r\n  \r\n  <BODY bgcolor=\"#ffffff\" alink=\"00aaaa\" link=\"008080\" vlink=\"008080\">\r\n  \r\n  <!-- Begin Header -->\r\n    [an error occurred while processing this directive]\r\n  <!-- End Header -->\r\n  \r\n  \r\n  <!-- Begin Body -->\r\n  \r\n  <br><br><center>\r\n<table width=90%>\r\n<tr>\r\n<td class=title>Publications & Technical Reports</td>\r\n<tr>\r\n  <td colspan=2><img width=\"100%\" height=\"2\"  src=\"/~dechter/images/black-fill.gif\"></td>\r\n</tr>\r\n</tr>\r\n</table>\r\n</center> \r\n<center>\r\n<table width=\"80%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n<tr valign=top>\r\n<td><b>R42a</td>\r\n | \r\n<br></td>\r\n</tr>\r\n\r\n<tr>\r\n<td colspan=2><div class=title>Value Iteration And Policy Iteration Algorithms For Markov Decision Problem</div>\r\n<tt>\r\nElena Pashenkova, Irina Rish (<A href=\"mailto:irinar@ics.uci.edu\">irinar@ics.uci.edu</A>) &\r\nRina Dechter (<A href=\"mailto:dechter@ics.uci.edu\">dechter@ics.uci.edu</A>)\r\n</tt></td></tr>\r\n</table>\r\n\r\n<table width=\"80%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n<tr><td>\r\n<br><div class=abstract>\r\n<b>Abstract</b><BR>\r\nIn this paper we consider computational aspects of decision-theoretic planning\r\nmodeled by Markov decision processes (MDPs). Commonly used algorithms, such as \r\nvalue iteration (VI) [Bellman, 1957] and several versions of modified policy \r\niteration (MPI) [Puterman, 1994] (a modification of the original Howard's policy\r\niteration (PI) [Howard, 1960]), are compared on a class of problems from the motion \r\nplanning domain. Policy iteration and its modifications are usually recommended \r\nas algorithms demonstrating a better performance than value iteration\r\n[Russel & Norvig, 1995], [Puterman, 1994]. However, our results show that their\r\nperformance is not always superior and depends on the parameters of a problem\r\nand the parameters of the algorithms, such as number of iterations in the value determination \r\nprocedure in MPI. Moreover, policy iteration applied to non-discounted\r\nmodels without special restrictions might not even converge to an optimal policy, as\r\nin case of the policy iteration algorithm introduced in [Russel & Norvig, 1995]. We\r\nalso introduce a new stopping criterion into value iteration based on policy changes.\r\nThe combined value-policy iteration (CVPI) algorithm proposed in the paper implements \r\nthis criterion and generates an optimal policy faster then both policy and\r\nvalue iteration algorithms.\r\n</div><br>\r\n\r\n<A href=\"r42a-mdp_report.ps\">\r\n<img align=left border=\"0\" src=\"/~dechter/images/down.gif\">&nbsp;&nbsp;<b>[ps] </b></a>\r\n<A target=blank href=\"r42a-mdp_report.pdf\">\r\n<b>[pdf]</b></a>\r\n</td></tr></table></center><br>\r\n\r\n<!-- End Body-->\r\n\r\n<!--- Begin Footer -->\r\n     <div id=\"footer\"><centeR>\r\n<A HREF=\"http://www.ics.uci.edu\">School of Information and Computer Science</A>\r\n<A HREF=\"http://www.uci.edu\">University of California, Irvine, CA 92697-3435</a>\r\n<A HREF=\"http://www.ics.uci.edu/~dechter\">Dr. Rina Dechter</A>\n\r\n<A HREF=\"mailto:dechter_at_ics.uci.edu\">dechter at ics.uci.edu</A>\r\n\n</center></div>\r\n<!--- End Footer -->\r\n\r\n</body>\r\n<html>\r\n", "encoding": "ascii"}