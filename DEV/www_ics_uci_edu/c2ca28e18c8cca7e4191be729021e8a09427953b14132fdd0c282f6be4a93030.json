{"url": "https://www.ics.uci.edu/~wscacchi/Papers/Vintage/Software_Productivity.html", "content": "<html><head>\r\n   <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\r\n   <meta name=\"description\" content=\"UNDERSTANDING SOFTWARE PRODUCTIVITY\">\r\n   <meta name=\"keywords\" content=\"kbspp\">\r\n   <meta name=\"resource-type\" content=\"document\">\r\n   <meta name=\"distribution\" content=\"global\">\r\n   <meta name=\"GENERATOR\" content=\"Mozilla/4.03 [en] (Win95; I) [Netscape]\"><title>UNDERSTANDING SOFTWARE PRODUCTIVITY</title></head>\r\n<body lang=\"EN\">\r\n\r\n<center>\r\n<h1>\r\nUNDERSTANDING SOFTWARE PRODUCTIVITY</h1></center>\r\n\r\n<center><b>WALT SCACCHI</b></center>\r\n\r\n<center><b>Information and Operations Management Department</b></center>\r\n\r\n<center><b>School of Business Administration</b></center>\r\n\r\n<center><b>University of Southern California</b></center>\r\n\r\n<center><b>Los Angeles, CA 90089-1421, USA</b></center>\r\n\r\n<center>(Appears in <i>Advances in Software Engineering and Knowledge Engineering</i>,\r\nD. Hurley (ed.),</center>\r\n\r\n<center>&nbsp;&nbsp;&nbsp; Volume 4, pp. 37-70, (1995).</center>\r\n\r\n<center><b>December 1994</b></center>\r\n\r\n<blockquote>What affects software productivity and how do we improve it?\r\nThis report examines the current state of the art in software productivity\r\nmeasurement. In turn, it describes a framework for understanding software\r\nproductivity, some fundamentals of measurement, surveys empirical studies\r\nof software productivity, and identifies challenges involved in measuring\r\nsoftware productivity. A radical alternative to current approaches is suggested:\r\nto construct, evaluate, deploy, and evolve a knowledge-based `software\r\nproductivity modeling and simulation system' using tools and techniques\r\nfrom the domain of software process engineering.</blockquote>\r\n\r\n<h1>\r\n<a name=\"SECTION00010000000000000000\"></a>Overview</h1>\r\nWhat affects software productivity and how do we improve it? This is a\r\nconcern near and dear to those who are responsible for researching and\r\ndeveloping large software systems. For example, Boehm [10] reports that\r\nby 1995, a 20% improvement in software productivity will be worth $45 billion\r\nin the U.S and $90 billion worldwide. As such, this report examines the\r\ncurrent state of the art in understanding software productivity. In turn,\r\nthis report describes some fundamentals of measurement, presents a survey\r\nof studies of software productivity, identifies variables apparently affecting\r\nsoftware productivity, and identifies alternative directions for research\r\nand practice in understanding what affects software productivity.\r\n\r\n<p>From the survey, it is apparent that existing software productivity\r\nmeasurement studies are fundamentally inadequate and potentially misleading.\r\nDepending on how and what indicators of software productivity are measured,\r\nit is possible to achieve results that show that modest changes in software\r\ndevelopment technologies lead to substantial productivity improvements\r\n(e.g., 300% in 5 years), while major changes to new technologies can lead\r\nto little productivity improvement. Different measurement strategies can\r\nshow an opposite trend. In short, how and what you measure determines how\r\nmuch productivity improvement you see, whether or not productivity is actually\r\nimproved.\r\n\r\n</p><p>This report advocates a radical alternative to current approaches to\r\nmeasuring and understanding what affects software productivity: to construct,\r\nevaluate, and deploy knowledge-based <i>software productivity modeling\r\nand simulation systems.</i> Accordingly, effort should be directed at developing\r\na knowledge-based system that models and symbolically simulates how software\r\nproduction occurs in a given project setting. Such a modeling facility\r\ncould be used to simulate software production under various product requirements,\r\ndevelopment processes, project settings, and computing resource conditions.\r\nIt could also be used to incrementally capture information about the production\r\ndynamics of multiple software projects and thus improve the breadth of\r\nits coverage over time. As a result, this modelling technology could be\r\nused to articulate and update a computational knowledge-based `corporate\r\nmemory' of software production practices.\r\n\r\n</p><p>The potential payoff of such technology is substantial. This technology\r\nprovides a vehicle for delivering practical feedback that software developers\r\nand managers can use prior to and during a development project to help\r\nidentify what might improve their productivity. Such a knowledge-based\r\ntechnology would enable project managers, developers, or analysts to query\r\na model, conduct `what if' analysis, diagnose project development anomalies,\r\nand generate explanations about how certain project conditions affect productivity.\r\nSuch capabilities are not possible with current productivity measurement\r\ntechnologies.\r\n\r\n</p><p>Overall, this examination of software productivity primarily focuses\r\non the development of large-scale software systems (LSS). LSS refers to\r\ndelivered software systems developed by a team of developers, intended\r\nto be in sustained operation for a long time, and typically representing\r\n50K-500K+ source code statements. The choice of LSS is motivated by economic\r\nand practical considerations. LSS are expensive to develop and maintain\r\nso that even modest software productivity improvements can lead to substantial\r\nsavings. For example, it is reasonable to assume that 10,000 lines of code\r\nmay cost a development organization $100,000-250,000. For larger systems\r\nin the range of 50,000 to 250,000 lines of code, the cost may climb by\r\nas much as a factor of 4-25. In turn, it is reasonable to assume that software\r\nmaintenance costs over the total life of the system dominate software development\r\ncosts by a factor of 2-10. Small-scale programming productivity measurement\r\noften reveals more than <i>an order of magnitude variation</i> for different\r\npeople, different programs, or both [18,19], while large-scale programming\r\nefforts (with large staffs) can mitigate some of this variance.\r\n\r\n</p><p>An outline of the remainder of this report follows. Section 2 provides\r\na brief exposition of the science of measurement. This section serves to\r\nidentify some fundamental concerns in evaluating software productivity\r\nmeasures. Section 3 provides a select survey of studies that attempt to\r\nidentify and measure what affects software productivity. The results of\r\nthis survey are then summarized in Section 3.14 as a list of software projects\r\nattributes that contribute to productivity improvement. These three sections\r\nset the stage for Section 4 which provides a discussion of the measurable\r\nvariables that appear to affect software productivity. Section 5 follows\r\nwith a new direction for research into identifying what affects software\r\nproductivity, and how to improve it. We summarize our conclusions and the\r\nconsequences that follow from this endeavor.\r\n</p><h1>\r\n<a name=\"SECTION00020000000000000000\"></a>Notes on the Science of Measurement</h1>\r\nMeasurement is ultimately a quest for certainty and control: certainty\r\nin understanding the nature of some phenomenon so as to control, influence,\r\nor evaluate that phenomenon. In this paper, the phenomenon under study\r\nis <i>software production:</i> from system inception through delivery,\r\noperation and support. Accordingly, we want to understand how software\r\nis produced, how to measure its production, and ultimately, how to positively\r\ninfluence or control the rate of its production. Curtis [18] provides an\r\nappropriate background on some fundamental principles involved in measuring\r\nsoftware production characteristics, including measure validity and reliability,\r\nas well as instrumentation and modeling issues.\r\n\r\n<p>A desire to measure software production implies an encounter with the\r\nprocess of systematic or scientific inquiry. This implies the need to confront\r\nfundamental problems such as the role of measurement in theory development,\r\nhypothesis testing and verification, and performance evaluation. It also\r\nimplies understanding the relationship between measurement and instrumentation-the\r\nartifacts employed to collect/measure data on the phenomenon under study.\r\nInstrumentation in turn raises questions for how to simplify or make trade-offs\r\nin:\r\n</p><ul>\r\n<li>\r\nconvenience of data collection versus cost of alternative instrumentation,\r\ncollection, or sampling strategies.</li>\r\n\r\n<li>\r\n&nbsp;ease of rendering or displaying the results of data analysis for\r\ndifferent audiences (e.g., internal management presentations versus journal\r\npublication).</li>\r\n\r\n<li>\r\n&nbsp;how to handle (or delete) anomalous data collected with survey instruments.</li>\r\n\r\n<li>\r\n&nbsp;use of collected data to monitor, evaluate, and intervene in the\r\nphenomenon under study.</li>\r\n\r\n<li>\r\n&nbsp;developing a narrative, diagrammatic, or operational abstraction\r\nof the phenomenon that is the source of the data collected.</li>\r\n</ul>\r\nOther fundamental concerns on the use of measurements include how to account\r\nfor the influence of unmeasured units, the uniformity and consistency of\r\nmeasured units, how to rationalize the construction of composite measures,\r\nand how to rationalize measurement scales and normalizations. All of these\r\nconcerns must be addressed in developing and sustaining an effort for measuring\r\nsoftware production.\r\n\r\n<p>As such, what types of measures are appropriate for understanding software\r\nproductivity? Productivity in most studies inside and out of the software\r\nworld is usually expressed as a <i>ratio</i> of output units produced per\r\nunit of input effort. This simple relation carries some important considerations:\r\nfor example, that productivity measures are comparable when counting the\r\nsame kind of outputs (e.g., lines of source code) and inputs (person-months\r\nof time). Likewise, that a software development effort with productivity\r\n2X is twice as productive as another effort whose productivity is X. Therefore,\r\nhow outputs and inputs are defined are critical concerns if they are to\r\nbe related as a ratio-type measure. As will become apparent through the\r\nsurvey that follows, other measure types - nominal, ordinal, and interval\r\n- are also appropriate indicators to characterize the variables that shape\r\nsoftware productivity.\r\n\r\n</p><p>In the next section, a survey of studies of software productivity measurement\r\nshows there is often a substantial amount of difference with respect to\r\nthe degree of rigor and the use of accepted analytical methods.\r\n</p><h1>\r\n<a name=\"SECTION00030000000000000000\"></a>A Sample of Software Productivity\r\nMeasurement Studies</h1>\r\nA number of researchers have sought through empirical investigations to\r\ndetermine whether some software development attribute, tool, technique,\r\nor some combination of these has a significant impact on software production.\r\nThese studies primarily focus on the development of LSS Twelve major software\r\nproductivity measurement studies are reviewed including those at IBM, TRW,\r\nNASA, and ITT, as well as at international sites. In addition, a number\r\nof other theoretical and empirical studies of programmer productivity,\r\ncost-benefit analysis, software cost estimation, and a software productivity\r\nimprovement program are reviewed. Together, these studies provide a loosely-grounded\r\nbasis for identifying a number of project characteristics that affect software\r\nproductivity.\r\n<h2>\r\n<a name=\"SECTION00031000000000000000\"></a>IBM Federal Systems Division</h2>\r\nWalston and Felix [56] conducted the classic study in this area. The authors\r\nstate that a major difficulty arises in trying to identify and measure\r\nwhich independent variables can be used to estimate software development\r\nproductivity, cost, and size. For example, they measured software productivity\r\nin terms of number of lines of code produced per person-hour. However,\r\nstaff time was measured by the duration of the complete development project,\r\nrather than just the coding phase. Thus, we have no information as to what\r\npercent of each measured project's effort was dedicated to code production\r\nversus other necessary development activities. This omission tends to distort\r\nthe results of their analysis.\r\n<h2>\r\n<a name=\"SECTION00032000000000000000\"></a>IBM DP Services Organization</h2>\r\nAlbrecht [2,3] developed the `function point' measure to compare the productivity\r\nin 24 projects that developed business applications. A function point is\r\na composite measure of a number of program atributes including the number\r\nof inputs, outputs, function calls, file accesses, etc. that are multiplied\r\nbe weighting factors then added together. These systems Albretch examined\r\nranged in size from 3K to 318K lines of code written in either DMS, PL/1\r\nor COBOL and developed over a 5 year period (1974-1978). Albrecht claims\r\nthat over this period for the programs studied, software productivity,\r\nas measured with function points, increased 3 to 1. He finds that developers\r\nusing DMS (a database management system language) are more productive than\r\nthose writing in PL/1, who in turn were more productive than those writing\r\nCOBOL. The application systems developed tended over time to be increasingly\r\ninteractive (vs. batch), accessing large data files/databases to produce\r\nreports. Also, during the 5 year period, developers progressively began\r\nto practice structured coding, top-down implementation and HIPO documentation.\r\nSuch development techniques would seem to lead to more function points\r\nappearing in source code. That is, poorly structured code will tend to\r\nhave fewer functions points than well-structured code conforming to the\r\nsame specification. Thus, structured code can produce a higher function\r\npoint measure, and therefore appear to be produced more productively.\r\n\r\n<p>But a number of confounding factors appear in Albrecht's results which\r\nundercut the validity of his reported productivity improvement claims.\r\nFor example, his formula for computing function point values incorporate\r\nweighting multipliers which he reports produced reliable results. However,\r\nhe does not discuss how these weights were determined, or how to determine\r\nthem when other programming languages and software applications are to\r\nbe measured. He also indicates that as department manager, he instructed\r\nhis program supervisors to collect this function point data. To some extent\r\nthen, his supervisors were encouraged to have their programs developed\r\nin ways that would lead to more function points produced per unit of effort.\r\nHowever, it is unclear whether the function point technique works equally\r\nwell on non-business application systems that do not rely on accessing\r\nlarge files, retrieving selected data, performing some computations on\r\nthe data, and producing various reports. Thus, it is unclear whether the\r\n3 to 1 productivity improvement that Albrecht claims is due to (a) shifts\r\nin the choice of programming language to those that produce more favorable\r\nmeasures, (b) alternative program development techniques, (c) choice of\r\nmultiplier weights, (d) management encouragement for collecting data that\r\nsubstantiates (and rewards) measured improvement.\r\n</p><h2>\r\n<a name=\"SECTION00033000000000000000\"></a>Equitable Life Organizations</h2>\r\nBehrens [5] also utilizes Albrecht's function point measures to compare\r\nsoftware productivity in 25 application system projects developed in various\r\nlife insurance companies from 1980 to 1981. His results are consistent\r\nwith Albrecht's in supporting the contention that project size, development\r\n(computing) environment, and programming language impact software productivity.\r\nIn particular, he finds that small project teams produce source code with\r\nmore function points than large teams in a comparable amount of time. He\r\nalso finds that developers working online are more productive than those\r\nworking in a batched computing environment. We can also observe that in\r\nlarge projects, software runs tend to become more batch-like as their size\r\ngrows, and the amount of computing resources they require grows.\r\n<h2>\r\n<a name=\"SECTION00034000000000000000\"></a>TRW Defense Systems Group</h2>\r\nBoehm [9,12] sought to identify avenues for improving software productivity\r\nbased primarily on TRW's Software Cost Estimation Program, SCEP. This program\r\nserved as an aid in developing cost estimates for competitive proposals\r\non large government software projects. The program estimates the cost of\r\na software project as a function of program size expressed in delivered\r\nsource instructions and a number of other cost drivers. Experience with\r\nSCEP in turn gave rise to the development of the COCOMO software cost estimation\r\nmodel presented in [9]. Boehm recognized that software cost drivers are\r\neffectively the inverse of productivity (or `benefit') drivers. He found,\r\nfor example, that personnel/team capability and product complexity had\r\nthe greatest affect in driving software costs and productivity. Thus, high\r\nstaff capability and low product complexity lead to high productivity/low\r\ncost software production. Conversely, low staff capability and high product\r\ncomplexity similarly imply low productivity/high cost software production.\r\nThrough his experience with these cost estimation models, Boehm was able\r\nto develop quantitative support for the relative contribution of different\r\nsoftware development characteristics that affect software cost and productivity.\r\n<h2>\r\n<a name=\"SECTION00035000000000000000\"></a>Australia-70 Study</h2>\r\nLawrence [39] conducted a study of 278 commercial applications developed\r\nin 23 medium-to-large organizations in Australia. The organizations and\r\napplications studies included those in government agencies, manufacturing\r\nand mining concerns, and banking and insurance firms. He performed a multivariate\r\nanalysis of productivity variance using a combination of computing environment\r\nand organizational factors. His use of multivariate analysis of variance\r\nis in direct contrast to the preceding software productivity studies that\r\nemploy only univariate analysis.\r\n\r\n<p>Lawrence observed that source lines of code, number of statements, number\r\nof procedure invocations, number of functional units, and number of transfers\r\nof control are all highly correlated. Other researchers have substantiated\r\nthis as well. As such, he chose to employ the number of procedural lines\r\nof code divided by the total time put into the programming job by the programmer\r\nfrom the receipt of program specifications to completion of program testing.\r\nThat is, Lawrence was interested in measuring the productivity of individual\r\nprogrammers who in turn were developing small programs (50-10000 lines\r\nof code). He found that programmer productivity increases with better turnaround,\r\nbut decreases with online source code testing and interface to a database.\r\nIn contrast to Albretch, Lawrence does not define what interface to a database\r\nmeans, nor whether the organizations he studied employed database management\r\nsystems. Thus, it is not possible to determine whether Albretch and Lawrence\r\nagree on the productivity impact of the use of database management systems.\r\nHowever, Lawrence also found that programming experience beyond the first\r\nyear on the job, structured programming, and walkthroughs contribute little\r\nto productivity improvement.\r\n</p><h2>\r\n<a name=\"SECTION00036000000000000000\"></a>NASA/SEL</h2>\r\nBailey and Basili [4] found higher productivity over the entire system\r\nlife cycle to be associated with the use of a disciplined programming methodology,\r\nparticularly in the early stages of system development. Their findings\r\nindicate that productivity measures, as well as other resource utilization\r\nestimates must be specific to the organizational setting and local computing\r\nenvironment to provide the most accurate measures. Standard, program-oriented\r\nproductivity or cost estimation measures will provide less accurate information\r\nthan those measures that account for characteristics of the organization\r\nand its computing environment. Mohanty [44] and Kemerer [30] also found\r\nsimilar results in their independent examinations of different software\r\ncost estimation models.\r\n<h2>\r\n<a name=\"SECTION00037000000000000000\"></a>IBM</h2>\r\nThadhani [54] and Lambert [37] examined the effects of good computer services\r\non programmer and project productivity during application program development.\r\nIn particular, their studies examine the effects of short response times,\r\nprogrammer's skills, and program complexity on programmer productivity.\r\nThadhani reports that programmers were twice as productive when their system's\r\naverage response time was 0.25 seconds (or less) than when it averaged\r\n2 seconds or more. However, in a review of this and other similar studies,\r\nConte and colleagues [17] report that average response time is not as critical\r\nas a narrow variance in expected response time. That is, programmers should\r\nbe more productive when their system's response time is fast, consistent,\r\nand relatively predictable from the computing task at hand.\r\n\r\n<p>Both Thadhani and Lambert assert that unexpected delay in response time\r\nto trivial computing tasks (e.g., processing simple editor or shell commands,\r\nor compiling a small program) is psychologically disruptive to the programmer.\r\nSuch delays they argue cause a longer delay than the actual elapsed time.\r\nSince LSS development efforts can entail thousands or more of such trivial\r\ntask transactions, that cumulative time will represent a significant cost\r\nto the project. Essentially, they argue that response time has an impact\r\non LSS development projects, so that ample processing resources are critical\r\nto enhancing software productivity. Subsequently, this could be viewed\r\nas evidence in favor of providing individual programmers more processing\r\nresources such as through the adoption of powerful personal computing workstations\r\nas a way to improve software productivity. That is, if programmers currently\r\nmust share a small number of heavily loaded computer systems, then providing\r\neach programmer with a workstation should improve their collective productivity\r\n[43].\r\n</p><h2>\r\n<a name=\"SECTION00038000000000000000\"></a>ITT Advanced Technology Center</h2>\r\nVosburg and associates [55] produced perhaps the most substantial study\r\nof large-scale software productivity to date. They examined software production\r\ndata for 44 programming projects in 17 different ITT subsidiaries in nine\r\ndifferent countries. Data on programming productivity, quality, and cost\r\nwere collected from the records of completed projects by means of a questionnaire\r\nanswered by project managers. Software systems ranged in size from 5,000\r\nto 500,000 coded statements, with a median size of 22,000 statements. Statement\r\ncounts include language processing directives, macro calls, and file inclusion\r\nstatements, but not comments or blank lines. Their study covered a variety\r\nof software systems including telecommunications switches, programming\r\ntools, operating systems, electronic defense systems, and process control.\r\nIn total, they represent more than 2.3 million coded statements and 1500\r\nperson-years of effort.\r\n\r\n<p>The authors focused on classifying productivity drivers according to\r\nthe ability of a software project manager to control them. They identify\r\ntwo types of factors: <i>product</i>-related factors that are not usually\r\ncontrollable by a project manager, and <i>production process</i>-related\r\nfactors that are controllable by managers and thus provide opportunity\r\nfor productivity improvement.\r\n\r\n</p><p>The product-related factors they identify include:\r\n</p><ul>\r\n<li>\r\n<i>computing resource constraints:</i> productivity decreases when software\r\nbeing developed has timing, memory utilization, and CPU occupancy constraints.</li>\r\n\r\n<li>\r\n&nbsp;<i>program complexity:</i> productivity decreases when software is\r\nprimarily operating systems, real-time command and control, and fault-tolerant\r\napplications that require extensive error detection, rollback and recover\r\nroutines.</li>\r\n\r\n<li>\r\n&nbsp;<i>customer participation:</i> productivity increases with customer\r\napplication experience and participation in requirements and specification\r\narticulation.</li>\r\n\r\n<li>\r\n&nbsp;<i>size of program product:</i> productivity decreases as the number\r\nof lines of code increases.</li>\r\n</ul>\r\nThe production process-related factors they identify include:\r\n<ul>\r\n<li>\r\n<i>concurrent hardware-software development:</i> productivity decreases\r\nwith concurrent development of hardware.</li>\r\n\r\n<li>\r\n&nbsp;<i>development computer size:</i> productivity increases as computer\r\nsize (processor speed, main and secondary storage capacity) increases.</li>\r\n\r\n<li>\r\n&nbsp;<i>requirements and specifications stability:</i> productivity increases\r\nwith accurate and stable system requirements and specifications.</li>\r\n\r\n<li>\r\n&nbsp;<i>use of modern programming practices:</i> productivity increases\r\nwith extensive use of top-down design, modular design, design reviews,\r\ncode inspections, and quality-assurance programs.</li>\r\n\r\n<li>\r\n&nbsp;<i>personnel experience:</i> productivity increases with more experienced\r\nsoftware development personnel.</li>\r\n</ul>\r\nOverall, they find that product-related and process-related factors account\r\nfor approximately the same amount of variance (one-third for each set)\r\nin productivity enhancement.\r\n\r\n<p>In conclusion, the authors suggest that improving programming productivity\r\nrequires much more than the isolated implementation of new technologies\r\nand policies. In their view, `To be successful, a productivity improvement\r\nprogram must address the entire spectrum of productivity issues. Key features\r\nof such a program are management commitment and an integrated approach'\r\n(pp. 151-152).\r\n</p><h2>\r\n<a name=\"SECTION00039000000000000000\"></a>Australia-80 Study</h2>\r\nJeffrey [26] describes a comparative study of software productivity among\r\nsmall teams in 38 development projects in three Austrialian firms. Each\r\nfirm used one programming language in its projects, but different from\r\nthat used by the other two firms. Software systems in the projects ranged\r\nfrom very small (200 LOC) to large (<tt>?`\\&gt;</tt>100K LOC), while their\r\ndevelopment team size ranged from 1-4 developers for 19 projects, and 3-8\r\nfor the other projects. As a result of his analysis, Jeffrey asserts (a)\r\nthere is an optimal staff level which depends on the language used and\r\nthe size of the resulting software system, and (b) adding staff beyond\r\nthe optimal point decreases productivity and increases total development\r\nelasped time. However, due to the small sample size (three firms), small\r\nteam size vis-a-vis individual programmer variations [19], and other common\r\nanalytical shortcomings in defining input and output measures, the generality\r\nof the assertions is limited.\r\n<h2>\r\n<a name=\"SECTION000310000000000000000\"></a>Commerical U.S. Banks</h2>\r\nCerveny and Joseph [15] report on their study software enhancement productivity\r\nin 200 U.S. commercial banks. Each bank was required by a change in national\r\ntax laws to implement new interest reporting requirements. Thus, all banks\r\nhad to satisfy the same set of tax law requirements. Cerveny and Joseph\r\nfound that banks which employed structured design and programming techniques\r\ntook twice the effort as those banks that used non-structured techniques,\r\nor that purchased and integrated commercial software packages. Effort in\r\ntheir study represents person hours expended for analysis, programming,\r\nand project management activities, which is data apparently collected on\r\na routine basis by the banks in the study. They do not report any measure\r\nof source code changes that accompany the measured effort. However, they\r\nreport that banks that employed structured techniques did so for auditing\r\nand control purposes, but generally lacked CASE tools to support the structured\r\ntechniques. Thus, it is unclear what the net change in software productivity\r\nmight be if CASE tools that support structured design and programming techniques\r\nwould have been empolyed.\r\n<h2>\r\n<a name=\"SECTION000311000000000000000\"></a>U.S. vs. Japan Study</h2>\r\nIn a provocative yet systematic comparison of industrial software productivity\r\nin the U.S. and Japan, Cusumano and Kemerer [21] argue that Japanese software\r\ndevelopment capabilities are comparable to those found in the U.S. [20].\r\nTheir analyses examined data from 24 U.S. and 16 Japanese development efforts\r\ncollected from software project managers who completed questionnaires.\r\nTheir project sample varied in terms of appplication type, programming\r\nlanguage used, programming language and application type, and hardware\r\nplatforms, full-time (versus part-time) staff effort by development phase,\r\npercentage of code reuse during development, code defect density, and number\r\nof tools/methods used per project. However, the researchers note that their\r\nsample of projects was not random, and that the software project managers\r\nmay have only reported on their best projects. Cusamano and Kemerer employed\r\nFortran-equivalent noncomment source lines of code as the output measure\r\n[27], and person-years of effort as the input measure, as well as both\r\nparametric and non-parametric statistical test where appropriate. While\r\nthey report that software productivity appears on the surface to be greater\r\nin Japan than in the U.S., the differences that were observed were not\r\nfound to be statistically significant.\r\n<h2>\r\n<a name=\"SECTION000312000000000000000\"></a>Other studies of Productivity\r\nand Cost Evaluation</h2>\r\nT.C. Jones [27] at IBM was among the first to recognize that measures of\r\nprogramming productivity and quality in terms of lines of code, and cost\r\nof detecting and removing code defects are inherently paradoxical. They\r\nare paradoxical in that lines of code per unit of effort tend to emphasize\r\nlonger rather than efficient or high-quality programs. Similarly, high-level\r\nprogramming languages tend to be penalized when compared to assembly programs,\r\nsince modern programs may utilize fewer lines of code than assembly routines\r\nto realize the same computational procedure. Cost of code defect detection\r\nand removal tends to indicate that it costs less to repair poor quality\r\nprograms than high quality programs. Thus, Jones' results undercut the\r\nutility of the findings reported by Walston and Felix [55] which are subject\r\nto these paradoxes. As an alternative, Jones recommends separating productivity\r\nmeasures into work units and cost units, while program quality be measured\r\nby defect removal efficiency and defect prevention.\r\n\r\n<p>Chrysler [16] sought to identify some basic determinants of programming\r\nproductivity by examining programming activities in a single organization.\r\nHe sought to identify (1) what characteristics of the time to complete\r\na programming (coding) task can be objectively measured before the task\r\nis begun, and (2) what programmer skill attributes are related to time\r\nto complete the task. His definition of programming task assumes that the\r\nprogram's specifications, `the instructions to the programmer regarding\r\nthe performance required by the program', must be sufficiently detailed\r\nto incorporate the objective variables that can be measured to determine\r\nthese relationships. Although he studied a sample of 36 COBOL programs,\r\nhe does not describe their size, nor account for the number of programmers\r\nworking on each. His results are similar in kind to those of Albrecht,\r\nfinding that programming productivity can be estimated primarily from (1)\r\nprogrammer experience at the current computing facility, (2) number of\r\ninput files, (3) number of input edits, (4) number of procedures and procedure\r\ncalls, and (5) number of input fields.\r\n\r\n</p><p>King and Schrems [34] provide the classic survey of problems encountered\r\nin applying cost-benefit analysis to system development and operation.\r\nTo no surprise, the `benefits' they identify represent commonly cited productivity\r\nimprovements. The authors observe that system development costs are usually\r\nunderestimated and difficult to control, while productivity improvements\r\nare overestimated and difficult to achieve. They observe that cost-benefit\r\n(or cost-productivity) analysis can be used as: (a) a planning tool for\r\nassistance in choosing among alternative technologies and allocating scarce\r\nresources among competing demands; (b) an auditing tool for performing\r\n<i>post hoc</i> evaluations of an existing project; and (c) a way to develop\r\n`quantitative' support in order to politically influence a resource allocation\r\ndecision.\r\n\r\n</p><p>Some of the problems they describe include (a) identifying and measuring\r\ncosts and benefits, (b) comparing cost-benefit alternatives, (c) cost accounting\r\ndilemmas, (d) problems in determining benefits, (e) everyday organizational\r\nrealities. For example, two cost accounting (or measurement) problems that\r\narise are <i>ommission of significant costs</i>, and <i>hidden costs</i>.\r\nOmitting significant costs occurs when certain costs are not measured,\r\nsuch as the time staff spend in design and review meetings, and the effort\r\nrequired to produce system design documents. Hidden costs arise in a number\r\nof ways, often as costs displaced either to others in the organization,\r\nor to a later time: for example, when a product marketing unit achieves\r\nthe early release of a software system before the developers have thoroughly\r\ntested it that customers find partially defective or suspect. If the developers\r\ntry to accomodate to the marketing unit's demands, then system testing\r\nplans are undercut or compromised, and system integrity is put in question\r\nfrom the developers point of view. The developers might later become demoralized\r\nand their productivity decrease if they are viewed by others or senior\r\nmanagement as delivering lower quality systems, especially when compared\r\nto other software development groups who do not have the same demands from\r\ntheir marketing units.\r\n\r\n</p><p>King and Schrems also note that conducting quality cost-benefits has\r\ndirect costs as well. For example, Capers Jones [28] reports that in its\r\nsoftware development laboratories, IBM spends the equivalent of 5% of all\r\ndevelopment costs on software measurement and analysis activities. More\r\ntypically, he observes, that most companies spend 1.5% to 3% of the cost\r\nof developing software to measure the kind of information IBM would collect\r\n[cf. 2,3,27,55]. Therefore, this article by King and Schrems can be recommended\r\nas background reading to those interested in conducting software cost vs.\r\nproductivity analysis.\r\n\r\n</p><p>Mohanty [44] compared the application of 20 software cost estimation\r\nmodels in use by large system development organizations. He entered data\r\ncollected from a large software project, then entered this data into each\r\nof the 20 cost estimation models. He found that the range of costs estimated\r\nwas nearly uniformly distributed, varying by an order of magnitude! This\r\nled him to conclude that almost no model can estimate the true cost of\r\nsoftware with any degree of accuracy. However, we could also conclude from\r\nhis analysis that each cost estimation model might in fact be accurate\r\nwithin the organizational setting where it was created and used. Although\r\ntwo different models may differ in their estimate of software development\r\ncosts by as much as a factor of 10, each model may reflect the cost accounting\r\nstructure for the organization where they were created. This means that\r\ndifferent cost estimation models, and by logical extension, productivity\r\nmodels, lead to differrent measured values which can show great variation\r\nwhen applied to software development projects. Also, the results of Kemerer's\r\n[30] study of software cost estimation models corroborates the same kind\r\nof findings that Mohanty`s study shows. However, Kemerer does go so far\r\nas to show how function points may be refined to improve their reliability\r\nas measures of program size and complexity [31,32], as well as tuned to\r\nproduce the better cost estimates [30]. But again, function points depend\r\nsolely upon program source code characteristics, and do not address production\r\nprocess or production setting variations, nor their contributing effects.\r\n\r\n</p><p>Romeu and Gloss-Soler [48] argue that most software productivity measurement\r\nstudies employ inappropriate statistical analysis techniques. They argue\r\nthat the type of productivity data usually reported is <i>ordinal</i> data\r\nrather than interval or ratio data. The parametric statistical techniques\r\nemployed by most software productivity analysts are inappropriate for ordinal\r\ndata, whereas non-parametric techniques are appropriate. The use of parametric\r\ntechniques on ordinal data results in apparently stronger relationships\r\n(e.g., correlations, regression slopes) than would be found with non-parametric\r\ntechniques. The consequence is that studies of productivity measurement\r\nclaiming statistically substantiated relationships based on inappropriate\r\nanalytical techniques are somewhat dubious, and the strength of the cited\r\nrelationship may not be as strong as claimed.\r\n\r\n</p><p>Boehm [9] reported that productivity on a software development project\r\nis most keenly affected by who develops the system and how well they are\r\norganized and managed as a team. Following this, Scacchi [50] reviewed\r\na number of published reports on the problems of managing large software\r\nengineering projects. He found, to no surprise, that when projects were\r\npoorly managed or poorly organized, productivity was substantially lower\r\nthan otherwise possible. Poor management can nullify the potential productivity\r\nenhancements attributable to improved development technologies. Scacchi\r\nidentified a number of strategies for managing software projects that focus\r\non improving the organization of software development work. These strategies\r\nidentify conditions in the workplace, and the skills and interests of the\r\ndevelopers as the basis for project-specific productivity drivers. For\r\nexample, developers who have a strong commitment to a project and the people\r\nassociated with it will be more productive, work harder, and produce higher\r\nquality software products. This commitment comes from the value the developers\r\nexpect to find in the products they produce. In contrast, if they do not\r\nvalue the products they are working on, then their commitment will be low\r\nand their productivity and quality of work will be lower. So an appropriate\r\nstrategy is to focus in organizing and managing the project to cultivate\r\nstaff commitment to each other and to the project's objectives [cf. 33].\r\nWhen developers are strongly committed to the project and to a team effort\r\n[38], they are more than willing to undertake the unplanned for system\r\nmaintenance and articulation work tasks needed to sustain productive work\r\nconditions [6,7]. Scacchi concludes that strategies for managing software\r\ndevelopment work have been overlooked as a major contributor to software\r\nproductivity improvement, and thus require further study and experimentation.\r\n\r\n</p><p>Boehm and associates at TRW [11] described the organization of a software\r\nproject whose objective was to develop an environment to enhance software\r\nproductivity by a factor of 2 in 5 years, and 4 in 10 years. The project\r\nbegan in 1981, and the article describes their progress after four years\r\nin assembling a software development environment that should be able to\r\nsupport TRW development projects. Surprisingly, their software environment\r\ncontains many tools for managing project communications and development\r\ndocumentation. This is because much of what gets delivered to a customer\r\nin a system is documentation, so tools that help develop what the customers\r\nreceives should improve customer satisfaction and thus project productivity.\r\nHowever, they do not report any experiences with this environment in a\r\nproduction project. But they report that developers that have used the\r\nenvironment believe it improved their development productivity 25% to 40%\r\n[cf. 24,45]. Nonetheless, they report that this productivity improvement\r\nwas realized at an additional capital investment of $10,000 per programmer.\r\nCurrent investigations in this project include the development and incorporation\r\nof a number of knowledge-based software development and project management\r\naids for additional LSS productivity improvements.\r\n\r\n</p><p>Capers Jones [28] provides the next study in his book on programming\r\nproductivity. Jones does an effective job at describing some of the problems\r\nand paradoxes that plague most software productivity and quality measures\r\nbased upon his previous studies [27]. For example, he observes that a line\r\nof source code is not an economic good, but it is frequently used in software\r\nproductivity measures as if it were-lines of code (or source statements)\r\nproduced per unit of time are not a sound indicator of economic productivity.\r\nIn response, he identifies more than 40 software development project variables\r\nthat can affect software production. This is the major contribution of\r\nthis work. However, the work is not without its faults. For example, Jones\r\nprovides `data' to support his examination of the effects of each variable\r\non comparable development projects. But his data, such as lines of source\r\ncode is odd is that it is often rounded to the most significant digit (e.g.,\r\n500, 10,000, or 500,000), and collected from unnamed sources. Thus, his\r\nmeasurements lack specificity and his data collection techniques lack sufficient\r\ndetail to substantiate his analysis.\r\n\r\n</p><p>Jones mentions that he relies upon his data for use in a quantitative\r\nsoftware productivity, quality, and reliability estimation model. However,\r\nhe does not discuss how his model works, or what equations it solves. This\r\nis in marked contrast to Boehm's [9] software cost and productivity estimation\r\nefforts where he both identifies the software project variables of interest,\r\nand also presents the analytical details of the COCOMO software cost estimation\r\nmodel that uses them. Thus, we must regard Jones's reported analysis with\r\nsome suspicion. Nonetheless, Jones does include an appendix that provides\r\na questionnaire he developed for collecting data for the cost/quality/reliability\r\nmodel his company markets. This questionnaire includes a variety of suggestive\r\nquestions that people collecting productivity data may find of interest.\r\n\r\n</p><p>In setting his sights on identifying software productivity improvements\r\nopportunities, Boehm [10] also identifies some of the dilemmas encountered\r\nin defining what things need to be measured to understand software productivity.\r\nIn departure from the studies surveyed in the previous section, Boehm observes\r\nthat software development <i>inputs</i> include: (a) different life cycle\r\ndevelopment phases each requiring different levels of effort and skill;\r\n(b) activities including documentation production, facilities management,\r\nstaff training, quality assurance, etc.; (c) support personnel such as\r\ncontract administrators and project managers; and (d) organizational resources\r\nsuch as computing platforms and communications facilities. Similarly, Boehm\r\nobserves that measuring software development <i>outputs</i> solely in terms\r\nof attributes of the delivered software (e.g., delivered source code statements)\r\nposes a number of dilemmas: (a) complex source code statements or complex\r\ncombinations of instructions usually receive the same weight as sequences\r\nof simple statements; (b) determining whether to count non-executable code,\r\nreused code, and carriage returns as code statements; and (c) whether to\r\ncount code before or after pre- or post-processing. For example, on this\r\nlast item, Boehm reports putting a compact Ada program through a pretty-printer\r\nfrequently may <i>triple</i> the number of source code lines. Even after\r\nreviewing other source code metrics, Boehms concludes that none of these\r\nmeasures is fundamentally more imformative than lines of code produced\r\nper unit of time. Thus, Boehm's observations add weight to our conclusion\r\nthat source code statement/line counts should be treated as an ordinal\r\nmeasure, rather than an interval or ratio measure, of software productivity.\r\nThis conclusion is especially appropriate when comparing such productivity\r\nmeasures across different studies.\r\n\r\n</p><p>In a comparative field study of software teams developing formal specifications,\r\nBendifallah and Scacchi [7] found that variation in specification teamwork\r\nproductivity and quality could best be explained in terms of recurring\r\nteamwork structures. They found six teamwork structures (ie, patterns of\r\ninteraction) recurring among all the teams in their study. Further, they\r\nfound that teams shifted from one structure to another for either planned\r\nor unplanned reasons. But more productive teams, as well as higher product\r\nquality teams, could be clearly identified in the observed patterns of\r\nteamwork structures. Lakhanpal's [38] study corroborates this finding showing\r\nworkgroup cohesion and collective capability is a more significant factor\r\nin team productivity than individual experience. Thus, the structures,\r\ncohesiveness, and shifting patterns of teamwork are also salient software\r\nproductivity variables.\r\n\r\n</p><p>In a study that does not actually examining the extent to which CASE\r\ntools may improve software productivity, Norman and Nunamaker [45] report\r\non what the software engineers they surveyed believed would improve software\r\nproductivity [cf. 24]. These software engineers answered questions about\r\nthe desirability and expected effectiveness of a variety of contemporary\r\nCASE mechanisms or methods. Norman and Nunamaker found that software engineers\r\nbelieve that CASE tools that enhance their ability to produce various analysis\r\nreports, screen displays, and structured diagrams will have the greatest\r\nexpected boost in software development productivity. But there is no data\r\navailable that systematically demonstrates if the expected gains are in\r\nfact realized, or to what level.\r\n\r\n</p><p>Kraut and colleagues [35] report on their study of organizational changes\r\nin worker productivity and quality of work-life resulting from the introduction\r\nof a large automated system. They surveyed the opinions of hundreds of\r\nsystem users in 10 different user sites. Through their analysis of this\r\ndata, Kraut and colleagues found that the system increased the productivity\r\nof certain classes or users, while decreasing it for other user classes.\r\nThey also found that while recurring user tasks were made easier, uncommon\r\nuser tasks were reported to be more difficult to complete. Finally, they\r\nfound that the distribution of user task knowledge shifted from old to\r\nnew loci within the user sites. So what if anything does this have to do\r\nwith software development productivity? The introduction of new software\r\ndevelopment tools and techniques might have a similar differential effect\r\non productivity, software development task configuration, and the locus\r\nof development task expertise. This effect might be most apparent in large\r\ndevelopment organizations employing hundreds or thousands of software developers,\r\nrather than in small development teams. In any event, Kraut and colleagues\r\nobserve that one needs to understand with web of relationships between\r\nthe organization of work between and among tasks, developers, and users,\r\nas well as the computing resources and software system designs in order\r\nto understand what affects productivity and quality of work-life [35].\r\n\r\n</p><p>Last, Bhansali and associates [8] report that programmers are two-to-four\r\ntimes more productive when using Ada versus Fortran or Pascal-like languages\r\naccording to their study data. However, as Ada contains language constructs\r\nnot present in these other languages, it is not clear what was significant\r\nin explaining the difference in apparent productivity. Similarly, they\r\ndo not indicate whether any of the source code involved was measured before\r\nor after pre-processing, which can affect source line counts, as already\r\nobserved [10].\r\n</p><h2>\r\n<a name=\"SECTION000313000000000000000\"></a>Information Technology and Productivity</h2>\r\nBrynjolfsson [14] provides a comprehensive review of empirical studies\r\nthat examine the relationship of information technology (IT) and productivity.\r\nIn this study, IT is broadly defined to include particular kinds of software\r\nsystems, such as transaction processing and strategic information systems,\r\nto general-purpose computing resources and services. Accordingly, he notes\r\nthat some studies examine the dollars spent on IT or different types of\r\nsoftware systems, compared to the overall profitability or productivity\r\nof the organizations that have invested in IT. Furthermore, his review\r\nexamines studies falling into manufacturing and service sectors within\r\nthe US economy, or in multiple economic sectors. However, none of the studies\r\nreviewed in the preceding sections of this report are included in his review.\r\n\r\n<p>The overall focus of his review is to examine the nature of the so-called\r\n`productivity paradox' that has emerged in recent public discussions about\r\nthe economic payoffs resulting from organizational investments in IT. In\r\nshort, the nature of this paradox indicates that there is little or no\r\nmeasurable contribution of IT to productivity of organizations within an\r\neconomic sector or to the national economy. His analysis then identifies\r\nfour issues that account for the apparent productivity paradox. These are:\r\n</p><ul>\r\n<li>\r\n<i>Mismeasurement</i> of IT inputs and outputs,</li>\r\n\r\n<li>\r\n&nbsp;<i>Lags</i> due to adaptation and learning how to most effectively\r\nutilize new IT,</li>\r\n\r\n<li>\r\n&nbsp;<i>Redistribution</i> of profits or payoffs attributal to IT, and</li>\r\n\r\n<li>\r\n&nbsp;<i>Mismanagement of IT</i> within industrial organizations.</li>\r\n</ul>\r\nIn a closer comparative examination of these studies, Brynjolfsson concludes\r\n<blockquote>`The closer one examines the data behind the studies of IT\r\nperformance, the more it looks like mismeasurement is at the core of the\r\nproductivity paradox.' [14, p. 74]</blockquote>\r\nThus, once again it appears that measuring and understanding the productivity\r\nimpact of new software systems or IT remains problematic, and that one\r\nsignificant underlying cause for this is found in the methods for measuring\r\nproductivity data.\r\n<h2>\r\n<a name=\"SECTION000314000000000000000\"></a>Summary of Software Development\r\nProductivity Drivers</h2>\r\nFrom a generous though somewhat naive review of the preceding studies,\r\na number of software productivity drivers can be identified. The generosity\r\ncomes from identifying the positive experiences or results reported in\r\nthe preceding studies, and the naivete comes from overlooking the fact\r\nthat many of the reported experiences or results are derived from analytically\r\nrestricted studies, or from dubious or flawed analytical methods. Further,\r\nmost studies fail to describe how they account for variation in productive\r\nability among individual programmers, which has been systemtically shown\r\nto vary by more than an order of magnitude [19]. That is, for very large\r\nsoftware systems (500K+ code statements), it seems likely that `average\r\nprogrammer' productivity dominates individual variations, while in smaller\r\nsystems (less than 50K code statements) or those developed by only a few\r\nprogrammers, then individual differences may dominate. Nearly all of the\r\nstudies cited above examined small systems to some extent. Nonetheless,\r\nif we take a positivist view, we find the following attributes of the software\r\napplication product being developed, the process by which it is developed,\r\nand the setting in which it is develop contribute favorably to improving\r\nsoftware productivity. However, we can neither reliably predict how much\r\nproductivity improvement should be expected, nor how to measure the individual\r\nor collective contribution of the attributes.\r\n\r\n<p>The attributes of a software project that facilitate high productivity\r\ninclude:\r\n\r\n</p><p><b>Software Development Environment Attributes:</b>\r\n</p><ul>\r\n<li>\r\nFast turnaround development activities and high-bandwidth processing throughput\r\n(may require more powerful or greater capacity computing resources)</li>\r\n\r\n<li>\r\n&nbsp;Substantial computing infrastructure (abundant computing resources\r\nand easy-to-access support system specialists)</li>\r\n\r\n<li>\r\n&nbsp;Contemporary software engineering tools and techniques (use of design\r\nand code development aids such as rapid prototyping tools, application\r\ngenerators, domain-specific (reusable) software components, etc., used\r\nto produce incrementally development and released software products.)</li>\r\n\r\n<li>\r\n&nbsp;System development aids for coordinating LSS projects (configuration\r\nmanagement systems, software testing tools, documentation management systems,\r\nelectronic mail, networked development systems, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Programming languages with constructs closely matched to application\r\ndomain concepts (e.g., object-oriented languages, spreadsheet languages)</li>\r\n\r\n<li>\r\n&nbsp;Process-centered software development environments that can accomodate\r\nmultiple shifting patterns of small group work structures</li>\r\n</ul>\r\n<b>Software System Product Attributes:</b>\r\n<ul>\r\n<li>\r\nDevelop small-to-medium complexity systems (complexity indicated by size\r\nof source code delivered, functional coupling, and functional cohesion)</li>\r\n\r\n<li>\r\n&nbsp;Reuse software that supports the information processing tasks required\r\nby the application</li>\r\n\r\n<li>\r\n&nbsp;No real-time or distributed systems software to be developed</li>\r\n\r\n<li>\r\n&nbsp;Minimal constraints for validation of data processing accuracy, security,\r\nand ease of alteration</li>\r\n\r\n<li>\r\n&nbsp;Stable system requirements and specifications</li>\r\n\r\n<li>\r\n&nbsp;Short development schedules to minimize chance for project circumstances\r\nto change</li>\r\n</ul>\r\n<b>Project Staff Attributes:</b>\r\n<ul>\r\n<li>\r\nSmall, well-organized project teams. Large teams should be organized into\r\nsmall groups of 3-7 experienced developers, comfortable working with each\r\nother</li>\r\n\r\n<li>\r\n&nbsp;Experienced software development staff (better if they are already\r\nfamiliar with application system domain, or similar system development\r\nprojects)</li>\r\n\r\n<li>\r\n&nbsp;Software developers and managers who collect and evaluate their own\r\nsoftware production data and are rewarded or acknowledged for producing\r\nhigh data value software</li>\r\n\r\n<li>\r\n&nbsp;A variety of teamwork structures and the patterns of shifts between\r\nthem during task performance.</li>\r\n</ul>\r\nThe factors that drive software costs up should be apparent from this list\r\nof productivity drivers. Software cost drivers are the opposite of productivity\r\ndrivers. For example, software without real-time performance should be\r\nproduced more productively or at lower cost than comparable software with\r\nreal-time performance requirements.\r\n\r\n<p>Also, it should be clear from this list that it is not always possible\r\nor desirable to achieve software productivity enhancements through all\r\nof the project characteristics listed above. For example, if the purpose\r\nof a project is to convert the operation of a real-time communications\r\nsystem from one computer and operating system to another computer-operating\r\nsystem combination, then only some of the characteristics may apply favorably,\r\nwhile others are inverted, occurring only as inhibitors. In this example,\r\nconversion suggests a high potential for substantial reuse of the existing\r\nsource code. However, if the new code added for the conversion affects\r\nthe system's real-time performance, or is spread throughout the system,\r\nthen productivity should decrease and the cost increase. Similarly, if\r\nthe conversion is performed by a well-organized team of developers already\r\nexperienced with the system, then they should complete the conversion more\r\nproductively than if a larger team of newly hired programmers is assigned\r\nthe same responsibility.\r\n\r\n</p><p>Finally, if instead of viewing software productivity improvement from\r\na generous and naive point of view, we seek to understand what affects\r\nsoftware productivity in a way that project managers and developers find\r\nmeaningful, then we need an approach fundamentally different than those\r\nsurveyed above. To achieve this, we must first articulate some of the analytical\r\nchallenges that must be taken into account. This challenge is the subject\r\nof Section 4. We also need to develop analytical instruments or tools that\r\nallow us to model and measure software production in ways that managers\r\nand developers can employ during LSS projects. This effort may lead us\r\naway from numbers and simple quantitative measures, and toward symbolic\r\nand qualitative models that incorporate nominal, ordinal, interval and\r\nratio measures of software production. The capacity to accomodate these\r\ntypes of measures is well within the capabilities of symbol processing\r\nsystems, but generally beyond strictly numerical productivity models. Ultimately,\r\nwe should articulate an operational knowledge-based model that represents\r\nthe software production process [22,23,40]. Such an operational model could\r\nthen provide both a framework and compatible computational vehicle for\r\nmeasuring software production, as well as accomodate simulations of how\r\nprojects work, and what might happen if certain project attributes were\r\naltered. This is the subject of Section 5.\r\n</p><h1>\r\n<a name=\"SECTION00040000000000000000\"></a>Challenges for Software Productivity\r\nMeasurement</h1>\r\nIn order to understand the variables that affect software productivity,\r\npeople interested in measuring it must be able to answer the following\r\nfive questions: (a) Why measure software productivity? (b) Who (or what)\r\nshould measure and collect software productivity data? (c) What should\r\nbe measured? (d) How to measure software productivity? (e) How to improve\r\nsoftware productivity? The purpose of asking these questions is to appreciate\r\nthe complexity of the answers as well as to see that different answers\r\nlead to different software production measurement strategies. Unfortunately,\r\nas we have begun to see in the preceding section, the answers made in practice\r\ncan lead to undesirable compromises in the analytical methods employed,\r\nor in the reliability of the results claimed.\r\n<h2>\r\n<a name=\"SECTION00041000000000000000\"></a>Why measure software productivity?</h2>\r\nTo date, a number of reasons for measuring software productivity have been\r\nreported. In simplest terms, the idea is to identify (and measure) how\r\nto reduce software development costs, improve software quality, and improve\r\nthe rate at which software is developed. In practical terms, this includes\r\nalternatives such as:\r\n<ul>\r\n<li>\r\nincrease the volume of work successfully accomplished by current staff\r\neffort,</li>\r\n\r\n<li>\r\n&nbsp;accomplish the same volume of work with a smaller staff,</li>\r\n\r\n<li>\r\n&nbsp;develop products of greater complexity or market value with the same\r\nstaff workload,</li>\r\n\r\n<li>\r\n&nbsp;avoid hiring additional staff to increase workload,</li>\r\n\r\n<li>\r\n&nbsp;rationalize higher levels of capital-to-staff investment,</li>\r\n\r\n<li>\r\n&nbsp;reduce error densities in delivered products, and decreasing the\r\namount of time and effort needed to rectify software errors,</li>\r\n\r\n<li>\r\n&nbsp;streamline or downsize software production operations,</li>\r\n\r\n<li>\r\n&nbsp;identify possible product defects earlier in development,</li>\r\n\r\n<li>\r\n&nbsp;identify resource utilization patterns to discover production bottlenecks\r\nand underutilized resources,</li>\r\n\r\n<li>\r\n&nbsp;identify high-output or responsive personnel to receive rewards,\r\nand</li>\r\n\r\n<li>\r\n&nbsp;identify low-output personnel for additional training or reassignment.</li>\r\n</ul>\r\nClearly, there are many reasons for measuring software productivity. However,\r\nonce again it may not be desirable to try to accomplish most or all of\r\nthese objectives through a single productivity measurement program. For\r\nexample, different people involved in a large software project may value\r\ncertain of these alternatives more than others. Similarly, each alternative\r\nimplies certain kinds of data be collected. This diversity may lead to\r\nconflicts over why and how to measure software productivity which, in turn,\r\nmay lead to a situation where the measured results are inaccurate, misleading,\r\nor regarded with suspicion [cf. 25,36]. Thus, a productivity measurement\r\nprogram must be carefully design to avoid creating conflicts, mistrust,\r\nor other conditions for mismeasurement within the software projects to\r\nbe studied. Involving the software developers and project managers in the\r\ndesign of the measurement instrument, data collection, and feedback program\r\ncan help minimize the potential organizational problems as well as gain\r\ntheir support.\r\n<h2>\r\n<a name=\"SECTION00042000000000000000\"></a>Who should measure software productivity\r\ndata?</h2>\r\nThe choice of who (or what) should collect and report software production\r\ndata is determined in part by the reasons for measuring productivity noted\r\nabove. The choices include:\r\n<ul>\r\n<li>\r\nProgrammer self report,</li>\r\n\r\n<li>\r\n&nbsp;Project or team manager,</li>\r\n\r\n<li>\r\n&nbsp;Outside analysts or observers,</li>\r\n\r\n<li>\r\n&nbsp;Automated performance monitors.</li>\r\n</ul>\r\nProgrammer or manager self-reported data are the least costly to collect,\r\nalthough they may be of limited accuracy. However, if productivity measures\r\nare to be used for personnel evaluation, then one should not expect high\r\nreliability or validity in self-reported data. Similarly, if productivity\r\nmeasures are employed as the basis of allocating resources or rewards,\r\nthen the data reporters will have an incentive to improve their reported\r\nproduction values. This is a form of the Hawthorne Effect, whereby people\r\nseek to accomodate their (software production) behavior to generate data\r\nvalues they think the data evaluators want to see.\r\n\r\n<p>Instead, we want to engender a software production measurement capability\r\nthat can feed back useful information to project managers and developers\r\nin a form that enhances their knowledge and experience over time. Outside\r\nobservers can often collect such information, but at a higher cost than\r\nself report. Similarly, automated production performance monitors may be\r\nof use, but this is still an emerging area of technology requiring more\r\ninsight for what should be measured and how. For example, Irving and associates\r\n[25] report that use of automated performance monitoring systems is associated\r\nwith perceived increase in productivity, more accurate assessment of worker\r\nperformance, and higher levels of organizational control. However, where\r\nsuch mechanisms have been employed, workers indicate that managers overemphasize\r\nsuch quantitative measures, and underemphasize quality of work in evaluating\r\nworker performance. Workers also reported increased stress, lower levels\r\nof job satisfaction, and a decrease in the quality of relationships with\r\npeers and managers. Ultimately, one would then expect that these negative\r\nfactors would decrease productivity and increase staff turnover. Thus,\r\nthe findings reported by Irving suggest some caution in the use of automated\r\nperformance monitors. Collecting high-quality production data and providing\r\nongoing constructive feedback to personnel in the course of a project is\r\nthus a longer-term goal.\r\n\r\n</p><p>Overall, if data quality or accuracy is not at issue, self-reported\r\nproduction data is sufficient. If causal behavior or organizational circumstances\r\ndo not need to be taken in account, then automated performance monitors\r\ncan be used. Similarly, if the desire is to show measured improvement in\r\nsoftware productivity, whether or not production improves, self-reported\r\nor automated data collection procedures will suffice.\r\n\r\n</p><p>On the other hand, if the intent of the productivity measurement program\r\nis to ascertain what affects software production, and what alternative\r\nwork arrangements might further improve productivity, then reliable and\r\naccurate data must be collected. Such data might best be collected by analysts\r\nor observers who have no vested interest in particular measured outcomes,\r\nnor who will collect data to be used for personnel evaluation. In turn,\r\nthe collected data should be analyzed and results fed back to project developers\r\nand managers in a form they can act upon. Again, this might be best facilitated\r\nthrough the involvement of representative developers and managers in the\r\ndesign of the data collection effort.\r\n</p><h2>\r\n<a name=\"SECTION00043000000000000000\"></a>What should be measured?</h2>\r\nThe choices over what to measure are many and complex. However, it is clear\r\nthat focusing on program product attributes such as lines or code, source\r\nstatements, or function points will not lead to significant insights on\r\nthe contributing or confounding effects of software production process\r\nor production setting characteristics of software productivity, nor vice\r\nversa. The studies in earlier sections clearly indicate that different\r\nLSS <i>product, process,</i> and <i>production setting</i> characteristics\r\nindividually and collectively affect software productivity. However, based\r\non this survey, there are some inconsistencies in determining which characteristics\r\naffect what increase or decrease in software productivity. As such, an\r\n<i>integrated</i> productivity measurement or improvement strategy must\r\naccount for characteristics of the products, processes, and settings to\r\ndelineate the potential interrelationships. This is necessary since we\r\ncannot predict beforehand which constituent variables will reveal the greatest\r\nsignificance or variance in different projects or computing environments.\r\nSimilarly, we should expect that software product, process, and setting\r\ncharacteristics will need to be measured using a combination of nominal,\r\nordinal, interval and ratio measures. As such, consider in turn, the following\r\nconstituents of software products, production processes, and production\r\nsettings.\r\n<h4>\r\n<a name=\"SECTION00043010000000000000\"></a>Software Products:</h4>\r\nSoftware projects produce a variety of outcomes other that source code.\r\nEach product is valuable to either the individual developers, project managers,\r\nproject organization, or the client. Therefore, we should not limit production\r\nmeasurement attention to only one product, especially if comparable effort\r\nis committed to producing other closely related products. The point here\r\nis that since software projects produce many products along the way, our\r\ninterest should be focussed on ascertaining the distribution of time, skill,\r\nteamwork, and value committed to developing each product. Accordingly,\r\nwe can see the following kinds of products resulting from a software development\r\nproject.\r\n<ul>\r\n<li>\r\nDelivered (new versus modified) <i>source statements</i> for successive\r\nsoftware life cycle development stages, including those automatically transformed\r\nor expanded by software tools, such as application generators.</li>\r\n\r\n<li>\r\n&nbsp;Software <i>development analyses</i> (knowledge about how a particular\r\nsystem was produced): requirements analysis, specifications, architectural\r\nand detailed designs, and test plans,</li>\r\n\r\n<li>\r\n&nbsp;<i>Application-domain knowledge</i>: knowledge generated and made\r\nexplicit about the problem domain (e.g., how to electronically switch a\r\nlarge volume of telephone message traffic under computer control),</li>\r\n\r\n<li>\r\n&nbsp;<i>Documents and artifacts</i>: internal and external reports, system\r\ndiagrams, and terminal displays produced for development schedule milestones,\r\ndevelopment analyses, user manuals, and system maintenance guides, and</li>\r\n\r\n<li>\r\n&nbsp;Improved <i>software development skills</i>, new occupational or\r\ncareer opportunities for project personnel, and new ways of cooperating\r\nin order to develop other software products.</li>\r\n</ul>\r\n\r\n<h4>\r\n<a name=\"SECTION00043020000000000000\"></a>Software Production Process:</h4>\r\nLSS are often produced through a multi-stage process commonly understood\r\nin terms of the system life cycle: from its inception through delivery,\r\nsustained operation, and retirement. But if requirements are frequently\r\nrenegotiated, if senior software engineers quit after the preliminary architectural\r\ndesign, or if there are no modern software requirements, specification,\r\nor design aids employed, then we might expect that the coding phase may\r\nshow comparatively low productivity, and that test and integration show\r\ncomparatively high cost. However, we should observe that none of the studies\r\ncited in Section 3 collected and analyzed data that addresses such issues.\r\n\r\n<p>Since each production process activity can produce valuable products,\r\nwhy is it conventional to measure the outcome of one of the smallest effort\r\nactivities in this process of developing large software systems, coding.\r\nA number of studies such as those reported by Boehm [9] indicate that coding\r\nusually consumes 10-20% of the total LSS development effort. On the other\r\nhand, software testing and integration consume the largest share, usually\r\nrepresenting 50% of the development effort. Early development activities\r\nconsume 10-15% each. Clearly, delivered software source code is a valuable\r\nproduct. However, it seems clear that code production depends on the outcomes\r\nand products of the activities that precede it.\r\n\r\n</p><p>In general, software projects do not progress in a simple sequential\r\norder from requirements analysis through specification, design, coding,\r\ntesting, integration, and delivery. However, this does not mean that such\r\nactivities are not performed with great care and management attention.\r\nQuite the contrary. Although the project may be organized and managed to\r\nproduce requirements, specification, design, and other documents according\r\nto planned schedule, the actual course of development work is difficult\r\nto accurately predict. Development task breakdowns and rework are common\r\nto many projects [6,7,41]. Experience suggests that software specifications\r\nget revised during later design stages, requirements change midway through\r\ndesign, software testing reveals inadequacies in certain specifications,\r\nand so forth [50,52]. Each of these events leads to redoing previously\r\naccomplished development work. As such, the life cycle development process\r\nis better understood not as a simple linear process, but rather as one\r\nwith many possible paths that can lead either forward or backward in the\r\ndevelopment cycle depending on the circumstantial events that arise, and\r\nthe project conditions that precede or follow from the occurence of these\r\nevents.\r\n\r\n</p><p>If we want to better estimate, measure, and understand the variables\r\nthat affect software production throughout its life cycle, we need to delineate\r\nthe activities that constitute the production process. We can then seek\r\nto isolate which tasks within the activities can dramatically impact overall\r\nproductivity. The activities of the software life cycle process to be examined\r\ninclude:\r\n</p><ul>\r\n<li>\r\nSystem requirements analysis: frequency and distribution of changes in\r\noperational system requirements throughout the duration of the project,</li>\r\n\r\n<li>\r\n&nbsp;Software requirements specifications (possibly including rapid prototypes):\r\nnumber and interrelationship of computational objects, attributes, relations\r\nand operations central to the critical components of the system (e.g.,\r\nthose in the kernel),</li>\r\n\r\n<li>\r\n&nbsp;Software architecture design: complexity of software architecture\r\nas measured by the number, interconnections, and functional cohesion of\r\nsoftware modules, together with comparable measures of project team organization.\r\nAlso, as measured by the frequency and distribution of changes in the configuration\r\nof both the software architecture and the team work structure.</li>\r\n\r\n<li>\r\n&nbsp;Detailed software unit design: time spent to design a module given\r\nthe number of project staff participating, and the amount of existing (or\r\nreused) system components incorporated into the system being developed,</li>\r\n\r\n<li>\r\n&nbsp;Software unit coding (or application generation): time to code designed\r\nmodules, and density of discovered inconsistencies (bugs) found between\r\na module's detailed design and its source code,</li>\r\n\r\n<li>\r\n&nbsp;Unit testing, integration, and system testing: ratio of time and\r\neffort allocated to (versus actually spent on) testing, effort spent to\r\nrepair detected errors, density of known error types, and the amount of\r\nautomated mechanisms employed to generate and evaluate test case results,</li>\r\n</ul>\r\nSimilar variables for consideration can also be articulated for other system\r\ndevelopment and evolution activities including quality assurance and configuration\r\nmanagement, preliminary customer (beta-site) testing, customer documentation\r\nproduction, delivery turnover, sustained operation and system evolution.\r\n\r\n<p>In addition, we must also appreciate that software production can be\r\norganized into different modes of manufacture and organization of work,\r\nincluding:\r\n</p><ul>\r\n<li>\r\nAd hoc problem solving and articulation work [6,7,33,41]</li>\r\n\r\n<li>\r\n&nbsp;Project-oriented job shop, which are typical for software development\r\nprojects [50]</li>\r\n\r\n<li>\r\n&nbsp;Batched job shops, for producing a family or small volume of related\r\nsoftware products</li>\r\n\r\n<li>\r\n&nbsp;Pipeline, where software production is organized in concurrent multi-stage\r\ndevelopment, and staff is specialized in particular development crafts\r\nsuch as `software requirement analysts', `software architects', or `coders'.</li>\r\n\r\n<li>\r\n&nbsp;Flexible software manufacturing systems, which represent one view\r\nof a `software factory of the future' [53].</li>\r\n\r\n<li>\r\n&nbsp;Transfer-line (or assembly line), where raw or unfinished information\r\nresources are brought to semi-skilled software craftspeople who perform\r\nhighly routinized and limited fabrication or assembly tasks.</li>\r\n</ul>\r\nAccordingly, the characteristics that distinguish these potential modes\r\nof software production from one another are their values along a set of\r\ndimensions that include (1) developer skill requirements, (2) ease of productivity\r\nmeasurement and instrumentation, (3) capitalization, (4) flexibility of\r\nprocedures to accomodate development anomalies, and (5) ability to adopt\r\nand assimilate software innovations.\r\n<h4>\r\n<a name=\"SECTION00043030000000000000\"></a>Software Production Setting:</h4>\r\nDoes the setting where software is produced make a difference on productivity?\r\nDo we expect that LSS production at, say, TRW Defense Systems Group is\r\ndifferent than at the MCC Software Technology Program Center, the Artificial\r\nIntelligence Laboratory at MIT, the Data Processing Center at Aetna Life\r\nand Casualty, or the Refinery Software Center at Exxon? Do we expect that\r\nLSS production in various development organization departments of the same\r\ncorporation is different? The answer to all is yes. Software production\r\nsettings differ in a number of ways including:\r\n<ul>\r\n<li>\r\nProgramming language in use (Assembly, Fortran, Cobol, C, C++, Ada, CommonLisp,\r\nSmalltalk, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Computing applications (telecommunications switch, command and control,\r\nAI research application, database management, structural analysis, signal\r\nprocessing, refinery process control, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Computers (SUN-4 workstations, DEC-VAX, Amdahl 580, Cray Y-MP, Symbolics\r\n3670, PC-clone, etc.) and operating systems (Unix variants, VAX-VMS, IBM-VM,\r\nZetalisp, MS-DOS, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Differences between host (development) and target (end-user) computing\r\nenvironment and setting, as well as between computing server and client\r\nsystems</li>\r\n\r\n<li>\r\n&nbsp;Software development tools (compilers, editors, application generators,\r\nreport generators, expert systems, etc.) and practices in use (hacking,\r\nstructured techniques, modular design, formal specification, configuration\r\nmanagement and QA, MIL-STD-2167A guidelines, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Personnel skill base (number of software staff with no college degree,\r\ndegree in non-technical field, BS CS/EE, MS CS/EE, Ph.D. CS/EE, etc.) and\r\nexperience in application area.</li>\r\n\r\n<li>\r\n&nbsp;Dependence on outside organizational units (system vendors, Marketing,\r\nBusiness Analysis and Planning, laboratory directors, change control committees,\r\nclients, etc.)</li>\r\n\r\n<li>\r\n&nbsp;Extent of client participation, their experience with similar application\r\nsystems, and their expectation for sustained system support</li>\r\n\r\n<li>\r\n&nbsp;Frequency and history of mid-project innovations in production computing\r\nenvironment (how often does a new release of the operating system, processor\r\nmemory upgrade, new computing peripherals introduced, etc. occur during\r\nprior development projects)</li>\r\n\r\n<li>\r\n&nbsp;Frequency and history of troublesome anomalies and mistakes that\r\narise during prior system development projects (e.g., schedule overshoot,\r\nbudget overrun, unexpected high staff turnover, unreliable new release\r\nsoftware, etc.)</li>\r\n</ul>\r\n\r\n<h2>\r\n<a name=\"SECTION00044000000000000000\"></a>How to measure software productivity?</h2>\r\nMeasuring software productivity presupposes an ability to construct a measurement\r\nprogram comparable to those employed in experimental designs for behavioral\r\nstudies [18]. This is necessary to insure that the measures employed are\r\nreliable, valid, accurate, and repeatable. This in turn implies that choices\r\nmust be made with respect to the following concerns:\r\n<h4>\r\n<a name=\"SECTION00044010000000000000\"></a>Productivity measurement research\r\ndesign and sampling strategy</h4>\r\nSimply put, there are at least three kinds of choices for research design:\r\nqualitative case studies, quantitative surveys, and triangulation studies.\r\nQualitative case studies can provide in-depth descriptive knowledge about\r\nhow software production work occurs, what sort of problems arise and when.\r\nSuch studies are usually one-shot affairs that are low-cost to initiate,\r\nemploy open-ended anthropological data collection strategies, usually require\r\noutside analysts, and produce rich, but not necessarily generalizable findings.\r\nMultiple, comparative case studies are much less common and require a greater\r\nsustained field research effort. van den Bosch and associates [13] describe\r\ncomparative qualitative case study designs for studying software production,\r\nwhile [6,7] provide detailed examples.\r\n\r\n<p>Quantitative survey studies employ some form of instrumentation such\r\nas a questionnaire to gather data in a manner well-suited for statistical\r\nanalysis. The survey sample must be carefully defined to insure reliable\r\nand valid statistical results. In constrast to qualitative studies, survey\r\nstudies require data analysis skills that are more widely available and\r\nsupported through automated statistical packages. Consider the following\r\nscenario of the sequence of activities entailed in the preparation and\r\nconduct of a quantitative study:\r\n</p><ol>\r\n<li>\r\nDevelop productivity data collection instrument (form or questionnaire)</li>\r\n\r\n<li>\r\n&nbsp;Pilot test and revise initial instrument to be sure that desired\r\ndata can be collected from subjects with modest effort</li>\r\n\r\n<li>\r\n&nbsp;Implement data collection activity and schedule with plans to follow-up\r\non first- and second-round non-respondents (never expect that everyone\r\nwill gladly cooperate with your data collection program)</li>\r\n\r\n<li>\r\n&nbsp;Validate and `clean' collected data (to remove or clarify ambiguous\r\nresponses)</li>\r\n\r\n<li>\r\n&nbsp;Code data into analytical variables, scale, and normalize</li>\r\n\r\n<li>\r\n&nbsp;Apply selected variables to univariate analysis to determine first-order\r\ndescriptive statistics, second-order variables, and factors for analysis\r\nof variance</li>\r\n\r\n<li>\r\n&nbsp;Select apparently significant first- and second-order variables from\r\nunivariate analysis for multivariate analysis, partial correlations, and\r\nregression analysis</li>\r\n\r\n<li>\r\n&nbsp;Formulate an analytical model of apparent quantitative relationship\r\nbetween factors</li>\r\n\r\n<li>\r\n&nbsp;Formulate descriptive model of analyzed statistical phenomenon to\r\nsubstantiate findings and recommendations.</li>\r\n</ol>\r\nThe chief drawback of surveys is that they usually do not capture the description\r\nof process phenomena, and so they provide low-resolution indicators of\r\ncausal relationships among measure variables. Therefore, surveys are best\r\nsuited for `snap-shot' studies, although multiple or longitudinal survey\r\nstudies are also possible but more costly and less common.\r\n\r\n<p>Triangulation studies attempt to draw from the relative strengths of\r\nboth qualitative and quantative studies to maximize analytical depth, generalizability\r\nand robustness. In short, triangulation studies seek to use qualitative\r\nfield studies to gain initial sensitivity to critical issues, use surveys\r\nbased on the field studies to identify the frequency and distribution of\r\nvariables that constitute these issues from a larger population, then derive\r\nfrom these a small select sample of projects/work groups for further in-depth\r\nexamination and verification. Such research designs are quite scarce in\r\nsoftware production measurement because of the cost and diversity of skills\r\nthey require. This may just be a way of saying that high-quality results\r\nrequire a substantial research investment. van den Bosch and associates\r\n[13] propose one such study design whose baseline cost is estimated in\r\nthe range of 1-3 person years of effort.\r\n</p><h4>\r\n<a name=\"SECTION00044020000000000000\"></a>Unit of analysis</h4>\r\n\r\n<dd>\r\nThe concern here is deciding what are the critical things to study. As\r\nwe indicated in Section 4.3, there is a multitude of factors that can potentially\r\naffect software productivity. However, it should be clear that all these\r\nfactors are not simultaneously affective. Instead, their influence may\r\nbe circumstantial or spread out over time and organizational space. Software\r\nproducts, production processes, and production setting characteristics\r\nall can be influential but not necessarily&nbsp; the same time or with\r\nthe same computing resources. This leads us to recognize that the subject\r\nfor our analysis of software productivity should be <i>the life history\r\nof a software project in terms of its evolving products, processes, and\r\nproduction settings.</i> An awareness of this also impinges on the choice\r\nfor research design and sampling strategy.</dd>\r\n\r\n<br>&nbsp;\r\n<h4>\r\n<a name=\"SECTION00044030000000000000\"></a>Level and terms of analysis</h4>\r\nTogether with the unit of analysis, the level and terms characterize the\r\nbasis for determining the scope and generalizability of a software productivity\r\nanalysis. The level of analysis indicates whether the resulting analysis\r\ncovers micro-level behavior, macro-level behavior, or some span in between.\r\nGiven the unit of analysis, should software productivity be examined at\r\nthe level of individual programmers, small work groups, software life cycle\r\nactivities, development organization, company, or industry? The level(s)\r\nchosen determines the granularity of data needed, as well as how it can\r\nbe aggregated to increase the scope and generalizability of the analysis.\r\nExperience suggests that analysis of data collected across three or more\r\nconsecutive levels provide very strong results (cf. [13,40,41,51]), as\r\nopposed to those cited in Section 3 which typically employ only one level.\r\nBut the greater the desired scope and generalizability, the more carefully\r\nsystematic the data collection and analysis must be.\r\n\r\n<p>The terms of analysis draw attention to the language and ontology of\r\na productivity analysis and the analyst. Assuming the unit and levels for\r\nanalysis, choices of which analytical vocabulary or rationale to use foreshadows\r\nthe outcomes and consequences implied by the analysis. Most analysis of\r\nsoftware productivity are framed in terms expressing economic `costs',\r\n`benefits', and `organizational impacts.' However, other rationales are\r\ncommonly employed which broaden the vocabulary and scope of an analysis.\r\nFor example, Kling and Scacchi [35] observe at least five different kinds\r\nof rationale are common: respectively, those whose terms emphasize (a)\r\nfeatures of the underlying technology, (b) attributes of the organization\r\nsetting, (c) improving relations between software people and management,\r\n(d) determining who can affect control over, or benefit from, a productivity\r\nmeasurement effort (addressing organizational politics), and (e) the ongoing\r\nsocial interactions and negotiations that characterize software production\r\nwork. The point of such diverse rationales and their implied terms of analysis\r\nis to recognize that no simple account can be rendered which completely\r\ndescribes what affects software productivity in a particular setting. Instead,\r\nwhat might be the best choice is to interpret an analysis in terms of each\r\nrationale to better identify which rationale is most informing in a particular\r\nsituation. But whatever the choice, the analysis will be constrained by\r\nthe terms built into the data collection instruments.\r\n</p><h2>\r\n<a name=\"SECTION00045000000000000000\"></a>How to improve software productivity?</h2>\r\nIn addressing this question, Boehm [10] identifies a number of strategies\r\nfor improving software productivity: get the best from people, make development\r\nsteps more efficient, eliminate development steps, eliminate rework, build\r\nsimpler products, and reuse components. These strategies draw attention\r\nto the development activities or processes that add value to an emerging\r\nsoftware system. Cusumano [20] independently reports on how these same\r\nstrategies are regularly practiced in various Japanese software factories\r\nto achieve software productivity improvements. However, Boehm does not\r\nindicate how these productivity improvement opportunities are or should\r\nbe measured to ascertain their effectiveness, nor can he or anyone else\r\nstate how much improvement each strategy or a combined strategy might realize.\r\n\r\n<p>Clearly, much more needs to be explained in order to begin to adequately\r\nanswer this question.\r\n</p><h2>\r\n<a name=\"SECTION00046000000000000000\"></a>Summary</h2>\r\nLarge-scale studies of software productivity (i.e., across multiple software\r\nprojects in many different settings) necessitate collecting of a plethora\r\nof data. The number and diversity of variables identified above indicate\r\nthat software productivity cannot be understood simply as a ratio of the\r\namount of source code statements produced for some unit of time. Instead,\r\nunderstanding software productivity requires a more systematic analysis\r\nof a variety of types of production measures, as well as their interrelationships.\r\nThis suggests that we need a more robust theoretical framework, analytical\r\nmethods, and support tools to address the dilemmas now apparent in understanding\r\nand measuring software productivity.\r\n<h1>\r\n<a name=\"SECTION00050000000000000000\"></a>Alternative Directions for Software\r\nProductivity Measurement and Improvement</h1>\r\nWe need a fundamental shift in understanding what affects software productivity.\r\nIn particular, new effort should be directed at the development of a knowledge-based\r\nsoftware productivity analysis system capable of modeling and simulating\r\nthe production dynamics of a software project in a specific setting. In\r\norder to develop such a system, it is appropriate to also develop project-specific\r\ntheories of software production, cultivate software productivity drivers,\r\nand develop techniques for utilizing qualitative (symbolic) project data.\r\n<h2>\r\n<a name=\"SECTION00051000000000000000\"></a>Develop Setting-Specific Theories\r\nof Software Production</h2>\r\nStandard measures, such as lines of code produced, represent data that\r\nare relatively easy to collect. However, they are also the least useful\r\nin informing our understanding for what affects, or how to improve software\r\nproductivity. We lack an articulated theory of software production. This\r\nreport identifies a number of elements that could be the constituents of\r\nsuch a theory. In principal, these include the software products, the processes\r\nwhich give rise to these products, and the computational and organizational\r\ncharacteristics that facilitate or inhibit the processes. Clearly, developing\r\nsuch theory is a basic research problem, and a problem that must be informed\r\nby systematic empirical examination of current software development projects\r\nand practices. Such theory could be used to construct new models, hypotheses,\r\nor measures that account for the production of large software systems in\r\ndifferent settings [cf. 4,18]. Similarly, such models and measures could\r\nbe <i>tuned</i> to better account for the mutual influence of product,\r\nprocess, and setting characteristics specific to a project. This in turn\r\ncould lead to simple, practical, and effective measures of software production\r\nthat give project managers and developers a source of information they\r\ncan use to improve the quality characteristics of their products, processes,\r\nand settings.\r\n<h2>\r\n<a name=\"SECTION00052000000000000000\"></a>Identify and Cultivate Software\r\nProductivity Drivers</h2>\r\nIn the apparent rush to measure software productivity, we may have lost\r\nsight of a fundamental concern: why are software developers as productive\r\nas they are in the presence of many technical and organizational constraints?\r\nThe potential for productivity improvement is not an inherent property\r\nof any new software development technology [35]. Instead, the people who\r\ndevelop software must effectively mobilize and transform whatever resources\r\nthey have available to construct software products. Software developers\r\nmust realize and articulate the potential for productivity improvement.\r\nNew software development technologies can facilitate this articulation.\r\nBut other technological impediments and organizational constraints can\r\nnullify or inhibit this potential. Thus, a basic concern must be to identify\r\nand cultivate software productivity drivers, whether such drivers are manifest\r\nas new computing resources, or alternative organizational or work arrangements.\r\n\r\n<p>Section 3.14 identifies a number of productivity drivers that (weakly)\r\nfollow from a number of software productivity measurement studies. These\r\ndrivers primarily represent technological resource alternatives. Related\r\nresearch [50,52] also identifies a set of project management strategies\r\nthat seek to improve software production through alternative social and\r\norganizational work arrangements. These strategies are identified through\r\nresearch investigations into the practice of software development in complex\r\nsettings [e.g., 6,7,35,41,52]. These studies are begining to show that\r\nthe development project's organizational history, idiosyncratic workplace\r\nincentives, investments in prior technologies and work arrangements, local\r\njob markets, occupational and career contingencies, and organizational\r\npolitics can dramatically affect software productivity potential, either\r\npositively or negatively [6,29,35,50]. Further, in some cases it appears\r\nthat such organizational and social conditions dominate the productivity\r\ncontribution attributable to in-place software development technologies.\r\nIn other words, in certain circumstances, changing the organization conditions\r\nor work arrangements might have far greater an effect in improving software\r\nproductivity potential than by merely trying to `fix things' by installing\r\nnew technology. Software productivity improvement will not come from better\r\nsoftware development technologies alone. Organizational and project management\r\nstrategies to improve software productivity potential must be identified,\r\nmade explicit, and supported.\r\n</p><h2>\r\n<a name=\"SECTION00053000000000000000\"></a>Develop Symbolic and Qualitative\r\nMeasures of Software Productivity</h2>\r\nWe should develop a rich understanding of how software production occurs\r\nin a small number of representative software projects. From this, we can\r\narticulate an initial qualitative process model of software production\r\nthat incorporates subjective and impressionistic data from local software\r\ndevelopment experts. Then use this model and data to determine what further\r\nquantitative data to collect, as a basis for refining and evolving the\r\nprocess model. Overall, the idea here is to first determine what we should\r\nmeasure before beginning to collect data. Data that is out there and easy\r\nto collect, such as lines of code, does not necessarily tell us anything\r\nabout how those lines of code were produced, what tools were used, what\r\nproblems were encountered, who wrote what code, etc. Instead, we should\r\nseek to be in touch with the people who develop software since it is reasonable\r\nto assume that they can identify their beliefs for what works well in their\r\nsituation, what enhances their productivity, and what improves the quality\r\nof their products. Quantitative data can then be used to substantiate or\r\nrefute the frequency and distribution of the findings described in qualitative\r\nterms. Subsequently, this should lead to the development of a family of\r\nprocess models that accounts for a growing range and scope of software\r\nproduction.\r\n<h2>\r\n<a name=\"SECTION00054000000000000000\"></a>Develop Knowledge-Based Systems\r\nthat Model Software Production</h2>\r\nWe should seek an integrated approach to capture and make explicit an empirically-grounded\r\nunderstanding of software production in a computational model. This model\r\nshould embody a computational framework for capturing, describing, and\r\napplying knowledge of how software development projects are carried out\r\nand managed [22,40,41,42]. New software process modelling technology in\r\nthe form of knowledge-based systems is emerging [e.g., 22,23,40]. This\r\ntechnology appears to be well-suited to support the acquisition, representation,\r\nand operationalization of the qualitative knowledge that exists within\r\na software development project. Readers interested in a specific realization\r\nof this approach should consult [40,41,42]. However, any software process\r\nengineering environment or knowledge engineering system capable of modeling,\r\nsimulating, and enacting software products, production processes, production\r\nsettings and their interrelationships could be employed.\r\n<h4>\r\n<a name=\"SECTION00054010000000000000\"></a>Knowledge acquisition:</h4>\r\nWe can acquire knowledge about software projects by conducting in-depth,\r\nobservational field studies [e.g, 7]. Ideally, such studies should be organized\r\nto facilitate comparative analysis. The data to be collected should account\r\nfor the concerns described in Section 4. This in turn requires the articulation\r\nof a scheme for data collection, coding, and analysis. The focus should\r\nbe directed at gathering and organizing information about the life history\r\nof a software development project in terms of its products, processes,\r\nand setting attributes described earlier. The goal is to be able to develop\r\na descriptive model of software production such that any analytical conclusion\r\ncan be traced back to the original data from which it emerged. Subsequently,\r\nthis descriptive model must capture the knowledge we seek in a form that\r\ncan then be represented and processed within a knowledge-based system.\r\n<h4>\r\n<a name=\"SECTION00054020000000000000\"></a>Knowledge representation:</h4>\r\nThe area of knowledge representation has long been an active area of research\r\nin the field of Artificial Intelligence. Thus discussions of topics or\r\napproaches can get easily bogged down in debates over implementation technology,\r\nphilosophy, and the like. Suffice to say that a knowledge organization\r\nscheme is essential, and that such a scheme must again accomodate the kinds\r\nsoftware production data outlined in Section 5. A suggestive starting point\r\nthat others are working from is the Schema Representation Language described\r\nby [49], and utilized by [47], or the Software Process Specification Language\r\n(SPSL) used in [40,41,42]. For example, in their representation of system\r\ndevelopment projects, Scacchi and colleagues [22,23,40,41,42,51] developed\r\na scheme for organizing and representing knowledge about organizational\r\nsettings, resource arrangements, development plans, actions, states, schedules,\r\nhistories, and expectations. In turn, they elaborate the relationships\r\nbetween these concepts using data derived from detailed narrative descriptions\r\nof system development projects [e.g., 6,33] to illustrate their approach.\r\nUltimately, the goal of such a scheme for representing knowledge about\r\nsoftware development projects is to facilitate computational analysis,\r\nsimulation, querying, and explanation [40,41].\r\n<h4>\r\n<a name=\"SECTION00054030000000000000\"></a>Knowledge operationalization:</h4>\r\nA knowledge base about software production projects provides an initial\r\nbasis for developing an operational model of software production. Such\r\na knowledge-based system requires (1) a knowledge base for storing facts,\r\nhueristics, and reasoning strategies according to the previous scheme,\r\n(2) a question-answering subsystem for retrieving facts stored or deduced\r\nfrom known relationships among facts, and (3) a simulator for exploring\r\nalternative trajectories for software development projects. Suggestive\r\nelaborations of such systems are available [22,23,40,47,49] and recommended.\r\nFor example, assuming an interesting knowledge base has already been stored,\r\nthe question-answering subsystem could be used to answer queries of the\r\nfollowing kinds: (1) who was the developer responsible for a particular\r\naction or situation, (2) what were the project development circumstances\r\nduring a particular time (schedule) interval, (3) when was a particular\r\ncircumstance true or when was the action done, (4) where was a specified\r\naction performed, (5) how was some software design task accomplished, and\r\n(6) why was a certain document produced. More specific questions such as\r\nthese can be answered by retrieval from the knowledge base, either by direct\r\nretrieval, property inheritance, or by inference rules [22,40,49].\r\n<h2>\r\n<a name=\"SECTION00055000000000000000\"></a>Simulate and measure the effects\r\nof productivity enhancements</h2>\r\nThe design of a knowledge-based system that simulates software production\r\nrequires an underlying computational model of development states, actions,\r\nplans, schedules, expectations (e.g., requirements), and histories in order\r\nto answer `what if' questions [40]. Ultimately, the operation of the simulator\r\ndepends upon the availability of a relevant knowledge base of facts, hueristics,\r\nand reasoning strategies found in development projects.\r\n\r\n<p>Consider the following scenario of the simulator use: We have developed\r\nor acquired a knowledge-based software production simulation system of\r\nthe kind outlined above. The simulator's user is a manager of a new project\r\nin a particular setting and wants to determine an acceptable schedule for\r\nthe project (and thus certain attributes affecting productivity). Knowledge\r\nabout the setting and the project have not yet been incorporated into the\r\nknowledge base. The user interacts with the simulator to elicit the relevant\r\nattributes of the setting, project, and schedule then enter them into the\r\nknowledge base. The user starts the simulation through an interactive question-answering\r\ndialog. The simulator would proceed to compare the particular facts related\r\nto the user's queries against prior project knowledge already accumulated\r\nin the knowledge base and tries to execute the proposed production schedule.\r\nThis would give rise to changes in the simulated project states, actions,\r\n(sub-)schedules, expectations, and histories consistent with those inferred\r\nfrom the hueristics and reasoning strategies. The simulation finishes when\r\nthe full schedule is executed, or halt when it reaches a state where it\r\nis inhibited. Such a state reflects a point in the project where some bottleneck\r\nemerges - for example, a key computing resource is overutilized, or some\r\nother precondition for a critical production step cannot be met [6,7,41,42].\r\nAnalysis of the conditions prevailing in the simulated project at this\r\npoint helps the user draw useful conclusions about critical interactions\r\nbetween various organizational units, development groups, and computing\r\nresource arrangements that facilitate productive work. The simulation may\r\nbe redone with different setting or project attributes in order to further\r\nexplore other hueristics for improving productivity in the project.\r\n\r\n</p><p>Ultimately, the simulation embodies a deep model of software production\r\nthat in turn can be further substantiated with quantified data as to the\r\nfrequency and distribution of actions, states, etc. arising in different\r\nsoftware development projects.\r\n</p><h2>\r\n<a name=\"SECTION00056000000000000000\"></a>An Approach</h2>\r\nThe approach to developing a knowledge-based software productivity modeling\r\nand simultion system described above is a radical departure from conventional\r\napproaches to understanding and measuring software productivity. Accordingly,\r\nthe following sequence of activities could be performed as a strategy for\r\nevaluating the utility of such an approach:\r\n<ul>\r\n<li>\r\nInitiate comparative case studies or surveys of current in-house software\r\nproduction practices. These studies serve to provide an initial baseline\r\ndata of software project products, production processes, and production\r\nsetting characteristics.</li>\r\n\r\n<li>\r\n&nbsp;Clean and analyze collected data using available skills and tools.\r\nThis is to provide a statement of baseline knowledge about apparent relationships\r\nbetween measured software production variables. This and the preceding\r\nstep correspond to `knowledge acquisition' activity described above.</li>\r\n\r\n<li>\r\n&nbsp;Codify subsets of available software project data in a knowledge\r\nspecification language, such as SPSL [40,41,42]. This step corresponds\r\nto an initial realization of the `knowledge representation' and `knowledge\r\noperationalization' activities described above.</li>\r\n\r\n<li>\r\n&nbsp;Demonstrate results in the computational language and processor suggested\r\nearlier [40].</li>\r\n\r\n<li>\r\n&nbsp;Embed the software productivity modeling and simulation system within\r\nan advanced CASE environment in order to demostrate its integration, access,\r\nand software production guidance on LSS development efforts [22,23,40,41,42,53].</li>\r\n</ul>\r\n\r\n<h1>\r\n<a name=\"SECTION00060000000000000000\"></a>Conclusions</h1>\r\nWhat affects software productivity and how do we improve it? This report\r\nexamines the state of the art in measuring and understanding software productivity.\r\nIn turn, it descries a framework for understanding software productivity,\r\nidentifies some fundamentals of measurement, and surveys selected studies\r\nof software productivity. This survey helps identify some of the recurring\r\nvariables that affect software productivity. As a results of the analysis\r\nof the shortcomings found in many of the surveyed studies, we then identify\r\nan alternative knowledge-based approach for research and practice in understanding\r\nwhat affects software productivity. This approach builds upon recent advances\r\nin modeling, simulating, and enacting software engineering processes situated\r\nwithin complex organizational settings. Also, this approach enables the\r\nconstruction of an organizational knowledge base on what affects software\r\nproductivity and how. Thus, we are optimistic about the potential for developing\r\nknowledge-based systems for modeling, simulating, and reasoning about software\r\ndevelopment projects as a new way to gain insight into what affects software\r\nproductivity.\r\n\r\n<p><b>Acknowledgements</b> This work was part of the USC System Factory\r\nProject, supported by contracts and grants from AT&amp;T, Northrop Corp.,\r\nOffice of Naval Technology through the Naval Ocean System Center, and Pacific\r\nBell. Additional support provided by the USC Center for Operations Management,\r\nEducation and Research, and the USC Center for Software Engineering. Preparation\r\nof the initial version of this report benefited from discussions and suggestions\r\nprovided by Dave Belanger, Chandra Kintala, Jerry Schwarz, and Don Swartout\r\nof the Advanced Software Concepts Department at AT&amp;T Bell Laboratories,\r\nMurray Hill, NJ. Pankaj Garg, Abdulaziz Jazzar, Peiwei Mi, and David Hurley\r\nprovided helpful comments on subsequent versions of this report. The collective\r\ninput of these people is appreciated, but not of their doing if misstated\r\nor misrepresented.\r\n</p><h1>\r\n<a name=\"SECTION00070000000000000000\"></a>References</h1>\r\n\r\n<ol>\r\n<li>\r\nAbdel-Hamid, T. and S. Madnick, Impact of Schedule Estimation on Software\r\nProject Behavior. <i>IEEE Software</i> <b>3</b>(4), (1986), 70-75.</li>\r\n\r\n<li>\r\n&nbsp;Albrecht, A., `Measuring Application Development Productivity', <i>Proc.\r\nJoint SHARE/GUIDE/IBM Application Development Symposium</i> (October, 1979),\r\n83-92.</li>\r\n\r\n<li>\r\n&nbsp;Albrecht, A. and J. Gaffney, `Software Function, Source Lines of\r\nCode, and Development Effort Prediction: A Software Science Validation',\r\n<i>IEEE Trans. Soft. Engr.</i> <b>SE-9</b>(6), (1983), 639-648.</li>\r\n\r\n<li>\r\n&nbsp;Bailey, J. and V. Basili, `A Meta-Model for Software Development\r\nResource Expenditures', <i>Proc. 5th. Intern. Conf. Soft. Engr.</i>, IEEE\r\nComputer Society, (1981), 107-116.</li>\r\n\r\n<li>\r\n&nbsp;Behrens, C.A., `Measuring the Productivity of Computer Systems Development\r\nActivities with Function Points', <i>IEEE Trans. Soft. Engr.</i> <b>SE-9</b>(6),\r\n(1983), 648-652.</li>\r\n\r\n<li>\r\n&nbsp;Bendifallah, S. and W. Scacchi, `Understanding Software Maintenance\r\nWork', <i>IEEE Trans. Soft. Engr.</i> <b>SE-13</b>(3), (1987), 311-323.</li>\r\n\r\n<li>\r\n&nbsp;Bendifallah, S. and W. Scacchi, `Work Structures and Shifts: An Empirical\r\nAnalysis of Software Specification Teamwork', <i>Proc. 11th. Intern. Conf.\r\nSoft. Engr.</i>, IEEE Computer Society, (1989), 345-357.</li>\r\n\r\n<li>\r\n&nbsp;Bhansali, P.V., B.K. Pflug, J.A. Taylor, and J.D. Wooley, `Ada Technology:\r\nCurrent Status and Cost Impact', <i>Proceedings IEEE</i>, <b>79</b>(1),\r\n(1991), 22-29.</li>\r\n\r\n<li>\r\n&nbsp;Boehm, B., <i>Software Engineering Economics</i> Prentice-Hall, Englewood\r\nCliffs, NJ (1981)</li>\r\n\r\n<li>\r\n&nbsp;Boehm, B.W., `Improving Software Productivity', <i>Computer</i>,\r\n<b>20</b>(8), (1987), 43-58.</li>\r\n\r\n<li>\r\n&nbsp;Boehm, B., M.. Penedo, E.D. Stuckle, R.D. Williams, and A.B. Pyster,\r\n`A Software Development Environment for Improving Productivity', <i>Computer</i>\r\n<b>17</b>(6), (1984), 30-44.</li>\r\n\r\n<li>\r\n&nbsp;Boehm, B. and R.W. Wolverton, `Software Cost Modelling: Some Lessons\r\nLearned', <i>J. Systems and Software</i> <b>1</b> (1980), 195-201.</li>\r\n\r\n<li>\r\n&nbsp;van den Bosch, F., J. Ellis, P. Freeman, L. Johnson, C. McClure,\r\nD. Robinson, W. Scacchi, B. Scheft, A. van Staa, and L. Tripp, `Evaluating\r\nthe Implementation of Software Development Life Cycle Methodology', <i>ACM\r\nSoftware Engineering Notes,</i> <b>7</b>(1), (1982), 45-61.</li>\r\n\r\n<li>\r\n&nbsp;Brynjolfsson, E., `The Productivity Paradox of Information Technology,'\r\n<i>Communications ACM</i>, <b>36</b>(12), (1993), 67-77.</li>\r\n\r\n<li>\r\n&nbsp;Cerveny, R.P., and D.A. Joseph, `A Study of the Effects of Three\r\nCommonly Used Software Engineering Strategies on Software Enhancement Productivity',\r\n<i>Information &amp; Management</i>, <b>14</b>, (1988), 243-251.</li>\r\n\r\n<li>\r\n&nbsp;Chrysler, E., `Some Basic Determinants of Computer Programming Productivity',\r\n<i>Communications ACM</i> <b>21</b>(6), (1978), 472-483.</li>\r\n\r\n<li>\r\n&nbsp;Conte, S., D. Dunsmore, and V. Shen, <i>Software Engineering: Models\r\nand Measures</i> Benjamin-Cummings, Palo Alto, CA (1986).</li>\r\n\r\n<li>\r\n&nbsp;Curtis, B., `Measurement and Experimentation in Software Engineering',\r\n<i>Proc. IEEE</i> <b>68</b>(9), (1980), 1103-1119.</li>\r\n\r\n<li>\r\n&nbsp;Curtis, B., `Substantiating Programmer Variability', <i>Proc. IEEE,</i>\r\n<b>69</b>(7), (1981).</li>\r\n\r\n<li>\r\n&nbsp;Cusumano, M., <i>Japan's Software Factories</i>, Oxford Univ. Press,\r\nNew York, (1991).</li>\r\n\r\n<li>\r\n&nbsp;Cusumano, M. and C.F. Kemerer, `A Quantitative Analysis of U.S. and\r\nJapanese Practice and Performance in Software Development', <i>Management\r\nScience</i>, <b>36</b>(11), (1990), 1384-1406.</li>\r\n\r\n<li>\r\n&nbsp;Garg, P.K., P. Mi, T. Pham, W. Scacchi, and G. Thunquest, `The SMART\r\nApproach to Software Process Engineering,' <i>Proc. 16th. Intern. Conf.\r\nSoftware Engineering</i>, Sorrento, Italy, IEEE Computer Society, (1994),\r\n341-350.</li>\r\n\r\n<li>\r\n&nbsp;Garg. P.K. and W. Scacchi, `On Designing Intelligent Software Hypertext\r\nSystems', <i>IEEE Expert</i>, <b>4</b>, (1989), 52-63.</li>\r\n\r\n<li>\r\n&nbsp;Hanson, S.J. and R.R. Kosinski, `Programmer Perceptions of Productivity\r\nand Programming Tools', <i>Communications ACM</i>, <b>28</b>(2), (1985),\r\n180-189.</li>\r\n\r\n<li>\r\n&nbsp;Irving, R., C. Higgins, and F. Safayeni, Computerized Performance\r\nMonitoring Systems: Use and Abuse, <i>Communications ACM,</i> <b>29</b>(8),\r\n(1986), 794-801.</li>\r\n\r\n<li>\r\n&nbsp;Jeffrey, D.R., `A Software Development Productivity Model for MIS\r\nEnvironments', <i>J. Systems and Soft.</i>, <b>7</b>, (1987), 115-125.</li>\r\n\r\n<li>\r\n&nbsp;Jones, T.C., `Measuring Programming Quality and Productivity', <i>IBM\r\nSystem J.</i> <b>17</b>(1), (1978), 39-63.</li>\r\n\r\n<li>\r\n&nbsp;Jones, C., <i>Programming Productivity,</i> McGraw-Hill, New York,\r\n(1986).</li>\r\n\r\n<li>\r\n&nbsp;Keen, P.G.W., `Information Systems and Organizational Change', <i>Communications\r\nACM,</i> <b>24</b>(1), (1981), 24-33.</li>\r\n\r\n<li>\r\n&nbsp;Kemerer, C.F., `An Empirical Validation of Software Cost Estimation\r\nModels', <i>Communications ACM</i>, <b>30</b>(5), (1987), 416-429.</li>\r\n\r\n<li>\r\n&nbsp;Kemerer, C.F., `Improving the Reliability of Function Point Measurement\r\n- An Empirical Study,' <i>IEEE Trans. Software Engineering</i>, <b>18</b>(11),\r\n(1992), 1011-1024.</li>\r\n\r\n<li>\r\n&nbsp;Kemerer, C.F., `Reliability of Function Point Measurement - A Field\r\nExperiment', <i>Communications ACM</i>, <b>36</b>(2), (1993), 85-97.</li>\r\n\r\n<li>\r\n&nbsp;Kidder, T. <i>The Soul of a New Machine,</i> Atlantic Monthly Press,\r\n(1981).</li>\r\n\r\n<li>\r\n&nbsp;King, J.L. and E. Schrems, `Cost-Benefit Analysis in Information\r\nSystems Development and Operation', <i>ACM Computing Surveys</i> <b>10</b>(1),\r\n(1978), 19-34.</li>\r\n\r\n<li>\r\n&nbsp;Kling, R. and W. Scacchi, `The Web of Computing: Computing Technology\r\nas Social Organization', <i>Advances in Computers</i> <b>21</b> (1982),\r\n3-87.</li>\r\n\r\n<li>\r\n&nbsp;Kraut, R., S. Dumais, and S. Koch, `Computerization, Productivity,\r\nand Quality of Work-Life', <i>Communications ACM</i>, <b>32</b>(2), (1989),\r\n220-238.</li>\r\n\r\n<li>\r\n&nbsp;Lambert, G.N., `A Comparative Study of System Response Time on Programmer\r\nDevelopment Productivity', <i>IBM Systems J.</i> <b>23</b>(1), (1984),\r\n36-43.</li>\r\n\r\n<li>\r\n&nbsp;Lakhanpal, B., `Understanding the Factors Influencing the Performance\r\nof Software Development Groups: An Exploratory Grou-Level Analysis,' <i>Information\r\nand Software Technology</i>, <b>35</b>(8), (1993), 468-471.</li>\r\n\r\n<li>\r\n&nbsp;Lawrence, M.J., `Programming Methodology, Organizational Environment,\r\nand Programming Productivity', <i>J. Systems and Software</i> <b>2</b>\r\n(1981), 257-269.</li>\r\n\r\n<li>\r\n&nbsp;Mi, P. and W. Scacchi, `A Knowledge-Based Environment for Modeling\r\nand Simulating Software Engineering Processes', <i>IEEE Trans. Knowledge\r\nand Data Engr.</i>, <b>2</b>(3), (1990), 283-294. Reprinted in <i>Nikkei\r\nArtificial Intelligence</i>, <b>20</b>(1) (1991), 176-191 (in Japanese).</li>\r\n\r\n<li>\r\n&nbsp;Mi. P. and W. Scacchi, `Modeling Articulation Work in Software Engineering\r\nProcesses,' <i>Proc. 1st. Intern. Conf. Software Process</i>, IEEE Computer\r\nSociety, Redondo Beach, CA, (1991)</li>\r\n\r\n<li>\r\n&nbsp;Mi, P. and W. Scacchi, `Process Integration for CASE Environments,'\r\n<i>IEEE Software</i>, <b>9</b>(2), (May 1992), 45-53. Reprinted in <i>Computer-Aided\r\nSoftware Engineering</i>, 2nd. Edition. Chikofsky (ed.), IEEE Computer\r\nSociety, (1993).</li>\r\n\r\n<li>\r\n&nbsp;Mittal, R., M. Kim, and R. Berg, `A Case Study of Workstation Usage\r\nDuring Early Phases of the Software Development Life Cycle', <i>Proc. ACM\r\nSIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development\r\nEnvironments</i>, (1986), 70-76.</li>\r\n\r\n<li>\r\n&nbsp;Mohanty, S.N., `Software Cost Estimation: Present and Future', <i>Software-Practice\r\nand Experience</i> <b>11</b> (1981), 103-121.</li>\r\n\r\n<li>\r\n&nbsp;Norman, R.J. and J.F. Nunamaker, `CASE Productivity Perceptions of\r\nSoftware Engineering Professionals', <i>Communications ACM</i>, <b>32</b>(9),\r\n(1989), 1102-1108.</li>\r\n\r\n<li>\r\n&nbsp;Pengelly, A, M. Norris, R. Higham, `Software Process Modelling and\r\nMeasurement - A QMS Case Study,' <i>Information and Software Technology</i>,\r\n<b>35</b>(6-7), 375-380.</li>\r\n\r\n<li>\r\n&nbsp;Reddy, Y.V., M.S. Fox, N. Husain, and M. McRoberts, `The Knowledge-Based\r\nSimulation System', <i>IEEE Software</i> <b>3</b>(2), (1986), 26-37.</li>\r\n\r\n<li>\r\n&nbsp;Romeu, J.L. and S.A. Gloss-Soler, `Some Measurement Problems Detected\r\nin the Analysis of Software Productivity Data and their Statistical Significance',\r\n<i>Proc. COMPSAC 83,</i> IEEE Computer Society, (1983), 17-24.</li>\r\n\r\n<li>\r\n&nbsp;Sathi, A., M.S. Fox, M. Greenberg, `Representation of Activity Knowledge\r\nfor Project Management', <i>IEEE Trans. Pattern Analysis and Machine Intelligence,</i>\r\n<b>7</b>(5), (1985), 531-552.</li>\r\n\r\n<li>\r\n&nbsp;Scacchi, W., `Managing Software Engineering Projects: A Social Analysis',\r\n<i>IEEE Trans. Soft. Engr.</i>, <b>SE-10</b>(1), (1984), 49-59.</li>\r\n\r\n<li>\r\n&nbsp;Scacchi, W., `On the Power of Domain-Specific Hypertext Environments',\r\n<i>J. Amer. Soc. Info. Sci.</i>, <b>40</b>(5), (1989).</li>\r\n\r\n<li>\r\n&nbsp;Scacchi, W., `Designing Software Systems to Facilitate Social Organization',\r\nin M.J. Smith and G. Salvendy (eds.), <i>Work with Computers</i>, Vol.\r\n12A, Advances in Humans Factors and Ergonomics, Elsevier, New York, (1989),\r\n64-72.</li>\r\n\r\n<li>\r\n&nbsp;Scacchi, W., `The Software Infrastructure for a Distributed System\r\nFactory', <i>Soft. Engr. J.</i>, <b>6</b>(5), (September 1991), 355-369.</li>\r\n\r\n<li>\r\n&nbsp;Thadhani, A.J., `Factors Affecting Programmer Productivity During\r\nApplication Development', <i>IBM Systems J.</i> <b>23</b>(1), (1984), 19-35.</li>\r\n\r\n<li>\r\n&nbsp;Vosburg, J., B. Curtis, R. Wolverton, B. Albert, H. Malec, S. Hoben\r\nand Y. Liu `Productivity Factors and Programming Environments', <i>Proc.\r\n7th. Intern. Conf. Soft. Engr.</i>, IEEE Computer Society, (1984), 143-152.</li>\r\n\r\n<li>\r\n&nbsp;Walton, C.E. and C.P. Felix, `A Method of Programming Measurement\r\nand Estimation', <i>IBM Systems J.</i> <b>16</b>(1), (1977), 54-65.</li>\r\n</ol>\r\n\r\n</body></html>", "encoding": "ascii"}