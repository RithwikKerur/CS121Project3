{"url": "https://www.ics.uci.edu/~eppstein/161/960123.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Bucket and radix sorting</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for January 23, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Bucket Sorting</h1>\n\nWe've seen various algorithms for sorting in O(n log n) time and a\nlower bound showing that O(n log n) is optimal. But the lower bound\nonly applies to comparison sorting. What if we're allowed to do\nother operations than comparisons? The results will have to depend\non what specific data type we want to sort; typical types might be\ninteger, floating point, or character string. \n\n<p>Let's start with a really simple example: We want to sort n\nintegers that are all in the range 1..n, no two the same. How\nquickly can we determine the sorted order?</p>\n\n<blockquote>Answer: O(1), without even computing anything you know\nit has to be 1,2,3,...n-1,n.</blockquote>\n\n<p>As a less stupid example, suppose all numbers are still in the\nrange 1..n but some might be duplicates. We can use an array to\ncount how many copies of each number there are:</p>\n\n<pre>\n    sort(int n, X[n])\n    {\n    int i,j, Y[n+1]\n    for (i = 0; i &lt; n; i++) Y[i] = 0;\n    for (i = 0; i &lt; n; i++) Y[X[i]]++;\n    for (i = 0, j = 0; i &lt; n; i++)\n        while(Y[i]-- &gt; 0) X[j++] = i\n    }\n</pre>\n\nThe three loops here take O(n) time (we go back to time instead of\ncounting comparisons since this algorithm doesnt use any\ncomparisons). \n\n<p>One not-completely-obvious point: one of the loops is nested.\nNormally when we see a collection of nested loops, the time bound\nis the product of the number of iterations of each loop. Why is\nthis nested loop O(n) rather than O(n^2)?</p>\n\n<blockquote>Answer: It's possible for the inner loop to execute as\nmany as n times, but not on all n iterations of the outer loop. The\neasiest way to see this is to match up the times when we increment\nthe array entries with the times when we decrement them. Since\nthere are only n increments, there are also only n\ndecrements.</blockquote>\n\n<p><a name=\"key\">This algorithm is already close to useful. But it\nis less likely that we want to sort numbers exactly, and more\nlikely that we want to sort records by some number derived from\nthem. (As an example, maybe we have the data from the UCI phone\nbook, and we want to sort all the entries by phone number. It's not\nso useful to sort just the phone numbers by themselves; we want to\nstill know which name goes with which number.)</a></p>\n\n<p>So suppose you have a list of n records each with a key that's a\nnumber from 1 to k (we generalize the problem a little so k is not\nnecessarily equal to n).</p>\n\n<p>We can solve this by making an array of linked lists. We move\neach input record into the list in the appropriate position of the\narray then concatenate all the lists together in order.</p>\n\n<pre>\n    bucket sort(L)\n    {\n    list Y[k+1]\n    for (i = 0; i &lt;= k; i++) Y[i] = empty\n    while L nonempty\n    {\n        let X = first record in L\n        move X to Y[key(X)]\n    }\n    for (i = 0; i &lt;= k; i++)\n    concatenate Y[i] onto end of L\n    }\n</pre>\n\nThere are two loops taking O(k) time, and one taking O(n), so the\ntotal time is O(n+k). This is good when k is smaller than n. E.g.\nsuppose you want to sort 10000 people by birthday; n=10000, k=366,\nso time = O(n). \n\n<h2>Stability</h2>\n\nWe say that a sorting algorithm is <i>stable</i> if, when two\nrecords have the same key, they stay in their original order. This\nproperty will be important for extending bucket sort to an\nalgorithm that works well when k is large. But first, which of the\nalgorithms we've seen is stable? \n\n<ul>\n<li>Bucket sort? Yes. We add items to the lists Y[i] in order, and\nconcatenating them preserves that order.</li>\n\n<li>Heap sort? No. The act of placing objects into a heap (and\nheapifying them) destroys any initial ordering they might\nhave.</li>\n\n<li>Merge sort? Maybe. It depends on how we divide lists into two,\nand on how we merge them. For instance if we divide by choosing\nevery other element to go into each list, it is unlikely to be\nstable. If we divide by splitting a list at its midpoint, and break\nties when merging in favor of the first list, then the algorithm\ncan be stable.</li>\n\n<li>Quick sort? Again, maybe. It depends on how you do the\npartition step.</li>\n</ul>\n\nAny comparison sorting algorithm can be made stable by modifying\nthe comparisons to break ties according to the original positions\nof the objects, but only some algorithms are automatically stable. \n\n<h2>Radix sort</h2>\n\nWhat to do when k is large? Think about the decimal representation\nof a number \n\n<pre>\n    x = a + 10 b + 100 c + 1000 d + ...\n</pre>\n\nwhere a,b,c etc all in range 0..9. These digits are easily small\nenough to do bucket sort. \n\n<pre>\n    radix sort(L):\n    {\n    bucket sort by a\n    bucket sort by b\n    bucket sort by c\n    ...\n    }\n</pre>\n\nor more simply \n\n<pre>\n    radix sort(L):\n    {\n    while (some key is nonzero)\n    {\n        bucket sort(keys mod 10)\n        keys = keys / 10\n    }\n    }\n</pre>\n\nThe only possibly strange part: Why do we do the sort least\nimportant digit first? For that matter, why do we do more than one\nbucket sort, since the last one is the one that puts everything\ninto place? \n\n<blockquote>Answer: If we're trying to sort things by hand we tend\nto do something different: first do a bucket sort, then recursively\nsort the values sharing a common first digit. This works, but is\nless efficient since it splits the problem up into many\nsubproblems. By contrast, radix sorting never splits up the list;\nit just applies bucket sorting several times to the same list. \n\n<p>In radix sorting, the last pass of bucket sorting is the one\nwith the most effect on the overall order. So we want it to be the\none using the most important digits. The previous bucket sorting\npasses are used only to take care of the case in which two items\nhave the same key (mod 10) on the last pass.</p>\n</blockquote>\n\n<p>Correctness:</p>\n\n<blockquote>We prove that the algorithm is correct by induction.\nThe induction hypothesis is that after i steps, the numbers are\nsorted by key modulo 10^i. Certainly after no steps, all numbers\nare the same modulo 1, and are therefore sorted by that value, so\nthe base case is true. Inductively, step i+1 sorts by key / 10^i.\nIf two numbers have the same value of key/10^i, the stability\nproperty of bucket sorting leaves them sorted by lower order\ndigits; and if they don't have the same value, the bucket sort on\nstep i+1 puts them in the right order, so in either case the\ninduction hypothesis holds. For i sufficiently large, taking the\nkeys mod 10^i doesn't change them, at which point the list is\nsorted.</blockquote>\n\n<p>Analysis:</p>\n\n<blockquote>The algorithm takes O(n) time per bucket sort.<br>\nThere are log_10 k = O(log n) bucket sorts.<br>\nSo the total time is O(n log k).</blockquote>\n\n<p>Is this ever the best algorithm to use?</p>\n\n<blockquote>Answer: No. If k is smaller than n, this takes O(n log\nk) while bucket sort takes only O(n). And if k is larger than n,\nthe O(n log k) taken by this method is worse than the O(n log n)\ntaken by comparison sorting.</blockquote>\n\n<p>How can we make it better?</p>\n\n<blockquote>Answer: don't use decimal notation. Multiplication and\ndivision is expensive, so it's better to use a base that's a power\nof 2 (this saves a constant factor but is an easy optimization).\nMore importantly, 10 is too small; we can increase the base up to\nas large as O(n) without increasing the bucket sort times\nsignificantly. If we use base n notation (or base some power of 2\nclose to n) we get time bounds of the form \n\n<pre>\n    O(n (1 + (log k)/(log n)))\n</pre>\n</blockquote>\n\nExample: sorting a million 32-bit numbers. If we use 2^16 (roughly\n64000) as our base (i.e. if we use this number in place of 10 in\nthe radix sorting pseudocode above) then the two O(k) loops are a\nminuscule fraction of the total time per bucket sort, and we only\nneed two bucket sorting passes to solve the problem. \n\n<p>With some more complicated methods one can make integer sorting\nalgorithms having bounds O(n loglog k) or O(n log n / log log n)\nbut these are of less practical interest, and are only good when k\nis enormously greater than n.</p>\n\n<h2>Sorting floating point numbers</h2>\n\nFloating point represents numbers roughly in the form x = a * 2^b.\nIf all numbers had same value of b, we could just bucket sort by a.\nDifferent b's complicate the picture but basically b is more\nimportant than a so sort by a first (radix sort) then one more\nbucket sort by b. \n\n<h2>Sorting character strings</h2>\n\nEssentially, alphabetical order no different than a base-26\nrepresentation of numbers and sorting by ascii character value is\nno different than a base-256 representation. As usual, the first\ncharacter is the most important and therefore the one you want to\nsave for last -- we can just work backwards through the strings\ndoing bucket sorts by the character values at each position. But\nthere are some details involved in applying this idea to get a\nquick algorithm. \n\n<p>What if strings have different lengths? There doesn't always\nexist a character in position i. But we can test i against the\nstring's length, and use character value 0 if the string is too\nshort since e.g. \"a\" should come before \"aardvark\". One way to\nthink about this is that we just \"pad\" the strings out with null\ncharacters to make them all the same length.</p>\n\n<p>However, this padding idea can significantly increase the size\nof the strings (or in other words it makes a sorting algorithm\nbased on it too slow). If you have n strings, most very short, but\none of length n, the time will be O(n^2) even though the input\nlength is O(n), so this idea of using radix sort for strings would\nend up no better than a bad comparison sort algorithm.</p>\n\n<p>The problem is that when we sort the strings by their i'th\ncharacter values, we're wasting a lot of work sorting the strings\nshorter than that. One possible solution would be to, in each\nbucket sort, only sort the strings longer than that length. How do\nwe find this list of long strings? We can do it by another call to\nbucket sorting! Just sort the strings by their lengths.</p>\n\n<p>There still remains a more subtle problem with this approach.\nRecall that bucket sort is efficient only when n&lt;k. In early\npasses of the radix sort algorithm, we'll only be sorting really\nlong strings, so there may be very few of them, and we won't have\nn&lt;k, so those bucket sorts may not be efficient</p>\n\n<p>Solution: We'll use the same array of k buckets for each radix\nsort pass, so putting things into buckets (distribute) is still\nfast. Concatenating nonempty buckets (coalesce) is also still fast.\nThe only slowdown comes from finding the nonempty buckets. So we\nmake a list of which buckets will be nonempty using bucket sorting\nin yet a third way! We sort pairs (i,string[i]) by two bucket\nsorts. The buckets of the second bucket sort give, for each i, a\nsorted list of the characters in position i of the strings, which\ntells us which buckets will be nonempty in the radix sort part of\nthe algorithm.</p>\n\n<p>This is getting pretty complicated and confusing, but it's not\ntoo bad once we express it in pseudocode:</p>\n\n<pre>\n    string sort(L)\n    {\n    make list of pairs (i,str[i])\n    bucket sort pairs by str[i]\n    bucket sort pairs by i (giving lists chars[i])\n    bucket sort strings by length\n    i = max length\n    L = empty\n    while (i &gt; 0)\n    {\n        concatenate strings of length = i before start of L\n        distribute into buckets by chars in position i\n        coalesce by concatenating buckets in chars[i]\n        i--\n    }\n    concatenate list of empty strings at start of L\n    return L\n    }\n</pre>\n\nThe time for the first three bucket sorts is O(N+k). Each remaining\npass takes time O(n_i) where n_i is the number of strings having a\ncharacter in position i, so the total time for the while loop is\nO(N) again. Overall, the algorithm's total time is O(N + k), where\nN is the total length of all strings and k is number of character\nvalues. \n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960123.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}