{"url": "https://www.ics.uci.edu/~ics1c/doc/spiders.html", "content": "<HTML>\n<HEAD>\n<base href=\"http://www.rpi.edu/~decemj/cmc/mag/1994/sep/spiders.html\">\n   <TITLE>New Spiders Roam the Web</TITLE>\n   <LINK REV=made HREF=\"mailto:decemj@rpi.edu\">\n</HEAD>\n<BODY>\n<i><a href=\"../../current/toc.html\">Computer-Mediated \nCommunication Magazine</a></i> / \nVolume 1, Number 5 / September 1, 1994 / Page 3<p>\n\n<HR>\n\n<H1>\nNew Spiders Roam the Web\n</H1>\n\nby \n<a href=\"http://www.rpi.edu/~decemj/index.html\">John December</a> \n(decemj@rpi.edu)\n<p>\n\nTHE WEB (August 28) \nTwo newer, smarter tools for finding and indexing resources on the \nWeb have been released this summer. The \n<a href=\"http://www.cmu.edu/\">Carnegie Mellon University</a>'s\n<a href=\"http://www.mt.cs.cmu.edu/cmt/CMT-home.html\">Center for \nMachine Translation</a> announced the public availability of its \n<a href=\"http://lycos.cs.cmu.edu/\">Lycos (TM)\nWWW search engine</a> on August 12th, and \nthe <a href=\"http://rd.cs.colorado.edu/~schwartz/IRTF.html\">Internet Research \nTask Force Research Group on Resource Discovery</a>'s \n<a href=\"http://rd.cs.colorado.edu/harvest/Home.html\">Harvest System</a>\nhas been presented in several papers during the summer.\nBoth systems are now in place for public use.\n<P>\n\nThe Lycos and Harvest systems attack a problem that \nhas plagued many information spaces before the Web--how \ncan a user find resources related to a topic or locate a \nspecific resource?   \nIn \n<a href=\"http://hoohoo.ncsa.uiuc.edu/ftp-interface.html\">ftpspace</a>, \nthere's <a href=\"http://web.nexor.co.uk/archie.html\">archie</a>;\nin <a href=\"gopher://gopher.micro.umn.edu/1\">gopherspace</a>, \nthere's <a href=\"gopher://gopher.unr.edu/11/veronica\">veronica</a>.\nFor the Web, there is a \nvariety of Web \n<a href=\"http://web.nexor.co.uk/mak/doc/robots/robots.html\">robots, \nwanderers, and spiders</a> that have been crawling through\nthe Web and collecting information about what they find. \nOliver McBryan's \n<a href=\"http://www.cs.colorado.edu/home/mcbryan/WWWW.html\">World-Wide \nWeb Worm</a>, released in March, was a very early ancestor\nto the newer species of spiders on the Web today.\nThe Worm collected a database of over 100,000 resources and still provides\nthe user with a search interface to its database (current to March 7, 1994).\nBoth Lycos and Harvest build on the \nWorm's techniques, provide more current databases, and \ncollect them in a more efficient manner.  \n<P>\n\n<H2>Lycos</H2>\n\nIn an email interview, \n<a href=\"http://fuzine.mt.cs.cmu.edu/mlm/home.html\">Dr. \nMichael L. Mauldin</a>, a developer of Lycos, \ndescribed the spider's unique features.  \nLycos' software ancestry is from \na program called \"Longlegs\" written by \n<a href=\"http://thule.mt.cs.cmu.edu:8001/jrrl-space/home-page.html\">John \nLeavitt</a> \nand <a href=\"http://www.mt.cs.cmu.edu/ehn/release/\">Eric Nyberg</a>, and \nthe term \"Lycos\" comes from \nthe arachnid family <i>Lycosidae</I>, which are large ground\nspiders that are very speedy and active at night, catching their\nprey by pursuit rather than in a web.\nLycos lives up to its name--rather than catching its \"prey\" (URLs\non a server) in a massive single-server sweeps, \nLycos uses an innovative, \nprobabilistic scheme to \nskip from sever to server in Webspace. \n<P>\n\nThe secret of Lycos' search technique lies in random choices\ntempered by preferences.   Lycos starts with a given URL and \ncollects information from the resource, including:\n<UL>\n<LI>Title \n<LI>Headings and Subheadings \n<LI>100 most \"weighty\" words (using an algorithm which considers\n\tword placement and frequencies, among other factors)\n<LI>First 20 lines \n<LI>Size in bytes \n<LI>Number of words \n</UL>\nLycos then adds the URL references in the resource\nto its queue.  To choose the next document to explore, \nLycos makes a random choice \n(among the http, gopher, and ftp references) \nwith built-in \"preferences\" for documents that \nhave multiple links into them (popular documents) and \na slight preference for shorter URLs (to keep the database\noriented to the Web's \"top\").\n<P>\n\n<HR>\n<BLOCKQUOTE>\n<B>\nWhile many early Web spiders \ninfested a particular server with a large number of rapid, \nsequential accesses, Lycos behaves.  \n</B>\n</BLOCKQUOTE>\n<HR>\nFirst, Lycos'\nrandom-search behavior avoids the \"multiple-hit\" problem.\nSecond, Lycos complies with \n<a href=\"http://web.nexor.co.uk/mak/doc/robots/norobots.html\">the \nstandard for robot exclusion</a> \nto keep unwanted robots off WWW servers, \nand identifies itself as \n'Lycos' when crawling, so that webmasters \ncan know when Lycos has hit their server.\n<P>\n\nWith more than 634,000 references in its database as of \nthe end of August, Lycos offers a huge database to locate\ndocuments matching a given query.  The \n<a href=\"http://lycos.cs.cmu.edu/cgi-bin/pursuit/\">search interface</a> \nprovides a way for users to find documents that contain \nreferences to a keyword, and to \nexamine a document outline, keyword list and\nan excerpt.  In this way, Lycos enables the user to determine\nif a document might be valuable <i>without having to retrieve it</i>.\nAccording to Dr. Mauldin, plans are in the works for\nallowing users to register pages and for other kinds of searching\nschemes.\nAnother related project underway is \n<a href=\"http://thule.mt.cs.cmu.edu:8001/jrrl-space/webants.html\">\nWebAnts</a> aimed at creating <i>cooperating</i> explorers, so \nthat an individual spider doesn't have to do all the work of \nfinding things on the Web or duplicate other spiders' efforts.\n\n<H2>The Harvest Project</H2>\n\nThe <a href=\"http://rd.cs.colorado.edu/harvest/\">The Harvest \nInformation Discovery and Access System</a> reaches beyond being \nmerely a spider, but involves a series of subsystems to create\nan efficient, flexible, and scalable way to locate information.\nHarvest is an ambitious project to provide a way to create indexes and \nprovide for efficient use of servers.\nWork on its development has been supported primarily by \n<a href=\"http://ftp.arpa.mil/\">Advanced Research Projects Agency</a>,\nwith other support from\n<a href=\"http://web.fie.com/web/fed/afr/\">Air Force Office of Scientific \nResearch (AFOSR)</a>, Hughes, \n<a href=\"gopher://stis.nsf.gov/11\">National Science Foundation</a>,\nand <a href=\"http://www.sun.com\">Sun</a>.\nHarvest is being \ndesigned and built by the\n<a href=\"http://rd.cs.colorado.edu/~schwartz/IRTF.html\">Internet \nResearch Task Force Research Group on Resource Discovery</a>.\n<P>\n\nThe philosophy behind the Harvest system is that it gathers information\nabout Internet resources and customizes views into what is \"harvested.\"\nAccording to developer\n<a href=\"http://rd.cs.colorado.edu/~schwartz/Home.html\">Mike Schwartz,</a> \n\"Harvest is much more than just a 'spider.'  \nIt's intended to be a\nscalable form of infrastructure for building and distributing content,\nindexing information, as well as for accessing Web information.\"\nThe complete capabilities of Harvest are beyond the scope of this \nnews article; for further information, the reader is directed to \n<a href=\"http://bruno.cs.colorado.edu/harvest/\">The Harvest \nInformation Discovery and Access System web page</a>.\n<P>\n\nHarvest consists of several subsystems.\nA Gatherer collects indexing information and \na <a href=\"http://rd.cs.colorado.edu/brokers/\">Broker</a>\nprovides a flexible interface to this information.\nA user can access a variety of collections of \ndocuments.  The \n<a href=\"http://rd.cs.colorado.edu/brokers/www-home-pages/query.html\">Harvest WWW Broker</a>, for example, includes content summaries of more than\n7,000 Web pages.  This databasa has a very flexible interface, providing\nsearch queries based on author, keyword, title, or URL-reference.\nWhile the Harvest database (the \nWWW pages) isn't yet as extensive as other spiders', its potential \nfor efficiently collecting a large amount is great.\n<P>\n\nOther subsystems further refine Harvest's capabilities.\nSubsystems for Indexing/Searching provides a ways for \nfor a variety of search engines to be used.  For example, \n<a href=\"http://glimpse.cs.arizona.edu:1994/\">Glimpse</a>\nsupports very rapid space-efficient searches with interactive \nqueries while \n<a href=\"http://canopus.cse.psu.edu/NEBFS/nebula.html\">Nebula</a>\nprovides fast searches for \nmore complex queries.  Another Harvest subsystem, \na Replicator, provides a way to mirror information the Brokers have \nand an Object Cache meets the demand for managing \nnetworked information by providing the capability to locate\nthe fastest-responding server to a query.\n<P>\n\nWhile spiders like the Worm could successfully crawl through \nWebspace in first part of 1994, \nthe rapid increase in the amount of information on the Web since \nthen make this same crawl difficult for the older spiders.\nHarvest's systems and subsystems are extensive and provide\nfor efficient, flexible operation, and its design \naddresses the very important issue of scalability.  \nSimilarly, the \n<a href=\"http://thule.mt.cs.cmu.edu:8001/jrrl-space/webants.html\">Web Ants</a> \nproject addresses this scalability issue through its vision of \ncooperating spiders crawling through the Web.\nThe promise for the future \nis that systems like Harvest and Lycos will provide\nusers with increasingly efficient ways to locate information on \nthe Nets.\n&#164\n<P>\n\n<HR>\n\n<i>\n<a href=\"toc.html\">This Issue </a> /\n<a href=\"../../index.html\">Index</a>\n</i>\n<p>\n\n</BODY>\n</HTML>\n", "encoding": "ascii"}