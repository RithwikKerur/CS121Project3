{"url": "https://www.ics.uci.edu/~thornton/ics46/Notes/AnalyzingRecursion/", "content": "<?xml version=\"1.0\" encoding=\"iso-8859-1\"?>\r\n<!DOCTYPE html PUBLIC\r\n \"-//W3C//DTD XHTML 1.1//EN\"\r\n \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\r\n\r\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\">\r\n\r\n<head>\r\n<meta http-equiv=\"content-type\" content=\"text/html; charset=iso-8859-1\" />\r\n<link rel=\"stylesheet\" href=\"../../course.css\" type=\"text/css\" />\r\n\r\n<title>ICS 46 Spring 2018, Notes and Examples: Asymptotic Analysis of Recursion</title>\r\n\r\n</head>\r\n\r\n<body>\r\n\r\n<div class=\"navbar\">\r\n\r\n<p>\r\nICS 46 Spring 2018 |\r\n<a href=\"../../index.html\">News</a> |\r\n<a href=\"../../CourseReference.html\">Course Reference</a> |\r\n<a href=\"../../Schedule.html\">Schedule</a> |\r\n<a href=\"../../ProjectGuide\">Project Guide</a> |\r\n<a href=\"../../Notes\">Notes and Examples</a> |\r\n<a href=\"http://www.ics.uci.edu/~thornton/\">About Alex</a>\r\n</p>\r\n\r\n<hr />\r\n\r\n</div>\r\n\r\n<div class=\"header\">\r\n\r\n<p>ICS 46 Spring 2018<br />\r\n   Notes and Examples: Asymptotic Analysis of Recursion</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Asymptotic analysis of simple recursive algorithms</p>\r\n\r\n<p>Some of the algorithms and data structures we've looked at so far &mdash; and many more than we'll see later this quarter &mdash; are best implemented recursively.  Since, in this course, we're interested not only in <i>how</i> things work, but also in <i>how well</i> things work, it becomes necessary for us to be able to perform the same kinds of analyses on recursive algorithms that we've done on iterative ones.  In some cases, we'll be able to do that without any additional mathematical tools, because sometimes the analysis is quite obvious when you stop and think a bit about how the algorithm works.</p>\r\n\r\n<p>As an example, consider the following recursive function for calculating <i>n</i>! (i.e., the factorial of <i>n</i>), given an input <i>n</i>.</p>\r\n\r\n<blockquote><pre>\r\nint factorial(int n)\r\n{\r\n    if (n == 0)\r\n        return 1;\r\n    else\r\n        return n * factorial(n - 1);\r\n}\r\n</pre></blockquote>\r\n\r\n<p>(We're leaving aside, for the time being, whether one would actually want to implement this function recursively in the first place.  In C++, the cost of function calls, relative to the total cost of what we're doing, would almost certainly lead us to want to write this as a loop instead; the overhead cost of the function calls will end up being a substantial portion of the overall time spent.  Other programming languages present different tradeoffs, so there are some in which a recursive solution would be appropriate or even preferable.  Nonetheless, let's proceed with our analysis, as our goal is to learn analytical approaches that we can apply to many situations.)</p>\r\n\r\n<p>If you don't read this function carefully, you might be tempted to say, just because of the overall shape of the code, that it runs in &Theta;(1) time.  It has no loops; it's just a simple <b>if</b> statement with a simple integer expression in each.  But if you consider what's actually happening here, you'll see that this conclusion can't possibly be right; if we ask for the factorial of 10, it's easy to see that more work will be done than if we ask for the factorial of 5.</p>\r\n\r\n<p>With a deeper understanding of what the function does, you'll probably be able to come up with a correct asymptotic result, even without doing any sort of math.  Ultimately, to calculate the factorial of <i>n</i>, we'll perform a sequence of multiplications; so how many will there be?  Let's consider what happens when <i>n</i> starts out at 5.</p>\r\n\r\n<blockquote><pre>\r\nfactorial(5) = 5 * factorial(4)   &rArr;   5 * 24 = <b><i>120</i></b>\r\nfactorial(4) = 4 * factorial(3)   &rArr;   4 * 6  = 24\r\nfactorial(3) = 3 * factorial(2)   &rArr;   3 * 2  = 6\r\nfactorial(2) = 2 * factorial(1)   &rArr;   2 * 1  = 2\r\nfactorial(1) = 1 * factorial(0)   &rArr;   1 * 1  = 1\r\nfactorial(0) = 1\r\n</pre></blockquote>\r\n\r\n<p>Is it coincidence that it required exactly five multiplications to calculate the factorial of <i>n</i>?  Consider the sequence for <b>factorial(3)</b> and for <b>factorial(10)</b> and you'll see that there's a pattern here: Calculating the factorial of <i>n</i> requires <i>n</i> multiplications and a total of <i>n</i> + 1 function calls.  There aren't any shortcuts; that's what is always required.  The function calls and the multiplications take a constant amount of time &mdash; integers in C++ are always the same size &mdash; so we would say that this function runs in &Theta;(<i>n</i>) time.</p>\r\n\r\n<p>Unfortunately, while it was fairly straightforward to unravel this algorithm in our minds and see the totality of what it does, not all recursive algorithms lend themselves to this kind of back-of-the-envelope analysis.  Sometimes, as we'll see later in this course, we need mathematical tools to help us to get to the bottom of these mysteries.</p>\r\n\r\n</div>\r\n\r\n<div class=\"section\">\r\n\r\n<hr />\r\n\r\n<p class=\"title\">Recurrences</p>\r\n\r\n<p>A <i>recurrence</i> maps inputs to an output in a way that's defined recursively.  (Some recurrences are mathematical functions, though not all of them are.)  Just as recursively-described data structures are quite often best processed by algorithms that are written recursively, recursive algorithms are quite often best described mathematically using recurrences.</p>\r\n\r\n<p>Considering our <b>factorial</b> function from above, we could describe its running time using the following recurrence:</p>\r\n\r\n<blockquote><pre>\r\nT(0) = a\r\nT(n) = b + T(n - 1)\r\n</pre></blockquote>\r\n\r\n<p>The recurrence T takes one input <i>n</i>, which represents the input that could be passed to <b>factorial</b>.  Based on that input, it calculates the time our <b>factorial</b> function would take to calculate its answer.  <i>a</i> and <i>b</i> are constants:</p>\r\n\r\n<ul>\r\n  <li>In the case where <i>n</i> = 0, <i>a</i> represents the length of time spent checking whether <i>n</i> is 0 (in the <b>if</b> statement), determining it is, and then returning 1 as a result.  On different machines, this will take different amounts of time, but, all things being equal other than the input <i>n</i>, they will be constant, so we'll name that constant <i>a</i>.</li>\r\n  <li>In the case where <i>n</i> &gt; 0, <i>b</i> represents the length of time spent doing everything <i>other than</i> the recursive call: checking whether <i>n</i> is 0, determining it isn't, subtracting 1 from <i>n</i>, multiplying the result of the recursive call by <i>n</i>, and returning the result.</li>\r\n</ul>\r\n\r\n<p>The only part of the function not described by <i>a</i> and <i>b</i> is the time spent in the recursive call to <b>factorial</b>.  But that would be determined using the same recurrence: it would be T(<i>n</i> - 1).</p>\r\n\r\n<p class=\"subtitle\">Reducing a recurrence to a closed form</p>\r\n\r\n<p>The problem with recurrences is that they can be difficult to glance at and quickly see an asymptotic notation that accurately describes them.  You can certainly learn some patterns and work them out in your head when you see recurrences that fit those patterns, but, generally, you sometimes need to work them out mathematically.  There are a number of techniques that can be used to reduce recurrences to a closed form, but we'll focus on one called <i>repeated substitution</i>, which will work on all of the recurrences we'll see this quarter (though, notably, doesn't always work).</p>\r\n\r\n<p>The idea behind repeated substitution is as simple as it sounds: substitute repeatedly.  But the goal isn't just to substitute repeatedly forever; the goal is to substitute repeatedly until we see a pattern develop.  When we see a pattern develop, we can prove that the pattern really holds in all cases, then use our knowledge of that pattern to remove the recursive term from the recurrence altogether, leaving behind a closed form (i.e., a mathematical function with no recursion in it); if we can do that, we'll quickly be able to determine the corresponding asymptotic notation for it.</p>\r\n\r\n<p>Let's use repeated substitution to reduce our recurrence above to a closed form.  Starting with T(<i>n</i>), we'll substitute what we know &mdash; that T(<i>n</i>) = <i>b</i> + T(<i>n</i> - 1) &mdash; a few times and see what develops.</p>\r\n\r\n<blockquote><pre>\r\nT(n) = b + T(n - 1)\r\n     = b + (b + T(n - 2))      <i>Because T(\"something\") = b + T(\"something\" - 1) for any \"something\"</i>\r\n     = 2b + T(n - 2)           <i>Some algebraic simplification</i>\r\n     = 2b + (b + T(n - 3))     <i>Second verse, same as the first</i>\r\n     = 3b + T(n - 3)           <i>Eureka!  A pattern is developing...</i>\r\n</pre></blockquote>\r\n\r\n<p>At this point, we see an interesting pattern emerging.  After the <i>j</i><sup><small>th</small></sup> substitution, we have:</p>\r\n\r\n<blockquote><pre>\r\nT(n) = jb + T(n - j)           <i>We believe this might be true for all j = 1, 2, 3, ...</i>\r\n</pre></blockquote>\r\n\r\n<p>We've seen that it's true for <i>j</i> = 1, 2, and 3.  Would it continue being true if we kept substituting?  How can we be sure?  On the one hand, we might feel pretty confident just by looking at the algebra we're doing, but if we wanted to prove it &mdash; or if the algebra left us less convinced &mdash; we could use a mathematical technique called <i>proof by induction</i>.</p>\r\n\r\n<p class=\"subtitle\">Interlude: Proof by induction</p>\r\n\r\n<p>Using a proof by induction on <i>j</i>, we can prove our pattern is the correct one for all <i>j</i> = 1, 2, 3, ..., in two steps:</p>\r\n\r\n<ul>\r\n  <li>Prove that it's true for <i>j</i> = 1.  (This is usually called the <i>base case</i>.)</li>\r\n  <li>Prove that if it's true for some <i>j</i> = <i>k</i>, it must also be true for <i>j</i> = <i>k</i> + 1.  (This is usually called the <i>inductive step</i>.)</li>\r\n</ul>\r\n\r\n<p>If we know those two things are true, then a very powerful sequence of implications follows:</p>\r\n\r\n<ul>\r\n  <li>We know it's true for <i>j</i> = 1, because our base case was proven.</li>\r\n  <li>Because we know it's true for <i>j</i> = 1, we're sure it's true for <i>j</i> = 2, thanks to the inductive step.</li>\r\n  <li>Because we know it's true for <i>j</i> = 2, we're sure it's true for <i>j</i> = 3, thanks to the inductive step.</li>\r\n  <li>And so on, forever!  So we know it's true for all <i>j</i> = 1, 2, 3, ...</li>\r\n</ul>\r\n\r\n<p>The proof by induction here is fairly short.</p>\r\n\r\n<ul>\r\n  <li><u>Base Case (<i>j</i> = 1): Prove that T(<i>n</i>) = 1<i>b</i> + T(<i>n</i> - 1).</u>\r\n    <ul>\r\n      <li>This is trivially true, because it's part of the recurrence itself, which states that T(<i>n</i>) = <i>b</i> + T(<i>n</i> - 1).</li>\r\n    </ul>\r\n  </li>\r\n  <li><u>Inductive Step: Assume that T(<i>n</i>) = <i>kb</i> + T(<i>n</i> - <i>k</i>), then prove that T(<i>n</i>) = (<i>k</i> + 1)<i>b</i> + T(<i>n</i> - (<i>k</i> + 1)).</u>\r\n    <ul>\r\n      <li>One substitution and some algebra gets us there:\r\n<blockquote><pre>\r\nT(n) = kb + T(n - k)\r\n     = kb + (b + T(n - k - 1))\r\n     = (k + 1)b + T(n - k - 1)\r\n     = (k + 1)b + T(n - (k + 1))\r\n</pre></blockquote>\r\n      </li>\r\n    </ul>\r\n  </li>\r\n</ul>\r\n\r\n<p class=\"subtitle\">Finishing up with our recurrence</p>\r\n\r\n<p>So, we've demonstrated conclusively that, after <i>j</i> substitutions &mdash; for any positive integer <i>j</i> &mdash; we have:</p>\r\n\r\n<blockquote><pre>\r\nT(n) = jb + T(n - j)\r\n</pre></blockquote>\r\n\r\n<p>This leads to an obvious question: So what?  Remember our goal here.  We want to turn this recurrence into a closed form, a function with no recursion left in it, so it'll be a function whose shape we understand more clearly.  If we know that the pattern above is true for all <i>j</i>, then we can let <i>j</i> be anything we'd like, including a convenient choice that takes our T(<i>n</i> - <i>j</i>) term and lets us replace it with something that isn't recursive.  We know that T(0) = <i>a</i>.  So, if <i>j</i> can be anything we want and it'll still be true, then how about if we let <i>j</i> = <i>n</i>?</p>\r\n\r\n<blockquote><pre>\r\nT(n) = nb + T(n - n)\r\n     = nb + T(0)\r\n     = nb + a\r\n</pre></blockquote>\r\n\r\n<p>At first, the function <i>nb + a</i> still looks complicated, but remember that both <i>a</i> and <i>b</i> are constants.  Any function of the form <i>nb</i> + <i>a</i>, where both <i>a</i> and <i>b</i> are constants, has a linear growth rate.  (As <i>n</i> gets large, <i>a</i> becomes irrelevant; meanwhile, the constant coefficient <i>b</i> doesn't affect the growth rate of the function.)</p>\r\n\r\n<p>So, to conclude, we see that T(n) &mdash; the time it takes to calculate the factorial of <i>n</i> using our recursive <b>factorial</b> function &mdash; is &Theta;(<i>n</i>).</p>\r\n\r\n</div>\r\n\r\n</body>\r\n</html>\r\n", "encoding": "ascii"}