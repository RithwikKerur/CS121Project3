{"url": "https://www.ics.uci.edu/~dan/pubs/DC-Sec1.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 1 </TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_1\">\n<H2> 1.  FUNDAMENTAL CONCEPTS</H2> </a>\n\n<A HREF=\"DataCompression.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tA brief introduction to information theory is\nprovided in this section.  The definitions and assumptions\nnecessary to a comprehensive discussion and evaluation of\ndata compression methods are discussed.  The following string of \ncharacters is used to illustrate the concepts defined:  \n<VAR>EXAMPLE</VAR> = <kbd>aa bbb cccc ddddd eeeeee fffffffgggggggg</kbd>.\n\n<a name=\"Sec_1.1\">\n<H3> 1.1  Definitions</H3> </a>\n\n\tA code is a mapping of <EM>source messages</EM> (words from the source \nalphabet <VAR>alpha</VAR>) into <EM>codewords</EM> (words of the code alphabet <VAR>beta</VAR>).  \nThe source messages are the basic units into which the string to be\nrepresented is partitioned.  These basic units may be single symbols\nfrom the source alphabet, or they may be strings of symbols. \nFor string <VAR>EXAMPLE</VAR>, <VAR>alpha</VAR> = { a, b, c, d, e, f, g, <VAR>space</VAR>}.  \nFor purposes of explanation, <VAR>beta</VAR> will be taken to be { 0, 1 }. \nCodes can be categorized as block-block, \nblock-variable, variable-block or variable-variable, where block-block\nindicates that the source messages and codewords are of fixed\nlength and variable-variable codes map variable-length source messages\ninto variable-length codewords.  A block-block code for <VAR>EXAMPLE</VAR> \nis shown in Figure 1.1 and a variable-variable code is given in Figure 1.2.\nIf the string <VAR>EXAMPLE</VAR> were coded using the Figure 1.1 code, the \nlength of the coded message would be 120; using Figure 1.2 the length would\nbe 30.  \n\n<PRE>\nsource message   codeword             source message   codeword\n\n     <VAR>a</VAR>             000                    <VAR>aa</VAR>             0\n     <VAR>b</VAR>             001                    <VAR>bbb</VAR>            1\n     <VAR>c</VAR>             010                    <VAR>cccc</VAR>           10\n     <VAR>d</VAR>             011                    <VAR>ddddd</VAR>          11\n     <VAR>e</VAR>             100                    <VAR>eeeeee</VAR>         100\n     <VAR>f</VAR>             101                    <VAR>fffffff</VAR>        101\n     <VAR>g</VAR>             110                    <VAR>gggggggg</VAR>       110\n   <VAR>space</VAR>           111                    <VAR>space</VAR>          111\n\nFigure 1.1: A block-block code     Figure 1.2: A variable-variable code.\n</PRE>\n\nThe oldest and most widely used codes,\nASCII and EBCDIC, are examples of block-block codes, mapping \nan alphabet of 64 (or 256) single\ncharacters onto 6-bit (or 8-bit) codewords.  These are not\ndiscussed, as they do not provide compression.\nThe codes featured in this survey are of the block-variable,\nvariable-variable, and variable-block types.  \n<P>\nWhen source \nmessages of variable length are allowed, the question\nof how a message <EM>ensemble</EM> (sequence of messages) is parsed into\nindividual messages arises.  Many of the algorithms \ndescribed here are <EM>defined-word schemes</EM>.  That is, the set of \nsource messages is determined prior to the invocation of the \ncoding scheme.  \nFor example, in text file processing each character may constitute\na message, or messages may be defined to consist of alphanumeric\nand non-alphanumeric strings.  In Pascal source code, each token\nmay represent a message.  All codes involving fixed-length source\nmessages are, by default, defined-word codes.\nIn <EM>free-parse</EM> methods, the coding algorithm itself parses the ensemble\ninto variable-length sequences of symbols.  Most of the known data\ncompression methods are defined-word schemes; the free-parse model\ndiffers in a fundamental way from the classical coding paradigm.\n<P>\n\tA code is <EM>distinct</EM> if each codeword is distinguishable\nfrom every other (i.e., the mapping from source messages to codewords is one-to-one).\nA distinct code is <EM>uniquely decodable</EM> if every codeword is identifiable\nwhen immersed in a sequence of codewords.  Clearly, each of these features is \ndesirable.  The codes of Figure 1.1 and Figure 1.2 are both distinct, but\nthe code of Figure 1.2 is not uniquely decodable.  For example, the coded\nmessage 11 could be decoded as either <kbd>ddddd</kbd> or <kbd>bbbbbb</kbd>.\nA uniquely decodable code\nis a <EM>prefix code</EM> (or <EM>prefix-free code</EM>) if it has the prefix property,\nwhich requires that no codeword is a proper prefix\nof any other codeword.  All uniquely decodable block-block and \nvariable-block codes are\nprefix codes.  The code with codewords { 1, 100000, 00 } is an example  \nof a code which is uniquely decodable but which does not have the prefix\nproperty.  Prefix codes are <EM>instantaneously\ndecodable</EM>; that is, they have the desirable property that\nthe coded message can be parsed into codewords\nwithout the need for lookahead.  In order to decode a message encoded using\nthe codeword set { 1, 100000, 00 }, lookahead is required.  \nFor example, the first codeword of the message 1000000001 is 1,\nbut this cannot be determined until the last (tenth) symbol of the message \nis read (if the string of zeros had been of odd length, then the first \ncodeword would have been 100000).\n<P>\nA <EM>minimal</EM> prefix\ncode is a prefix code such that if <VAR>x</VAR> is a proper prefix of some codeword, \nthen <VAR>x sigma</VAR> is either a codeword or a proper prefix of a codeword,\nfor each letter <VAR>sigma</VAR> in <VAR>beta</VAR>.  The set of codewords { <kbd>00, 01, 10</kbd> }\nis an example of a prefix code which is not minimal.  The fact that <kbd>1</kbd> is\na proper prefix of the codeword <kbd>10</kbd> requires that <kbd>11</kbd> be either a codeword\nor a proper prefix of a codeword, and it is neither.\nIntuitively, the minimality constraint prevents \nthe use of codewords which are longer than necessary.  In the above example \nthe codeword <kbd>10</kbd> could be replaced by the codeword <kbd>1</kbd>, yielding a \nminimal prefix code with shorter codewords.  The codes discussed\nin this paper are all minimal prefix codes.\n<P>\nIn this section, a <EM>code</EM> has been defined to be a mapping from a \nsource alphabet to a code alphabet; we now define related terms.\nThe process of transforming a source ensemble into a coded message\nis <EM>coding</EM> or <EM>encoding</EM>.  The encoded message may be\nreferred to as an <EM>encoding</EM> of the source ensemble.  The\nalgorithm which constructs the mapping and uses it to transform the\nsource ensemble is called the <EM>encoder</EM>.  The <EM>decoder</EM>\nperforms the inverse operation, restoring the coded message to its\noriginal form.\n\n<a name=\"Sec_1.2\">\n<H3> 1.2  Classification of Methods</H3> </a>\n\n\tIn addition to the categorization of data compression schemes\nwith respect to message and codeword lengths, these methods are\nclassified as either static or dynamic.\nA <EM>static</EM> method is one in which the mapping from the set of messages\nto the set of codewords is fixed before transmission begins, so that\na given message is represented by the same codeword\nevery time it appears in the message ensemble.  The classic static\ndefined-word scheme is Huffman coding [Huffman 1952].  In Huffman \ncoding, the assignment of codewords to source messages is based on the \nprobabilities with which the source messages appear in the \nmessage ensemble.  Messages which appear more frequently are represented\nby short codewords; messages with smaller probabilities map to longer \ncodewords.  These probabilities are determined before transmission begins.\nA Huffman code for the ensemble <VAR>EXAMPLE</VAR> is given in Figure 1.3.\nIf <VAR>EXAMPLE</VAR> were coded using this Huffman mapping, the length of the\ncoded message would be 117.\nStatic Huffman coding is discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>.  Other  static schemes are\ndiscussed in\n<a href=\"DC-Sec2.html#Sec_2\">Sections 2</a> and <a href=\"DC-Sec3.html#Sec_3\">3</a>. \n\n<PRE>\n   source message   probability      codeword\n\n        <VAR>a</VAR>             2/40           1001\n        <VAR>b</VAR>             3/40           1000\n        <VAR>c</VAR>             4/40           011\n        <VAR>d</VAR>             5/40           010\n        <VAR>e</VAR>             6/40           111\n        <VAR>f</VAR>             7/40           110\n        <VAR>g</VAR>             8/40           00\n      <VAR>space</VAR>           5/40           101\n\nFigure 1.3 -- A Huffman code for the message <VAR>EXAMPLE</VAR> (code length=117).\n</PRE>\n\n\tA code is <EM>dynamic</EM> if the mapping from the set of messages to the\nset of codewords changes over time.  For example, dynamic Huffman coding \ninvolves computing an approximation to the probabilities of occurrence \n\"on the fly\", as the ensemble is being\ntransmitted.  The assignment of codewords to messages is based on\nthe values of the relative frequencies of occurrence at each point in time.  \nA message <VAR>x</VAR> may\nbe represented by a short codeword early in the transmission because \nit occurs frequently at the beginning of the ensemble, even though its \nprobability of occurrence over the total ensemble is low.  Later, \nwhen the more probable messages begin to occur with higher frequency, \nthe short codeword will be mapped to one of the higher probability\nmessages and <VAR>x</VAR> will be mapped to a longer codeword.  As an illustration,\nFigure 1.4 presents a dynamic Huffman code table corresponding to the\nprefix <kbd>aa bbb</kbd> of <VAR>EXAMPLE</VAR>.  Although the frequency of <VAR>space</VAR>\nover the entire message is greater than that of <VAR>b</VAR>,  at this\npoint in time <VAR>b</VAR> has higher frequency and therefore is \nmapped to the shorter codeword.\n\n<PRE>\n   source message   probability      codeword\n\n        <VAR>a</VAR>             2/6            10\n        <VAR>b</VAR>             3/6            0\n      <VAR>space</VAR>           1/6            11\n\nFigure 1.4 -- A dynamic Huffman code table for the prefix\n              <kbd>aa bbb</kbd> of message <VAR>EXAMPLE</VAR>.\n</PRE>\n\n\tDynamic codes are also referred to in the literature as\n<EM>adaptive</EM>, in that they adapt to changes in ensemble characteristics\nover time.  The term adaptive will be used for the remainder of this\npaper; the fact that these codes adapt to changing characteristics is\nthe source of their appeal.  Some adaptive methods adapt to changing patterns\nin the source [Welch 1984] while others exploit locality of reference \n[Bentley et al. 1986].  Locality of reference is the tendency, common \nin a wide variety of text types, for a particular word to occur\nfrequently for short periods of time then fall into disuse for long\nperiods.\n<P>\nAll of the adaptive methods are <EM>one-pass</EM> methods; only\none scan of the ensemble is required. \nStatic Huffman coding requires two passes:\none pass to compute probabilities and determine the mapping, and a\nsecond pass for transmission.  Thus, as long as the encoding and decoding \ntimes of an adaptive method are not substantially greater than those of\na static method, the fact that an initial scan is not needed implies\na speed improvement in the adaptive case.   In addition, the mapping \ndetermined in the first pass of a static coding scheme \nmust be transmitted by the encoder to the decoder.  The mapping may\npreface each transmission (that is, each file sent), or a single mapping \nmay be agreed upon and used for multiple transmissions.\nIn one-pass methods the encoder defines and redefines the mapping dynamically, \nduring transmission.  The decoder must define and redefine the mapping in\nsympathy, in essence \"learning\" the mapping as codewords are received.\nAdaptive methods are discussed in\n<a href=\"DC-Sec4.html#Sec_4\">Sections 4</a> and <a href=\"DC-Sec5.html#Sec_5\">5</a>.  \n<P>\nAn algorithm may also be a <EM>hybrid</EM>, neither completely\nstatic nor completely dynamic.  In a simple hybrid scheme,\nsender and receiver maintain identical <EM>codebooks</EM> \ncontaining <VAR>k</VAR> static codes.  For each transmission, \nthe sender must choose one of the <VAR>k</VAR> previously-agreed-upon codes \nand inform the receiver of his choice (by transmitting first the\n\"name\" or number of the chosen code).\nHybrid methods are discussed further in\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a> and <a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>.\n\n<a name=\"Sec_1.3\">\n<H3> 1.3  A Data Compression Model</H3> </a>\n\n\tIn order to discuss the relative merits of data compression\ntechniques, a framework for comparison must be established.  There\nare two dimensions along which each of the schemes discussed here\nmay be measured, algorithm complexity and amount of compression.\nWhen data compression is used in a data transmission application,\nthe goal is speed.  Speed of transmission depends upon the number\nof bits sent, the time required for the encoder to generate the\ncoded message, and the time required for the decoder to recover\nthe original ensemble.  In a data storage application, although the\ndegree of compression is the primary concern, it is nonetheless\nnecessary that the algorithm be efficient in order for the\nscheme to be practical. \nFor a static scheme, there are three algorithms to analyze:\nthe map construction algorithm, the encoding algorithm, and the\ndecoding algorithm.  For a dynamic scheme, there are just two algorithms:\nthe encoding algorithm, and the decoding algorithm.\n<P>\n\tSeveral common measures of compression have been \nsuggested:  redundancy [Shannon and Weaver 1949], average message\nlength [Huffman 1952], and compression ratio [Rubin 1976; \nRuth and Kreutzer 1972].  These measures are defined below.  Related to each of these measures\nare assumptions about the characteristics of the source.\nIt is generally assumed in information theory that all statistical\nparameters of a message source are known with perfect accuracy\n[Gilbert 1971].  The most common model is that of a discrete\nmemoryless source; a source whose output is a sequence of letters\n(or messages),\neach letter being a selection from some fixed alphabet <kbd>a</kbd>,...\nThe letters are taken to be random, statistically independent\nselections from the alphabet, the selection being made according\nto some fixed probability assignment <VAR>p</VAR>(<kbd>a</kbd>),... [Gallager 1968].\nWithout loss of generality, the code alphabet is assumed\nto be {0,1} throughout this paper.  The modifications\nnecessary for larger code alphabets are straightforward.\n<P>\n\tIt is assumed that any cost associated with the code \nletters is uniform.  This is a reasonable assumption, although it\nomits applications like telegraphy where the code symbols are of\ndifferent durations.  The assumption is also important, since \nthe problem of constructing optimal codes over unequal code letter costs \nis a significantly different and more difficult problem.\nPerl et al. and Varn have developed algorithms for minimum-redundancy \nprefix coding in the case of arbitrary symbol cost and equal \ncodeword probability [Perl et al. 1975; Varn 1971].  \nThe assumption of equal probabilities mitigates the difficulty\npresented by the variable symbol cost.  For the more general unequal \nletter costs and unequal probabilities model, Karp has proposed an \ninteger linear programming approach [Karp 1961].  There have been \nseveral approximation algorithms proposed for this more difficult \nproblem [Krause 1962; Cot 1977; Mehlhorn 1980].\n<P>\n\tWhen data is compressed, the goal is to reduce redundancy,\nleaving only the informational content.  The measure of information\nof a source message <VAR>x</VAR> (in bits) is -lg <VAR>p</VAR>(<VAR>x</VAR>)\n[lg denotes the base 2 logarithm].  This definition\nhas intuitive appeal; in the case that <VAR>p</VAR>(<VAR>x</VAR>=1,\nit is clear that <VAR>x</VAR> is not at all informative since it had to occur.\nSimilarly, the smaller the value of <VAR>p</VAR>(<VAR>x</VAR>,\nthe more unlikely <VAR>x</VAR>\nis to appear, hence the larger its information content.  The reader\nis referred to Abramson for a longer, more elegant\ndiscussion of the legitimacy of this technical definition of the\nconcept of information [Abramson 1963, pp. 6-13].  \nThe average information content over\nthe source alphabet can be computed by weighting the information content\nof each source letter by its probability of occurrence, yielding the\nexpression SUM{i=1 to n} [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))].  This quantity is\nreferred to as the <VAR>entropy</VAR> of a source letter, or the entropy of the \nsource, and is denoted by <VAR>H</VAR>.   \nSince the length of a codeword for message <VAR>a</VAR>(<VAR>i</VAR>)\nmust be sufficient to carry the information content of <VAR>a</VAR>(<VAR>i</VAR>),\nentropy imposes a lower bound on the number of bits required for the\ncoded message.  The total number of bits must be at least as large as\nthe product of <VAR>H</VAR> and the length of the source ensemble.\nSince the value of <VAR>H</VAR> is generally not an integer, variable length\ncodewords must be used if the lower bound is to be achieved.\nGiven that message <VAR>EXAMPLE</VAR> is to be encoded one letter at a time,\nthe entropy of its source can be calculated using the probabilities \ngiven in Figure 1.3:\n<VAR>H</VAR> = 2.894, so that the minimum number of bits contained \nin an encoding of <VAR>EXAMPLE</VAR> is 116.\nThe Huffman code given in <a href=\"#Sec_1.2\">Section 1.2</a> does not quite\nachieve the theoretical minimum in this case.\n<P>\nBoth of these definitions of information content are due to \nShannon.  A derivation of the concept of entropy as it relates \nto information theory is presented by Shannon [Shannon and Weaver 1949].\nA simpler, more intuitive explanation of entropy is offered by Ash\n[Ash 1965].\n<P>\n\tThe most common notion of a \"good\" code is one which \nis <EM>optimal</EM> in the sense of having minimum redundancy.  <EM>Redundancy</EM>\ncan be defined as:  SUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>)\n- SUM [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))] where <VAR>l</VAR>(<VAR>i</VAR>) is\nthe length of the codeword representing message <VAR>a</VAR>(<VAR>i</VAR>).  The expression\nSUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>) represents the lengths of the codewords weighted\nby their probabilities of occurrence, that is, the average codeword length.\nThe expression SUM [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))] is entropy, <VAR>H</VAR>.  Thus,\nredundancy is a measure of the difference between average \ncodeword length and average information content.  If a code has \nminimum average codeword length for a given discrete probability distribution,\nit is said to be a minimum redundancy code.\n<P>\nWe define the term <EM>local redundancy</EM> to capture the notion\nof redundancy caused by local properties of a message ensemble,\nrather than its global characteristics.  While the model used for\nanalyzing general-purpose coding techniques assumes a random distribution\nof the source messages, this may not actually be the case.  In particular\napplications the tendency for messages to cluster in predictable patterns\nmay be known.  The existence of predictable patterns may be exploited \nto minimize local redundancy.  Examples of applications in which local\nredundancy is common, and methods for dealing with local redundancy,\nare discussed in\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a> and <a href=\"DC-Sec678.html#Sec_6.2\">Section 6.2</a>.\n<P>\nHuffman uses <EM>average message length</EM>, SUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>), as\na measure of the efficiency of a code.  Clearly the meaning of\nthis term is the average length of a <EM>coded</EM> message. \nWe will use the term <EM>average codeword length</EM> to represent\nthis quantity.  Since redundancy is defined to be average codeword\nlength minus entropy and entropy is constant\nfor a given probability distribution, minimizing average codeword\nlength minimizes redundancy.\n<P>\nA code is <EM>asymptotically optimal</EM> if it has the property\nthat for a given probability distribution, the ratio of average\ncodeword length to entropy approaches 1 as entropy tends to infinity.\nThat is, asymptotic optimality guarantees that average codeword length\napproaches the theoretical minimum (entropy represents information content,\nwhich imposes a lower bound on codeword length).\n<P>\n\tThe amount of compression yielded by a coding scheme can be\nmeasured by a <EM>compression ratio</EM>.  The term compression ratio\nhas been defined in several ways.  The definition   \n<VAR>C</VAR> = (average message length)/(average codeword length) \ncaptures the common meaning, which is a comparison of the length of the coded\nmessage to the length of the original ensemble [Cappellini 1985].  \nIf we think of the characters of the ensemble <VAR>EXAMPLE</VAR> as 6-bit ASCII\ncharacters, then the average message length is 6 bits.  The Huffman\ncode of\n<a href=\"#Sec_1.2\">Section 1.2</a> represents <VAR>EXAMPLE</VAR> in 117 bits,\nor 2.9 bits per character.  This yields a compression ratio of 6/2.9,\nrepresenting compression by a factor of more than 2.  Alternatively,\nwe may say that Huffman encoding produces a file whose size is\n49% of the original ASCII file, or that 49% compression has been achieved.\n \nA somewhat different definition of compression ratio, by Rubin,\n<VAR>C</VAR> = (<VAR>S</VAR> - <VAR>O</VAR> - <VAR>OR</VAR>)/<VAR>S</VAR>, includes the representation \nof the code itself in the transmission cost [Rubin 1976].  In this\ndefinition <VAR>S</VAR> represents the length of the source ensemble, <VAR>O</VAR> the \nlength of the output (coded message), and <VAR>OR</VAR> the size of the \"output \nrepresentation\" (eg., the number of bits required for the encoder to\ntransmit the code mapping to the decoder).  The quantity <VAR>OR</VAR> \nconstitutes a \"charge\" to an algorithm for transmission of information \nabout the coding scheme.  The intention is to measure the total size\nof the transmission (or file to be stored).\n\n<a name=\"Sec_1.4\">\n<H3> 1.4  Motivation</H3> </a>\n\n\tAs discussed in the Introduction, data compression has \nwide application in terms of information\nstorage, including representation of the abstract data type <EM>string</EM>\n[Standish 1980] and file compression.  Huffman coding is used for \ncompression in several file archival systems [ARC 1986; PKARC 1987], \nas is Lempel-Ziv coding, one of the adaptive schemes to be discussed \nin <a href=\"DC-Sec5.html#Sec_5\">Section 5</a>.\nAn adaptive Huffman coding technique is the basis for the <EM>compact</EM>\ncommand of the UNIX operating system, and the UNIX \n<EM>compress</EM> utility employs the Lempel-Ziv approach [UNIX 1984].\n<P>\n\tIn the area of data transmission, Huffman coding has been\npassed over for years in favor of block-block codes, notably ASCII.\nThe advantage of Huffman coding is in the average number of bits per character\ntransmitted, which may be much smaller than the lg <VAR>n</VAR> bits per\ncharacter (where <VAR>n</VAR> is the source alphabet size) of a block-block\nsystem.  The primary difficulty associated with variable-length\ncodewords is that the rate at which bits are presented to the \ntransmission channel will fluctuate, depending on the relative\nfrequencies of the source messages.  This requires buffering between\nthe source and the channel.  Advances in technology have both overcome\nthis difficulty and contributed to the appeal of variable-length\ncodes.  Current data networks allocate communication resources to\nsources on the basis of need and provide buffering as part of the \nsystem.  These systems require significant amounts of protocol, and \nfixed-length codes are quite inefficient for applications such as \npacket headers.  In addition, communication costs are beginning to\ndominate storage and processing costs, so that variable-length coding\nschemes which reduce communication costs are attractive even if they\nare more complex.  For these reasons, one could expect to see even\ngreater use of variable-length coding in the future.\n<P>\n\tIt is interesting to note that the Huffman coding algorithm,\noriginally developed for the efficient transmission of data, also\nhas a wide variety of applications outside the sphere of data\ncompression.  These include construction of optimal search trees\n[Zimmerman 1959; Hu and Tucker 1971; Itai 1976], list merging [Brent\nand Kung 1978], and generating optimal evaluation trees in the \ncompilation of expressions [Parker 1980].  Additional applications\ninvolve search for jumps in a monotone function of a single variable,\nsources of pollution along a river, and leaks in a pipeline [Glassey\nand Karp 1976].  The fact that this elegant combinatorial algorithm\nhas influenced so many diverse areas underscores its importance. \n\n<P>\n<A HREF=\"DataCompression.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}