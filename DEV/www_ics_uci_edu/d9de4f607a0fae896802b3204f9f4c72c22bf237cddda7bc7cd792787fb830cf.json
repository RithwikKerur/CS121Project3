{"url": "https://www.ics.uci.edu/~pattis/ICS-33/lectures/decoratorspackages.txt", "content": "\t\t\t\tDecorators\r\n\r\nWe have discussed decorators before. Typically we described them as a class\r\nthat takes an argument that supports some protocol (method calls) and returns\r\nan object that supports the same protocol. When the decorator object executes\r\nthese methods, it performs a bit differently than for the decorated object. The\r\ndecorator object typically stores a reference to the decorated object and calls\r\nmethods on it when necessary. Previously, we wrote decorarators for iterables\r\n(the iterator protocol, calling __iter__ and __next__ methods) using classes\r\nand also generator functions. For example, the Unique decorator took as an\r\nargument something that was iterable; iterating over the Unique object produced\r\nall the values from its iterable argument, but never yields the same one twice\r\n(the decorator object produces each value only once).\r\n\r\nThe examples in this lecture, and the Check_Annotations class in Programming\r\nAssignment #4, use classes to decorate functions by using the __call__ protocol\r\nto decorate/augment how functions are called. Although, some examples don't\r\nneed classes (and can use just functions) to do the decoration: we show such\r\ndecorators both ways.\r\n\r\nFinally, it is appropriate to put this material right after our discussion\r\nof recursion and functional programming. Decorating function calls is most\r\ninteresting when the functions are called many times. And, one call to a\r\nrecursive function typically results in it calling itself many times.\r\n\r\nWe will use the recursive factorial and fib (Fibonacci) functions in our\r\nexamples below. Both require an argument >= 0.\r\n\r\ndef factorial(n):\r\n    if n == 0:\r\n        return 1\r\n    else:\r\n        return n*factorial(n-1)\r\n\r\ndef fib(n):\r\n    if    n == 0: return 1                      # or, if 0 <= n <= 1: return 1\r\n    elif  n == 1: return 1\r\n    else:         return fib(n-1) + fib(n-2)\r\n\r\nThe factorial function is linearly-recursive: it calls itself once, recurring\r\ndownward (as illustrated in class, with call frames in a hand-simulation) to\r\nthe base case, then going upward returning results to the top. The factorial(n)\r\ndoes n function calls. The fib function always calls itself twice, going\r\ndownward, upward, downard, upward in a complex way, which leads to an\r\nexponential number of function calls. We will examine a picture of what it\r\ndoes, which requires a branching (tree-like) structure to illustrate.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nSpecial Python Syntax for Decorators\r\n\r\nIf Decorator is the name of a decorator (a class or a function) that takes one\r\nargument, we can use it to decorate a function object, by writing either\r\n\r\n  def f(params-annotation) -> result-annotation:\r\n      ...\r\n  f = Decorator(f)\r\n\r\nor by using the new decoarator syntax of @\r\n\r\n  @Decorator\r\n  def f(params-annotation) -> result-annotation:\r\n\r\nwhich both have the same meaning. @Decorator is applied when a module is loaded,\r\nas is indicated by the first form.\r\n\r\nWe can also use multiple decorators on functions. The meaning of\r\n\r\n@Decorator1\r\n@Decorator2\r\ndef f(...)\r\n    ...\r\n\r\nis equivalent to writing\r\n\r\nf = Decorator1(Decorator2(f))\r\n\r\nso Decorator1 decorates the result of Decorator2 decorating f; the decorators\r\nare applied in the reverse of the order they appear (with the closest one to\r\nthe decorated object applying first).\r\n\r\nWe have seen a decorator name \"staticmethod\", which we have used to decorate\r\nmethods defined in classes: those methods that don't have a self parameter\r\nand are mostly called in the form Classname.staticmethodname(....). But we can\r\ncall such methods using objects, just like non-static methos. When called by\r\nobjects in the class, e.g., o.staticmethodname(args), the FEOOP translates\r\ncalls to static methods without passing o as the first argument: it is called\r\nlike just Classname.statimethodname(args). This change is what the staticmethod\r\ndecorator accomplishes.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nExamples of Function Decorators\r\n\r\nHere are three decorators for functions (three are classes; some can be written\r\neasily as functions too).\r\n\r\n(1) Track_Calls remembers the function it is decorating and initializes the\r\ncalls counter to 0; the decorator object overloads the __call__ method so that\r\nall calls to the decorator object increment its calls counter and then actually\r\ncalls the decorated function (which if recursive, increments the calls counter\r\nfor every recursive call). Once the function's value is computed and returned,\r\nthe calls counter instance name can be accessed (via the called method) and\r\nreset (via the reset method) for tracking further calls.\r\n\r\nclass Track_Calls:\r\n    def __init__(self,f):\r\n        self._f = f\r\n        self._calls = 0\r\n    \r\n    def __call__(self,*args,**kargs):  # bundle arbitrary arguments to this call\r\n        self._calls += 1\r\n        return self._f(*args,**kargs)  # unbundle arbitrary arguments to call f\r\n\r\n    def called(self):\r\n        return self._calls\r\n    \r\n    def reset(self):\r\n        self._calls = 0\r\n\r\nSo, if we wrote\r\n\r\n@Track_Calls\r\ndef factorial(n):\r\n    if n == 0:\r\n        return 1\r\n    else:\r\n         return n*factorial(n-1)\r\n\r\nwhich is equvalent to \r\n\r\ndef factorial(n):\r\n    if n == 0:\r\n        return 1\r\n    else:\r\n         return n*factorial(n-1)\r\nfactorial = Track_Calls(factorial)\r\n\r\nthen the name factorial would refer to a Track_Calls object, whose self._f\r\nrefers to the actual function object factorial defined and whose self._calls\r\nis initialized to 0.\r\n\r\nExamine the picture that shows how a Track_Calls object decorates the factorial\r\nfunction object (available on the Weekly Schedule). Note that the name factorial\r\nis bound to a Track_Calls object.\r\n\r\nIf we called factorial(3), Python executes the factorial.__call__(3) method on\r\nthe factorial object, which by the FEOOP would call\r\nTrack_Calls.__call__(factorial,3), which first increments factorial._calls and\r\nthen calls factorial._f(3) - the original factorial function object: its body\r\nwould recursively call factorial(2) -recall the name factorial is bound to a\r\nTrack_Calls object- which Python executes as factorial.__call__(2), again\r\nincrementing ._calls and initiating a recursive call.\r\n\r\nThe main take-away is that every time Python calls factorial (including\r\nrecursively inside factorial) it finds the binding for factorial, which is the\r\nTrack_Calls object.\r\n\r\nThis process continues, and ultimately factorial.__call__(3) returns 6 with\r\nfactorial.calls = 4. We have seen before that factorial(n) calls itself for\r\nn, n-1, n-2, ... 0 for a total of n+1 times. We could write\r\n\r\n  factorial.reset()\r\n  print(factorial(10))\r\n  print(factorial.called())\r\n\r\nExamine the picture accompanying this lecture, showing the fib function (whose\r\nbody does two recursive calls) and all the recursive calls it makes when called\r\nwith a variety of numbers. Ignore the colors for now. For example calling fib(2)\r\n= 2 does a total of 3 calls to fib (counting itself), fib(3) = 3 does a total\r\nof 5 calls, fib(4) = 5 does a total of 9 calls, fib(5) = 8 does a total of 15\r\ncalls, and fib(6) = 13 does a total of 25 calls. We can use Track_Calls to\r\nverify these numbers (and compute fib for bigger numbers). For example\r\nfib(10) = 89 does a total of 177 calls, and fib(20) = 10,946 and does a total\r\nof 21,891 calls.\r\n\r\nThe number of calls needed to compute fib(n) is greter than the value of fib(n).\r\n\r\nRun the program accompanying this lecture to see the value (and number of\r\nfunction calls) needed to evaluate fib(0) through fib(30). Notice that the last\r\nfew calculations take a noticable amount of time: computing fib(30) requires\r\n2,692,537 function calls!\r\n\r\nWe can also write the Track_Calls class decorator as the following function\r\n\r\ndef track_calls(f):\r\n    def call(*args,**kargs):\r\n        call._calls += 1\r\n        return f(*args,**kargs)\r\n\r\n    call._calls = 0         # define calls attribute on call function-object!\r\n    return call\r\n\r\nHere we define an inner-function named call, which is returned by track_calls.\r\nBefore returning call's function object, a calls attribute is defined for that\r\nfunction object and initialzed to 0; inside the call function that instance\r\nname is incremented before the original function (f) is called and the value it\r\ncomputes returned.\r\n\r\nWe know objects have namespaces stored in __dict__ of the object, and function\r\nobjects are just a kind of objects. After executing \r\n\r\n  factorial = track_calls(factorial)       # Returns the call function object\r\n\r\nWe can examine and rebind factorial.calls: e.g, print(factorial.calls) and\r\nfactorial.calls = 0.\r\n\r\nIn fact, we can define the track_calls function below, so that we can call the\r\nmethods called/reset on it. Here we bind the attributes reset/called to\r\nfunctions (on named - because it executes a statement - and one lambda, because\r\nit just returns a value)\r\n\r\ndef track_calls(f):\r\n    def call(*args,**kargs):\r\n        call._calls += 1\r\n        return f(*args,**kargs)\r\n\r\n    call._calls = 0              # define calls, reset, and called attributes on\r\n                                 # call function-object; bind the first to a\r\n    def reset(): call._calls = 0 # data object, the next two to function objects\r\n    call._reset = reset\r\n\r\n    call.called = lambda : call._calls\r\n    \r\n    return call\r\n\r\nIn this case we can write (exactly as we did for the Track_Calls class):\r\n\r\nfactorial = track_calls(factorial)       # Returns the call function object\r\nfactorial.reset()\r\nprint(factorial(10))\r\nprint(factorial.called())\r\n\r\nRemember that when calling factorial.reset(), Python will first try to find the\r\nreset attribute in the factorial function object (which is the call function\r\nobject track_calls returns). This attribute will be found there: it has been set\r\nby defining the reset function and then adding it to the namespace of the calls\r\nfunction object, which track_calls returns, and is bound to the name factorial.\r\n\r\n    def reset(): call._calls = 0\r\n    call.reset = reset\r\n \r\nI will continue to show equivalent class/function definitions for the decorators\r\ndescribed below, but in a simplified form (like the original track_calls above).\r\nAt the end of this lecture, I will briefly explain the reason for using classes\r\ninstead of functions: the ability to overload the __getattr__ function, which is\r\nuseful when using multiple decorators at the same time.\r\n\r\n------------------------------------------------------------------------------\r\n\r\n(2) Memoize remembers the function it is decorating and initializes a dict to\r\n{}. It will use this dict to cache (keep track of and be able to access quickly)\r\nthe arguments to calls, which are associated in the dict with the values\r\nultimately returned by function calls with those arguments. The decorator object\r\noverloads the __call__ method so that all calls to the decorator object first\r\ncheck to see if the arguments are already cached in the dict (if so, the value\r\ncomputed for these arguments is there too). If the arguments are there, their\r\nassociated value is returned immediately, without executing the code in the\r\ndecorated function; if the arguments are not there, the decorated function is\r\ncalled, and its answer is cached in the dict with the function's arguments, and\r\nthe answer is returned. \r\n\r\nFor simplicity here, I'm assuming all arguments are positional (so no **kargs).\r\nAlso,since the arguments are used as keys in a dictionary, they must be\r\nimmutable/hashable (which ints are). If the weren't, we could try to convert\r\neach argument an immutable one for use in the cache dict: e.g,, convert a list\r\ninto an equivalent tuple.\r\n\r\nIn this way, a function never has to compute the same value twice. Memoization\r\nis useful for multiply-recursive calls, as in the fibonacci function (not so\r\nmuch in factorial, where it computes each value only once), where the fib of\r\na value might be computed many times: fib(2) is computed 5 times when computing\r\nfib(6).\r\n\r\nclass Memoize:\r\n    def __init__(self,f):\r\n        self._f = f\r\n        self._cache = {}\r\n\r\n    def __call__(self,*args):\r\n        if args in self._cache:\r\n            return self._cache[args]\r\n        else:\r\n            answer = self._f(*args)        # Recursive calls will set cache too\r\n            self._cache[args] = answer\r\n\t    return answer\r\n\r\n    def reset_cache(self):\r\n        self._cache = {}\r\n\r\nExamine the picture accompanying this lecture, showing the Fibonacci function.\r\nWhen decorated by Memoize, the only calls that actually compute a result are\r\nthose in green. As each green function calls finishes, it caches its result in\r\nthe dictionary. Afterward, calling fib with that argument again will obtain the\r\nresult from the cache immediately (obviating the need for all the calls in\r\nwhite). With memoization, calling fib(n) does a total of n+1 function calls.\r\n\r\nRun the program accompanying this lecture to see the value (and number of\r\nfunction calls) needed to evaluate fib(0) through fib(30). This time, uncomment\r\n@Memoize and the fib.reset_cache() call in the loop. Notice that all the\r\ncalculations are done instantaneously: with memoization, computing fib(30)\r\nrequires only 31 function calls (with cached values computed immediately\r\nmillions of times).\r\n\r\nMemoization can convert a recursive function requiring an exponential number of \r\nfunction calls to one requiring just a linear number.\r\n\r\nWe can also write memoize as a function, to return a wrapper function (it can\r\nbe named anything) that does the same operations as the class above. Here is\r\na version written in the style of track_calls, declaring the data cache and\r\nthe method reset_cache as attributes of the wrapper function object.*\r\n----\r\n*We have been picturing a function object just by its code. Actually, a function\r\nobject is a real object with a __dict__ for storing its attributes. One\r\nattribute is __code__; in the example below we create more attributes. We did\r\nthe same kind of thing showing how partial is implemented in Python.\r\n----\r\n\r\ndef memoize(f):\r\n    def wrapper(*args):\r\n        if args in wrapper._cache: \r\n            return wrapper._cache[args]\r\n        else:\r\n            answer = f(*args)\r\n            wrapper._cache[args] = answer\r\n            return answer\r\n\r\n    wrapper._cache = {}\r\n    \r\n    def reset_cache():\r\n        wrapper._cache = {}\r\n    wrapper.reset_cache = reset_cache\r\n\r\n    return wrapper\r\n\r\nWe can also use the following code, which defines cache as a local variable in\r\nmemoize (unlike what we did above, defining it as an attribute of the wrapper\r\nfunction object). \r\n\r\ndef memoize(f):\r\n    cache = {}\r\n    def wrapper(*args):\r\n        if args in cache: \r\n            return cache[args]\r\n        else:\r\n            answer = f(*args)\r\n            cache[args] = answer\r\n            return answer\r\n\r\n    def reset_cache():\r\n        cache.clear()\r\n    wrapper.reset_cache = reset_cache\r\n\r\n    return wrapper\r\n\r\nFinally, we could also define the reset_cache function inside memoize as\r\n\r\n    def reset_cache():\r\n        nonlocal cache          # Allows cache in enclosing scope to be updated\r\n        cache = {}\r\n\r\nHere we need the declaration \"nonlocal cache\" inside the reset_cache function,\r\nso we can rebind the cache variable declared outside this function. Normally,\r\nbinding any variable makes it local to the function in which it is bound. The\r\nnonlocal declaration is like the global declaration that we have studied, but\r\nit first looks in the enclosing scope instead of immediately looking in the\r\nglobal scope.\r\n\r\nIn the previous version of memoize, inside reset_cache we called clear to mutate\r\nthe cache, which does not involve rebinding: looking up names that are not being\r\nbound uses the LEGB rule, so it finds that name in the enclosing scope.\r\n\r\n------------------------------------------------------------------------------\r\n\r\n(3) Illustrate_Recursive remembers the function it is decorating and\r\ninitializes a tracing variable to False. The decorator object overloads the\r\n__call__ method so that all calls to the decorator object just return the\r\nresult of calling the decorated function (if tracing is off). Calling\r\n.illustrate(...) on the decorator calls the illustrate method, which sets up\r\nfor tracing, and then uses __call__ to trace all entrances and exists to the\r\ndecorated function printing indented/outdented information for each function\r\ncall/return.\r\n\r\nclass Illustrate_Recursive:\r\n    def __init__(self,f):\r\n        self._f = f\r\n        self._trace = False\r\n        \r\n    def illustrate(self,*args,**kargs):\r\n        self._indent = 0\r\n        self._trace = True\r\n        answer = self.__call__(*args,**kargs)\r\n        self._trace = False\r\n        return answer\r\n    \r\n    def __call__(self,*args,**kargs):\r\n        if self._trace:\r\n            if self._indent == 0:\r\n                print('Starting recursive illustration'+30*'-')\r\n            print (self.i_ndent*\".\"+\"calling\", self._f.__name__+str(args)+str(kargs))\r\n            self._indent += 2\r\n        answer = self._f(*args,**kargs)\r\n        if self._trace:\r\n            self._indent -= 2\r\n            print (self._indent*\".\"+self._f.__name__+str(args)+str(kargs)+\" returns\", answer)\r\n            if self._indent == 0:\r\n                print('Ending recursive illustration'+30*'-')\r\n        return answer\r\n\r\nRun the program accompanying this lecture to example of this trace (and others\r\nby changing the program. Here is what the program prints for a call to factorial\r\nand fibonacci.\r\n\r\n@Illustrate_Recursive\r\ndef factorial(n):\r\n    if n == 0:\r\n        return 1\r\n    else:\r\n        return n*factorial(n-1)\r\n\r\nprint(factorial.illustrate(5))\r\n\r\nStarting recursive illustration------------------------------\r\ncalling factorial(5,){}\r\n..calling factorial(4,){}\r\n....calling factorial(3,){}\r\n......calling factorial(2,){}\r\n........calling factorial(1,){}\r\n..........calling factorial(0,){}\r\n..........factorial(0,){} returns 1\r\n........factorial(1,){} returns 1\r\n......factorial(2,){} returns 2\r\n....factorial(3,){} returns 6\r\n..factorial(4,){} returns 24\r\nfactorial(5,){} returns 120\r\nEnding recursive illustration------------------------------\r\n\r\nFactorial is a linear recursive function (one recursive call in its body) so\r\nthe structure of its recursion is simple. Each factorial calls the one below\r\nit (indented); when the bottom one returns 1 for its base case, each call above\r\nit (unindented) can compute and return its result.\r\n\r\nHere is an illustration of a tail-recursive version of factorial. Notice that\r\nthat the call to the factorial_helper function is illustrated. Recall that\r\nPython does NOT change tail-recursive functions to be iterative.\r\n\r\ndef factorial(n):\r\n    @Illustrate_Recursive\r\n    def factorial_helper(n,acc):\r\n        if n == 0:\r\n            return acc\r\n        else:\r\n            return factorial_helper(n-1,acc*n)\r\n    return factorial_helper.illustrate(n,1)\r\n    \r\nprint(factorial(5))     # For Illustrate_Recursive of tail-recursive helper\r\n\r\nStarting recursive illustration------------------------------\r\ncalling factorial_helper(5, 1){}\r\n..calling factorial_helper(4, 5){}\r\n....calling factorial_helper(3, 20){}\r\n......calling factorial_helper(2, 60){}\r\n........calling factorial_helper(1, 120){}\r\n..........calling factorial_helper(0, 120){}\r\n..........factorial_helper(0, 120){} returns 120\r\n........factorial_helper(1, 120){} returns 120\r\n......factorial_helper(2, 60){} returns 120\r\n....factorial_helper(3, 20){} returns 120\r\n..factorial_helper(4, 5){} returns 120\r\nfactorial_helper(5, 1){} returns 120\r\nEnding recursive illustration------------------------------\r\n\r\n\r\nNow, on to illustrating the non-linear recursive function fib.\r\n\r\n@Illustrate_Recursive\r\ndef fib(n):\r\n    if    n == 0: return 1\r\n    elif  n == 1: return 1\r\n    else:         return fib(n-1) + fib(n-2)\r\n\r\nprint(fib.illustrate(5))\r\n\r\nStarting recursive illustration------------------------------\r\ncalling fib(5,){}\r\n..calling fib(4,){}\r\n....calling fib(3,){}\r\n......calling fib(2,){}\r\n........calling fib(1,){}\r\n........fib(1,){} returns 1\r\n........calling fib(0,){}\r\n........fib(0,){} returns 1\r\n......fib(2,){} returns 2\r\n......calling fib(1,){}\r\n......fib(1,){} returns 1\r\n....fib(3,){} returns 3\r\n....calling fib(2,){}\r\n......calling fib(1,){}\r\n......fib(1,){} returns 1\r\n......calling fib(0,){}\r\n......fib(0,){} returns 1\r\n....fib(2,){} returns 2\r\n..fib(4,){} returns 5\r\n..calling fib(3,){}\r\n....calling fib(2,){}\r\n......calling fib(1,){}\r\n......fib(1,){} returns 1\r\n......calling fib(0,){}\r\n......fib(0,){} returns 1\r\n....fib(2,){} returns 2\r\n....calling fib(1,){}\r\n....fib(1,){} returns 1\r\n..fib(3,){} returns 3\r\nfib(5,){} returns 8\r\nEnding recursive illustration------------------------------\r\n\r\nThe fib function is NOT a linear recursive function: its body contains two\r\nrecursive calls. So the structure of its recursion becomes much more\r\ncomplicated. This is why the fib function is a good one on which to test both\r\nTrack_Calls and Memoize. Even for fairly small arguments (under 30), it\r\nproduces a tremendous number of calls and can be sped-up tremendously by\r\nmemoizing it.\r\n\r\nThe mns function in the previous lecture (minimum number of stamps) does even\r\nmore than two recursive calls: it does a recursive call one for each\r\nstamp denomination in the list constructor. It too can be vastly sped-up by\r\nusing memoize: see the modules whose names end in \"fast\" in the stamps download;\r\nrun its code to see completely computed results produced very quickly.\r\n\r\nBelow traces a the decorated mns function call: mns.illustrate(10,(1,6,14,57))\r\n\r\nAs you can see, thinking about the execution of this function is unlikely to\r\ngive you insight. There are way too many elephants.\r\n\r\n\r\nStarting recursive illustration------------------------------\r\ncalling mns(10, (1, 6, 14, 57)){}\r\n..calling mns(9, (1, 6, 14, 57)){}\r\n....calling mns(8, (1, 6, 14, 57)){}\r\n......calling mns(7, (1, 6, 14, 57)){}\r\n........calling mns(6, (1, 6, 14, 57)){}\r\n..........calling mns(5, (1, 6, 14, 57)){}\r\n............calling mns(4, (1, 6, 14, 57)){}\r\n..............calling mns(3, (1, 6, 14, 57)){}\r\n................calling mns(2, (1, 6, 14, 57)){}\r\n..................calling mns(1, (1, 6, 14, 57)){}\r\n....................calling mns(0, (1, 6, 14, 57)){}\r\n....................mns(0, (1, 6, 14, 57)){} returns 0\r\n..................mns(1, (1, 6, 14, 57)){} returns 1\r\n................mns(2, (1, 6, 14, 57)){} returns 2\r\n..............mns(3, (1, 6, 14, 57)){} returns 3\r\n............mns(4, (1, 6, 14, 57)){} returns 4\r\n..........mns(5, (1, 6, 14, 57)){} returns 5\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(6, (1, 6, 14, 57)){} returns 1\r\n........calling mns(1, (1, 6, 14, 57)){}\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(1, (1, 6, 14, 57)){} returns 1\r\n......mns(7, (1, 6, 14, 57)){} returns 2\r\n......calling mns(2, (1, 6, 14, 57)){}\r\n........calling mns(1, (1, 6, 14, 57)){}\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(1, (1, 6, 14, 57)){} returns 1\r\n......mns(2, (1, 6, 14, 57)){} returns 2\r\n....mns(8, (1, 6, 14, 57)){} returns 3\r\n....calling mns(3, (1, 6, 14, 57)){}\r\n......calling mns(2, (1, 6, 14, 57)){}\r\n........calling mns(1, (1, 6, 14, 57)){}\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(1, (1, 6, 14, 57)){} returns 1\r\n......mns(2, (1, 6, 14, 57)){} returns 2\r\n....mns(3, (1, 6, 14, 57)){} returns 3\r\n..mns(9, (1, 6, 14, 57)){} returns 4\r\n..calling mns(4, (1, 6, 14, 57)){}\r\n....calling mns(3, (1, 6, 14, 57)){}\r\n......calling mns(2, (1, 6, 14, 57)){}\r\n........calling mns(1, (1, 6, 14, 57)){}\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(1, (1, 6, 14, 57)){} returns 1\r\n......mns(2, (1, 6, 14, 57)){} returns 2\r\n....mns(3, (1, 6, 14, 57)){} returns 3\r\n..mns(4, (1, 6, 14, 57)){} returns 4\r\nmns(10, (1, 6, 14, 57)){} returns 5\r\nEnding recursive illustration------------------------------\r\n\r\nIf we memoize mns and trace it for the same problem, we get the following\r\nsimpler (but still quite complex) trace.\r\n\r\nStarting recursive illustration------------------------------\r\ncalling mns(10, (1, 6, 14, 57)){}\r\n..calling mns(9, (1, 6, 14, 57)){}\r\n....calling mns(8, (1, 6, 14, 57)){}\r\n......calling mns(7, (1, 6, 14, 57)){}\r\n........calling mns(6, (1, 6, 14, 57)){}\r\n..........calling mns(5, (1, 6, 14, 57)){}\r\n............calling mns(4, (1, 6, 14, 57)){}\r\n............mns(4, (1, 6, 14, 57)){} returns 4\r\n..........mns(5, (1, 6, 14, 57)){} returns 5\r\n..........calling mns(0, (1, 6, 14, 57)){}\r\n..........mns(0, (1, 6, 14, 57)){} returns 0\r\n........mns(6, (1, 6, 14, 57)){} returns 1\r\n........calling mns(1, (1, 6, 14, 57)){}\r\n........mns(1, (1, 6, 14, 57)){} returns 1\r\n......mns(7, (1, 6, 14, 57)){} returns 2\r\n......calling mns(2, (1, 6, 14, 57)){}\r\n......mns(2, (1, 6, 14, 57)){} returns 2\r\n....mns(8, (1, 6, 14, 57)){} returns 3\r\n....calling mns(3, (1, 6, 14, 57)){}\r\n....mns(3, (1, 6, 14, 57)){} returns 3\r\n..mns(9, (1, 6, 14, 57)){} returns 4\r\n..calling mns(4, (1, 6, 14, 57)){}\r\n..mns(4, (1, 6, 14, 57)){} returns 4\r\nmns(10, (1, 6, 14, 57)){} returns 5\r\nEnding recursive illustration------------------------------\r\n\r\n\r\n----------\r\n\r\nDelegation of attribute lookup: Classes are better than functions for decorators\r\n\r\nWhen using (multiple) decorators, we need a way to translate attribute accesses\r\non the decorator object into attribute accesses on the decorated object. There\r\nis no simple mechanism to do this with functions, but it easy to do with \r\nclasses: by overloading the __getattr__ method as follows (which should be done\r\nto all three classes above).\r\n\r\n    def __getattr__(self, attr):        # if attr not here, try self._f\r\n        return getattr(self._f ,attr)\r\n\r\nSo, if we use the following two decorators\r\n\r\n    @Track_Calls\r\n    @Illustrate_Recursive\r\n    def fib(....):\r\n        ...\r\n\r\nfib is a Track_Calls object, whose ._f attribute is an Illustrate_Recursive \r\nobject, whose ._f attribute is the actual fib function. If we then wrote\r\nfib.illustrate(5) Python would try to find the illustrate attribute of the\r\nTrack_Calls object; there is no such attribute there, so it fails and then\r\ncalls the __getattr__ of the Track_Calls class, which \"translates\" the failed\r\nattribute access into trying to get the same attribute from the ._f object\r\n(from the decorated class object, an object of the Illustrate_Recursive class,\r\nwhich does define such an attribute, as a method which can be called).\r\n\r\nGenerally this is called delegation: where an \"outer\" object that does not have\r\nsome attribute delegates the attribute reference to an inner object which might\r\ndefine it. Decorators often use exactly this form of delegation, so the\r\ndecorator object can process its attributes and delegate looking up other\r\nattributes to the decorated object.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nNow we will study one last interesting combination of using decorators and the\r\nfunctools.partial function (discussed in the previous lecture). Let's look at\r\nthe following simple decorator, which takes a function and its name as\r\narguments; every time the function is called the decorator prints the\r\nfunction's name and the result it computes.\r\n\r\nclass Trace:\r\n    def __init__(self,f,f_name):\r\n        self._f = f\r\n        self._f_name = f_name\r\n        \r\n    def __call__(self,*args,**kargs):\r\n        result = self._f(*args,**kargs)\r\n        print(self._f_name+'called, returned: '+str(result))\r\n        return result\r\n\r\nNow suppose we want to decorate a function f defined simple as\r\n\r\ndef f(x):\r\n    return 2*x\r\n\r\nIf we write\r\n\r\n@Trace\r\ndef f(x):\r\n    return 2*x\r\n\r\nPython raises a TypeError exception, because the __init__ for Trace requires\r\ntwo arguments, not one. We can avoid the @ form of decorators and write\r\n\r\ndef f(x):\r\n    return 2*x\r\nf = Trace(f,'f')\r\n\r\nto solve the problem, but without using the @Decorator form. Can we do something\r\nto use this form? The problem is that we have have two arguments to __init__ but\r\n@Decorator requires just one, so we can use functools.partial to pre-supply the\r\nsecond argument.\r\n\r\nWe can write\r\n\r\nf_Trace = functools.partial(Trace,f_name='f')\r\n@f_Trace\r\ndef f(x):\r\n    return 2*x\r\n\r\nBut even that is a bit clunky, because we are defining the name f_Trace but\r\nusing it only once (we are not likely to trace other functions named f). We\r\ndon't need this name; instead we can write\r\n\r\n@functools.partial(Trace,f_name='f')\r\ndef f(x):\r\n    return 2*x\r\n\r\ndirectly using the result returned from partial as the decorator. This allows\r\nus to use the standard @Decorator form (with possibly more than one decorator).\r\n\r\nNow calling f(1) would return the result 2 and cause Python to print\r\n\r\n  f called, returned: 2\r\n\r\n------------------------------------------------------------------------------\r\n\r\nProblems:\r\n\r\n1) Define a class that decorates function calls so that it keeps count of how\r\nmany times the function was called with each combination of arguments. Write\r\na report function that returns a list of 2-tuples, each containing the argument\r\nand the number of times the function was called with that argument, sorted from\r\nthe most freqeuently to least frequently called argument).  Hint: this is\r\nsimilar to what Track_Calls and Memoize does.\r\n\r\n2) Define a Memoize class whose constructor also has a max_to_remember argument,\r\nwhich limits the size of the dict to that number of entries (if the argument is\r\nNone, memoize all calls in the ditc). It should remember only the most recent\r\nmax_to_remember arguments. Hint: do this by keeping a list (really representing\r\na queue with oldest and youngest values) and a dict in synch as follows:\r\n  (a) if the args are IN the dict, just return the result.\r\n  (b) if the args are NOT IN the dict, and the list HAS ROOM, add the args to\r\n         the list (at the end: youngest) and to the dict (with their computed\r\n         value)\r\n  (c) if the args are NOT IN the dict, and the list has NO MORE ROOM, pops the\r\n         value out of index 0 (oldest) in the list and delete that as a key from\r\n         the dict, then add the args to the list (at the end) and to the dict\r\n         (with their computed value) \r\nFinally, write a function name info that returns a 4-tuple containing the\r\nnumber of hits (times the function was callled on memoized arguments), misses\r\n(times the function was callled on arguments that weren't memoized), the\r\ncurrent size of the list/dict, and the maximum size.\r\n", "encoding": "ascii"}