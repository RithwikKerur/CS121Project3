{"url": "https://www.ics.uci.edu/~pazzani/Students.html", "content": "<HTML>\n<head>\n<title> Michael J. Pazzani: Students and Affiliated Researchers </title>\n</head>\n\n<body BGCOLOR=\"#FFFFFF\">\n<h1>  Michael J. Pazzani: Research Group </h1>\n\n\n<h2>Graduate Students</h2>\n<ul>\n\n\n</ul>\n\n<h2>Ph.D. Graduates</h2>\n<li><a href=http://www.ics.uci.edu/~sbay>Stephen Bay</a>\n<Li><a href =http://www.ics.uci.edu/~dbillsus>Daniel Billsus</a>\n<li><a href=http://www.ics.uci.edu/~eamonn>Eamonn Keogh</a>\n\n<ul> \n<li><a href =\"http://www.ics.uci.edu/~wogulis\"><b>James Wogulis</b></a> <a\nhref=\"wogulis.ps\">  An Approach to Repairing and Evaluating\nFirst-Order Theories Containing Multiple Concepts and Negation.</a> (1.2M)\nThis dissertation addresses the problem of theory revision in machine\nlearning.  The task requires the learner to minimally revise an\ninitial incorrect theory such that the revised theory explains a given\nset of training data. A learning system, A3, is presented that solves\nthis task.  The main contributions of this dissertation include the\nlearning system A3 that can revise theories containing multiple\nconcepts expressed as function-free first-order Horn clauses, an\napproach to repairing theories containing negation, and the\nintroduction of a distance metric between theories to evaluate the\ndegree of revision performed.  Experimental evidence is presented that\ndemonstrates A3's ability to solve the theory revision task.\nAssumptions commonly made by other approaches to theory revision such\nas whether a theory needs to be generalized or specialized with\nrespect to misclassified examples are shown to be incorrect for\ntheories containing negation.  A3 is able to repair theories\ncontaining negation and demonstrates a simple, general approach to\nidentifying types of errors in a theory using a single mechanism for\nhandling positive and negative examples as well as examples of\nmultiple concepts.  The syntactic distance between two theories is\nproposed as an evaluation metric for theory revision systems. This\ndistance is defined in terms of the minimum number of edit operations\nrequired to transform one theory into another. This allows for a\nprecise measurement of how much a theory has been revised and allows\nfor comparison of different systems' abilities to perform minimal\nrevisions.  This distance metric is also used by A3 in order to bias\nit towards finding minimal revisions that accurately explain the data.\nThe distance metric also leads to insights about the theory revision\ntask. In particular, it is shown that the theory revision task is\nunderconstrained if the additional goal of learning a particular\ncorrect theory is to be met. Without additional constraints, there are\npotentially many accurate revisions that are far apart syntactically.\nIt is shown that providing examples of multiple concepts in the theory\ncan provide some of these constraints.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~schulenb\"><b>David\nSchulenburg</b></a> <a href=\"schulenb.ps\">Learning and Using Context\nin a connectionist Model of Language Understanding</a> (2.5M) Natural\nlanguages are ambiguous.  This is especially true for non-literal\nfigurative constructs such as metaphors and indirect speech acts.\nEven literal text suffers from problems of ambiguity as exemplified by\ntext containing words having multiple meanings.  Understanding such\nambiguous text is a fairly simple task for us humans; it is well\nrecognized that context is often used by people to aid in the\nresolution of these problems of ambiguity.  This dissertation presents\na discussion of a computational model, PIP, which was designed to\naddress the issue of ambiguity in natural language understanding.  By\nincorporating a textual context during the understanding process, PIP\nis able to disambiguate text containing ambiguous constructs such as\nmetaphors and indirect speech acts.  Furthermore, the entire\nunderstanding process is uniform in the sense that the exact same\nmechanisms are used to process both literal and non-literal\n(ambiguous) text; no special processing ``rules'' are necessary to\ndeal with the ambiguity.  The underlying computational model of PIP is\nthe feed-forward artificial neural network.  The incorporation of the\ntextual context is accomplished by a recurrent relation between the\ncontext that is being constructed and the network input for processing\nthe words of the text.  In this fashion is PIP able to use the context\ndirectly as it processes text.  PIP has demonstrated its effectiveness\non sets of text which include ambiguous lexemes, metaphors, and\nindirect speech acts.  By using the context constructed from earlier\nsentences in these texts, PIP is able to derive the intended meaning\nof the ambiguous sentences at the end of the texts.  It is shown that\nwithout use of the context, PIP is unable to produce the intended\nmeaning and in many cases, cannot decide on any meaning to give to the\nambiguous sentences.\n\n<p><li> <a href= \"http://www.isle.org/~ali\"><b> Kamal Ali</b></a> <a href=\"ali.ps\">Learning Probabilistic Relational Concept Descriptions</a> (1.6M) \nThis dissertation presents results in the area of multiple models\n(multiple classifiers), learning probabilistic relational (first order)\nrules from noisy, \"real-world\" data and reducing  the small disjuncts\nproblem - the problem whereby learned rules that cover few training examples\nhave high error rates on test data.\n<p>\nSeveral results are also presented in the arena of multiple models.  The\nmultiple models approach in relevant to the problem of making accurate\nclassifications in ``real-world'' domains since it facilitates evidence\ncombination which is needed to accurately learn on such domains.\nIt is also useful when learning from small training data samples in which \nmany models appear to be equally \"good\" w.r.t. the given evaluation metric.\nSuch models often have quite varying error rates on test data so in such\nsituations, the single model method has problems. Increasing search only\npartly addresses this problem whereas the multiple models approach has the\npotential to be much more useful.\n\nThe most important result of the multiple models research is that the\n*amount* of error reduction afforded by the multiple models approach is\nlinearly correlated with the degree to which the individual models make\nerrors in an uncorrelated manner. This work is the first to model the degree\nof error reduction due to the use of multiple models.  It is also shown that\nit is possible to learn models that make less correlated errors in domains\nin which there are many ties in the search evaluation metric during\nlearning.  The third major result of the research \non multiple models is the realization that models should be learned that\nmake errors in a negatively-correlated manner rather than those that make\nerrors in an uncorrelated (statistically independent) manner.\n\nThe thesis also presents results on learning probabilistic first-order rules\nfrom relational data.  It is shown that learning a class description for\neach class in the data - the one-per-class approach - and attaching\nprobabilistic estimates to the learned rules allows accurate classifications\nto be made on real-world data sets.  The thesis presents the system HYDRA\nwhich implements this approach.  It is shown that the resulting\nclassifications are often more accurate than those made by three existing\nmethods for learning from noisy, relational data.  Furthermore, the learned\nrules are relational and so are more expressive than the attribute-value\nrules learned by most induction systems.\n<p>\nFinally, results are presented on the small-disjuncts problem in which rules\nthat apply to rare subclasses have high error rates\nThe thesis presents the first approach that is simultaneously successful\nat reducing the error rates of small disjucnts while also reducing the\noverall error rate by a statistically significant margin. The previous\napproach which aimed to reduce small disjunct error rates only did so at the\nexpense of increasing the error rates of large disjuncts.\nIt is shown that the one-per-class approach reduces error rates for such\nrare rules while not sacrificing the error rates of the other rules.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~brunk\"><b>Cliff Brunk</b></a>\n<a href=\"brunk.ps\">An Investigation of Knowledge Intensive Approaches to \nConcept Learning and Theory Refinement</a> (1.6M) Concept learning algorithms have been used to solve difficult problems\nin fields ranging from medical diagnosis to astronomy.  In spite of\ntheir successful application, most concept learners are only able to\nutilize knowledge that is expressed in the form of a set of training\nexamples.  Relevant knowledge from other sources can not be utilized\neven when it is available.  Evidence is presented that using knowledge\nfrom other sources leads to more accurate learned models.\nThis dissertation is an investigation of techniques for utilizing\nknowledge in the form of an approximate theory to facilitate concept\nlearning.  The techniques explored are divided into two classes:\ntheory-guided learning algorithms which use the approximate theory to\nconstrain the search for a new concept description, and theory\nrevision algorithms which attempt to repair the approximate theory.\nWhile both techniques are more accurate than approaches that ignore\nthe information contained in the approximate theory, experimental\nevidence indicates that theory revision algorithms produce more\naccurate models.  Furthermore, these models are structurally more\nsimilar to both the approximate theory and the \"ideal\" theory, than\nthose produced by theory-guided learning algorithms.\nThe main contributions of this dissertation include: a new approach to\ntheory-guided learning, a conceptual framework for comparing and\nevaluating theory revision algorithms, enhanced techniques for\nidentifying and repairing errors within a theory, and a lexically\nenhanced approach to evaluating repairs. Lexically enhanced theory\nrevision is a novel technique for utilizing a previously unused form\nof knowledge contained in the approximate theory.  The technique uses\nlexical information contained in the term names of the approximate\ntheory to prefer repairs that are lexically more coherent.  Evidence\nindicates that this further reduces the structural difference between\nthe revised theory and the \"ideal\" theory.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~cmerz\"><b>Christopher Merz</b></a>\n<a href=\"merz.ps\">Classification and Regression by Combining Models</a> \nTwo novel methods for combining predictors are introduced in this \nthesis; one for the task of regression, and the other for the task of \nclassification.  The goal of combining the predictions of a set of \nmodels is to form an improved predictor.  This dissertation \ndemonstrates how a combining scheme can rely on the stability of the \nconsensus opinion and, at the same time, capitalize on the unique \ncontributions of each model.\n<p>\nAn empirical evaluation reveals that the new methods \nconsistently perform as well or better than existing combining schemes \nfor a variety of prediction problems.  The success of these algorithms is \nexplained empirically and analytically by demonstrating how they \nadhere to a set of theoretical and heuristic guidelines.\n<p>\nA byproduct of the empirical investigation is the evidence that \nexisting combining methods fail to satisfy one or more of the \nguidelines defined.  The new combining approaches satisfy these \ncriteria by relying upon Singular Value Decomposition as a tool for \nfiltering out the redundancy and noise in the predictions of the learn \nmodels, and for characterizing the areas of the example space where \neach model is superior.  The SVD-based representation used in the new \ncombining methods aids in avoiding sensitivity to correlated \npredictions without discarding any learned models.  Therefore, the \nunique contributions of each model can still be discovered and \nexploited.  An added advantage of the combining algorithms derived in \nthis dissertation is that they are not limited to models generated by \na single algorithm; they may be applied to model sets generated by a \ndiverse collection of machine learning and statistical \nmodeling methods.\n<p>\nThe three main contributions of this dissertation are:\n<ol><li>The introduction of two new combining methods capable of \nrobustly combining classification and regression estimates, and \napplicable to a broad range of model sets.\n<li> An in-depth analysis revealing how the new methods address the \nspecific problems encountered in combining multiple learned models.\n<li> A detailed account of existing combining methods and an \nassessment of where they fall short in the criteria for combining \napproaches.\n</ol>\n\n</ul>\n\n\n\n<HR>\n\n<P>\n\n<ADDRESS>\n<A HREF=\"http://www.ics.uci.edu/~pazzani\">Michael J. Pazzani</A><br>\n<A HREF=\"http://www.ics.uci.edu/\">Department of Information and Computer Science,</A><br>\n<A HREF=\"http://www.uci.edu/\">University of California, Irvine</A><br>\nIrvine, CA 92697-3425 <br>\n<A href=\"mailto:pazzani@ics.uci.edu\">pazzani@ics.uci.edu </A>\n</ADDRESS>\n</BODY></HTML>\n", "encoding": "ascii"}