{"url": "https://www.ics.uci.edu/~eppstein/280/cluster.html", "content": "<HTML><HEAD>\n<TITLE>Computational Statistics: Clustering</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Clustering</H2>\n\n<I>Clustering</I> refers to several related problems: partitioning a set\nof input points into a fixed number of \"closely related\" subsets;\nfinding a small number of representative center points;\nor matching the point distribution to a family of overlapping continuous\ndistributions.\nThis multiplicity of definitions can actually be helpful, as some major\nclustering algorithms (particularly K-means) work by going back and forth\nbetween the partition and representative-point view of the problem.\n\n<H3>Data Models</H3>\n\nThere are a couple different ways of finding a data model for clustering.\n\n<P>Most naturally, perhaps, the model can be like that for\n<A HREF=\"point.html\">single point estimation</A>:\nthere are some number <I>k</I> of actual data points,\nwhich the data model chooses among.\nThe noise model then adds errors to these points, and we should\nestimate their locations by partitioning them somehow and then applying\na single point estimation method to each subset.\n\n<P>Alternatively, the data model can be that there are <I>k</I>\ndifferent overlayed random distributions of points (such as Gaussians),\nthe noise model merely samples random points from each distribution,\nand the clustering task is to infer the distributions' parameters.\nThe difference with this second model is that it no longer makes\nsense to sharply classify which point is part of which cluster;\nthe best one can do is estimate the likelihood that a given point\nbelongs to a given cluster.\nFor instance, if the data form two crossed Gaussians,\na point in the central crossing region might be equally likely to belong\nto either distribution.\n\n<P>Despite these principles, however, many clustering algorithms use ad\nhoc ideas of choosing a partition that optimizes some functional or\nother, without regard for how it fits into a data model.  Even\nprincipled approaches such as K-means (which can be viewed as based on\nmax likelihood ideas) can't be proven to converge to the correct\nclusters or even to find the global max likelihood solution.\n\n<H3>How many clusters to use?</H3>\n\nThis is a hard question. For now let's just say that the number of\nclusters is given as input.  There has been some work (Smyth?) on\nautomatically inferring the correct number of clusters; some\nK-means-like clustering methods can merge or split clusters and\nhopefully converge on something; alternatively, the problem of\nhierarchical clustering can be viewed as simultaneously solving\nclustering for each possible number of clusters.\n\n<H3><A NAME=\"k-means\">K-means</A></H3>\n\nThe most commonly used clustering method can be seen as a Bayesian\n(max likelihood) approach to the point model of clustering.\nIt is an iterative hill-climbing technique that is not guaranteed\nto find any global maximum likelihood solution, and about which\nlittle can be said theoretically, but works well in practice\nand can be adapted to various different noise models.\n\n<P>The basic idea is to maintain two estimates:\nan estimate of the center locations for each cluster,\nand a separate estimate of the partition of the data points\naccording to which one goes into which cluster.\nThen, one estimate can be used to refine the other.\n\n<P>If we have an estimate of the center locations,\nthen (with reasonable prior assumptions) the max likelihood solution\nis that each data point\nshould belong to the cluster with the nearest center.\nHere \"nearest\" should be measured according to a distance\ndetermined by the noise model, as in <A HREF=\"point.html#dist\">single\npoint estimation</A>, but in practice is always Euclidean distance.\nTherefore, from a set of center locations we can compute a new partition:\nform the <I>Voronoi diagram</I> of the centers,\nthat is, a partition of space into the regions nearest each center,\nand make a cluster from the set of points in each Voronoi cell.\n\n<P>Conversely, if we have a partition of the data points into clusters,\nthe max likelihood estimate of the center locations reduces\nto <I>k</I> independent single point estimation problems;\nif likelihood is related to variance, the max likelihood estimator is just the centroid.\n\n<P>Therefore the K-means algorithms proceeds by a sequence of phases in\nwhich it alternates between moving data points to the cluster of the\nnearest center, and moving center points to the nearest centroid.\nThere are variants depending on whether points are moved one at a time\nor in batches; Kanungo et al.\n[<A HREF=\"bib.html#KMNPSW99\">KMNPSW99</A>]\nhave looked at applying computational geometry data structures\nto speed up each iteration of the algorithm.\n\n<H3><A NAME=\"opt\">Optimal Solutions for Few Centers</A></H3>\n\nWhen the number of problems and the problem's dimension\nare both small, clustering may become amenable to an exact algorithmic\napproach.  The general type of problem considered in this area\nis to partition the points into (usually) two subsets,\nin order to minimize some function of the subsets.\n\n<P>For example, the two-center problem\nseeks to find a partition into two subsets that minimizes the maximum\ncircumradius.  One can come up with models for which this is a\nmax-likelihood solution (e.g. the noise model is that the points may be\nmoved arbitrarily within a bounded but unknown radius, with larger radii\nbeing less likely than smaller ones) but this seems to be working in the\nwrong direction: one should start with a noise model and derive\nalgorithms, not vice versa.\nSince each point may safely be clustered with its nearest circumradius,\nthe optimal partition is formed by the Voronoi diagram of the two points,\nwhich is just a line.\nEarly two-center algorithms found the optimal partition by testing all\nO(n<sup>2</sup>) ways of dividing the data points by a line.\nMore recently, it has been discovered that the problem can be solved\nin time O(n&nbsp;polylog(n)) by more sophisticated geometric searching\nalgorithms\n[<A HREF=\"bib.html#E97\">E97</A>].\n\n<P>The problem of minimizing the sum of distances from each point to its\ncenter is known as the 2-median, since it generalizes the one-dimensional\nmedian problem (which minimizes the sum of distances to the single center).\nAgain, the optimal partition is by a line, so a fast algorithm exists.\nther problems for which such a line partition works, and therefore\nfast exact algorithms are known\ninclude minimizing the sum (or any monotone combination)\nof the circumradii\n[<A HREF=\"bib.html#E92\">E92</A>].\n\n<P>The problem of finding a partition that minimizes the sum of intra-cluster distances (or, equivalently, maximizes the sum of inter-cluster distances)\nis known as the \"Euclidean max cut\".\nThe partition is always formed by a circle [Schulman?]\nand so the optimal solution can be found in polynomial time by testing\nall O(n<sup>3</sup>) ways of dividing the data points by a circle.\n\n<P>Other two-center-like problems for which efficient algorithms are\nknown include finding partitions which minimize the maximum radius,\nenclosing square size, or\nenclosing rectangle area, perimeter, or diameter\n[<A HREF=\"bib.html#HS91\">HS91</A>],\nminimize the sum of the two cluster diameters\n[<A HREF=\"bib.html#H92\">H92</A>],\nand minimizing the maximum width of the two clusters\n[<A HREF=\"bib.html#AS94\">AS94</A>,\n<A HREF=\"bib.html#GKS98\">GKS98</A>].\nThis last problem can be thought of as one of clustering the data along\ntwo lines rather than two points (with the L<sub>infinity</sub> criterion\nused within each cluster).\n\n<H3><A NAME=\"approx\">Approximation and NP-hardness</A></H3>\n\nMost or all of the problems discussed above are NP-hard when the number\nof clusters is variable\n[<A HREF=\"bib.html#BE96\">BE96</A>].\nFurther,\nit is often not possible even to approximate the function being optimized\n(e.g. the maximum cluster radius) arbitrarily well, unless P=NP.\nHowever, there exist simple greedy approximation algorithms\nthat get within a constant factor; e.g.\nrepeatedly choosing a new cluster center at the data point farthest\nfrom previously chosen centers, and then grouping all unchosen points at\ntheir nearest center, will approximate the radius within a factor of two.\nUnfortunately, it is difficult or impossible to say much about\napproximating the actual cluster locations rather than merely\napproximating the criterion used to find the clusters.\n\n<H3>Overlayed Distributions</H3>\n\n<P>I vaguely recall that Smyth has done some work on the problem of\ninferring the parameters of a set of overlayed Gaussians, using some\nkind of Bayesian approach.  References and details?\n\n<P>There are several ways of defining the problem formally.\nFrom the Bayesian point of view, one should attempt to find the set\nD of\ndistributions that maximizes the log-likelihood\n<DIV ALIGN=CENTER>\n    log Prior[D] + sum(log Prob[x<sub>i</sub> | D])\n</DIV>\nwhere the (log Prior) term represents the a priori assumptions on which\ndistributions are more likely, and may be omitted (null hypothesis).\nAlternatively, one could attempt to minimize the discrepancy\n<DIV ALIGN=CENTER>\n    max<sub>S</sub> measure(S) - (# points in S)/(total # points)\n</DIV>\nwhere S is maximized over some simple class of functions e.g. halfspaces.\n\n<P>One important application of the overlayed distribution model is in\nthe belief-propagation approach to coding theory: one assumes that\nmessage bits that are sent out as signals with two discrete values (0\nand 1) come back as values drawn from two random distributions.  One\nneeds to estimate these distributions in order to derive beliefs\n(Bayesian probabilities) about which measurements are 0's and 1's; the\nbeliefs are then modified by a belief network to perform error\ncorrection.  This problem fits very well into the discrete algorithmic\napproach, since the number of data points (message length) is high, and\n(at least in the simplest cases) one only needs to find two clusters in\none-dimensional data.  However, I don't know of any theoretical-CS work\non this sort of problem.\n\n<P>The work mentioned above on finding a clustering that minimizes the\nmaximum cluster width can be also be viewed as a problem in which the\nanswer consists of overlayed distributions (two infinite strips that\ntogether cover the point set).\n\n<H3><A NAME=\"eps\">Geometric Sampling</A></H3>\n\nOne way to view sampling is as a way of finding a representative subset\nof the data. It may be ok (or even desired) that a large cluster in the\nactual data be represented by several centers in the cluster output,\nbut it should not be possible for a large cluster of data points to be\nmissing a representative.\nThis can be formalized with the terminology of geometric sampling,\nan area originally developed by statisticians but one that has been\nused extensively within computational geometry as a technique for\nderiving deterministic algorithms from randomized ones, since the\ncluster centers derived from the theory behave in many ways like random\nsamples.\n\n<P>Define a <em>range space</em> to consist of a \"universe\" U of objects\n(often but not always points in <b>R</b><sup>d</sup>) together with a\nfamily F of subsets of U (for instance, halfspaces of <b>R</b><sup>d</sup>).\nWe'll call a set R in F a \"range\".  \nFor any finite set S (not necessarily in F),\nand any positive value epsilon,\ndefine an\n<em>epsilon-net</em> for S to be a subset N of S such that,\nfor any range R,\n<DIV ALIGN=CENTER>\nif |R&nbsp;intersect&nbsp;S|&nbsp;&gt;&nbsp;epsilon|S|,\nthen R&nbsp;intersect&nbsp;N must be nonempty.\n</DIV>\nIn other words, N touches all large ranges in S.\nSimilarly, define an\n<em>epsilon-approximation</em> to be a subset A of S such that,\nfor any range R,\n<DIV ALIGN=CENTER CELLPADDING=0 CELLSPACING=0 BORDER=0>\n-epsilon&nbsp;&lt;&nbsp;|S&nbsp;intersect&nbsp;R|/|S|&nbsp;-&nbsp;|A&nbsp;intersect&nbsp;R|/|A|&nbsp;&lt;&nbsp;epsilon.\n</DIV>\nIn other words, R covers approximately the same fraction of A as it does\nof S.\nAny set S is always an epsilon-approximation for itself, and any\nepsilon-approximation is always an epsilon-net.\nBut the important result about these range spaces is that,\nfor many important geometric examples, one can find much smaller\nepsilon-nets and epsilon-approximations.  In fact, the size\nof a net or approximation can be made to be a constant independent of S.\n\n<P>Specifically, let F<sub>S</sub> denote the family of sets formed\nby intersecting ranges with S,\nand define the \"scaffold dimension\" of a range space\nto be the maximum of\n<DIV ALIGN=CENTER>\nlog&nbsp;|F<sub>S</sub>|&nbsp;/&nbsp;log&nbsp;|S|\n</DIV>\nmaximized over all finite sets S in U.\nFor instance, consider sets of points in the plane, with closed halfspace ranges.\nFor any halfspace containing more than one point of S,\npass through some pair of the points in the range.\nSo, if S contains n points\nthe size of F<sub>S</sub> can be at most\nn(n-1) + n + 1; with a little more care one can see that there are fewer\nthan n<sup>2</sup> such sets.  Therefore, the scaffold dimension of this\nrange space is st most two.  In general, it is safe to think of the scaffold\ndimension as being roughly the number of points needed to determine a range,\nso the scaffold dimension of circles would be three, and the scaffold\ndimension of ellipses would be five.\n(There is also an alternate definition of the dimension of a range space\ncalled the \"Vapnik-Chervonenkis dimension\", but this is bounded iff the\nscaffold dimension is bounded, and most proofs involving VC dimension \ncan be expressed more directly in terms of the scaffold dimension.)\n\n<P><B>Theorem:</B> For any set S in a range space with scaffold\ndimension d, there exists an epsilon-net with size\nO(d/epsilon&nbsp;log(d/epsilon)) and an epsilon-approximation\nwith size O(d/epsilon<sup>2</sup>&nbsp;log(d/epsilon)).\n\n<P>The proof begins by observing that a\nrandom sample of size O(1/epsilon&nbsp;log&nbsp;|F<sub>S</sub>|)\nor O(1/epsilon<sup>2</sup>&nbsp;log&nbsp;|F<sub>S</sub>|) is with\nconstant probability\nan epsilon-net or epsilon-approximation respectively.\nAlso, an epsilon/2-approximation of an epsilon/2-approximation\nis itself an epsilon-approximation.\nSo, by induction, one can assume that there is a small epsilon/2-approximation,\ntake a random sample of it, and get (with constant probability)\nan even smaller epsilon-approximation.\nThen once the result for epsilon-approximations is proven,\none can take a random sample of a small epsilon-approximation to get a\nsmall epsilon-net.\nAlthough the above construction is randomized,\nthere exist deterministic algorithms for constructing epsilon-nets\nand epsilon-approximations, with time bounds of the form O(n)\nwhenever d and 1/epsilon are bounded by O(1)\n[citation to be filled in later].\n\n<P>It would be reasonable to use an epsilon-net for the range space of\ndisks (or ellipses) as a form of clustering: the definitions above\nwould imply that every disk-shaped or elliptical cluster\nof more than epsilon*n points would have a representative.\n\n<P>As I mentioned above, epsilon-nets and epsilon-approximations\nhave been used extensively in geometric algorithms.  As an example\nof a statistical application, consider the regression depth problem:\nthe appropriate range space is the family of double wedges bounded by one\nvertical and one non-vertical hyperplane.  The regression depth\nof a given hyperplane (measured as a fraction of the size of the overall data\nset) is within epsilon of the regression depth of the hyperplane\nrelative to an epsilon-approximation of the data.  Therefore,\nafter linear-time preprocessing (computing an epsilon-approximation),\nwe can compute good approximations to the regression depth in constant time.\nFurther, the deepest plane with respect to the approximation\nmust be within epsilon of the deepest plane for the overall data set,\nso we can approximate the depth of the deepest plane to within any (1-epsilon)\nfactor in linear total time\n[<A HREF=\"bib.html#SW98\">SW98</A>].\n\n<H3><A NAME=\"sep\">Other Types of Clustering Problems</A></H3>\n\nThere has been recent interest in problems of \"robust separation\":\nafter you have partitioned the points into two clusters somehow\n(or maybe a binary classification is already given as input),\nhow quickly\ncan you find a boundary plane that separates one side from the other\nin a way that maximizes the distance of any point from the boundary?\nThis is equivalent to finding the closest pair of points in the convex\nhulls of the two sides (note this is not the same as the closest pair of\ndata points) since the separating plane is the perpendicular bisector of\nthe segment between these two points.\nThe problem can be solved in linear time by LP-type methods\n[<A HREF=\"bib.html#G95\">G95</A>]\nsimilar to those used for the <A HREF=\"point.html#Lp\">smallest\nenclosing ball</A>.\nI am not aware of any implementation of these ideas, but perhaps\nG&auml;rtner's miniball code [<A HREF=\"bib.html#G99\">G99</A>]\ncould be adapted to this problem.\n\n<P>Another clustering-like problem is to find a single cluster among a\nset of points, ignoring other less tightly clustered points.\nEssentially, we discussed this already\nunder <A HREF=\"point.html#LMS\">robust single-point estimation</A>,\nbut the emphasis is different (here we might allow many more\nthan n/2 outliers) so algorithmically one can optimize for the case\nwhen the number of points in the cluster is small.\n\n<H2><A HREF=\"tree.html\">NEXT: Hierarchical Clustering</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"cluster.html\" --></SMALL>\n</BODY></HTML>\n", "encoding": "ascii"}