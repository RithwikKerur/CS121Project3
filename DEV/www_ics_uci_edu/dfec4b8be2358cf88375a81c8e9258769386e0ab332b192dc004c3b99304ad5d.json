{"url": "https://www.ics.uci.edu/~dan/pubs/DataCompression.html", "content": "<HTML>\n<HEAD>\n<TITLE> Data Compression </TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<H3> Debra A. Lelewer and Daniel S. Hirschberg </H3>\n\n<H2> Table of Contents </H2>\n\n Abstract <br>\n INTRODUCTION <br>\n1.  <a href=\"DC-Sec1.html#Sec_1\">    FUNDAMENTAL CONCEPTS</a><br>\n1.1 <a href=\"DC-Sec1.html#Sec_1.1\">  Definitions</a><br>\n1.2 <a href=\"DC-Sec1.html#Sec_1.2\">  Classification of Methods</a><br>\n1.3 <a href=\"DC-Sec1.html#Sec_1.3\">  A Data Compression Model</a><br>\n1.4 <a href=\"DC-Sec1.html#Sec_1.4\">  Motivation</a><br>\n2.  <a href=\"DC-Sec2.html#Sec_2\">    SEMANTIC DEPENDENT METHODS</a><br>\n3.  <a href=\"DC-Sec3.html#Sec_3\">    STATIC DEFINED-WORD SCHEMES</a><br>\n3.1 <a href=\"DC-Sec3.html#Sec_3.1\">  Shannon-Fano Coding</a><br>\n3.2 <a href=\"DC-Sec3.html#Sec_3.2\">  Static Huffman Coding</a><br>\n3.3 <a href=\"DC-Sec3.html#Sec_3.3\">  Universal Codes and Representations of the Integers</a><br>\n3.4 <a href=\"DC-Sec3.html#Sec_3.4\">  Arithmetic Coding</a><br>\n4.  <a href=\"DC-Sec4.html#Sec_4\">    ADAPTIVE HUFFMAN CODING</a><br>\n4.1 <a href=\"DC-Sec4.html#Sec_4.1\">  Algorithm FGK</a><br>\n4.2 <a href=\"DC-Sec4.html#Sec_4.2\">  Algorithm V</a><br>\n5.  <a href=\"DC-Sec5.html#Sec_5\">    OTHER ADAPTIVE METHODS</a><br>\n5.1 <a href=\"DC-Sec5.html#Sec_5.1\">  Lempel-Ziv Codes</a><br>\n5.2 <a href=\"DC-Sec5.html#Sec_5.2\">  Algorithm BSTW</a><br>\n6.  <a href=\"DC-Sec678.html#Sec_6\">  EMPIRICAL RESULTS</a><br>\n7.  <a href=\"DC-Sec678.html#Sec_7\">  SUSCEPTIBILITY TO ERROR</a><br>\n7.1 <a href=\"DC-Sec678.html#Sec_7.1\">Static Codes</a><br>\n7.2 <a href=\"DC-Sec678.html#Sec_7.2\">Adaptive Codes</a><br>\n8.  <a href=\"DC-Sec678.html#Sec_8\">  NEW DIRECTIONS</a><br>\n9.  <a href=\"DC-Sec678.html#Sec_9\">  SUMMARY</a><br>\n    <a href=\"DC-references.html\">    REFERENCES</a><br>\n<P>\n\n<H2> Abstract </H2>\n\nThis paper surveys a variety of data compression methods\nspanning almost forty years of research, from\nthe work of Shannon, Fano and Huffman in the late 40's to a \ntechnique developed in 1986.  \n\tThe aim of data compression is to reduce redundancy \nin stored or communicated data, thus increasing effective\ndata density.  Data compression has important application in \nthe areas of file storage and distributed systems.\n<P>\n\tConcepts from information theory, as they relate to the goals \nand evaluation of data compression methods, are discussed briefly. \nA framework \nfor evaluation and comparison of methods is constructed and applied \nto the algorithms presented.  Comparisons of both theoretical and \nempirical natures are reported and possibilities for future research \nare suggested.\n\n<H2> INTRODUCTION </H2>\n\n\tData compression is often referred to as coding, where\ncoding is a very general term encompassing any special representation\nof data which satisfies a given need.  Information\ntheory is defined to be the study of efficient coding and its \nconsequences, in the form of speed of transmission and probability\nof error [Ingels 1971].  Data compression may be viewed as a\nbranch of information theory in which the primary objective is\nto minimize the amount of data\nto be transmitted.  The purpose of this paper is to present and\nanalyze a variety of data compression algorithms.\n<P>\nA simple characterization of data compression is that it involves\ntransforming a string of characters in some representation (such\nas ASCII) into a new string (of bits, for example) which contains the same information \nbut whose length is as small as possible.  Data compression has\nimportant application in the areas of data transmission and data\nstorage.   Many data processing applications require storage of\nlarge volumes of data, and the number of such applications is\nconstantly increasing as the use of computers extends to new\ndisciplines.  At the same time, the proliferation of computer \ncommunication networks is resulting in massive transfer of \ndata over communication links.\nCompressing data to be stored or transmitted reduces storage\nand/or communication costs.  When the amount of data to be transmitted\nis reduced, the effect is that of increasing the capacity of\nthe communication channel.  Similarly, compressing a file to half of\nits original size is equivalent to doubling the capacity of the\nstorage medium.  It may then become feasible to store the data\nat a higher, thus faster, level of the storage hierarchy and reduce\nthe load on the input/output channels of the computer system. \n<P>\nMany of the methods to be discussed in this paper are implemented\nin production systems.  The UNIX utilities <EM>compact</EM> and \n<EM>compress</EM> are based on methods to be discussed in\n<a href=\"DC-Sec4.html#Sec_4\">Sections 4</a> and\n<a href=\"DC-Sec5.html#Sec_5\">5</a> respectively [UNIX 1984].  Popular file archival \nsystems such as <EM>ARC</EM> and <EM>PKARC</EM> employ techniques presented\nin <a href=\"DC-Sec3.html#Sec_3\">Sections 3</a>\nand <a href=\"DC-Sec5.html#Sec_5\">5</a> [ARC 1986; PKARC 1987].  The savings achieved\nby data compression can be dramatic; reduction as high as 80%\nis not uncommon [Reghbati 1981].  Typical values of compression provided\nby <EM>compact</EM> are:  text (38%), Pascal source (43%),\nC source (36%) and binary (19%).  <EM>Compress</EM> generally\nachieves better compression (50-60% for text such as\nsource code and English), and takes less time to compute [UNIX 1984].\nArithmetic coding (<a href=\"DC-Sec3.html#Sec_3.4\">Section 3.4</a>)\nhas been reported to reduce\na file to anywhere from 12.1 to 73.5% of its original size\n[Witten et al. 1987].  Cormack reports that data compression\nprograms based on Huffman coding\n(<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>) reduced the size\nof a large student-record database by 42.1% when only some of\nthe information was compressed.  As a consequence\nof this size reduction, the number of disk operations required\nto load the database was reduced by 32.7% [Cormack 1985].  \nData compression\nroutines developed with specific applications in mind have\nachieved compression factors as high as 98% [Severance 1983].\n<P>\nWhile coding for purposes of data security\n(cryptography) and codes which guarantee a certain level of \ndata integrity (error detection/correction) are topics worthy of \nattention, these do not fall\nunder the umbrella of data compression.  With the exception of a \nbrief discussion of the susceptibility to error of the methods \nsurveyed (<a href=\"DC-Sec678.html#Sec_7\">Section 7</a>), a discrete noiseless channel is assumed.  \nThat is, we assume a system in which a sequence of symbols chosen from\na finite alphabet can be transmitted from one point to another without the \npossibility of error.  Of course, the coding schemes described here \nmay be combined with data security or error correcting codes.  \n<P>\n\tMuch of the available literature on data compression\napproaches the topic from the point of view of data transmission.\nAs noted earlier, data compression is of value in\ndata storage as well.  Although this discussion \nwill be framed in the terminology of data transmission, compression\nand decompression of data files for storage is essentially the same task\nas sending and receiving compressed data over a communication channel.\n\tThe focus of this paper is on algorithms for data\ncompression; it does not deal with hardware aspects of\ndata transmission.  The reader is referred to Cappellini\nfor a discussion of techniques with natural \nhardware implementation [Cappellini 1985].  \n<P>\n\tBackground concepts in the form of terminology and a\nmodel for the study of data compression are provided\nin <a href=\"DC-Sec1.html#Sec_1\">Section 1</a>.  Applications of data compression\nare also discussed in <a href=\"DC-Sec1.html#Sec_1\">Section 1</a>,\nto provide motivation for the material which follows.\n<P>\nWhile the primary focus of this\nsurvey is data compression methods of general utility,\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a>\nincludes examples from the literature in which ingenuity applied\nto domain-specific problems has yielded interesting coding\ntechniques.\nThese techniques are referred to as <EM>semantic dependent</EM> since they\nare designed to exploit the context and semantics of the data to \nachieve redundancy reduction.  Semantic dependent techniques include \nthe use of quadtrees, run length encoding, or difference mapping \nfor storage and transmission of image data [Gonzalez and Wintz 1977; \nSamet 1984].  \n<P>\n\tGeneral-purpose techniques, which assume no knowledge of the\ninformation content of the data, are described in\n<a href=\"DC-Sec3.html#Sec_3\">\nSections 3-5</a>.  These descriptions are sufficiently detailed to\nprovide an understanding of the techniques.  The reader will need\nto consult the references for implementation details.  In most \ncases, only worst-case analyses of the\nmethods are feasible.  To provide a more realistic picture\nof their effectiveness, empirical data is presented in\n<a href=\"DC-Sec678.html#Sec_6\">Section 6</a>.  The susceptibility to error of the algorithms surveyed\nis discussed in\n<a href=\"DC-Sec678.html#Sec_7\">Section 7</a> and possible directions for future\nresearch are considered in\n<a href=\"DC-Sec678.html#Sec_8\">Section 8</a>.\n\n<P>\n<A HREF=\"DC-Sec1.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "encoding": "ascii"}