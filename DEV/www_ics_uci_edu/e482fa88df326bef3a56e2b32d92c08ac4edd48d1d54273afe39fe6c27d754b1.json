{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/aa2.txt", "content": "       Computing Complexity Classes of Code, Applications, Big-Omega/Theta\r\n\r\n\r\nIn this lecture we will learn how to compute the complexity class of all the\r\ncode in a function (and therefore the complexity class of the function)\r\nstatically (without RUNNING the function) by using simple rules for composition.\r\nWe will see how apply these rules in various functions and combinations of\r\nfunctions in classes using the lens of analysis of algorithms, and compare\r\nfundamentally differerent algorithms for solving the same problem. Next, we\r\nwill compare some time and space metrics for array vs. linked list\r\nimplementations of simple data types (e.g., queue).\r\n\r\nFinally, we will discuss Big-Omega/Big-Theta notations and discuss lower bounds\r\non the complexity classes of problems (not functions). We will revisit this\r\ntopic later in the quarter to prove a non-trivial (and interesting) lower bound\r\non sorting.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nComposing Complexity Classes: Sequential and Nested Statements\r\n\r\nIn this section we will learn how to combine complexity class information about\r\nsimple operations into complexity information about complex operations\r\n(composed from simple operations). The goal is to be able to analyze all the\r\nstatements in a functon/method and determine the complexity class of executing\r\nthat function/method.\r\n\r\n----------\r\n\r\nLaw of Addition for big-O notation\r\n\r\n O(f(n)) + O(g(n))  is  O( f(n) + g(n) )  is  O( max(f(n),g(n)) )\r\n\r\nThat is, we when adding complexity classes, we bring the two complexity classes\r\ninside the O(...). Ultimately, O( f(n) + g(n) ) results in the BIGGER of the two\r\ncomplexity class (because we drop lower added terms). So,\r\n\r\nO(N) + O(Log N)  =  O(N + Log N)  =  O(max(N,Log N)) = O(N)\r\n\r\nbecause N is the faster growing function; also\r\n\r\nO(N) + O(N)  =  O(2N)  =  O(N)\r\n\r\nbecause we also drop constants using big-O notation. Yes, it could take twice\r\nas long, but the growth rate is still linear.\r\n\r\nThis rule helps us understand how to compute the complexity of doing some \r\nSEQUENCE of operations: executing some statement that is O(f(n)) followed by\r\nexecuting some statement that is O(g(n)). Executing all the statements\r\nSEQUENTAILLY, the first followed by the second, is O(f(n)) + O(g(n)) which is\r\nO( f(n) + g(n) ) which is O(max(f(n),g(n))).\r\n\r\nFor example, if some function call f(...) is O(N) and another function call\r\ng(...) is O(N Log N), then doing the sequence\r\n\r\n   f(...);\r\n   g(...);\r\n\r\nis O(N) + O(N Log N) = O(N + N Log N) = O(N Log N).  Of course,\r\ndoing the sequence\r\n\r\n  f(...);\r\n  f(...);\r\n\r\nis O(N) + O(N) which is O(N + N) which is O(2N) which is O(N).\r\n\r\nIn an if statement, we execute a sequence: first, always the test code; second,\r\nthe code in the block that the test's value indicates to execute. Note that for\r\nan if statment like\r\n\r\n  if (test)    \t assume complexity of test is O(T)\r\n     block 1     assume complexity of block 1 is O(B1)\r\n  else\r\n     block 2     assume complexity of block 2 is O(B2)\r\n\r\nThe complexity class for the if is O(T) + max(O(B1),O(B2)). The test is always\r\nevaluated first; next, one of the two blocks is always executed. In the worst\r\ncase, the if will execute the block in the largest complexity class. As an\r\nexample, given\r\n\r\n  if (test)    \t complexity is O(N)\r\n     block 1     complexity is O(N^2)\r\n  else\r\n     block 2     complexity is O(N)\r\n\r\nThe complexity class for the if is O(N) + max (O(N^2),O(N))) = O(N) + O(N^2) =\r\nO(N + N^2) = O(N^2). This is because in the worst case, the test will be true\r\nand therefore block 1 will execute. Note that the test is always executed so\r\n\r\n  if (test)    \t complexity is O(N)\r\n     block 1     complexity is O(N)\r\n  else\r\n     block 2     complexity is O(1)\r\n\r\nhas a complexity class of O(N) + max(O(N),O(1)) = O(N) + O(N)= O(2N) = O(N).\r\n\r\n----------\r\n\r\nLaw of Multiplcation for big-O notation\r\n\r\n O(f(n)) * O(g(n)) is O( f(n) * g(n) )\r\n\r\nIf we repeat O(N) times an O(f(N)) process, the resulting complexity is\r\nO(N)*O(f(N)) = O( N*f(N) ). An example of this is, if some function call f(...)\r\nis O(N^2), then executing that call N times (in the following loop)\r\n\r\n  for (int i=0; i<N; ++i)\r\n    f(...);\r\n\r\nthe complexity is O(N)*O(N^2) = O(N*N^2) = O(N^3)\r\n\r\nThis rule helps us understand how to compute the complexity of doing some \r\nstatement INSIDE THE BODY controlled by a looping statement that is repeating\r\nit. We multiply the complexity class of the number of repetitions by the\r\ncomplexity class of the statement being repeated.\r\n\r\nCompound statements can be analyzed by composing the complexity classes of\r\ntheir constituent statements. For sequential statements the complexity classes\r\nare added; for statements repeated inside a loop the complexity class of the\r\nloop and statements are multiplied.\r\n\r\nLet's use the tools discussed above to analyze (determine their complexity\r\nclasses) three different functions that each have the same arguments and each\r\ncompute the same result: whether or not an array contains only unique values\r\n(no duplicates). We will assume in all three examples that the length of the\r\narray is N.\r\n\r\nI will omit computing the complexity classes of parameter transmission: copying\r\narguments to parameters. In all cases, parameter transmission is O(1): a pointer\r\nand an integer are copied to the parameters a and N. Even though the array a\r\nhas size N, only a single pointer value (the address of the first element in\r\nthe array) is copied. If we were to pass an ArrayQueue with N values, the\r\ncomplexity of copying it would be O(N), although if we passed the parameter as\r\n\"const &\", the complexity would be O(1): passing a reference is equivalent to\r\npassing an address (which is an integer, which is O(1)).\r\n\r\n1) Algorithm 1: An array is unique if each value in the array does not occur in\r\nany later indexes (can you explain why we don't have to check earlier indexes?).\r\n\r\nbool is_unique (int [] a, int N) {\r\n    for (int i=0; i<N; ++i)\t\tO(N)\r\n      for (int j=i+1; j<N; ++j)\t\tO(N) - min 0;max N-1;average N/2: O(N)\r\n        if (a[j] == a[i])\t\tO(1) - two array accesses, ==\r\n          return false;\t\t\tO(1) - never executed in worst case\r\n                                               so won't appear in formula below\r\n    return true;\t\t\tO(1)\r\n}\r\n\r\nThe formula for the complexity class for executing the entire function is\r\nO(N)*O(N)*O(1) + O(1) = O(N^2+1) = O(N^2). So we know from the previous\r\nlecture that if we double the size of the array, this function takes 4 times as\r\nlong to execute in the worst case. Note that in the worst case, the\r\n\"return false\" is not executed (because that would stop the loop prematurely),\r\nso its O(1) term does not appear in the formula for the complexity class of the\r\nfunction.\r\n\r\n2) Algorithm 2: An array is unique if when we sort its values, no adjacent\r\nvalues are equal: if there were any duplicate values, sorting the array would\r\nput these duplicate values next to each other. Here we copy the array so as to\r\nnot change the order of the parameter's array: we can prove that copying the\r\narray does increase the running time of the function, but it does not affect\r\nits complexity class.\r\n\r\nbool is_unique2 (int a[], int N) {\r\n  int copy[N];\t\t\t       O(1) - allocation of any-sized array\r\n  for (int i=0; i<N; ++i)\t       O(N)\r\n    copy[i] = a[i];  \t\t       O(1) - two array accesses and =\r\n\r\n  qsort(copy,N,sizeof(int),\t       O(N Log N) - for C++'s qsort\r\n      [](const void* a, const void* b){return (*(int*)a - *(int*)b);});\r\n\r\n  for (int i=0; i<N-1; ++i)   \t       O(N) - really N-1, but that is O(N)\r\n      if (copy[i] == copy[i+1])\t       O(1) - two array accesses and ==\r\n          return false;\t\t       O(1) - never executed in worst case\r\n                                              so won't appear in formula below\r\n  return true;\t \t\t       O(1)\r\n}\r\n\r\nThe formula for the complexity class for executing the entire function is\r\nO(1) + O(N)*O(1) + O(N Log N) + O(N)*O(1) + O(1) =\r\nO(1 + N + N Log N + N + 1)                       =\r\nO(N Log N + 2N + 2)                              = O(N Log N)\r\n\r\nSo the complexity class for this algorithm/function is lower than the first\r\nalgorithm/function. The complexity class for sorting the array is dominant: it\r\ndoes most of the work (based on its highest complexity class). If we double the\r\nsize of a, this function takes a bit more than twice the amount of time. In\r\nN Log N, N doubles and Log N gets a tiny bit bigger (i.e., Log 2N = 1 + Log N;\r\ne.g., Log 2000 = 1 + Log 1000 = 11, so compared to 1000 Log 1000, 2000 Log 2000\r\ngot 2.2 times bigger, or 10% bigger than just doubling).\r\n\r\nLooked at another way if T(N) = c*(N Log N), then T(2N) = c*(2N Log 2N). Since\r\nLog 2N is 1 + Log N, T(2N) = c*2N Log N + c*2N. So\r\n\r\n\r\n  T(2N)     c*2N Log N + c*2N     c*2N Log N       c*2N              2\r\n-------- = ------------------- = ------------ + ----------- = 2 + -------\r\n  T(N)         c N Log N           c N Log N     c N Log N         Log N\r\n\r\nWhen N = 1,000, the sum is 2+2/10 = 2.2; when N = 1,000,000, the sum is 2+2/20\r\n= 2.1; when N = 1,000,000,000, the sum is 2+2/30 = 2.0666... So the T(2N)/T(N)\r\nratio here depends on N, but for bigger and bigger N the ratio converges to 2.\r\n\r\nSo, doubling N doubles the time + a bit more (a bit that gets smaller as\r\nN->infinity, but gets smaller slowly because Log N grows so slowly).\r\n\r\n3) Algorithm 3: An array is unique if when we insert its values into a set, its\r\nsize is the same as the size of the array: if duplicate values were inserted\r\ninto the set, its size would be smaller than the size of the array by exactly\r\nthe number of duplicates in the array. We will examine various implementations\r\nof Sets during the quarter, and learn that by using hashing, most Set\r\noperations (including insertion) are O(1). Therefore\r\n\r\nbool is_unique3 (int a[], int N) {\r\n  ics::HashSet<int> s;\t\t       O(1) //Ignored here: the hash function\r\n\r\n  for (int i=0; i<N; ++i)\t       O(N)\r\n    s.insert(a[i]);  \t\t       O(1) - for sets implemented by Hashing\r\n\r\n  return s.size() == N;\t       \t       O(1)\r\n}\r\n\r\nThe formula for the complexity class for executing the entire function is\r\nO(1) + O(N)*O(1) + O(1) = O(1 + N + 1) = O(N + 2) = O(N). So the complexity\r\nclass for this algortihm/function is even lower than the first and second\r\nalgorithms/functions. If we double the size of the array, this function takes\r\njust twice the amount of time.\r\n\r\nWe can speed-up this function (but only for arrays that actually have\r\nduplicates) by using the return value of the insert function to immediately\r\ncompute false (if insert ever returns 0, meaning the inserted value was already\r\nin the Set). In arrays with duplicates, the insertion loop in the code below\r\ndoesn't run to completion. But the function below will run more slowly on\r\narrays with no duplicates, because it does a bit of extra work inside each for\r\nloop (checking == 0), and for non-duplicate arrays the loop runs until the end.\r\n\r\nbool is_unique3a (int a[], int N) {\r\n  ics::HashSet<int> s;\r\n\r\n  for (int i=0; i<N; ++i) {\r\n    if (s.insert(a[i]) == 0)\r\n      return false;\r\n\r\n  return true;\r\n}\r\n\r\nSo, in the worst case, this function has the same complexity class as\r\nis_unique3; it is likely a bit faster for non-unique arrays (return false\r\nsooner) but a bit slower for unique arrays (because of the extra \"if\" testing\r\nduring each iteration). So adding the extra testing can decrease orincrease\r\nthe running time, depending on the actual data.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nUsing a Priority Queue (implementable 3 ways):\r\n\r\nWe will now look at the solution of a few problems (combining operations on a\r\npriority queue: pq) and how the complexity class of the result is affected by\r\nthree different classes/implementations of priority queues.\r\n\r\nIn a priority queue, we can enqueue/dequeue values to the data structure. A\r\ncorrectly working priority queue always dequeues the maximum value remaining in\r\nthe priority queue. \r\n\r\nFor the problems we examine below, all we need to know is the complexity class\r\nof the \"enqueue\" and \"dequeue\" operations.\r\n\r\n                    enqueue        dequeue\r\n\t         +-------------+-------------+\r\nImplementation 1 |    O(1)     |    O(N)     |  unordered array/linked list\r\n\t         +-------------+-------------+ \r\nImplementation 2 |    O(N)     |    O(1)     |  ordered array/linked list\r\n\t         +-------------+-------------+\r\nImplementation 3 |  O(Log N)   |  O(Log N)   |  binary heap (to be studied)\r\n\t         +-------------+-------------+\r\n\r\nImplementation 1 works by keeping an unordered array or linked list of values:\r\nit enqueues a new value into the pq by appending it at the rear of an array (or\r\nthe front of a linked list): O(1); it dequeues the highest priority value by\r\nscanning through the array (or linked list) to find the highest, which is O(N)\r\n(because we must look at every value), and then removing that value (O(N) in\r\nthe worst case for arrays, because we must copy every value in the data\r\nstructure after it, and it might be first; O(1) for linked lists).\r\n\r\nImplementation 2 works by keeping an ordered (by priority) array or linked list\r\nof values: it enqueues the new value into the pq by scanning the array (or\r\nlinked list) for the right spot to put it (O(N) in the worst case), and putting\r\nit there, which is O(N) for arrays because other values must be copied and O(1)\r\nfor linked lists. Arrays store their highest priority at the rear, linked lists\r\nat the front; it dequeues the highest priority value from the rear of the array\r\n(or front of the linked list),which is O(1).\r\n\r\nImplementation 3, which is discussed later in ICS-46, uses a binary heap tree\r\n(not a binary search tree) to implement both operations with \"middle\" complexity\r\nO(Log N): which is greater than O(1) but less than O(N). Which complexity class\r\ndo you think O(Log N) is closer to?\r\n\r\nProblem 1: Suppose that we wanted to use the priority queue to sort N values:\r\nwe enqueue N values and then dequeue N values (first the highest, next the\r\nsecond highest, ...). Here is the complexity of these combined operations for\r\neach implementation.\r\n\r\nImplementation 1: N*O(1)     + N*O(N)     = O(N   + N^2)     = O(N^2)\r\nImplementation 2: N*O(N)     + N*O(1)     = O(N^2 + N)       = O(N^2)\r\nImplementation 3: N*O(Log N) + N*O(Log N) = O(NLogN + NLogN) = O(NLogN)\r\n\r\nHere, Implementation 3 has the lowest complexity class for all the combined\r\noperations. Implementations 1 and 2 each do one operation quickly but the other\r\nslowly; since the slowest operation determines the complexity class, both are\r\nequally slow. The complexity class O(Log N) is between O(1) and O(N);\r\nsurprisingly, it is actually \"closer\" to O(1) than O(N), even though it does\r\ngrow -because it grows so slowly; yes, O(1) doesn't grow at all, but in the\r\nUniverse of 10^90 particles of matter (so N is always < 10^90)  Log 10^90 =\r\nLog (10^3)^30 = 300: a constant, even if a large one.\r\n\r\nProblem 2: Suppose we wanted to use the priority queue to find the 10 biggest\r\n(of N) values: we would enqueue N values and then dequeue 10 values. Here is\r\nthe complexity of these combined operations for each implementation..\r\n\r\nImplementation 1: N*O(1)     + 10*O(N)     = O(N   + 10N)     = O(11N) = O(N)\r\nImplementation 2: N*O(N)     + 10*O(1)     = O(N^2 + 1)       = O(N^2)\r\nImplementation 3: N*O(Log N) + 10*O(Log N) = O(NLogN + LogN)  = O(NLogN)\r\n\r\nHere, Implementation 1 has the lowest complexity class for the combined\r\noperations. That makes sense, as the operation done many times (enqueue) is\r\nvery simple (enqueue to the end of an array/the front of a linked list) and the\r\noperation done a constant number of times (10, independent of N) is the\r\nexpensive operation (dequeue). It even beats the complexity of Implementation 3.\r\nSo, as N gets bigger, implementation 1 will eventually become faster than the\r\nother two.\r\n\r\nSo, sometimes there is NOT a \"best all the time\" implementation of a data type.\r\nWe need to know what problem we are solving (the complexity classes of all the\r\noperations in various implementations of the data type and the number of times\r\nthat we must do these operations) to choose the most efficient implementation\r\nfor solving the problem.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nAnalyzing Array Doubling (vs Linked List Allocation):\r\n\r\nAssume that we are using an array to store a simple ordered collection (e.g., a\r\nqueue; and assume we just enqueue values, but never dequeue any). Assume we\r\nstart by allocating (using \"new\") a 1 element array.\r\n\r\n1) When we enqueue the 1st value, we just store it in index 0 of the array; that\r\narray is now filled.\r\n\r\n2) When we enqueue the 2nd value, we must reallocate (again using \"new\") a 2\r\nelement array, and then copy all the values from the old 1 element array into\r\nit, and then we can store the 2nd value into index 1; that array is now filled.\r\n\r\n3) When we enqueue the 3rd value, we must reallocate (again using \"new\") a 4\r\nelement array, and then copy all the values from the old 2 element array into\r\nit, and then we can store the 3rd value into index 2; there is 1 more available\r\nindex in the array (index 3).\r\n\r\n4) When we enqueue the 4th value, we just store it at index 3.\r\n\r\n5) When we enqueue the 5th value, we must reallocate (again using \"new\") an 8\r\nelement array, and then copy all the values from the old 4 element array into\r\nit, and then we can store the 5th value into index 4; there are 3 more\r\navailable indexes in the array (indexes 5-7).\r\n\r\n6-8) When we enqueue the 6th, 7th, and 8th values, we just store them into\r\nindexes 5, 6, 7.\r\n\r\n9) When we enqueue the 9th value, we must reallocate (again using \"new\") a 16\r\nelement array, and then copy all the values from the old 8 element array into\r\nit, and then we can store the 9th value into index 8; there are 7 more\r\navailable indexes in the array (indexes 9-15).\r\n\r\n10-16) When we enqueue the 10th, 11th, ... 16th values, we just store them at\r\nindexes 9, ... 15.\r\n\r\n17) When we enqueue the 17th value, we must reallocate (again using \"new\") a 32\r\nelement array, and then copy all the values from the old 16 element array into\r\nit, and then store the 17h value into index 16; there are 15 more available\r\nindexes in the array (index 17-31).\r\n\r\nWe can make a table illustrating the total number of times \"new\" is called and\r\nthe total amount of copying needed to enqueue the Nth value into a queue\r\nrepresented by an array. Notice that for every value 1 beyond a perfect power\r\nof 2, we must call another \"new\" and copy some more values.\r\n\r\n enqueue| times \"new\" | total copied\r\n        |   called    |  values\r\n--------+-------------+--------------------------\r\n   1    |     1       |    0   \r\n   2    |     2       |    1 (= 0 +  1 more copy) \r\n  3-  4 |     3       |    3 (= 1 +  2 more copies)\r\n  5-  8 |     4       |    7 (= 3 +  4 more copies)\r\n  9- 16 |     5       |   15 (= 7 +  8 more copies)\r\n 17- 32 |     6       |   31 (=15 + 16 more copies)\r\n 33- 64 |     7       |   63 (=31 + 32 more copies)\r\n 65-128 |     8       |  127 (=63 + 64 more copies)\r\n ...\r\n\r\nSo, when we enqueue values into an array, to store between 2^(M-1)+1 and 2^M\r\nvalues takes at most M+1 calls to \"new\" and 2^M - 1 copied values. Another way\r\nto think of this is to store N values requires about Log2 N calls to \"new\" and\r\ncopies N values.\r\n\r\nAnother way to think about this is that to get to an array with 1,025 values we\r\nhave to double at 512 and do 512 copies; to get to an array with 512 values we\r\nhave to double at 256 and do 256 copies; to get to an array with 256 values we\r\nhave to double at 128 and do 128 copies; ...\r\n\r\nSo how many copies in total do we do?\r\n\r\n   512 + 256 + 128 + 64 + 32 + 16 + 8 + 2 + 1\r\n\r\nNote that the series is in the form  s = a + ar + ar^2 + ar^3 + ... ar^n\r\n (plug in n=9, a=2^n, r=1/2)\r\n\r\n  multiplying by r:  rs =     ar + ar^2 + ar^3 + ... ar^n + ar^(n+1)\r\n\r\n  subtracting    s - rs = a - ar^(n+1)   (all the other terms cancel each other)\r\n\r\n  factoring      s(1-r) = a(1 - r^(n+1))\r\n\r\n  divide by 1-r  s      = a(1 - r^(n+1))/(1-r)\r\n\r\nplug in a=512,r=1/2: s  = 512(1 - 1/2^(n+1))/ 1/2\r\n\r\nFor large n, 1/2^(n+1) is very close to 0 (when n = 9, 1/2^10 is < .001),\r\nso let's assume it is 0.\r\n\r\n  s ~ 512(1-0) / 1/2 = 512 * 2 = 1,024\r\n\r\nso to get to 1,025 values added to an array by doubling its size, we have to do\r\nabout 1,024 copies, so again the number of copies is linear in the number of\r\nvalues added to the array. Doing N adds is O(N).\r\n\r\nContrast this to using a linked list (with a rear cache) for this collection.\r\nTo store N values requires calling \"new\" N times (allocating a new LN for each\r\nvalue) and performing 0 copies.\r\n\r\nThe \"new\" and copy operations are best thought to be O(1). So the complexity\r\nclass for using an array is\r\n\r\n  (Log2 N)*news + N*copies = \r\n  (Log2 N)*O(1) + N*O(1) = O(Log2 N  +  N) = O(N)\r\n\r\nand the complexity class for using a linked list is\r\n\r\n  N*O(1) = O(N)\r\n\r\nSo both implementations are O(N). Which is actually faster? Based on complexity\r\nclasses, we do not know. But if you measure the times needed for both, we will\r\ntypically find that array implementation is faster. Here is some insight into\r\nwhy.\r\n\r\nAlthough calling \"new\" and copying values are both O(1), the constants are\r\ndifferent. Let's assume an overly simplistic model that calling \"new\" requires\r\n20 instructions and copying requires 10 instructions. This means that\r\n\r\n  # instructions needed to add N values into an array      = 10N + 20 Log2 N\r\n  # instructions needed to add N values into a linked list = 20N\r\n\r\nNote that the dominant term in both is N: arrays have a smaller constant but\r\nhave a second -lower order- term. Now let's look at these formulas for\r\na few different values of N (but spanning a huge range, from 4 to 1,000,000).\r\n\r\nFor N = 4 (Log2 4 = 2):\r\n# array instructions (#ai) = 40 + 40 = 80;\r\n# linked list instructions (#lli) = 80.\r\nSo, #ai/#lli = 80/80 = 1; so they run at about the same speed.\r\n\r\nFor N = 8 (Log2 8 = 3):\r\n#ai = 80 + 60 = 140;\r\n#lli = 160.\r\n#ai/#lli = 140/160 = .875; so the array uses 87.5% of the time of a linked list\r\n\r\nFor N = 64 (Log2 32 = 5):\r\n#ai = 320 + 100 = 420;\r\n#lli = 640.\r\n#ai/#lli = 420/640 ~ .66; so the array uses 66% of the time of a linked list\r\n\r\nFor N = 1,000:\r\n#ai = 10,000 + 200 = 10,200;\r\n#lli = 20,000\r\n#ai/#lli = .51; so the array uses 51% of the time of a linked list (about 1/2)\r\n\r\nFor N = 1,000,000:\r\n#ai = 10,000,000 + 400 = 10,000,400;\r\n#lli = 20,000,000.\r\n#ai/#lli = .50002; so the array uses about 50% of the time of a linked list\r\n\r\nSo, as N -> infniity, the logarithmic term is dominated by the linear term in\r\n#ai, so #ai/#lli -> .5 (the ratio of the cost of a new divided by the cost of\r\na copy); so for large N, filling an array takes about 50% of the time of filling\r\na linked list: it is about twice a fast.\r\n\r\nIf the coefficient for allocation is four times the coefficient for copying,\r\nusing an array becomes four times as fast. So if we actually time both methods\r\nfor a large N, we can compute the ratio of the instructions needed to do a new\r\ndivided by the # of instructions to do a copy.\r\n\r\nNow let's look a bit at analyzing the space for these data structures. First,\r\nlet's assume we are storing a queue of int, so storing each value requires 1\r\nword of memory. Also, let's assume that a pointer (used in the linked list)\r\nrequires 1 word of memory too.\r\n\r\n  1) For the array implementation, if we are storing N values we need between\r\n  N+2 and 2(N-1)+2=2N memory locations: e.g., if we need to store 1,024 values\r\n  we would need 1,026 memory locations (1,024 int values in a filled array + 1\r\n  each to store the array's length and the number of array values used); but if\r\n  we need to store 1,025 values we would need 2050: 2,048 int values in a\r\n  filled array (because of doubling) + 1 each to store the array's length and\r\n  the number of array values used).\r\n\r\n  2) For the linked implementation, if we are storing N values at exactly 2N\r\n  memory locations (one each for the int value and the .next field pointer).\r\n  So storing 1,024 values requires 2,048 memory locations.\r\n\r\nSo, for storing ints, arrays always need <= the storage space used by a linked\r\nlist, even if 1/2 the array contain no values (because every linked list node\r\ncontains two storage locations: a value and a pointer)!\r\n\r\nSome words of caution. At the time a new array is allocated, we are using\r\nN values (the old array) + 2N values (the new array) of data. We can get rid \r\nof the old array as soon as we copy its values into the new array. So, actually\r\nin the worst case we might need about 3N memory locations -temporarily- to\r\nstore N values, right when doubling the array.\r\n\r\nAlso, if the queue stored data bigger than integers, this analysis is flipped.\r\nIf each value in the data structure were an object containing 10 memory\r\nlocations, then storing 1,024 in a filled array would required 10,240 + 2 memory\r\nlocations and storing 1,025 of these values in an array that was just doubled\r\nto size 2,048 would require 20,480 + 2 memory locations. A linked list would\r\nneed 11,264 memory locations to store these values (10,240 for the data\r\nand 1,024 for the pointers). So, unless an array is close to filled, a linked\r\nlist would occupy less memory when each value stored in the array/list is large.\r\n\r\nOne way to return to the original \"storing int\" analysis is to use pointers to\r\nobjects for all the data. Since pointers and ints both take up the same amount\r\nof space, the original analysis holds. In such a case (assuming each pointer to\r\ndata pointed to an object occupying 10 words of storage), 80% of the storage\r\nwould be occupied up by the data itself and only 20% by the array/linked list\r\nthat stores this pointers.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nBig-O:\r\n\r\nRecall the formal definition of big-O notation, which bounds a function from\r\nabove. A function f(n) is O(g(n)) -often written \"in the complexity class of\r\nO(g(n))\" if there are  values c and n0 such that  f(n) <= c g(n) for all n>n0.\r\n\r\nTypically the \"f\" function we are interested in measures the effort (often the\r\namount of time) it takes for some algorithm a (coded in some language as\r\na method m) to run, which we write either Ta(N) or Tm(N). Note that Ta(N) is\r\nO(N), then Ta(N) is also O(N^2), O(N^3), etc. because these functions are even\r\nbigger bounds: if f(n) <= c1 n then f(n) <= c2 n^2, etc. Typically, though, we\r\nare looking for the SMALLEST complexity class that bounds some algorithm or\r\nmethod.\r\n\r\n\r\nBig-Omega:\r\n\r\nBig-Omega notation bounds a function from below instead of from above.\r\nThe definition starts similarly to big-O notation: A function f(n) is\r\nOmega(g(n)) if there are values c and n0 such that f(n) >= c g(n) for all\r\nn>n0.\r\n\r\nNotice the <= in big-O notation has been replaced with a >= in big-Omega\r\nnotation.  Although big-O notation is mostly used to analyze \"algorithms\",\r\nbig-Omega notation is mostly used to analyze \"problems\". With big-O notation\r\nwe analyze one SPECIFIC algorithm/method to determine an upper bound on its\r\nperformance. In big-Omega notation we analyze all possible algorithms/methods\r\nto determine a lower bound on performance. This second task is much harder.\r\n\r\nNow let's return to using big-Omega notation for finding the lower bound of\r\nproblems. It is trivial to prove that any algorithm that solves the \"find the\r\nmaximum of an unordred array problem\" is Omega(N) because it has to look at\r\nleast at every value in the array: if it missed looking at some value in the\r\narray, that value might be the biggest, and the algorithm would return the\r\nwrong value.\r\n\r\nInteresting lower bounds on problems are much harder to prove than upper bounds\r\non algorithms. The lower bound on a problem is much more general: it says,\r\n\"for ANY ALGORITHM that solves this problem, it will take AT LEAST g(n)\r\noperations\". Whereas, for upper bounds we are analyzing something much\r\nmore concrete: one actual agorithm; we say, \"for this particular algorithm,\r\nit will take AT MOST g(n) operations.\"\r\n\r\nOften the only lower bound that we can get on a problem is trivial -like that\r\nwe must examine every value in an array. Later in the quarter we will examine\r\nan interesting/beautiful lower bound for sorting via comparisons: such a\r\nproblem is Omega(N Log N). We also will examine sorting algorithms that are\r\nO(N Log N). This means that within comparison based sorting, we have optimally\r\nsolved the problem according to its complexity class: any algorithm to solve\r\nthis problem requires work proportional to N Log N and we have an algorithm that\r\nsolves this problem in work proportional to N Log N. So, a new algorithm might\r\nhave a better/smaller constant (which is very important, once we have resolved\r\nin which complexity class is the problem), but a better algorithm cannot have a\r\nlower complexity class: the lower bound forces its complexity class to at least\r\nbe O(N Log N).\r\n\r\nOne interesting example of a LACK of an obvious lower and upper bounds concerns\r\nmatrix multiplication. When we multiply two NxN matrices we get another NxN\r\nmatrix. Since the input matrices have N^2 values and the result has N^2 values,\r\nwe know trivially that this problem is Omega(N^2): it must at least look at\r\n2N^2 inputs and produce N^2 outputs. But, the standard algorithm to multiply\r\nmatrices is O(N^3): each of the N^2 entries in the resulting matrix is computed\r\nby an inner-product requiring N multiplications and additions (one matrix's\r\nrow values times the other matrix's column value), which itself is O(N).\r\n\r\nSo there is a gap between the complexity class of the problem (the lower bound\r\nfor the problem is Omega(N^2)) and the complexity class of one solution (the\r\nupper bound for the standard matrix multiplication algorithm is O(N^3)). Either\r\nwe should be able to improve the lower bound by raising it: by proving more\r\nwork is always needed; or we should be able to improve the upper bound by \r\nlowering it: finding a better algorithm and proving that it needs to do less\r\nwork than the standard one.\r\n\r\nIn the 60s, a Computer Scientist named Strassen devised an algorithm to solve\r\nthis problem in O(N ^ Log 7): N raised to the power of Log (base 2) of 7, which\r\nis ~N^2.8 (recall Log (base 2) of 8 = 3 so Log (base 2) of 7 will be a bit less\r\nthan 3), somewhat better than N^3 but still higher than N^2. But the gap between\r\nlower bound and upper bound has narrowed.\r\n\r\nIn the 90s two Computer Scientists, Coopersmith and Winograd, devised an\r\nalgorithm whose complexity is O(N^2.376). Interestingly enough -and not often\r\nthe case- the constant on this algorithm is so huge, the n0 for which is starts\r\nbeing faster than Strassen is bigger than matrices easily storable on today's\r\ncomputers (more than billions of values).\r\n\r\nIn 2002, a computer scientist named Raz proved a new lower bound of\r\nOmega(N^2 LogN), which is the product of N^2 and LogN, and is bigger than\r\nOmega(N^2).\r\n\r\nSo, at this point we know the actualy complexity of the problem, call it c(n)\r\nis somewhere between N^2 Log N and N^2.376. Note that N^.376 is  close to the\r\ncube root of N, so N^2.376 = N^.376 * N^2 is close to cube_root(N)*N^2. So the\r\nactual term before N^2 is going to lie somewhere between LogN and cube_root(N).\r\n\r\nFor more information, check http://en.wikipedia.org/wiki/Strassen_algorithm\r\n\r\nIn summary, better algorithms decrease the big-O complexity class, better lower\r\nbound proofs increase the big-Omega minimal complexity. If the big-O and\r\nbig-Omega bounds are the same functions, then we have discovered an optimal\r\nalgorithm to solve the problem. Well, best to say \"optimal within a constant\",\r\nas other algorithms in the same (optimal) complexity class might exhibit a\r\nsmaller constant and be faster. That is what big-O notation \"throws away\".\r\n\r\n-----\r\n\r\nSometimes we do want to prove just that some function f(n) is Omega(g(n)).\r\nFor example, we want to prove that f(n) = 5n^2 + 3nlogn + 2n + 5 is Omega(n^2).\r\nSo, we need to prove that c n^2 <= 5n^2 + 3nlogn + 2n + 5 for all n>n0. We can\r\neasily ignore all positive lower order terms. That is,\r\n\r\n  f(n) >= f(n) - 3nlogn -2n - 5 (for all n>1) =  5n^2, so\r\n  f(n) >= 5n^2 >= 4n^2 (choosing c = 4, for all n > 0)\r\n\r\nfollowing the inequalities (and reversing how it is shown),\r\n4n^2 <= 5n^2 <= f(n) (for all n>0). That is,\r\n4n^2 <= 5n^2 <= 5n^2 + 3nlogn + 2n + 5\r\nbecause for all problem sizes (which are positive) 3logn, 2n, and 5 are >=0\r\n\r\nso f(n) is Omega(n^2).\r\n\r\nBy a similar subtraction we can can ignore all positive lower-order  terms.\r\n\r\nLikewise, we can see that for f(n) = 2n - 100log n, f(n) is O(n) because we\r\ncan choose c to be 1, so we need to know when\r\n\r\n  2n - 100log n >= n \r\n  n - 100log n >= 0   (subtract 1n from each side)\r\n  n >= 100log n       (add 100log n to each side)\r\n\r\nIt is not easy to solve this inequality, but log 1024 is 10, and 100log 1024\r\nis 1000, so n >= 100log n for n = 1024, and n grows faster than log n, so for\r\nbigger n, n is even bigger than 100log n.\r\n \r\nBefore using big-Omega to analyze the lower bounds  for algorithms we will\r\nanalyze two simple functions to compute their big-Omega lower bounds.\r\n\r\nIf f(n) = 3n^2 - 2n - 10, then we can choose c = 2 and solve for n such that\r\n   3n^2 - 2n - 10 > 2n^2\r\n    n^2 - 2n - 10 > 0\t\tsubtract n^2 from each side\r\n    n^2 - 2n > 10\t\tadd 10 to each side\r\n    n(n-2) > 10\t\t\tfactor\r\n\r\nNotice that n(n-2) increases whenever n increases, and for n = 5, n(n-2) is\r\n15, which is > 10. So for all n >= 5,  f(n) > 2n^2 (with constant c). \r\n\r\nIn summary, for any simply expressed polynomial function:\r\n   f(x) = c1 x^n + c2 x^n-1 + .... + c\r\n\r\nit is both O(x^n) and Omega(x^n).\r\n\r\nWe will return to this kind of analysis later in the notes, when we analyze\r\nupper and lower bounds for the n! function, which is not a simple polynomial.\r\n\r\nFinally if f(n) = n, then\r\nf is O(N), O(N^2), O(2^N), etc. all are upper bounds on the growth of f\r\nf is Omega(N), Omega(Log N), Omega(1) etc. all are lower bounds on the growth\r\nof F\r\n\r\n-----\r\n\r\n\r\n\r\nBig-Theta:\r\n\r\nThis brings us to our final notation. Big-Theta notation is a combination of\r\nbig-O and big-Omega, which bounds a function from below and above. A function\r\nf(n) is Theta(g(n)) if there are values c1, c2, and n0 such that\r\n   c1 g(n) <= f(n) <= c2 g(n) for all n>n0.\r\n\r\nWe use Theta notation for two purposes. First, we use it to show that the O\r\nnotation is \"tight\" not only is some function O(g(n)) but we cannot really find\r\na smaller complexity class because it is Omega(g(n)) too.\r\n\r\nFor example, we proved f(n) = 5n^2 + 3nlogn + 2n + 5 is O(n^2) (for c2=15 and\r\nn0=1) and we proved above that f(n) is Omega(n^2) (for c1 = 4 and n0=1) so\r\nwe have our c1, c2, and n0 (n0 is generally the bigger of the two, but here\r\nboth are 0). So talking about f(n) in terms of the n^2 complexity class makes\r\nsense for an upper (O) and lower (Omega) bound. So, if we say a function f(n) is\r\nTheta(g(n)) it means c1 g(n) <= f(n) <= c2 g(n) for all n>n0.\r\n\r\nWe also use Theta notation to mean that we have found an optimal (within a\r\nconstant) algorithm for a problem: if our algorithm is O(g(n))and the problem\r\nis Omega(g(n)) then our solution's complexity class, g(n), is as good as we can\r\nget. We will see that as a problem, sorting with comparisons is Omega(N Log N)\r\nand we will see various sorting algorithms (mergesort and heapsort) that are\r\nO(N Log N) so sorting is Theta(N Log N).\r\n\r\nFinally, to make matters a bit worse (but more truthful), there is a sorting\r\nalgorithm called quicksort that is O(N^2) in the worst case, but O(N Log N)\r\nfor almost all inputs. In fact, the constant for a method implementing this\r\nalgorithm is typically smaller (by a factor of 2-3) than the constant for\r\nmergesort and heapsort, two other sorting algorithms that guarantee to run in\r\nO(N Log N).\r\n\r\nSo, even though quicksort's worst complexity class is higher than other fast\r\nsorting algorithms, its average performance is in the same complexity class as\r\nother fast sorting algorithms, and its constant is actually lower. So, choosing\r\nthe best algorithm is a bit more complicated than just finding one in the\r\nlowest complexity class. Note that on a few problems, quicksort can take much\r\nlonger than mergesort or heapsort, but those problems are very infrequent.\r\n\r\n\r\nAs a final example illustrating big-O and big-Omega (applied directly to a\r\nfunction), let's find simple (but not very precise) upper and lower bounds for\r\nthe function n!  Recall n! = 1 x 2 x ... x n. To get a simple upper bound we\r\ncan replace each multiplicand by n. \r\n\r\n  n! = 1 x 2 x 3 ... x n  <  n x n x n ... x n  =  n^n\r\n\r\nSo n! is O(n^n).\r\n\r\nTo get a lower bound, bound we can replace the first half of the multiplicands\r\nby 2, and the second half by n/2. (OK, so 2 isn't smaller than 1, but so long\r\nas n>8, the product of the first 1/2 of the terms is bigger than 2^(n/2) because\r\n4 is 2^2)\r\n\r\n                             (first half)        (second half)\r\n  n! = 1 x 2 x 3 ... x n  >  2 x .... x 2   x   n/2 x ... x n/2 =\r\n\r\n     2^(n/2) x (n/2)^(n/2) = n^(n/2) = (n^n)^(1/2) = sqrt(n^n)\r\n\r\nSo n! is Omega(sqrt(n^n))\r\n\r\nThese are very different functions, so we cannot use Theta here, but we know\r\n\r\n  sqrt(n^n) <= n! <= n^n\r\n\r\nSo n! grows at least at the rate sqrt(n^n) but less than the rate n^n.\r\n\r\nStirlings formula for approximating n!, which is very accurate for large n, has\r\n\r\n  n! ~ sqrt(2*pi*n)*(n/e)^n.\r\n\r\nNotice that the grown rate of this function, after removing constant multipliers\r\nis sqrt(n)*n^n*(1/e)^n. By Stirling's approxiation, 10! ~ 3,598,718 which is\r\nvery close to the correct answer (3,628,800): its absolute error is 30,082 but\r\nits relative error is 30,082/3,628,800 = .83%. As n gets bigger the Stirling's\r\napproximation gets closer to n!.\r\n", "encoding": "ascii"}