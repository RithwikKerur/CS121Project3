{"url": "https://www.ics.uci.edu/~taylor/ICS221/papers/McIlroyComponents.txt", "content": "Scanned from P. Naur and B. Randell, \"Software Engineering, Report on\na conference sponsored by the NATO Science Committee, Garmisch,\nGermany, 7th to 11th October 1968\", Scientific Affairs Division, NATO,\nBrussels, 1969, 138-155.  Layout has not been preserved.  Isolated\nmisprints in the original have been corrected, and a hodge-podge of\nEnglish and American spellings has been resolved in favor of American.\nScanning errors may remain.\n\t\t\t\t\t- MDM, 15 October 1998\n\n8.2.  MASS PRODUCED SOPTWARE COMPONENTS, BY M.D. McILROY\n\n                ABSTRACT\n\n        Software components (routines), to be widely applicable to\ndifferent machines and users, should be available in families arranged\naccording to precision, robustness, generality and time-space performance.\nExisting sources of components - manufacturers, software houses, users'\ngroups and algorithm collections - lack the breadth of interest or\ncoherence of purpose to assemble more than one or two members of such\nfamilies, yet software production in the large would be enormously helped\nby the availability of spectra of high quality routines, quite as\nmechanical design is abetted by the existence of families of structural\nshapes, screws or resistors.  The talk will examine the kinds of\nvariability necessary in software components, ways of producing useful\ninventories, types of components that are ripe for such standardization,\nand methods of instituting pilot production.\n\n                The Software Industry is Not Industrialized\n\n        We undoubtedly produce software by backward techniques.  We\nundoubtedly get the short end of the stick in confrontations with hardware\npeople because they are the industrialists and we are the crofters.\nSoftware production today appears in the scale of industrialization\nsomewhere below the more backward construction industries.  I think its\nproper place is considerably higher, and would like to investigate the\nprospects for mass-production techniques in software.\n\n        In the phrase `mass production techniques,' my emphasis is on\n`techniques' and not on mass production plain.  Of course mass production,\nin the sense of limitless replication of a prototype, is trivial for\nsoftware.  But certain ideas from industrial technique I claim are\nrelevant.  The idea of subassemblies carries over directly and is well\nexploited.  The idea of interchangeable parts corresponds roughly to our\nterm `modularity,' and is fitfully respected.  The idea of machine tools\nhas an analogue in assembly programs and compilers.  Yet this fragile\nanalogy is belied when we seek for analogues of other tangible symbols of\nmass production.  There do not exist manufacturers of standard parts, much\nless catalogues of standard parts.  One may not order parts to individual\nspecifications of size, ruggedness, speed, capacity, precision or character\nset.\n\n        The pinnacle of software is systems - systems to the exclusion of\nalmost all other considerations.  Components, dignified as a hardware\nfield, is unknown as a legitimate branch of software.  When we undertake to\nwrite a compiler, we begin by saying `What table mechanism shall we build.'\nNot, `What mechanism shall we use?'  but `What mechanism shall we build?'\nI claim we have done enough of this to start taking such things off the\nshelf.\n\n                Software Components\n\n        My thesis is that the software industry is weakly founded, and that\none aspect of this weakness is the absence of a software components\nsubindustry.  We have enough experience to perceive the outline of such a\nsubindustry.  I intend to elaborate this outline a little, but I suspect\nthat the very name `software components' has probably already conjured up\nfor you an idea of how the industry could operate.  I shall also argue that\na components industry could be immensely useful, and suggest why it hasn't\nmaterialized.  Finally I shall raise the question of starting up a `pilot\nplant' for software components.\n\n        The most important characteristic of a software components industry\nis that it will offer families of routines for any given job.  No user of a\nparticular member of a family should pay a penalty, in unwanted generality,\nfor the fact that he is employing a standard model routine.  In other\nwords, the purchaser of a component from a family will choose one tailored\nto his exact needs.  He will consult a catalogue offering routines in\nvarying degrees of precision, robustness, time-space performance, and\ngenerality.  He will be confident that each routine in the family is of\nhigh quality - reliable and efficient.  He will expect the routine to be\nintelligible, doubtless expressed in a higher level language appropriate to\nthe purpose of the component, though not necessarily instantly compilable\nin any processor he has for his machine.  He will expect families of\nroutines to be constructed on rational principles so that families fit\ntogether as building blocks.  In short, he should be able safely to regard\ncomponents as black boxes.\n\n        Thus the builder of an assembler will be able to say `I will use a\nString Associates A4 symbol table, in size 500x8,' and therewith consider\nit done.  As a bonus he may later experiment with alternatives to this\nchoice, without incurring extreme costs.\n\n                A Familiar Example\n\n        Consider the lowly sine routine.  How many should a standard\ncatalogue offer?  Off hand one thinks of several dimensions along which we\nwish to have variability:\n\n        Precision, for which perhaps ten different approximating functions\n                might suffice\n        Floating-vs-fixed computation\n        Argument ranges 0-pi/2, O-2pi, also -pi/2 to pi/2,\n        -pi to pi, -big to +big\n        Robustness - ranging from no argument validation through signaling\n                of complete loss of significance, to signaling of specified\n                range violations.\n\nWe have here 10 precisions, 2 scalings, 5 ranges and 3 robustnesses The\nlast range option and the last robustness option are actually arbitrary\nparameters specifiable by the user.  This gives us a basic inventory of 300\nsine routines.  In addition one might expect a complete catalogue to\ninclude a measurement-standard sine routine, which would deliver (at a\nprice) a result of any accuracy specified at run time.  Another dimension\nof variability, which is perhaps difficult to implement, as it caters for\nvery detailed needs is\n\n        Time-space tradeoff by table lookup, adjustable in several\n                `subdimensions':\n                (a) Table size\n                (b) Quantization of inputs (e.g., the inputs are known to\n                    be integral numbers of degrees)\n        Another possibility is\n                (c) Taking advantage of known properties of expected input\n                    sequences, for example profiting from the occurrence of\n\t\t    successive calls for sine and cosine of the same argument.\n\n        A company setting out to write 300 sine routines one at a time and\nhoping to recoup on volume sales would certainly go broke.  I can't imagine\nsome of their catalogue items ever being ordered.  Fortunately the cost of\noffering such an `inventory' need not be nearly 300 times the cost of\nkeeping one routine.  Automated techniques exist for generating\napproximations of different degrees of precision.  Various editing and\nbinding techniques are possible for inserting or deleting code pertinent to\neach degree of robustness.  Perhaps only the floating-vs-fixed dichotomy\nwould actually necessitate fundamentally different routines.  Thus it seems\nthat the basic inventory would not be hard to create.\n\n        The example of the sine routine re-emphasizes an interesting fact\nabout this business.  It is safe to assert that almost all sines are\ncomputed in floating point these days, yet that would not justify\ndiscarding the fixed point option, for that could well throw away a large\npart of the business in distinct tailor-made routines for myriads of small\nprocess-control and other real-time applications on all sorts of different\nhardware.  `Mass production' of software means multiplicity of what\nmanufacturing industry would call `models,' or `sizes' rather than\nmultiplicity of replicates of each.\n\n                Parameterized Families of Components\n\n        One phrase contains much of the secret of making families of\nsoftware components:  `binding time.'  This is an `in' phrase this year,\nbut it is more popular in theory than in the field.  Just about the only\napplications of multiple binding times I can think of are sort generators\nand the so-called `Sysgen' types of application:  filling in parameters at\nthe time routines are compiled to control table sizes, and to some extent\nto control choice among several bodies of code.  The best known of these,\nIBM's OS/360 Sysgen is indeed elaborate - software houses have set\nthemselves up as experts on this job.  Sysgen differs, though, in a couple\nof ways from what I have in mind as the way a software components industry\nmight operate.\n\n        First, Sysgen creates systems not by construction, but rather by\nexcision, from an intentionally fat model.  The types of adjustment in\nSysgen are fairly limited.  For example it can allocate differing amounts\nof space to a compiler, but it can't adjust the width of list link fields\nin proportion to the size of the list space.  A components industry on the\nother hand, not producing components for application to one specific\nsystem, would have to be flexible in more dimensions, and would have to\nprovide routines whose niches in a system were less clearly delineated.\n\n        Second, Sysgen is not intended to reduce object code or running\ntime.  Typically Sysgen provides for the presetting of defaults, such as\nwhether object code listings are or are not standard output from a\ncompiler.  The entire run-time apparatus for interrogating and executing\noptions is still there, even though a customer might guarantee he'd never\nuse it were it indeed profitable to refrain.  Going back to the sine\nroutine, this is somewhat like building a low precision routine by\ncomputing in high precision and then carefully throwing away the less\nsignificant bits.\n\n            Having shown that Sysgen isn't the exact pattern for a \ncomponents industry, I hasten to add that in spirit it is almost the only\nway a successful components industry could operate.  To purvey a rational\nspectrum of high quality components a fabricator would have to systematize\nhis production.  One could not stock 300 sine routines unless they were all\nin some sense instances of just a few models, highly parameterized, in which\nall but a few parameters were intended to be permanently bound before run\ntime.  One might call these early-bound parameters `sale time' parameters.\n\n     Many of the parameters of a basic software component will be\nqualitatively different from the parameters of routines we know today.\nThere will be at least\n\n         Choice of Precision.  Taken in a generalized sense precision\nincludes things like width of characters, and size of address or pointer\nfields.\n\n        Choice of Robustness.  The exact tradeoff between reliability and\ncompactness in space and time can strongly affect the performance of a\nsystem.  This aspect of parameterization and the next will probably rank\nfirst in importance to customers.\n\n        Choice of Generality.  The degree to which parameters are left\nadjustable at run time.\n\n        Choice of Time-space behavior.\n\n        Choice of Algorithm.  In numerical routines, as exemplified by\nthose in the CACM, this choice is quite well catered for already.  For\nnonnumerical routines, however, this choice must usually be decided on the\nbasis of folklore.  As some nonnumerical algorithms are often spectacularly\nunsuitable for particular hardware, a wide choice is perhaps even more\nimperative for them.\n\n        Choice of Interfaces.  Routines that use several inputs and yield\nseveral outputs should cone in a variety of interface styles.  For example,\nthese different styles of communicating error outputs should be available:\n\n        a.  Alternate returns\n        b.  Error code return\n        c.  Call an error handler\n        d.  Signal (in the sense of PL/I)\n\nAnother example of interface variability is that the dimensions of matrix\nparameters should be receivable in ways characteristic of several major\nprogramming languages.\n\n        Choice of Accessing method.  Different storage accessing\ndisciplines should be supported, so that a customer could choose that best\nfitting his requirements in speed and space, the addressing capabilities of\nhis hardware, or his taste in programming style.\n\n        Choice of Data structures.  Already touched upon under the topic of\ninterfaces, this delicate matter requires careful planning so that\nalgorithms be as insensitive to changes of data structure as possible.\nWhen radically different structures are useful for similar problems (e.g.,\nincidence matrix and list representations for graphs), several algorithms\nmay be required.\n\n                Application Areas\n\n        We have to begin thinking small.  Despite advertisements to the\neffect that whole compilers are available on a `virtually off-the-shelf'\nbasis, I don't think we are ready to make software subassemblies of that\nsize on a production basis.  More promising components to begin with are\nthese:\n\n        Numerical approximation routines.  These are very well understood,\nand the dimensions of variability for these routines are also quite clear.\nCertain other numerical processes aren't such good candidates; root finders\nand differential equation routines, for instance are still matters for\nresearch, not mass production.  Still other `numerical' processes, such as\nmatrix inversion routines, are simply logical patterns for sequencing that\nare almost devoid of variability.  These might be sold by a components\nindustry for completeness' sake, but they can be just as well taken from\nthe CACM.\n\n        Input-output conversion.  The basic pieces here are radix\nconversion routines, some trivial scanning routines, and format crackers.\nFrom a well-designed collection of families it should be possible to\nfabricate anything from a simple on-line octal package for a small\nlaboratory computer to a Fortran IV conversion package.  The variability\nhere, especially in the matter of accuracy and robustness is substantial.\nConsiderable planning will evidently be needed to get sufficient\nflexibility without having too many basically different routines.\n\n        Two and three dimensional geometry.  Applications of this sort are\ngoing on a very wide class of machines, and today are usually kept\nproprietary.  One can easily list a few dozen fundamental routines for\ngeometry.  The sticky dimension of variability here is in data structures.\nDepending on which aspect of geometrical figures is considered fundamental\n- points, surfaces, topology, etc.  - quite different routines will be\nrequired.  A complete line ought to cater for different abstract\nstructures, and also be insensitive to concrete structures.\n\n        Text processing.  Nobody uses anybody else's general parsers or\nscanners today, partly because a routine general enough to fulfill any\nparticular individual needs probably has so much generality as to be\ninefficient.  The principle of variable binding times could be very\nfruitfully exploited here.  Among the corpus of routines in this area\nwould be dictionary builders and lookup routines, scanners, and output\nsynthesizers, all capable of working on continuous streams, on unit\nrecords, and various linked list formats, and under access modes suitable\nto various hardware.\n\n        Storage management.  Dynamic storage allocation is a popular topic\nfor publication, about which not enough real knowledge yet exists.  Before\nconstructing a product line for this application, one ought to do\nconsiderable comparison of known schemes working in practical environments.\nNevertheless storage management is so important, especially for text\nmanipulation, that it should be an early candidate.\n\n                The Market\n\n        Coming from one of the larger sophisticated users of machines, I\nhave ample opportunity to see the tragic waste of current software writing\ntechniques.  At Bell Telephone Laboratories we have about 100 general\npurpose machines from a dozen manufacturers.  Even though many are\ndedicated to special applications, a tremendous amount of similar software\nmust be written for each.  All need input-output conversion, sometimes only\nsingle alphabetic characters and octal numbers, some full-blown Fortran\nstyle I/O. All need assemblers and could use macroprocessors, though not\nnecessarily compiling on the same hardware.  Many need basic numerical\nroutines or sequence generators.  Most want speed at all costs, a few want\nconsiderable robustness.\n\n        Needless to say much of this support programming is done\nsuboptimally, and at a severe scientific penalty of diverting the machine's\nowners from their central investigations.  To construct these systems of\nhigh-class componentry we would have to surround each of some 50 machines\nwith a permanent coterie of software specialists.  Were it possible quickly\nand confidently to avail ourselves of the best there is in support\nalgorithms, a team of software consultants would be able to guide\nscientists towards rapid and improved solutions to the more mundane support\nproblems of their personal systems.\n\n        In describing the way Bell laboratories might use software\ncomponents, I have intended to described the market in microcosm.  Bell\nlaboratories is not typical of computer users.  As a research and\ndevelopment establishment, it must perforce spend more of its time\nsharpening its tools, and less using them than does a production computing\nshop.  But it is exactly such a systems-oriented market toward which a\ncomponents industry would be directed.\n\n        The market would consist of specialists in system building, who\nwould be able to use tried parts for all the more commonplace parts of\ntheir systems.  The biggest customers of all would be the manufacturers.\n(Were they not it would be a sure sign that the offered products weren't\ngood enough.)  The ultimate consumer of systems based on components ought\nto see considerably improved reliability and performance, as it would\nbecome possible to expend proportionally more effort on critical parts of\nsystems, and also to avoid the now prevalent failings of the more mundane\nparts of systems, which have been specified by experts, and have then been\nwritten by hacks.\n\n                Present Day Suppliers\n\n        You may ask, well don't we have exactly what I've been calling for\nalready in several places?  What about the CACM collected algorithms?  What\nabout users groups?  What about software houses?  And what about\nmanufacturers' enormous software packages?\n\n        None of these sources caters exactly for the purpose I have in\nmind, nor do I think it likely that any of them will actually evolve to\nfill the need.\n\n        The CACM algorithms, in a limited field, perhaps come closer to\nbeing a generally available off-the-shelf product than do the commercial\nproducts, but they suffer some strong deficiencies.  First they are an\ningathering of personal contributions, often stylistically varied.  They\nfit into no plan, for the editor can only publish that which the authors\nvolunteer.  Second, by being effectively bound to a single compilable\nlanguage, they achieve refereeability but must perforce completely avoid\nalgorithms for which Algol is unsuited or else use circumlocutions so\nabominable that the product can only be regarded as a toy.  Third, as an\nadjunct of a learned society, the CACM algorithms section can not deal in\nlarge numbers of variants of the same algorithm.  Variability can only be\nprovided by expensive run time parameters\n\n        User's groups I think can be dismissed summarily, and I will spare\nyou a harangue on their deficiencies.\n\n        Software houses generally do not have the resources to develop\ntheir own product lines; their work must be financed, and large financing\ncan usually only be obtained for large products.  So we see the software\nhouses purveying systems, or very big programs, such as Fortran compilers,\nlinear programming packages or flowcharters.  I do not expect to see any\nsoftware house advertising a family of Bessel functions or symbol tabling\nroutines in the predictable future.\n\n        The manufacturers produce unbelievable amounts of software.\nGenerally, as this is the stuff that gets used most heavily it is all\npretty reliable, a good conservative grey, that doesn't include the best\nroutine for anything, but that is better than the average programmer is\nlikely to make.  As we heard yesterday manufacturers tend to be rather\npragmatic in their choice of methods.  They strike largely reasonable\nbalances between generality and specificity and seldom use absolutely\ninappropriate approaches in any individual software component.  But the\nprofit motive wherefrom springs these virtues also begets their prime\nhangup - systems now.  The system comes first; components are merely\nannoying incidentals.  Out of these treadmills I don't expect to see high\nclass components of general utility appear.\n\n                A Components Factory\n\n        Having shown that it is unlikely to be born among the traditional\nsuppliers of software I turn now to the question of just how a components\nindustry might get started.\n\n        There is some critical size to which the industry must attain\nbefore it becomes useful.  Our purveyor of 300 sine routines would probably\ngo broke waiting for customers if that's all he offered, just as an\nelectronics firm selling circuit modules for only one purpose would have\ntrouble in the market.\n\n        It will take some time to develop a useful inventory, and during\nthat time money and talent will be needed.  The first source of support\nthat comes to mind is governmental, perhaps channeled through\nsemi-independent research corporations.  It seems that the fact that\ngovernment is the biggest user and owner of machines should provide\nsufficient incentive for such an undertaking that has promise for making an\nacross-the-board improvement in systems development.\n\n        Even before founding a pilot plant, one would be wise to have\ndemonstrated techniques for creating a parameterized family of routines for\na couple of familiar purposes, say a sine routine and a Fortran I/O module.\nThese routines should be shown to be usable as replacements in a number of\nradically different environments.  This demonstration could be undertaken\nby a governmental agency, a research contractor, or by a big user, but\ncertainly without expectation of immediate payoff.\n\n        The industrial orientation of a pilot plant must be constantly\nborne in mind.  I think that the whole project is an improbable one for\nuniversity research.  Research-caliber talent will be needed to do the job\nwith satisfactory economy and reliability, but the guiding spirit of the\nundertaking must be production oriented.  The ability to produce members of\na family is not enough.  Distribution, cataloguing, and rational planning\nof the mix of product families will in the long run be more important to\nthe success of the venture than will be the purely technical achievement.\n\n        The personnel of a pilot plant should look like the personnel on\nmany big software projects, with the masses of coders removed.  Very good\nplanning, and strongly product-minded supervision will be needed.  There\nwill be perhaps more research flavor included than might be on an ordinary\nsoftware project, because the level of programming here will be more\nabstract:  Much of the work will be in creating generators of routines\nrather than in making the routines themselves.\n\n        Testing will have to be done in several ways.  Each member of a\nfamily will doubtless be tested against some very general model to assure\nthat sale-time binding causes no degradation over runtime binding.  Product\ntest will involve transliterating the routines to fit in representative\nhardware.  By monitoring the ease with which fairly junior people do\nproduct test, managers could estimate the clarity of the product, which is\nimportant in predicting customer acceptance.\n\n        Distribution will be a ticklish problem.  Quick delivery may well\nbe a components purveyor's most valuable sales stimulant.  One instantly\nthinks of distribution by communication link.  Then even very small\ncomponents might be profitably marketed.  The catalogue will be equally\nimportant.  A comprehensive and physically condensed document like the\nSears-Roebuck catalogue is what I would like to have for my own were I\npurchasing components.\n\n        Once a corpus of product lines became established and profit\npotential demonstrated, I would expect software houses to take over the\nindustry.  Indeed, were outside support long needed, I would say the\nventure had failed (and try to forget I had ever proposed it).\n\n                     Touching on Standards\n\n        I don't think a components industry can be standardized into\nexistence.  As is usual with standards, it would be rash to standardize\nbefore we have the models.  Language standards, provided they are loose\nenough not to prevent useful modes of computation, will of course be\nhelpful.  Quite soon one would expect a components industry to converge on\na few standard types of interface.  Experience will doubtless reveal other\nstandards to be helpful, for example popular word sizes and character sets,\nbut again unless the standards encompass the bulk of software systems (as\ndistinguished from users), the components industry will die for lack of\nmarket.\n\n                Summary\n\n        I would like to see components become a dignified branch of\nsoftware engineering.  I would like to see standard catalogues of routines,\nclassified by precision, robustness, time-space performance, size limits,\nand binding time of parameters.  I would like to apply routines in the\ncatalogue to any one of a large class of often quite different machines,\nwithout too much pain.  I do not insist that I be able to compile a\nparticular routine directly, but I do insist that transliteration be\nessentially direct.  I do not want the routine to be inherently inefficient\ndue to being expressed in machine independent terms.  I want to have\nconfidence in the quality of the routines.  I want the different types of\nroutine in the catalogue that are similar in purpose to be engineered\nuniformly, so that two similar routines should be available with similar\noptions and two options of the same routine should be interchangeable in\nsituations indifferent to that option.\n\n        What I have just asked for is simply industrialism, with\nprogramming terms substituted for some of the more mechanically oriented\nterms appropriate to mass production.  I think there are considerable areas\nof software ready, if not overdue, for this approach.\n\n8.2.1.  DISCUSSION\n\nRoss:  What Mcllroy has been talking about are things we have been playing\nwith.  For example, in the AED system we have the so-called\nfeature-feature.  This enables us to get round the problem of loaders.  We\ncan always embed our system in whatever loader system is available.  The\nproblem of binding is very much interlocked there, so we are at the mercy\nof the environment.  An example is a generalized alarm reporting system in\nwhich you can either report things on the fly, or put out all kinds of\ndynamic information.  The same system gives 14 different versions of the\nalarm handling.  Macro-expansion seems to me to be the starting place for\nsome of the technical problems that have to be solved in order to put these\nvery important ideas into practice.\n\nMcIlroy:  It seems that you have automated some of types variability that I\nthought were more speculative.\n\nOpler:  The TOOL system produced six years ago for Honeywell was\ncomplementary to the one McIlroy described.  It has facilities for putting\nthings together, but it did not provide the components.  The difficulty we\nhad was that we produced rudimentary components to see how the system would\nwork, but the people for whom we developed the system did not understand\nthat they were to provide their own components, so they just complained\nthat the system was not good.  But I am very enthusiastic about what you\nsuggest.\n\nPerlis:  The GP system of the first Univac was a system for developing\npersonalized software as long as you stayed on that machine.  The authors\nof this system asked me:  how would one generalize this to other computers?\nThey did not know how to do it at the time, and I suppose it has not been\ndone.  I have a question for Mcllroy.  I did not hear you mention what to\nme is the most obvious of parameterizations, namely to build generalized\nbusiness data file handling systems.  I understand that Informatics has one\nout which everybody says is OK, but - .  This seems to be a typical\nattitude to parameterized systems.\n\nMcIlroy:  My reason for leaving that out is that this is an area that I\ndon't know about.\n\nPerlis:  Probably it would be one of the easiest areas, and one with the\nmost customers.  Before d'Agapeyeff talks I have another comment.\n[Laughter].  Specialists in every part of software have a curious vision of\nthe world:  All parts of software but his are simple and easily\nparameterized; his is totally variable.\n\nd'Agapeyeff:  There is no package which has received more attention from\nmanufacturers than file handling.  Yet there is hardly a major system that\nI know of that is relying solely on the standard system produced by the\nmanufacturer.  It is extremely difficult to construct this software in a\nway that is efficient, reliable, and convenient for all systems and where\nthe nature of the package does not impose itself upon the user.  The reason\nis that you cannot atomize it.  Where work has been successful it tends to\nbe concerned with packages that have some structure.  When you get down to\nsmall units it is not economic to make them applicable to a large set of\nusers, using different machines with different languages, and to do all the\nbinding work, such that it doesn't take twice as long to find out how to\nload it.  The problems with Sysgen are not to be dispensed with, they are\ninherent.  But why do we need to take atoms down from the shelf?  What you\nwant is a description which you can understand, because the time taken to\ncode it into your own system is really very small.  In that way you can\ninsert your own nuances.  The first step in your direction should be better\ndescriptions.\n\nEndres:  Two notes of caution:  You discarded the algorithms in the Comm.\nACM in part because they are written in high-level language, so I\nunderstand that you refer to routines written in a more machine oriented\nlanguage.  I think you oversimplify the problem of transliteration.  Or do\nyou assume a de facto machine standard?  Second question:  You refer to the\nproblems of Sysgen, where you cut out pieces from a large collection.  If\ninstead you want to put together systems, I think the problems of Sysgen\nbecome a dimension larger.  Who will bear this cost, and maintain the\nsystem?\n\nMcIlroy:  The algorithms in the Comm. ACM effectively use one language,\nwhich is suitable for a particular class of applications.  This may not be\nthe right one for things like input/output packages.  On the second\nquestion:  I am convinced, with you, that at first it will be harder to\nbuild systems by accretion, rather than by excision.  The people who build\ncomponents will have to be skilled systems builders, not run of the mill\nusers.\n\nKjeldaas:  I strongly favor this idea.  I think the examples mentioned are\nwithin the state of the art.  However, later we will want macros needing\nparameters having more intricate relations, for instance if you want some\nfunctional relationship between the parameters.  We will need some language\nfor describing the parameters.  Another point:  documentation can also be\nincluded in this.  When you have given the parameters to the program, you\ncan give the same parameters to the documentation, and the documentation\nfor the particular use can be produced automatically.  Catering for\ndifferent machines will raise big problems, needing research.\n\nKolence:  May I stress one point:  Mcllroy stated that the\nindustrialization is concerned with the design, not the replication\nprocess.  We are concerned with a mass design problem.  In talking about\nthe implementation of software components, the whole concept of how one\ndesigns software is ignored.  Yet this is a key thing.\n\nNaur:  What I like about this is the stress on basic building principles,\nand on the fact that big systems are made from smaller components.  This\nhas a strong bearing on education.  What we want in education, particularly\nat the more elementary level, is to start indoctrinating the knowledge of\nthe components of our systems.  A comparison with our hardware colleagues\nis relevant.  Why are they so much more successful than we are?  I believe\nthat one strong reason is that there is a well established field of\nelectronic engineering, that the young people start learning about Ohm's\nLaw at the age of fourteen or thereabouts, and that resistors and the like\nare known components with characteristics which have been expounded at\nlength at the early level of education.  The component principles of our\nsystems must be sorted out in such a form that they can be put into\nelementary education.\n\nGill:  Two points:  first on the catalogue question.  I hope we can do\nbetter than the Sears-Roebuck catalogue.  Surely what we want is a\ncomputerized conversational catalogue.  Second point:  what is it that you\nactually sell when you sell a piece of software, what exactly does a\nsoftware contract look like?\n\nBarton:  Mcllroy's talk was so well done that it took me about three\nminutes to realize what is wrong with this idea.  Another compliment:  If I\nwere running Intergalactic Software, I would hire Mcllroy for a manager.\nNow the serious point:  Over the last few years I have taught the ACM\nCourse `Information Structures' and used the game not to let anyone code or\nwrite anything in any programming language at all.  We have just thought\nabout data representations.  If in this way you get people over the habit\nof writing code right away, of thinking procedurally, then some very\ndifferent views on information representations come to view.  In McIlroy's\ntalk about standard components having to do with data structures I have the\nfeeling that this is not a problem to take out of the universities yet.\nNow a heretical view:  I don't think we have softened up enough things in\nmachines yet.  I don't think we will get anywhere trying to quantify the\nspace-time trade-off unless we discard fixed word sizes, fixed character\nsizes, fixed numerical representations, altogether in machines.  Without\nthese, the thing proposed by Mcllroy will prove to be just not quite\npractical.\n\nFraser:  I wish to take issue with d'Agapeyeff.  I think it will be\npossible to parameterize data representation and file management.  From a\nparticular file system experience I learned two lessons:  first, there are\na large number of parameters, to be selected in a non-mutually-exclusive\nmanner.  The selection of the parameters is so complicated that it is\nappropriate to put a compiler on the front end of the software distribution\nmechanism.  Perhaps we are talking more about compilers than we realize.\nConcerning catalogues:  in England a catalogue of building materials is a\nvery ad hoc catalogue, you have left hand flanges to go with left hand\ngates, etc.  I think the catalogue is likely to be ad hoc in that nature,\nrather than like an electronics catalogue where the components are more\ninterchangeable.\n\n        The second issue is the question of writing this compiler.  Our\nfile management generator effectively would generate a large number of\ndifferent file management systems, very considerably in excess of the 500\nthat Mcllroy mentioned.  There was no question of testing all of these.  We\nproduced an ad hoc solution to this problem, but until more research is\ndone on this problem I don't think McIlroy's suggestion is realistic.\n\nGraham:  I will speak of an adjunct to this idea.  In Multics we used a\nsubset of PL/I, although PL/I is quite inadequate, in that the primitive\noperations of the language are not really suited for system design.  In\nMultics you do a lot of directory management, simple operations like adding\nand deleting entries, but in a complicated directory.  With a higher-level\nlanguage with these operations as primitives one could easily write a new\nsystem.  By simulating the primitives one could test the performance of the\nsystem before actually building it.  If one had Mcllroy's catalogue stored\nin the system, with the timings of a lot of routines, then the simulation\nbacking up this higher-level language could in fact refer to the catalogue\nand use the actual timings for a new machine that this company offered and\nget realistic timings.  Another point, I wish to rebutt McIlroy's suggestion\nthat this is not for universities; I think it is.  There are very difficult\nproblems in this area, such as parameterizing more sophisticated routines,\nin particular those in the compiler area.  These are fit for universities.\n\nBemer:  I agree that the catalogue method is not a suitable one.  We don't\nhave the descriptors to go searching.  There is nothing so poorly described\nas data formats, there are no standards, and no sign that they are being\ndeveloped.  Before we have these we won't have the components.\n\nMcIlroy:  It is for that reason that I suggest the Sears-Roebuck type now.\nOn-line searching may not be the right answer yet.\n", "encoding": "ascii"}