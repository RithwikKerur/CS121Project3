{"url": "https://www.ics.uci.edu/~pattis/ICS-46/lectures/notes/speedup.txt", "content": "\t\t\"Small\" Efficiency Improvement for HashEquivalence\r\n\r\nIn this lecture we will move away from looking at complexity classes for the\r\nEquivalence class and focus on various way to improve (the constant for)\r\nperformance.\r\n\r\n0) I timed my solution with the standard compress_to_root algorithm for\r\n   N = 200,000 and merge_factor = 5. I did this five times, with the following\r\n   results \r\n\r\n   1.51; 1.498; 1.498; 1.482; 1.498 : average = 1.498\r\n\r\n   Note that the # of remaining classes was 8-17, and the MaxHeight was 2.\r\n\r\n1) Next, I pre-allocated compress_set as private instance variable. In this way\r\n   its constructor/destructor is called just once -when the Equivalence  is\r\n   constructed/destructed- and not once each time compress_to_root is called.\r\n   But note that it must be cleared before return, so that the next time\r\n   compress_to_root is called it is empty.\r\n\r\n   Edits\r\n     Remove: local compress_set declaration\r\n     Add   : ics::HashSet<T> compress_set; //in private\r\n     Add   : ..., compress_set(1,thash)    //in constructor\r\n     Add   : compress_set.clear();         //at end of compress_to_root\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .936; .936; .952; .936;  .936: average .939 = 62.6% of previous\r\n\r\n2) Next, I changed the HashSet to an ArraySet for compress_set, because I\r\n   expect the set size to be small, and for small sets, the simpler\r\n   implementation is likely to be faster. You will find the average size of\r\n   this set in Quiz #8 and see it is small.\r\n\r\n   Edits   \r\n     Add   : #include \"array_set.hpp\"        //at top; HashSet still used too\r\n     Change: ics::ArraySet<T> compress_set;  //in private\r\n     Change: ..., compress_set(5)            //in constructor: preallocate 5\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .515; .499; .484; .514; .499: average .502 = 53.5% of previous\r\n                                                33.5% of original\r\n\r\n3) Next, I changed the ArraySet to an ArrayQueue: all I do with the values is\r\n   iterate over them and the values are known to not be duplicates, so a set,\r\n   which checks for duplicates, is wasting time. Note, I left the name as\r\n   compress_set even though I should rename it compress_queue.\r\n\r\n   Edits   \r\n     Change: #include \"array_queue.hpp\"\t\t\t\t//at top\r\n     Change: ics::ArrayQueue<T> compress_set;\t\t\t//in private\r\n     change: compress_set.insert(a) -> compress_set.enqueue(a)  //compress_to_root\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .530; .515; .546; .530; .530: average .530 = 100.6% of previous\r\n\r\n   Although this implementation is slower, we can continue simplifying queues\r\n   in a way that we could not simplify sets.\r\n\r\n4) Next, I replaced the iterator by a loop dequeuing values (and using those\r\n   dequeued values). Becaue the ArrayQueue is empty after dequeuing all values,\r\n   I don't have to clear it at the end of compress_to_set.\r\n\r\n   Edits   \r\n   Replace: while (!compress_set.empty())  //last for loop in compress_to_root\r\n              parent[compress_set.dequeue()] = to_root;\r\n   Remove: compress_set.clear();\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .453; .468; .452; .452; .468: average .459 = 91.4% of time after 2 (not 3)\r\n                                                30.6% of original\r\n\r\n5) Next, I replaced the Queue by a Stack, whose push/pop operations are\r\n   simple/faster than enqueue/dequeue\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .453; .453; .452; .437; .437: average .446 = 97.2% of previous\r\n                                                29.7% of original\r\n\r\n6) Next, I removed the compress_set data structure all together, by using\r\n   two loops on the parent map: once to find the root, once to make each node\r\n   refer to the root.\r\n\r\n   Edits:\r\n     Remove: #include \"array_stack_.hpp\" and all references to compress_set\r\n     Replace body of compress_to_root by the following code\r\n     \r\n     T a_original = a;\r\n     T a_root;\r\n     while ( (a_root = parent[a]) != a )\r\n       a = a_root;\r\n\r\n     a = a_original;\r\n     while (a != a_root) {\r\n       T next = parent[a];\r\n       parent[a] = a_root;\r\n       a = next;\r\n     }\r\n\r\n     return a_root;\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .468; .468; .453; .468; .468: average .465 = 104% of previous\r\n                                                31.2% of original\r\n\r\n   This implementation is slower, so I discarded it and went back to\r\n   implementation 5.\r\n\r\n7) Next, I removed the precondition check in this private helper method: the\r\n   two public methods compress_to_root is called in (in_same_class and \r\n   merge_classes_of) already do this precondition check so it is redundant.\r\n\r\n   Edits\r\n     Remove (or comment out) precondition check\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .421; .421; .436; .406; .405: average .418 = 93.7% of step 5 (not previous)\r\n                                                27.9% of original\r\n\r\n8) Next, I added back the precondition check but replaced the looping code with\r\n   a simple recursive solution. Note\r\n\r\n   Edits\r\n     Replace: the body with the following code\r\n\r\n     T a_parent = parent[a];\r\n     if (a_parent == a)\r\n       return a;\r\n\r\n     a_parent = compress_to_root(a_parent);\r\n     parent[a] = a_parent;\r\n\r\n     return a_parent;\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .453; .453; .452; .437; .468: average .453 = 108% of previous\r\n\r\n   So, this code is a bit faster than the looping code that included the same\r\n   precondition check. As you might expect, adding back the precondition check\r\n   slows down this code, because the recursive call performs the precondition\r\n   check more than once: it performs it on each recursive call (and recursive\r\n   calls are guaranteed to pass the tests); see below for a net improvement\r\n   once the precondition checking code is removed again.\r\n\r\n9) Next, I removed the precondition check just as I did in 7. Note that with\r\n   recursive calls doing this precondition each time, we expect an even better\r\n   performance improvement, because the precondition check is performed on each\r\n   (recursive) call.\r\n\r\n   Edits\r\n     Remove (or comment out) precondition check\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .405; .390; .406; .406; .390: average .399 = 95.5% of time after 7 (not 8)\r\n                                                26.6% of original\r\n\r\n   So in this case (once the precondition check is removed) the recursive code\r\n   runs faster than the iterative code.\r\n\r\n10) Next, I used the property of erase (returning its old associated value) in\r\n   merge_classes_of to simplify the manipulation of root_size map\r\n\r\n   Edits\r\n     Replace (in merge_classes)\r\n       if (root_size[a_root] < root_size[b_root]) {\r\n         parent[a_root] = b_root;\r\n         root_size[b_root] = root_size.erase(a_root)+root_size[b_root];\r\n      }else{\r\n        parent[b_root] = a_root;\r\n        root_size[a_root] = root_size[a_root]+root_size.erase(b_root);\r\n      }\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .490; .406; .406; .390; .390: average .396 = 99.2% of previous\r\n                                                26.4% of original\r\n\r\n11) Next, I used += in the code above to increment the root_size maps, which\r\n    avoids doing two [] operators for the same root\r\n\r\n   Edits\r\n   Replace (in merge_classes)\r\n     if (root_size[a_root] < root_size[b_root]) {\r\n       parent[a_root] = b_root;\r\n       root_size[b_root] += root_size.erase(a_root);\r\n     }else{\r\n       parent[b_root] = a_root;\r\n      root_size[a_root] += root_size.erase(b_root);\r\n    }\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .390; .390; .390; .390; .390: average .390 = 98.5% of previous\r\n                                                26.0% of original\r\n\r\n12) Finally, I removed the precondition checks in public functions: caution!\r\n    I will talk in class about checking preconditions a bit more.\r\n\r\n   I timed this compress_to_root algorithm 5 times\r\n   .390; .406; .3390; .406; .390: average .396 = 1.02% of previous\r\n\r\n  This is strange, beecause the execution time seemed to go up even though\r\n  code was removed!\r\n\r\nGoing back to implementation 11, the code (while still in the same complexity\r\nclass) uses just 26.0% of the time of the original code that I wrote, running\r\nover 3 times faster. Although we have focused on complexity class analysis of\r\ncode this quarter, there are many interesting tricks (understandable at the\r\nlevel of data structures) that we can perform to improve the performance of our\r\ncode. Although recent optimizations didn't gain much efficiency.\r\n\r\nAs a reminder, using Array_Equivalence would take about 40 minutes to solve the\r\nproblem once (2,370 seconds), which Hash_Equivalence (with the changes made\r\nhere) now solves in .39 seconds.\r\n\r\n------------------------------------------------------------------------------\r\n\r\nAdded Fall 15:\r\n\r\nI can think of one other interesting change to make, but it is harder than the\r\nones already made, and requires a new data structure. Imagine that we stored\r\njust one Map (call it lookup): from T -> pointer to a node. Each node would\r\ncontain 3 values: the value for T, a pointer to its parent (another node - or\r\nthe same node for roots), and its size (which is stored everywhere, but is\r\nrelevant only for node representing roots).\r\n\r\nNow, to do a compression, we would use the lookup map to find the node, then\r\nfollow the parent pointers up to its root (not using any map to find the root).\r\nNow, to go from a node to its parent does not require looking in a map: it\r\nrequires only following a pointer. In fact, we would change the parameter of\r\nthe helper method compress_to_root to not take a value of type T, but to take a\r\npointer to the node that represents T (using the lookeup map, in the public\r\nmethods that are passed a value for T and then call compress_to_root).\r\n\r\nI conjecture that this might cut the time by another reasonable factor (based on\r\ndoing fewer map look-ups: even with maps implemented by hash tables, that is\r\nmuch slower than just following a pointer.\r\n", "encoding": "ascii"}