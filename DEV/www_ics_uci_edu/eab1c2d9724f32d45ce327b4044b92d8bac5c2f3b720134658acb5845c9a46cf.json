{"url": "https://www.ics.uci.edu/~eppstein/161/960312.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>NP-Completeness</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for March 12, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>NP-Completeness</h1>\n\nSo far we've seen a lot of good news: such-and-such a problem can\nbe solved quickly (in close to linear time, or at least a time that\nis some small polynomial function of the input size). \n\n<p>NP-completeness is a form of bad news: evidence that many\nimportant problems can't be solved quickly.</p>\n\n<h2>Why should we care?</h2>\n\n<p>These NP-complete problems really come up all the time. Knowing\nthey're hard lets you stop beating your head against a wall trying\nto solve them, and do something better:</p>\n\n<ul>\n<li>Use a heuristic. If you can't quickly solve the problem with a\ngood worst case time, maybe you can come up with a method for\nsolving a reasonable fraction of the common cases.</li>\n\n<li>Solve the problem approximately instead of exactly. A lot of\nthe time it is possible to come up with a provably fast algorithm,\nthat doesn't solve the problem exactly but comes up with a solution\nyou can prove is close to right.</li>\n\n<li>Use an exponential time solution anyway. If you really have to\nsolve the problem exactly, you can settle down to writing an\nexponential time algorithm and stop worrying about finding a better\nsolution.</li>\n\n<li>Choose a better abstraction. The NP-complete abstract problem\nyou're trying to solve presumably comes from ignoring some of the\nseemingly unimportant details of a more complicated real world\nproblem. Perhaps some of those details shouldn't have been ignored,\nand make the difference between what you can and can't solve.</li>\n</ul>\n\n<h2>Classification of problems</h2>\n\nThe subject of <i>computational complexity theory</i> is dedicated\nto classifying problems by how hard they are. There are many\ndifferent classifications; some of the most common and useful are\nthe following. (One technical point: these are all really defined\nin terms of yes-or-no problems -- does a certain structure exist\nrather than how do I find the structure.) \n\n<ul>\n<li><b>P</b>. Problems that can be solved in polynomial time. (\"P\"\nstands for polynomial.) These problems have formed the main\nmaterial of this course.</li>\n\n<li><b>NP</b>. This stands for \"nondeterministic polynomial time\"\nwhere nondeterministic is just a fancy way of talking about\nguessing a solution. A problem is in NP if you can quickly (in\npolynomial time) test whether a solution is correct (without\nworrying about how hard it might be to find the solution). Problems\nin NP are still relatively easy: if only we could guess the right\nsolution, we could then quickly test it.</li>\n</ul>\n\n<p><i>NP does not stand for \"non-polynomial\"</i>. There are many\ncomplexity classes that are much harder than NP.</p>\n\n<ul>\n<li><b>PSPACE</b>. Problems that can be solved using a reasonable\namount of memory (again defined formally as a polynomial in the\ninput size) without regard to how much time the solution\ntakes.</li>\n\n<li><b>EXPTIME</b>. Problems that can be solved in exponential\ntime. This class contains most problems you are likely to run into,\nincluding everything in the previous three classes. It may be\nsurprising that this class is not all-inclusive: there are problems\nfor which the best algorithms take even more than exponential\ntime.</li>\n\n<li><b>Undecidable</b>. For some problems, we can prove that there\nis no algorithm that always solves them, no matter how much time or\nspace is allowed. One very uninformative proof of this is based on\nthe fact that there are as many problems as there real numbers, and\nonly as many programs as there are integers, so there are not\nenough programs to solve all the problems. But we can also define\nexplicit and useful problems which can't be solved.</li>\n</ul>\n\nAlthough defined theoretically, many of these classes have\npractical implications. For instance P is a very good approximation\nto the class of problems which can be solved quickly in practice --\nusually if this is true, we can prove a polynomial worst case time\nbound, and conversely the polynomial time bounds we can prove are\nusually small enough that the corresponding algorithms really are\npractical. NP-completeness theory is concerned with the distinction\nbetween the first two classes, P and NP. \n\n<h2>Examples of problems in different classes</h2>\n\n<a name=\"x1\"><b>Example 1: Long simple paths</b></a>. \n\n<p>A <i>simple path</i> in a graph is just one without any repeated\nedges or vertices. To describe the problem of finding long paths in\nterms of complexity theory, we need to formalize it as a yes-or-no\nquestion: given a graph G, vertices s and t, and a number k, does\nthere exist a simple path from s to t with at least k edges? A\nsolution to this problem would then consist of such a path.</p>\n\n<p>Why is this in NP? If you're given a path, you can quickly look\nat it and add up the length, double-checking that it really is a\npath with length at least k. This can all be done in linear time,\nso certainly it can be done in polynomial time.</p>\n\n<p>However we don't know whether this problem is in P; I haven't\ntold you a good way for finding such a path (with time polynomial\nin m,n, and K). And in fact this problem is NP-complete, so we\nbelieve that no such algorithm exists.</p>\n\n<p>There are algorithms that solve the problem; for instance, list\nall 2^m subsets of edges and check whether any of them solves the\nproblem. But as far as we know there is no algorithm that runs in\npolynomial time.</p>\n\n<p> <a name=\"x2\"><b>Example 2: Cryptography</b></a>.</p>\n\n<p>Suppose we have an encryption function e.g.</p>\n\n<pre>\n    code=RSA(key,text)\n</pre>\n\nThe \"RSA\" encryption works by performing some simple integer\narithmetic on the code and the key, which consists of a pair (p,q)\nof large prime numbers. One can perform the encryption only knowing\nthe product pq; but to decrypt the code you instead need to know a\ndifferent product, (p-1)(q-1). \n\n<p>A standard assumption in cryptography is the \"known plaintext\nattack\": we have the code for some message, and we know (or can\nguess) the text of that message. We want to use that information to\ndiscover the key, so we can decrypt other messages sent using the\nsame key.</p>\n\n<p>Formalized as an NP problem, we simply want to find a key for\nwhich code=RSA(key,text). If you're given a key, you can test it by\ndoing the encryption yourself, so this is in NP.</p>\n\n<p>The hard question is, how do you find the key? For the code to\nbe strong we hope it isn't possible to do much better than a brute\nforce search.</p>\n\n<p>Another common use of RSA involves \"public key cryptography\": a\nuser of the system publishes the product pq, but doesn't publish p,\nq, or (p-1)(q-1). That way anyone can send a message to that user\nby using the RSA encryption, but only the user can decrypt it.\nBreaking this scheme can also be thought of as a different NP\nproblem: given a composite number pq, find a factorization into\nsmaller numbers.</p>\n\n<p>One can test a factorization quickly (just multiply the factors\nback together again), so the problem is in NP. Finding a\nfactorization seems to be difficult, and we think it may not be in\nP. However there is some strong evidence that it is not NP-complete\neither; it seems to be one of the (very rare) examples of problems\nbetween P and NP-complete in difficulty.</p>\n\n<p> <a name=\"x3\"><b>Example 3: Chess</b></a>.</p>\n\n<p>We've seen in the news recently a match between the world chess\nchampion, Gary Kasparov, and a very fast chess computer, Deep Blue.\nThe computer lost the match, but won one game and tied others.</p>\n\n<p>What is involved in chess programming? Essentially the sequences\nof possible moves form a tree: The first player has a choice of 20\ndifferent moves (most of which are not very good), after each of\nwhich the second player has a choice of many responses, and so on.\nChess playing programs work by traversing this tree finding what\nthe possible consequences would be of each different move.</p>\n\n<p>The tree of moves is not very deep -- a typical chess game might\nlast 40 moves, and it is rare for one to reach 200 moves. Since\neach move involves a step by each player, there are at most 400\npositions involved in most games. If we traversed the tree of chess\npositions only to that depth, we would only need enough memory to\nstore the 400 positions on a single path at a time. This much\nmemory is easily available on the smallest computers you are likely\nto use.</p>\n\n<p>So perfect chess playing is a problem in PSPACE. (Actually one\nmust be more careful in definitions. There is only a finite number\nof positions in chess, so in principle you could write down the\nsolution in constant time. But that constant would be very large.\nGeneralized versions of chess on larger boards are in PSPACE.)</p>\n\n<p>The reason this deep game-tree search method can't be used in\npractice is that the tree of moves is very bushy, so that even\nthough it is not deep it has an enormous number of vertices. We\nwon't run out of space if we try to traverse it, but we will run\nout of time before we get even a small fraction of the way through.\nSome pruning methods, notably \"alpha-beta search\" can help reduce\nthe portion of the tree that needs to be examined, but not enough\nto solve this difficulty. For this reason, actual chess programs\ninstead only search a much smaller depth (such as up to 7 moves),\nat which point they don't have enough information to evaluate the\ntrue consequences of the moves and are forced to guess by using\nheuristic \"evaluation functions\" that measure simple quantities\nsuch as the total number of pieces left.</p>\n\n<p> <a name=\"x4\"><b>Example 4: Knots</b></a>.</p>\n\n<p>If I give you a three-dimensional polygon (e.g. as a sequence of\nvertex coordinate triples), is there some way of twisting and\nbending the polygon around until it becomes flat? Or is it\nknotted?</p>\n\n<p>There is an algorithm for solving this problem, which is very\ncomplicated and has not really been adequately analyzed. However it\nruns in at least exponential time.</p>\n\n<p>One way of proving that certain polygons are not knots is to\nfind a collection of triangles forming a surface with the polygon\nas its boundary. However this is not always possible (without\nadding exponentially many new vertices) and even when possible <a\nhref=\"http://www.ics.uci.edu/~eppstein/pubs/p-3poly.html\">it's\nNP-complete to find these triangles</a>.</p>\n\n<p>There are also some heuristics <a href=\"hyperbolic\">based on\nfinding a non-Euclidean geometry for the space outside of a\nknot</a> that work very well for many knots, but are not known to\nwork for all knots. So this is one of the rare examples of a\nproblem that can often be solved efficiently in practice even\nthough it is theoretically not known to be in P.</p>\n\n<p>Certain related problems in higher dimensions (is this\nfour-dimensional surface equivalent to a four-dimensional sphere)\nare provably undecidable.</p>\n\n<p> <a name=\"x5\"><b>Example 5: Halting problem</b></a>.</p>\n\n<p>Suppose you're working on a lab for a programming class, have\nwritten your program, and start to run it. After five minutes, it\nis still going. Does this mean it's in an infinite loop, or is it\njust slow?</p>\n\n<p>It would be convenient if your compiler could tell you that your\nprogram has an infinite loop. However this is an undecidable\nproblem: there is no program that will always correctly detect\ninfinite loops.</p>\n\n<p>Some people have used this idea as evidence that people are\ninherently smarter than computers, since it shows that there are\nproblems computers can't solve. However it's not clear to me that\npeople can solve them either. Here's an example:</p>\n\n<pre>\n    main()\n    {\n    int x = 3;\n    for (;;) {\n        for (int a = 1; a &lt;= x; a++)\n        for (int b = 1; b &lt;= x; b++)\n            for (int c = 1; c &lt;= x; c++)\n            for (int i = 3; i &lt;= x; i++)\n                if(pow(a,i) + pow(b,i) == pow(c,i))\n                exit;\n        x++;\n    }\n    }\n</pre>\n\nThis program searches for solutions to Fermat's last theorem. Does\nit halt? (You can assume I'm using a multiple-precision integer\npackage instead of built in integers, so don't worry about\narithmetic overflow complications.) To be able to answer this, you\nhave to understand the recent proof of Fermat's last theorem. There\nare many similar problems for which no proof is known, so we are\nclueless whether the corresponding programs halt. \n\n<h2>Problems of complexity theory</h2>\n\nThe most famous open problem in theoretical science is whether P =\nNP. In other words, if it's always easy to check a solution, should\nit also be easy to find the solution? \n\n<p>We have no reason to believe it should be true, so the\nexpectation among most theoreticians is that it's false. But we\nalso don't have a proof...</p>\n\n<p>So we have this nice construction of complexity classes P and NP\nbut we can't even say that there's one problem in NP and not in P.\nSo what good is the theory if it can't tell us how hard any\nparticular problem is to solve?</p>\n\n<h2>NP-completeness</h2>\n\nThe theory of NP-completeness is a solution to the practical\nproblem of applying complexity theory to individual problems.\nNP-complete problems are defined in a precise sense as the hardest\nproblems in P. Even though we don't know whether there is any\nproblem in NP that is not in P, we can point to an NP-complete\nproblem and say that if there are any hard problems in NP, that\nproblems is one of the hard ones. \n\n<p>(Conversely if everything in NP is easy, those problems are\neasy. So NP-completeness can be thought of as a way of making the\nbig P=NP question equivalent to smaller questions about the\nhardness of individual problems.)</p>\n\n<p>So if we believe that P and NP are unequal, and we prove that\nsome problem is NP-complete, we should believe that it doesn't have\na fast algorithm.</p>\n\n<p>For unknown reasons, most problems we've looked at in NP turn\nout either to be in P or NP-complete. So the theory of\nNP-completeness turns out to be a good way of showing that a\nproblem is likely to be hard, because it applies to a lot of\nproblems. But there are problems that are in NP, not known to be in\nP, and not likely to be NP-complete; for instance the code-breaking\nexample I gave earlier.</p>\n\n<h2>Reduction</h2>\n\nFormally, NP-completeness is defined in terms of \"reduction\" which\nis just a complicated way of saying one problem is easier than\nanother. \n\n<p>We say that A is easier than B, and write A &lt; B, if we can\nwrite down an algorithm for solving A that uses a small number of\ncalls to a subroutine for B (with everything outside the subroutine\ncalls being fast, polynomial time). There are several minor\nvariations of this definition depending on the detailed meaning of\n\"small\" -- it may be a polynomial number of calls, a fixed constant\nnumber, or just one call.</p>\n\n<p>Then if A &lt; B, and B is in P, so is A: we can write down a\npolynomial algorithm for A by expanding the subroutine calls to use\nthe fast algorithm for B.</p>\n\n<p>So \"easier\" in this context means that if one problem can be\nsolved in polynomial time, so can the other. It is possible for the\nalgorithms for A to be slower than those for B, even though A &lt;\nB.</p>\n\n<p>As an example, consider the Hamiltonian cycle problem. Does a\ngiven graph have a cycle visiting each vertex exactly once? Here's\na solution, using longest path as a subroutine:</p>\n\n<pre>\n    for each edge (u,v) of G\n    if there is a simple path of length n-1 from u to v\n        return yes      // path + edge form a cycle\n    return no\n</pre>\n\nThis algorithm makes m calls to a longest path subroutine, and does\nO(m) work outside those subroutine calls, so it shows that\nHamiltonian cycle &lt; longest path. (It doesn't show that\nHamiltonian cycle is in P, because we don't know how to solve the\nlongest path subproblems quickly.) \n\n<p>As a second example, consider a polynomial time problem such as\nthe minimum spanning tree. Then for every other problem B, B &lt;\nminimum spanning tree, since there is a fast algorithm for minimum\nspanning trees using a subroutine for B. (We don't actually have to\ncall the subroutine, or we can call it and ignore its results.) <a\nname=\"cook\"></a></p>\n\n<h2>Cook's Theorem</h2>\n\nWe are now ready to formally define NP-completeness. We say that a\nproblem A in NP is NP-complete when, for every other problem B in\nNP, B &lt; A. \n\n<p>This seems like a very strong definition. After all, the notion\nof reduction we've defined above seems to imply that if B &lt; A,\nthen the two problems are very closely related; for instance\nHamiltonian cycle and longest path are both about finding very\nsimilar structures in graphs. Why should there be a problem that\nclosely related to all the different problems in NP?</p>\n\n<p>Theorem: an NP-complete problem exists.</p>\n\n<p>We prove this by example. One NP-complete problem can be found\nby modifying the halting problem (which without modification is\nundecidable).</p>\n\n<blockquote><b>Bounded halting</b>. This problem takes as input a\nprogram X and a number K. The problem is to find data which, when\ngiven as input to X, causes it to stop in at most K\nsteps.</blockquote>\n\n<p>To be precise, this needs some more careful definition: what\nlanguage is X written in? What constitutes a single step? Also for\ntechnical reasons K should be specified in <i>unary</i> notation,\nso that the length of that part of the input is K itself rather\nthan O(log K).</p>\n\n<p>For reasonable ways of filling in the details, this is in NP: to\ntest if data is a correct solution, just simulate the program for K\nsteps. This takes time polynomial in K and in the length of\nprogram. (Here's one point at which we need to be careful: the\nprogram can not perform unreasonable operations such as arithmetic\non very large integers, because then we wouldn't be able to\nsimulate it quickly enough.)</p>\n\n<p>To finish the proof that this is NP-complete, we need to show\nthat it's harder than anything else in NP. Suppose we have a\nproblem A in NP. This means that we can write a program PA that\ntests solutions to A, and halts within polynomial time p(n) with a\nyes or no answer depending on whether the given solution is really\na solution to the given problem. We can then easily form a modified\nprogram PA' to enter an infinite loop whenever it would halt with a\nno answer. If we could solve bounded halting, we could solve A by\npassing PA' and p(n) as arguments to a subroutine for bounded\nhalting. So A &lt; bounded halting. But this argument works for\nevery problem in NP, so bounded halting is NP-complete.</p>\n\n<h2>How to prove NP-completeness in practice</h2>\n\nThe proof above of NP-completeness for bounded halting is great for\nthe theory of NP-completeness, but doesn't help us understand other\nmore abstract problems such as the Hamiltonian cycle problem. \n\n<p>Most proofs of NP-completeness don't look like the one above; it\nwould be too difficult to prove anything else that way. Instead,\nthey are based on the observation that if A &lt; B and B &lt; C,\nthen A &lt; C. (Recall that these relations are defined in terms of\nthe existence of an algorithm that calls subroutines. Given an\nalgorithm that solves A with a subroutine for B, and an algorithm\nthat solves B with a subroutine for C, we can just use the second\nalgorithm to expand the subroutine calls of the first algorithm,\nand get an algorithm that solves A with a subroutine for C.)</p>\n\n<p>As a consequence of this observation, if A is NP-complete, B is\nin NP, and A &lt; B, B is NP-complete. In practice that's how we\nprove NP-completeness: We start with one specific problem that we\nprove NP-complete, and we then prove that it's easier than lots of\nothers which must therefore also be NP-complete.</p>\n\n<p>So e.g. since Hamiltonian cycle is known to be NP-complete, and\nHamiltonian cycle &lt; longest path, we can deduce that longest\npath is also NP-complete.</p>\n\n<p>Starting from the bounded halting problem we can show that it's\nreducible to a problem of simulating circuits (we know that\ncomputers can be built out of circuits, so any problem involving\nsimulating computers can be translated to one about simulating\ncircuits). So various circuit simulation problems are NP-complete,\nin particular Satisfiability, which asks whether there is an input\nto a Boolean circuit that causes its output to be one.</p>\n\n<p>Circuits look a lot like graphs, so from there it's another easy\nstep to proving that many graph problems are NP-complete. Most of\nthese proofs rely on constructing <i>gadgets</i>, small subgraphs\nthat act (in the context of the graph problem under consideration)\nlike Boolean gates and other components of circuits.</p>\n\n<p>There are many problems already known to be NP-complete, and\nlisted in the bible of the subject:</p>\n\n<blockquote>Computers and Intractibility:<br>\n A guide to the theory of NP-completeness<br>\n Michael R. Garey and David S. Johnson<br>\n W. H. Freeman, 1979.</blockquote>\n\nIf you suspect a problem you're looking at is NP-complete, the\nfirst step is to look for it in Garey and Johnson. The second step\nis to find as similar a problem as you can in Garey and Johnson,\nand prove a reduction showing that similar problem to be easier\nthan the one you want to solve. If neither of these works, you\ncould always go back to the methods described in the rest of this\nclass, and try to find an efficient algorithm... \n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960312.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}