{"url": "https://www.ics.uci.edu/~ihler/papers/abs.html#ssp05", "content": "<html>\r\n<head>\r\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/~ihler/ihler.css\">\r\n<title>Alexander Ihler: Publication abstracts</title>\r\n</head>\r\n\r\n<body>\r\n<div id=\"navleft\">\r\n<CENTER>\r\n<img src=\"/~ihler/ati04.jpg\" ><br>\r\n   <H2>Alexander Ihler </H2>\r\n<p>Associate Professor</p>\r\n<p>Information & Computer Science, UC Irvine</p><br>\r\n<H3>\r\n  Bren Hall 4066<br>\r\n  ph: 949-824-3645<br>\r\n  fx: 949-824-4056<br>\r\n  ihler (at) ics.uci.edu /<br>\r\n  ihler (at) alum.mit.edu<br>\r\n</H3>\r\n</CENTER>\r\n<HR noShade>\r\n<p>\r\n<a href=\"/~ihler/\">Home</a> <br>\r\n<a href=\"/~ihler/pubs.php\">Publications</a> <br>\r\n<a href=\"/~ihler/code/\">Code</a> <br>\r\nRecent Classes<br>\r\n&nbsp&nbsp<a href=\"https://canvas.eee.uci.edu/courses/12835/\">CS178, ML & Data Mining</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/Classes\">... archive of older offerings</a> <br>\r\n<!--\r\n&nbsp&nbsp<a href=\"https://canvas.eee.uci.edu/courses/3226/assignments/syllabus\">CS171, Intro to AI</a> <br>\r\n&nbsp&nbsp<a href=\"https://canvas.eee.uci.edu/courses/3287/assignments/syllabus\">CS273a, Intro to ML</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2015F-179\">CS179, Graphical Models</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2015W-178\">CS178, Machine Learning</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2015W-273a\">CS273a, Machine Learning</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2011F-171\">CS171, Intro to AI</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2011S-271\">CS271, Intro to AI</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2011S-274a\">CS274A, Prob. Learning</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2010S-295\">CS295 Spring '10</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/Classes/2010W-274a\">CS274A, Prob. Learning</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/pmwiki/Classes/2008S\">CS274A Spring'08</a> <br>\r\n&nbsp&nbsp<a href=\"http://sli.ics.uci.edu/pmwiki/Classes/2008W\">CS295 Winter'08</a> <br>\r\n-->\r\n<a href=\"http://sli.ics.uci.edu/\">Group Wiki</a> <br>\r\n<br>\r\n<a href=\"/~ihler/bio.php\">Bio</a> and \r\n<!-- <a href=\"/~ihler/resume.shtml\">Resume</a> (<a href=\"/~ihler/resume.pdf\">PDF</a>) <br> -->\r\n<a href=\"/~ihler/ihler-cv.pdf\">CV</a> <br>\r\n<br>\r\n<a href=\"/~ihler/personal\">Personal</a> <br>\r\n<!--<a href=\"/~ihler/links.php\">Links</a> <br> -->\r\n<a href=\"http://sli.ics.uci.edu/Ihler-Photos/Main\">Photos</a> <br>\r\n</p>\r\n<hr noShade>\r\n<a href=\"http://www.sharynmuzik.com/\" style=\"color:#CCCCCC\">Piano</a>\r\n</div>\r\n<div class=\"content\">\r\n\r\n\r\n<h1>Abstracts</h1>\r\nLoaded<a name=nips99></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning Informative Statistics: A Nonparametric Approach</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Fisher, Ihler, Viola</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  We discuss an information theoretic approach for categorizing and\n  modeling dynamic processes.  The approach can learn a compact and\n  informative statistic which summarizes past states to predict future\n  observations. Furthermore, the uncertainty of the prediction is\n  characterized nonparametrically by a joint density over the learned\n  statistic and present observation. We discuss the application of the\n  technique to both noise driven dynamical systems and random\n  processes sampled from a density which is conditioned on the\n  past. In the first case we show results in which both the dynamics\n  of random walk and the statistics of the driving noise are\n  captured. In the second case we present results in which a\n  summarizing statistic is learned on noisy random telegraph waves\n  with differing dependencies on past states. In both cases the\n  algorithm yields a principled approach for discriminating processes\n  with differing dynamics and/or dependencies. The method is grounded\n  in ideas from information theory and nonparametric statistics.\n\n<br><p>[<a href=\"bib.html#nips99\"> BibTex </a>] | [<a href=\"nips99.pdf\"> PDF </a>] | [<a href=\"nips99.ps\"> PS </a>] </p><br><hr noshade>\n<a name=icassp01></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Estimators for Online Signature Authentication</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We present extensions to our previous work in modelling dynamical processes.\nThe approach uses an information theoretic criterion for searching over\nsubspaces of the past observations, combined with a nonparametric density\ncharacterizing its relation to one-step-ahead prediction and uncertainty.\nWe use this methodology to model handwriting stroke data, specifically\nsignatures, as a dynamical system and show that it is possible to learn\na model capturing their dynamics for use either in synthesizing realistic\nsignatures and in discriminating between signatures and forgeries even\nthough no forgeries have been used in constructing the model.\nThis novel approach yields promising results even for small training sets.\n\n<br><p>[<a href=\"bib.html#icassp01\"> BibTex </a>] | [<a href=\"icassp01.pdf\"> PDF </a>] | [<a href=\"icassp01.ps\"> PS </a>] </p><br><hr noshade>\n<a name=tr2551></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sudderth, Ihler, Freeman, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In applications of graphical models arising in fields such as computer vision,\n  the hidden variables of interest are most naturally specified by continuous,\n  non-Gaussian distributions. However, due to the limitations of existing inference\n  algorithms, it is often necessary to form coarse, discrete approximations to\n  such models. In this paper, we develop a <i>nonparametric belief propagation</i>\n  (NBP) algorithm, which uses stochastic methods to propagate kernel-based approximations\n  to the true continuous messages. Each NBP message update is based on an efficient\n  sampling procedure which can accomodate an extremely broad class of potential\n  functions, allowing easy adaptation to new application areas. We validate our\n  method using comparisons to continuous BP for Gaussian networks, and an application\n  to the stereo vision problem.\n\n<br><p>[<a href=\"bib.html#tr2551\"> BibTex </a>] | [<a href=\"tr2551.pdf\"> PDF </a>] | [<a href=\"tr2551.ps\"> PS </a>] </p><br><hr noshade>\n<a name=ipsn03></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Hypothesis Testing over Factorizations for Data Association</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The issue of data association arises frequently in sensor\nnetworks; whenever multiple sensors and sources are present, it\nmay be necessary to determine which observations from different\nsensors correspond to the same target.  In highly uncertain\nenvironments, one may need to determine this correspondence\nwithout the benefit of an \\emph{a priori} known joint\nsignal/sensor model.  This paper examines the data association\nproblem as the more general hypothesis test between factorizations\nof a single, learned distribution.  The optimal test between known\ndistributions may be decomposed into model-dependent and\nstatistical dependence terms, quantifying the cost incurred by\nmodel estimation from measurements compared to a test between\nknown models.  We demonstrate how one might evaluate a two-signal\nassociation test efficiently using kernel density estimation\nmethods to model a wide class of possible distributions, and show\nthe resulting algorithm's ability to determine correspondence in\nuncertain conditions through a series of synthetic examples. We\nthen describe an extension of this technique to multi-signal\nassociation which can be used to determine correspondence while\navoiding the computationally prohibitive task of evaluating all\nhypotheses. Empirical results of the approximate approach are\npresented.\n\n<br><p>[<a href=\"bib.html#ipsn03\"> BibTex </a>] | [<a href=\"ipsn03.pdf\"> PDF </a>] | [<a href=\"ipsn03.ps\"> PS </a>] </p><br><hr noshade>\n<a name=cvpr03></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sudderth, Ihler, Freeman, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In many applications of graphical models arising in computer vision, the hidden variables of interest\r\n  are most naturally specified by continuous, non-Gaussian distributions. There exist inference algorithms\r\n  for discrete approximations to these continuous distributions, but for the high-dimensional variables\r\n  typically of interest, discrete inference becomes infeasible. Stochastic methods such as particle \r\n  filters provide an appealing alternative. However, existing techniques fail to exploit the rich\r\n  structure of the graphical models describing many vision problems. Drawing on ideas from regularized \r\n  particle filters and belief propagation (BP), this paper develops a nonparametric belief propagation\r\n  (NBP) algorithm applicable to general graphs. Each NBP iteration uses an efficient sampling procedure\r\n  to update kernel-based approximations to the true, continuous likelihoods. The algorithm can accomodate \r\n  an extremely broad class of potential functions, including nonparametric representations. Thus, NBP \r\n  extends particle filtering methods to the more general vision problems that graphical models can \r\n  describe. We apply the NBP algorithm to infer component interrelationships in a parts-based face model,\r\n  allowing location and reconstruction of occluded features.\r\n<br><p>[<a href=\"bib.html#cvpr03\"> BibTex </a>] | [<a href=\"cvpr03.pdf\"> PDF </a>] | [<a href=\"cvpr03.ps\"> PS </a>] </p><br><hr noshade>\n<a name=nips03></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Efficient Multiscale Sampling from Products of Gaussian Mixtures</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Sudderth, Freeman, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The problem of approximating the product of several Gaussian mixture\ndistributions arises in a number of contexts, including the nonparametric\nbelief propagation (NBP) inference algorithm and the training of\nproduct of experts models.\nThis paper develops two multiscale algorithms for sampling from a\nproduct of Gaussian mixtures, and compares their performance to existing\nmethods.  The first is a multiscale variant of previously proposed\nMonte Carlo techniques, with comparable\ntheoretical guarantees but improved empirical convergence rates.\nThe second makes use of approximate kernel density evaluation\nmethods to construct a fast approximate sampler, which is\nguaranteed to sample points to within a tunable parameter $\\epsilon$\nof their true probability.\nWe compare both multiscale samplers on a set of computational examples\nmotivated by NBP, demonstrating significant improvements over existing\nmethods.\n\n<br><p>[<a href=\"bib.html#nips03\"> BibTex </a>] | [<a href=\"nips03.pdf\"> PDF </a>] | [<a href=\"nips03.ps\"> PS </a>] </p><br><hr noshade>\n<a name=ipsn04></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation for Self-Calibration in Sensor Networks</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Moses, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Automatic self-calibration of ad-hoc sensor networks is a critical need\nfor their use in military or civilian applications.  In general,\nself-calibration involves the combination of absolute location information\n(e.g. GPS) with relative calibration information (e.g. time delay or\nreceived signal strength between sensors) over regions of the network.\nFurthermore, it is generally desirable to distribute the computational\nburden across the network and minimize the amount of inter-sensor\ncommunication.  We demonstrate that the information used for sensor\ncalibration is fundamentally local with regard to the network topology and\nuse this observation to reformulate the problem within a graphical model\nframework.  We then demonstrate the utility of \\emph{nonparametric belief\npropagation} (NBP), a recent generalization of particle filtering, for\nboth estimating sensor locations and representing location uncertainties.\nNBP has the advantage that it is easily implemented in a distributed\nfashion, admits a wide variety of statistical models, and can represent\nmulti-modal uncertainty.  We illustrate the performance of NBP on several\nexample networks while comparing to a previously published nonlinear least\nsquares method.\n\n<br><p>[<a href=\"bib.html#ipsn04\"> BibTex </a>] | [<a href=\"ipsn04.pdf\"> PDF </a>] | [<a href=\"ipsn04.ps\"> PS </a>] </p><br><hr noshade>\n<a name=icassp04></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation for Sensor Network Self-Calibration</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Moses, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Automatic self-calibration of ad-hoc sensor networks is a critical need\nfor their use in military or civilian applications.  In general,\nself-calibration involves the combination of absolute location information\n(e.g. GPS) with relative calibration information (e.g. estimated distance\nbetween sensors) over regions of the network. We formulate the\nself-calibration problem as a graphical model, enabling application of\nnonparametric belief propagation (NBP), a recent generalization of\nparticle filtering, for both estimating sensor locations and representing\nlocation uncertainties. NBP has the advantage that it is easily\nimplemented in a distributed fashion, can represent multi-modal\nuncertainty, and admits a wide variety of statistical models.   This last\npoint is particularly appealing in that it can be used to provide\nrobustness against occasional high-variance (outlier) noise.  We\nillustrate the performance of NBP using Monte Carlo analysis on an example\nnetwork.\n\n<br><p>[<a href=\"bib.html#icassp04\"> BibTex </a>] | [<a href=\"icassp04.pdf\"> PDF </a>] | [<a href=\"icassp04.ps\"> PS </a>] </p><br><hr noshade>\n<a name=tr2601></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Communications-Constrained Inference</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In many applications, particularly power-constrained sensor networks, it is important to\nconserve the amount of data exchanged while maximizing the utility of that data for some\ninference task. Broadly, this tradeoff has two major cost components\u2014the representation\u2019s\nsize (in distributed networks, the communications cost) and the error incurred by its use (the\ninference cost).\n<p>    We analyze this tradeoff for a particular problem: communicating a particle-based \nrepresentation (and more generally, a Gaussian mixture or kernel density estimate). We begin by\ncharacterizing the exact communication cost of these representations, noting that it is less than\nmight be suggested by traditional communications theory due to the invariance of the represen-\ntation to reordering. We describe the optimal, lossless encoder when the generating distribution\nis known, and pose a sub-optimal encoder which still benefits from reordering invariance.\n<p>    However, lossless encoding may not be sufficient. We describe one reasonable measure of\nerror for distribution-based messages and its consequences for inference in an acyclic network,\nand propose a novel density approximation method based on KD-tree multiscale representations\nwhich enables the communications cost and a bound on error to be balanced efficiently. We\nshow several empirical examples demonstrating the method\u2019s utility in collaborative, distributed\nsignal processing under bandwidth or power constraints.\n\n<br><p>[<a href=\"bib.html#tr2601\"> BibTex </a>] | [<a href=\"tr2601.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=tsp04></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Hypothesis Tests for Statistical Dependency</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Determining the structure of dependencies among a set of variables is a\ncommon task in many signal and image processing applications, including\nmulti-target tracking and computer vision.  In this paper we present an\ninformation-theoretic, machine learning approach to problems of this type.\nWe cast this problem as a hypothesis test between factorizations of\nvariables into mutually independent subsets. We show that the likelihood\nratio can be written as sums of two sets of Kullback-Leibler (KL)\ndivergence terms. The first set captures the structure of the statistical\ndependencies within each hypothesis, while the second set measures the\ndetails of model differences between hypotheses.  We then consider the\ncase when the signal prior models are unknown, so that the distributions\nof interest must be estimated directly from data, showing that the second\nset of terms is (asymptotically) negligible and quantifying the loss in\nhypothesis separability when the models are completely unknown.  We\ndemonstrate the utility of nonparametric estimation methods for such\nproblems, providing a general framework for determining and distinguishing\nbetween dependency structures in highly uncertain environments.\nAdditionally, we develop a machine learning approach for estimating lower\nbounds on KL-divergence and mutual information from samples of\nhigh-dimensional random variables for which direct density estimation is\ninfeasible. We present empirical results in the context of three\nprototypical applications: association of signals generated by sources\npossessing harmonic behavior, scene correspondence using video imagery,\nand detection of coherent behavior among sets of moving objects.\n\n<br><p>[<a href=\"bib.html#tsp04\"> BibTex </a>] | [<a href=\"tsp04.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips04></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Message Errors in Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Belief propagation (BP) is an increasingly popular method of performing\napproximate inference on arbitrary graphical models.  At times, even\nfurther approximations are required, whether from quantization or other\nsimplified message representations or from stochastic approximation\nmethods.  Introducing such errors into the BP message computations has\nthe potential to adversely affect the solution obtained.  We analyze this\neffect with respect to a particular measure of message error, and show\nbounds on the accumulation of errors in the system.  This leads both to\nconvergence conditions and error bounds in traditional and approximate BP\nmessage passing. \n\n<br><p>[<a href=\"bib.html#nips04\"> BibTex </a>] | [<a href=\"nips04.pdf\"> PDF </a>] | [<a href=\"nips04.ps\"> PS </a>] </p><br><hr noshade>\n<a name=icassp05></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Estimating Dependency and Significance for High-Dimensional Data</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Siracusa, Tieu, Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Understanding the dependency structure of a set of variables is a\nkey component in various signal processing applications which involve\ndata association. The simple task of detecting whether any\ndependency exists is particularly difficult when models of the data\nare unknown or difficult to characterize because of high-dimensional\nmeasurements. We review the use of nonparametric tests for characterizing\ndependency and how to carry out these tests with highdimensional\nobservations. In addition we present a method to assess\nthe significance of the tests.\n\n<br><p>[<a href=\"bib.html#icassp05\"> BibTex </a>] | [<a href=\"icassp05.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=jsac05></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation for Sensor Network Self-Calibration</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Moses, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Automatic self-localization is a critical need for the effective use of\nad-hoc sensor networks in military or civilian applications.  In general,\nself-localization involves the combination of absolute location\ninformation (\\eg GPS) with relative calibration information (\\eg distance\nmeasurements between sensors) over regions of the network. Furthermore, it\nis generally desirable to distribute the computational burden across the\nnetwork and minimize the amount of inter-sensor communication.  We\ndemonstrate that the information used for sensor localization is\nfundamentally local with regard to the network topology and use this\nobservation to reformulate the problem within a graphical model framework.\nWe then present and demonstrate the utility of \\emph{nonparametric belief\npropagation} (NBP), a recent generalization of particle filtering, for\nboth estimating sensor locations and representing location uncertainties.\nNBP has the advantage that it is easily implemented in a distributed\nfashion, admits a wide variety of statistical models, and can represent\nmulti-modal uncertainty. Using simulations of small- to moderately-sized\nsensor networks, we show that NBP may be made robust to outlier\nmeasurement errors by a simple model augmentation, and that judicious\nmessage construction can result in better estimates. Furthermore, we\nprovide an analysis of NBP's communications requirements, showing that\ntypically only a few messages per sensor are required, and that even low\nbit-rate approximations of these messages can have little or no\nperformance impact.\n\n<br><p>[<a href=\"bib.html#jsac05\"> BibTex </a>] | [<a href=\"jsac05.pdf\"> PDF </a>] | [<a href=\"jsac05.ps\"> PS </a>] </p><br><hr noshade>\n<a name=jmlr05></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Loopy Belief Propagation: Convergence and Effects of Message Errors</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Belief propagation (BP) is an increasingly popular method of performing\napproximate inference on arbitrary graphical models.  At times, even\nfurther approximations are required, whether due to quantization of the\nmessages or model parameters, from other simplified message or model\nrepresentations, or from stochastic approximation methods.  The\nintroduction of such errors into the BP message computations has the\npotential to affect the solution obtained adversely.  We analyze the\neffect resulting from message approximation under two particular measures\nof error, and show bounds on the accumulation of errors in the system.\nThis analysis leads to convergence conditions for traditional BP message\npassing, and both strict bounds and estimates of the resulting error in\nsystems of approximate BP message passing.\n\n<br><p>[<a href=\"bib.html#jmlr05\"> BibTex </a>] | [<a href=\"jmlr05.pdf\"> PDF </a>] | [<a href=\"jmlr05.ps\"> PS </a>] </p><br><hr noshade>\n<a name=ssp05></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Particle Filtering Under Communications Constraints</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Fisher, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Particle filtering is often applied to the problem of object tracking\nunder non-Gaussian uncertainty; however, sensor networks frequently\nrequire that the implementation be local to the region of interest,\neventually forcing the large, sample-based representation to be moved\namong power-constrained sensors. We consider the problem of successive\napproximation (i.e., lossy compression) of each sample-based density\nestimate, in particular exploring the consequences (both theoretical and\nempirical) of several possible choices of loss function and their\ninterpretation in terms of future errors in inference, justifying their\nuse for measuring approximations in distributed particle filtering.\n\n<br><p>[<a href=\"bib.html#ssp05\"> BibTex </a>] | [<a href=\"ssp05.pdf\"> PDF </a>] | [<a href=\"ssp05.ps\"> PS </a>] </p><br><hr noshade>\n<a name=uai06></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick Breaking Representation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Porteous, Ihler, Smyth, Welling</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Nonparametric Bayesian approaches to clustering,\ninformation retrieval, language modeling\nand object recognition have recently shown great\npromise as a new paradigm for unsupervised data\nanalysis. Most contributions have focused on the\nDirichlet process mixture models or extensions\nthereof for which efficient Gibbs samplers exist.\nIn this paper we explore Gibbs samplers for\ninfinite complexity mixture models in the stick\nbreaking representation. The advantage of this\nrepresentation is improved modeling flexibility.\nFor instance, one can design the prior distribution\nover cluster sizes or couple multiple infi-\nnite mixture models (e.g., over time) at the level\nof their parameters (i.e., the dependent Dirichlet\nprocess model). However, Gibbs samplers for in-\nfinite mixture models (as recently introduced in\nthe statistics literature) seem to mix poorly over\ncluster labels. Among others issues, this can have\nthe adverse effect that labels for the same cluster\nin coupled mixture models are mixed up. We\nintroduce additional moves in these samplers to\nimprove mixing over cluster labels and to bring\nclusters into correspondence. An application to\nmodeling of storm trajectories is used to illustrate\nthese ideas.\n\n<br><p>[<a href=\"bib.html#uai06\"> BibTex </a>] | [<a href=\"uai06.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=spm06></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Distributed Fusion in Sensor Networks</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Cetin, Chen, Fisher, Ihler, Moses, Wainwright, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Distributed inference methods developed for graphical models comprise a principled\napproach for data fusion in sensor networks. The application of these methods, however,\nrequires some care due to a number of issues that are particular to sensor networks.\nChief of among these are the distributed nature of computation and\ndeployment coupled with communications bandwidth and energy constraints typical\nof many sensor networks. Additionally, information sharing in a sensor network necessarily\ninvolves approximation. Traditional measures of distortion are not sufficient to characterize the\nquality of approximation as they do not address in an explicit manner the resulting impact on inference\nwhich is at the core of many data fusion problems. While both graphical models and a distributed\nsensor network have network structures associated with them, the mapping is not one to one.\nAll of these issues complicate the mapping of a particular inference problem to a given sensor network\nstructure. Indeed, there may be a variety of mappings with very different characteristics with\nregard to computational complexity and utilization of resources. Nevertheless, it is the case that\nmany of the powerful distributed inference methods have a role in information fusion for sensor\nnetworks. In this article we present an overview of research conducted by the authors that has\nsought to clarify many of the important issues at the intersection\nof these domains. We discuss both theoretical issues and prototypical\napplications in addition to suggesting new lines of reasoning.\n\n<br><p>[<a href=\"bib.html#spm06\"> BibTex </a>] | [<a href=\"spm06.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=kdd06></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Adaptive Event Detection with Time-Varying Poisson Processes</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Hutchins, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Time-series of count data are generated in many different contexts, such as web access logging, freeway traffic monitoring, and security logs associated with buildings. Since this data measures the aggregated behavior of individual human beings, it typically exhibits a periodicity in time on a number of scales (daily, weekly,etc.) that reflects the rhythms of the underlying human activity and makes the data appear non-homogeneous. At the same time, the data is often corrupted by a number of bursty periods of unusual behavior such as building events, traffic accidents, and so forth. The data mining problem of finding and extracting these anomalous events is made difficult by both of these elements. In this paper we describe a framework for unsupervised learning in this context, based on a time-varying Poisson process model that can also account for anomalous events. We show how the parameters of this model can be learned from count time series using statistical estimation techniques. We demonstrate the utility of this model on two datasets for which we have partial ground truth in the form of known events, one from freeway traffic data and another from building access data, and show that the model performs significantly better than a non-probabilistic, threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and make inferences about the detected events (e.g., popularity or level of attendance). Our experimental results indicate that the proposed time-varying Poisson model provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.\n<br><p>[<a href=\"bib.html#kdd06\"> BibTex </a>] | [<a href=\"kdd06.pdf\"> PDF </a>] | [<a href=\"kdd06.ps\"> PS </a>] </p><br><hr noshade>\n<a name=nips06></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning Time-Intensity Profiles of Human Activity Using Nonparametric Bayesian Models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Data sets that characterize human activity over time through collections of timestamped\nevents or counts are of increasing interest in application areas as humancomputer\ninteraction, video surveillance, and Web data analysis. We propose a\nnon-parametric Bayesian framework for modeling collections of such data. In\nparticular, we use a Dirichlet process framework for learning a set of intensity\nfunctions corresponding to different categories, which form a basis set for representing\nindividual time-periods (e.g., several days) depending on which categories\nthe time-periods are assigned to. This allows the model to learn in a data-driven\nfashion what \u0093factors\u0094 are generating the observations on a particular day, including\n(for example) weekday versus weekend effects or day-specific effects corresponding\nto unique (single-day) occurrences of unusual behavior, sharing information\nwhere appropriate to obtain improved estimates of the behavior associated\nwith each category. Applications to real\u0096world data sets of count data involving\nboth vehicles and people are used to illustrate the technique.\n\n<br><p>[<a href=\"bib.html#nips06\"> BibTex </a>] | [<a href=\"nips06.pdf\"> PDF </a>] | [<a href=\"nips06.ps\"> PS </a>] </p><br><hr noshade>\n<a name=physd07></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Graphical Models for Statistical Inference and Data Assimilation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Kirshner, Ghil, Robertson, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In data assimilation for a system which evolves in time, one combines past and current observations with a model of the dynamics of the\nsystem, in order to improve the simulation of the system as well as any future predictions about it. From a statistical point of view, this process can\nbe regarded as estimating many random variables which are related both spatially and temporally: given observations of some of these variables,\ntypically corresponding to times past, we require estimates of several others, typically corresponding to future times.\n<p>\nGraphical models have emerged as an effective formalism for assisting in these types of inference tasks, particularly for large numbers of\nrandom variables. Graphical models provide a means of representing dependency structure among the variables, and can provide both intuition\nand efficiency in estimation and other inference computations. We provide an overview and introduction to graphical models, and describe how\nthey can be used to represent statistical dependency and how the resulting structure can be used to organize computation. The relation between\nstatistical inference using graphical models and optimal sequential estimation algorithms such as Kalman filtering is discussed. We then give\nseveral additional examples of how graphical models can be applied to climate dynamics, specifically estimation using multi-resolution models of\nlarge-scale data sets such as satellite imagery, and learning hidden Markov models to capture rainfall patterns in space and time.\n\n<br><p>[<a href=\"bib.html#physd07\"> BibTex </a>] | [<a href=\"physd07.pdf\"> PDF </a>] | [<a href=\"http://dx.doi.org/10.1016/j.physd.2006.08.023\"> Link </a>] </p><br><hr noshade>\n<a name=uai07></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Accuracy Bounds for Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The belief propagation algorithm is widely applied to perform\napproximate inference on arbitrary graphical models, in part due to\nits excellent empirical properties and performance. However, little\nis known theoretically about when this algorithm will perform well.\nUsing recent analysis of convergence and stability properties in\nbelief propagation and new results on approximations in binary\nsystems, we derive a bound on the error in BP's estimates for\npairwise Markov random fields over discrete--valued random variables.\nOur bound is relatively simple to compute, and compares favorably\nwith a previous method for bounding the accuracy of\nbelief propagation.\n\n<br><p>[<a href=\"bib.html#uai07\"> BibTex </a>] | [<a href=\"uai07.pdf\"> PDF </a>] | [<a href=\"uai07.ps\"> PS </a>] </p><br><hr noshade>\n<a name=camsap07></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Modeling Count Data from Multiple Sensors: A Building Occupancy Model</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Hutchins, Ihler, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Knowledge of the number of people in a building at a given time is\ncrucial for applications such as emergency response.  Sensors can be\nused to gather noisy measurements which when combined, can be used\nto make inferences about the location, movement and density of\npeople. In this paper we describe a probabilistic model for\npredicting the occupancy of a building using networks of\npeople-counting sensors. This model provides robust predictions\ngiven typical sensor noise as well as missing and corrupted data\nfrom malfunctioning sensors.  We experimentally validate the model\nby comparing it to a baseline method using real data from a network\nof optical counting sensors in a campus building.\n\n<br><p>[<a href=\"bib.html#camsap07\"> BibTex </a>] | [<a href=\"camsap07.pdf\"> PDF </a>] | [<a href=\"camsap07.ps\"> PS </a>] </p><br><hr noshade>\n<a name=nips07></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Adaptive Bayesian Inference</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Acar, Ihler, Mettu, Sumer</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  Motivated by stochastic systems in which observed evidence and\n  conditional dependencies between states of the network change over\n  time, and certain quantities of interest (marginal distributions,\n  likelihood estimates etc.) must be updated, we study the problem of\n  \\emph{adaptive inference} in tree-structured Bayesian networks.  We\n  describe an algorithm for adaptive inference that handles a broad\n  range of changes to the network and is able to maintain marginal\n  distributions, MAP estimates, and data likelihoods in all expected\n  logarithmic time.  We give an implementation of our algorithm and\n  provide experiments that show that the algorithm can yield up to two\n  orders of magnitude speedups on answering queries and responding to\n  dynamic changes over the sum-product algorithm.\n\n<br><p>[<a href=\"bib.html#nips07\"> BibTex </a>] | [<a href=\"nips07.pdf\"> PDF </a>] | [<a href=\"nips07.ps\"> PS </a>] </p><br><hr noshade>\n<a name=tkdd07></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning to detect events with Markov-modulated Poisson processes</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Hutchins, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p> Time-series of count data occur in many different contexts, including Internet navigation logs, freeway traffic monitoring, and security logs associated with buildings. In this article we describe a framework for detecting anomalous events in such data using an unsupervised learning approach. Normal periodic behavior is modeled via a time-varying Poisson process model, which in turn is modulated by a hidden Markov process that accounts for bursty events. We outline a Bayesian framework for learning the parameters of this model from count time-series. Two large real-world datasets of time-series counts are used as testbeds to validate the approach, consisting of freeway traffic data and logs of people entering and exiting a building. We show that the proposed model is significantly more accurate at detecting known events than a more traditional threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and to make inferences about different aspects of events such as number of vehicles or people involved. The results indicate that the Markov-modulated Poisson framework provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.\n\n<br><p>[<a href=\"bib.html#tkdd07\"> BibTex </a>] | [<a href=\"http://doi.acm.org/10.1145/1297332.1297337\"> Link </a>] </p><br><hr noshade>\n<a name=uai08></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Adaptive Inference in General Graphical Models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Acar, Ihler, Mettu, Sumer</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  Many algorithms and applications involve repeatedly solving\n  variations of the same inference problem; for example we may want to\n  introduce new evidence to the model or perform updates to\n  conditional dependencies. The goal of \\emph{adaptive inference} is\n  to take advantage of what is preserved in the model and perform\n  inference more rapidly than from scratch.  In this paper, we\n  describe techniques for adaptive inference on general graphs that\n  support marginal computation and updates to the conditional\n  probabilities and dependencies in logarithmic time. We give\n  experimental results for an implementation of our algorithm, and\n  demonstrate its potential performance benefit in the study of\n  protein structure.\n<br><p>[<a href=\"bib.html#uai08\"> BibTex </a>] | [<a href=\"uai08.pdf\"> PDF </a>] | [<a href=\"uai08.ps\"> PS </a>] </p><br><hr noshade>\n<a name=kdd08></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Fast Collapsed Gibbs Sampling for Latent Dirichlet Allocation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Porteous, Newman, Ihler, Asuncion, Smyth, Welling</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In this paper we introduce a novel collapsed Gibbs sampling method for the\nwidely used latent Dirichlet allocation (LDA) model. Our\nnew method results in significant speedups on real world text\ncorpora. Conventional Gibbs sampling schemes for LDA require\nO(K) operations per sample where K is the number of\ntopics in the model. Our proposed method draws equivalent\nsamples but requires on average significantly less then K operations per sample.\nOn real-word corpora FastLDA can be as much as 8\ntimes faster than the standard collapsed Gibbs sampler for LDA. No approximations are\nnecessary, and we show that our fast sampling\nscheme produces exactly the same results as the standard\n(but slower) sampling scheme. Experiments on four real world\ndata sets demonstrate speedups for a wide range of\ncollection sizes. For the PubMed collection of over 8 million\ndocuments with a required computation time of 6 CPU months for LDA,\nour speedup of 5.7 can save 5 CPU months of computation.\n\n<br><p>[<a href=\"bib.html#kdd08\"> BibTex </a>] | [<a href=\"kdd08.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=kdsd08></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Probabilistic Analysis of a Large Scale Urban Traffic Sensor Data Set</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Hutchins, Ihler, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Real-world sensor time series are often significantly noisier\nand more difficult to work with than the relatively clean\ndata sets that tend to be used as the basis for experiments\nin many research papers. In this paper we report on a large\ncase-study involving statistical data mining of over 300 million \nmeasurements from 1700 freeway traffic sensors over a\nperiod of seven months in Southern California. We discuss\nthe challenges posed by the wide variety of different sensor\nfailures and anomalies present in the data. The volume and\ncomplexity of the data precludes the use of manual \nvisualization or simple thresholding techniques to identify these\nanomalies. We describe the application of probabilistic \nmodeling and unsupervised learning techniques to this data set\nand illustrate how these approaches can successfully detect\nunderlying systematic patterns even in the presence of \nsubstantial noise and missing data\n\n<br><p>[<a href=\"bib.html#kdsd08\"> BibTex </a>] | [<a href=\"kdsd08.pdf\"> PDF </a>] | [<a href=\"http://www.springerlink.com/content/n216717854378473/fulltext.pdf\"> Link </a>] </p><br><hr noshade>\n<a name=aistats09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Particle Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, McAllester</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The popularity of particle filtering for inference in Markov chain\nmodels defined over random variables with very large or continuous\ndomains makes it natural to consider sample--based versions of belief\npropagation (BP) for more general (tree--structured or loopy) graphs.\nAlready, several such algorithms have been proposed in the\nliterature.  However, many questions remain open about the behavior\nof particle--based BP algorithms, and little theory\nhas been developed to analyze their performance.\nIn this paper, we describe a generic particle belief propagation (PBP)\nalgorithm which is closely related to previously proposed methods.\nWe prove that this algorithm is consistent, approaching the true BP\nmessages as the number of samples grows large.\nWe then use concentration bounds to analyze the finite-sample\nbehavior and give a convergence rate for the algorithm on\ntree--structured graphs.  Our convergence rate is $O(1/\\sqrt{n})$\nwhere $n$ is the number of samples, independent of the domain size\nof the variables.\n\n<br><p>[<a href=\"bib.html#aistats09\"> BibTex </a>] | [<a href=\"aistats09.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=plos09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Circadian Clock Genes Contribute to the Regulation of Hair Follicle Cycling</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lin, Kumar, Geyfman, Chudova, Ihler, Smyth, Paus, Takahashi, Andersen</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The hair follicle renews itself by repeatedly cycling among growth, regression,\nand rest phases. One function of hair follicle cycling is to allow seasonal\nchanges in hair growth. Understanding the regulation of hair follicle cycling\nis also of interest because abnormal regulation of hair cycle control genes is\nresponsible for several types of human hair growth disorders and skin cancers.\nWe report here that <i>Clock</i> and <i>Bmal1</i> genes,\nwhich control circadian rhythms, are also important for the regulation of hair\nfollicle cycling, a biological process of much longer duration than 24 hours.\nDetailed analysis of skin from mice mutated for central clock genes indicates a\nsignificant delay in the progression of the hair growth phase. We show that\nclock genes affect the expression of key cell cycle control genes and that\nkeratinocytes in a critical compartment of the hair follicles in\n<i>Bmal1</i> mutant mice are halted in the G1 phase of the cell\ncycle. These findings provide novel insight into circadian control mechanisms\nin modulating the progression of cyclic biological processes on different time\nscales.\n<br><p>[<a href=\"bib.html#plos09\"> BibTex </a>] | [<a href=\"plos09.pdf\"> PDF </a>] | [<a href=\"http://www.plosgenetics.org/article/info:doi/10.1371/journal.pgen.1000573\"> Link </a>] </p><br><hr noshade>\n<a name=ssp09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Adaptive Updates for MAP Configurations with Applications to Bioinformatics</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Acar, Ihler, Mettu, Sumer</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Many applications involve repeatedly computing the optimal, maximum\n<i>a posteriori</i> (MAP) configuration of a graphical model as the\nmodel changes, often slowly or incrementally over time, e.g., due to\ninput from a user.\nSmall changes to the model often require\nupdating only a small fraction of the MAP configuration, suggesting\nthe possibility of performing updates faster than recomputing from\nscratch.\nIn this paper we present an algorithm for efficiently performing such\nupdates under arbitrary changes to the model.\nOur algorithm is within a logarithmic factor of the optimal and is\nasymptotically never slower than re-computing from-scratch: if a\nmodification to the model requires $m$ updates to the MAP\nconfiguration of $n$ random variables, then our algorithm requires\n$O(m\\log{(n/m)})$ time; re-computing from scratch requires $O(n)$\ntime.\nWe evaluate the practical effectiveness of our algorithm by\nconsidering two problems in genomic signal processing, CpG region\nsegmentation and protein sidechain packing, where a MAP configuration\nmust be repeatedly updated.  Our results show significant speedups\nover recomputing from scratch.\n\n<br><p>[<a href=\"bib.html#ssp09\"> BibTex </a>] | [<a href=\"ssp09.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=allerton09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>A Low Density Lattice Decoder via Non-parametric Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Bickson, Ihler, Avissar, Dolev</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The recent work of Sommer, Feder and Shalvi presented a new family of codes called low density lattice codes (LDLC)\nthat can be decoded efficiently and approach the capacity of the AWGN channel. A linear time iterative decoding\nscheme which is based on a message-passing formulation on a factor graph is given.\n<br>\nIn the current work we report our theoretical findings regarding the relation between the LDLC decoder and belief propagation. We show that the LDLC decoder is an instance of non-parametric belief propagation and further connect it to the Gaussian belief propagation algorithm. Our new results enable borrowing knowledge from the non-parametric and Gaussian belief propagation domains into the LDLC domain. Specifically, we give more general convergence conditions for convergence of the LDLC decoder (under the same assumptions of the original LDLC convergence analysis). We discuss how to extend the LDLC decoder from Latin square to full rank, non-square matrices. We propose an efficient construction of sparse generator matrix and its matching decoder. We report preliminary experimental results which show our decoder has comparable symbol to error rate compared to the original LDLC decoder.\n<br><p>[<a href=\"bib.html#allerton09\"> BibTex </a>] | [<a href=\"allerton09.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=tr09-06></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Bounding Sample Errors in Approximate Distributed Latent Dirichlet Allocation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Newman</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Latent Dirichlet allocation (LDA) is a popular algorithm for discovering structure in\nlarge collections of text or other data.  Although its complexity is linear in the data\nsize, its use on increasingly massive collections has created considerable interest in\nparallel implementations.  ``Approximate distributed'' LDA, or AD-LDA,\napproximates the popular collapsed Gibbs sampling algorithm for LDA models while\nrunning on a distributed architecture.  Although this algorithm often appears to perform well\nin practice, its quality is not well understood or easily assessed.  In this work, we\nprovide some theoretical justification of the algorithm, and modify AD-LDA to track an\nerror bound on its performance.  Specifically, we upper-bound the probability of making\na sampling error at each step of the algorithm (compared to an exact, sequential Gibbs sampler),\ngiven the samples drawn thus far.  We show empirically that our bound is sufficiently tight to give\na meaningful\nand intuitive measure of approximation error in AD-LDA, allowing the user to understand the\ntrade-off between accuracy and efficiency.\n\n<br><p>[<a href=\"bib.html#tr09-06\"> BibTex </a>] | [<a href=\"tr09-06.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Particle-Based Variational Inference for Continuous Systems</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Frank, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Since the development of loopy belief propagation, there has been considerable\nwork on advancing the state of the art for approximate inference over\ndistributions defined on discrete random variables. Improvements include\nguarantees of convergence, approximations that are provably more accurate, and\nbounds on the results of exact inference.  However, extending these methods  to\ncontinuous-valued systems has lagged behind.  While several methods have been\ndeveloped to use belief propagation on systems with continuous values, recent \nadvances for discrete variables have not as yet been incorporated.\n\nIn this context we extend a recently proposed particle-based belief propagation\nalgorithm to provide a general framework for adapting discrete message-passing\nalgorithms to inference in continuous systems.  The resulting\nalgorithms behave similarly to their purely discrete counterparts, extending\nthe benefits of these more advanced inference techniques to the continuous\ndomain.\n\n<br><p>[<a href=\"bib.html#nips09\"> BibTex </a>] | [<a href=\"nips09.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=bioinf09></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Bayesian detection of non-sinusoidal periodic patterns in circadian expression data</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Chudova, Ihler, Lin, Andersen, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p><b>Motivation:</b> Cyclical biological processes such as cell division and circadian regulation produce coordinated periodic expression of thousands of genes. Identification of such genes and their expression patterns is a crucial step in discovering underlying regulatory mechanisms. Existing computational methods are biased toward discovering genes that follow sine-wave patterns.\n<br>\n<b>Results:</b> We present an analysis of variance (ANOVA) periodicity detector and its Bayesian extension that can be used to discover periodic transcripts of arbitrary shapes from replicated gene expression profiles. The models are applicable when the profiles are collected at comparable time points for at least two cycles. We provide an empirical Bayes procedure for estimating parameters of the prior distributions and derive closed-form expressions for the posterior probability of periodicity, enabling efficient computation. The model is applied to two datasets profiling circadian regulation in murine liver and skeletal muscle, revealing a substantial number of previously undetected non-sinusoidal periodic transcripts in each. We also apply quantitative real-time PCR to several highly ranked non-sinusoidal transcripts in liver tissue found by the model, providing independent evidence of circadian regulation of these genes.\n<br>\n<b>Availability:</b> MATLAB software for estimating prior distributions and performing inference is available for download from http://www.datalab.uci.edu/resources/periodicity/.\n<br>\n<b>Contact:</b> dchudova@gmail.com\n<br><p>[<a href=\"bib.html#bioinf09\"> BibTex </a>] | [<a href=\"http://bioinformatics.oxfordjournals.org/cgi/reprint/btp547\"> Link </a>] </p><br><hr noshade>\n<a name=bioinf10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Estimating Replicate Time-Shifts Using Gaussian Process Regression</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Lin, Anderson, Smyth, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p><b>Motivation:</b> Time-course gene expression datasets provide important insights into dynamic aspects of biological processes, such as circadian rhythms, cell cycle and organ development. In a typical microarray time-course experiment, measurements are obtained at each time point from multiple replicate samples. Accurately recovering the gene expression patterns from experimental observations is made challenging by both measurement noise and variation among replicates' rates of development. Prior work on this topic has focused on inference of expression patterns assuming that the replicate times are synchronized. We develop a statistical approach that simultaneously infers both (i) the underlying (hidden) expression profile for each gene, as well as (ii) the biological time for each individual replicate. Our approach is based on Gaussian process regression (GPR) combined with a probabilistic model that accounts for uncertainty about the biological development time of each replicate.\n<br>\n<b>Results:</b> We apply GPR with uncertain measurement times to a microarray dataset of mRNA expression for the hair-growth cycle in mouse back skin, predicting both profile shapes and biological times for each replicate. The predicted time shifts show high consistency with independently obtained morphological estimates of relative development. We also show that the method systematically reduces prediction error on out-of-sample data, significantly reducing the mean squared error in a cross-validation study.\n<br>\n<b>Availability:</b> Matlab code for GPR with uncertain time shifts is available at http://sli.ics.uci.edu/Code/GPRTimeshift/\n<br>\n<b>Contact:</b> ihler@ics.uci.edu\n<br><p>[<a href=\"bib.html#bioinf10\"> BibTex </a>] | [<a href=\"http://bioinformatics.oxfordjournals.org/cgi/reprint/26/6/770\"> Link </a>] </p><br><hr noshade>\n<a name=aistats10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning with Blocks: Composite Likelihood and Contrastive Divergence</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Asuncion, Liu, Ihler, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Composite likelihood methods provide a wide spectrum of computationally efficient \ntechniques for statistical tasks such as parameter estimation and model selection.  \nIn this paper, we present a formal connection between the optimization of composite \nlikelihoods and the well-known contrastive divergence algorithm. In particular, we \nshow that composite likelihoods can be stochastically optimized by performing a \nvariant of contrastive divergence with random-scan blocked Gibbs sampling. By using \nhigher-order composite likelihoods, our proposed learning framework makes it \npossible to trade off computation time for increased accuracy. Furthermore, one can \nchoose composite likelihood blocks that match the model's dependence structure, \nmaking the optimization of higher-order composite likelihoods computationally \nefficient.  We empirically analyze the performance of blocked contrastive \ndivergence on various models, including visible Boltzmann machines, conditional \nrandom fields, and exponential random graph models, and we demonstrate that using \nhigher-order blocks improves both the accuracy of parameter estimates and the rate \nof convergence.\n\n<br><p>[<a href=\"bib.html#aistats10\"> BibTex </a>] | [<a href=\"aistats10.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=icml10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Particle Filtered MCMC-MLE with Connections to Contrastive Divergence</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Asuncion, Liu, Ihler, Smyth</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Learning undirected graphical models such as Markov random fields is an\nimportant machine learning task with applications in many domains.\nSince it is usually intractable to learn these models exactly, various \napproximate learning techniques have been developed, such as contrastive \ndivergence (CD) and Markov chain Monte Carlo maximum likelihood estimation \n(MCMC-MLE).  In this paper, we introduce particle filtered MCMC-MLE, which is \na sampling-importance-resampling version of MCMC-MLE with \nadditional MCMC rejuvenation steps.  \nWe also describe a unified view of (1) MCMC-MLE, (2) our particle filtering approach,\nand (3) a stochastic approximation procedure known as persistent contrastive\ndivergence.  We show how these approaches are related to each other\nand discuss the relative merits of each approach.\nEmpirical results on various undirected models demonstrate that \nthe particle filtering technique we propose in this paper\ncan significantly outperform MCMC-MLE.  Furthermore, in certain \ncases, the proposed technique is faster than persistent CD.  \n\n<br><p>[<a href=\"bib.html#icml10\"> BibTex </a>] | [<a href=\"icml10.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=cvpr10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Covering Trees and Lower Bounds on Quadratic Assignment</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Yarkony, Fowlkes, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Many computer vision problems involving feature correspondence among images can\nbe formulated as an assignment problem with a quadratic cost function. Such\nproblems are computationally infeasible in general but recent advances in\ndiscrete optimization such as tree-reweighted belief propagation (TRW) \noften provide high-quality solutions.  In this paper, we improve upon\nthese algorithms in two ways.  First, we introduce covering trees, a variant of\nTRW which provide the same bounds on the MAP energy as TRW with far fewer\nvariational parameters. Optimization of these parameters can be carried out\nefficiently using either fixed--point iterations (as in TRW) or sub-gradient\nbased techniques.  Second, we introduce a new technique that utilizes bipartite\nmatching applied to the min-marginals produced with covering trees in order to\ncompute a tighter lower-bound for the quadratic assignment problem.  We apply\nthis machinery to the problem of finding correspondences with pairwise energy\nfunctions, and demonstrate the resulting hybrid method outperforms TRW alone\nand a recent related subproblem decomposition algorithm on benchmark image\ncorrespondence problems.\n\n<br><p>[<a href=\"bib.html#cvpr10\"> BibTex </a>] | [<a href=\"cvpr10.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Negative Tree-reweighted Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We introduce a new class of lower bounds on the log partition function of a\nMarkov random field which makes use of a reversed Jensen's inequality.\nIn particular, our method approximates the intractable distribution using\na linear combination of spanning trees with negative weights.  This technique\nis a lower-bound counterpart to the tree-reweighted belief propagation algorithm,\nwhich uses a convex combination of spanning trees with positive weights to\nprovide corresponding upper bounds.  We develop algorithms to optimize and\ntighten the lower bounds over the non-convex set of valid parameter values. \nOur algorithm generalizes mean field approaches (including na\\\"ive and \nstructured mean field approximations), which it includes as a limiting case.\n\n<br><p>[<a href=\"bib.html#uai10\"> BibTex </a>] | [<a href=\"uai10.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=cacm10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Nonparametric Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sudderth, Ihler, Isard, Freeman, Willsky</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Continuous quantities are ubiquitous in models of real-world phenomena,\nbut are surprisingly difficult to reason about automatically.  Probabilistic\ngraphical models such as Bayesian networks and Markov random fields,\nand algorithms for approximate inference such as belief propagation,\nhave proven to be powerful\ntools in a wide range of applications in statistics and artificial intelligence.\nHowever, applying these methods to models with continuous variables remains\na challenging task.  In this work we describe an extension of belief propagation\nto continuous variable models, generalizing particle filtering and Gaussian mixture\nfiltering techniques for time series to more complex models.  We illustrate the\npower of the resulting nonparametric belief propagation algorithm via two applications:\nkinematic tracking of visual motion and distributed localization in sensor networks.\n\n<br><p>[<a href=\"bib.html#cacm10\"> BibTex </a>] | [<a href=\"http://cacm.acm.org/magazines/2010/10/99503-nonparametric-belief-propagation/pdf?dl=no\"> Link </a>] </p><br><hr noshade>\n<a name=_tkde10></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Understanding Errors in Approximate Distributed Latent Dirichlet Allocation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Newman</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Latent Dirichlet allocation (LDA) is a popular algorithm for discovering semantic structure in\nlarge collections of text or other data.  Although its complexity is linear in the data\nsize, its use on increasingly massive collections has created considerable interest in\nparallel implementations.  ``Approximate distributed'' LDA, or AD-LDA,\napproximates the popular collapsed Gibbs sampling algorithm for LDA models while\nrunning on a distributed architecture.  Although this algorithm often appears to perform well\nin practice, its quality is not well understood theoretically or easily assessed on new data.  In this work, we\ntheoretically justify the approximation, and modify AD-LDA to track an\nerror bound on performance.  Specifically, we upper-bound the probability of making\na sampling error at each step of the algorithm (compared to an exact, sequential Gibbs sampler),\ngiven the samples drawn thus far.  We show empirically that our bound is sufficiently tight to give\na meaningful\nand intuitive measure of approximation error in AD-LDA, allowing the user to track the\ntrade-off between accuracy and efficiency while executing in parallel.\n\n<br><p>[<a href=\"bib.html#_tkde10\"> BibTex </a>] | [<a href=\"_tkde10.pdf\"> PDF </a>] | [<a href=\"http://www.computer.org/portal/web/csdl/doi/10.1109/TKDE.2011.29\"> Link </a>] </p><br><hr noshade>\n<a name=aistats11c></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Revisiting MAP Estimation, Message Passing and Perfect Graphs</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Foulds, Navaroli, Smyth, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Given a graphical model, one of the most useful queries is to find the most\nlikely configuration of its variables.  This task, known as the maximum a\nposteriori (MAP) problem, can be solved efficiently via message passing\ntechniques when the graph is a tree, but is NP-hard for general graphs.\nJebara (2009) shows that the MAP problem can be converted into the\nstable set problem, which can be solved in polynomial time for a broad class of\ngraphs known as perfect graphs via a linear programming relaxation technique.\nThis is a result of great theoretical interest.  However, the article\nadditionally claims that max-product linear programming (MPLP) message passing\ntechniques of Globerson & Jaakkola (2007) are also guaranteed to solve these\nproblems exactly and efficiently.  We investigate this claim, show that it does\nnot hold in general, and attempt to repair it with several alternative message\npassing algorithms.\n\n<br><p>[<a href=\"bib.html#aistats11c\"> BibTex </a>] | [<a href=\"aistats11c.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aistats11b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Multicore Gibbs Sampling in Dense, Unstructured Graphs</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Xu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Multicore computing is on the rise, but algorithms such as Gibbs sampling are\nfundamentally sequential and may require close consideration to be made\nparallel.  Existing techniques either exploit sparse problem structure or make\napproximations to the algorithm; in this work, we explore an alternative to\nthese ideas.  We develop a parallel Gibbs sampling algorithm for shared-memory\nsystems that does not require any independence structure among the variables\nyet does not approximate the sampling distributions.  Our method uses a\nlook-ahead sampler, which uses bounds to attempt to sample variables before the\nresults of other threads are made available.  We demonstrate our algorithm on\nGibbs sampling in Boltzmann machines and latent Dirichlet allocation (LDA).  We\nshow in experiments that our algorithm achieves near linear speed-up in the\nnumber of cores, is faster than existing exact samplers, and is nearly as fast\nas approximate samplers while maintaining the correct stationary distribution.\n\n<br><p>[<a href=\"bib.html#aistats11b\"> BibTex </a>] | [<a href=\"aistats11b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aistats11a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning Scale Free Networks by Reweighted L1 regularization</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Methods for L1-type regularization have been widely used in Gaussian\ngraphical model selection tasks to encourage sparse structures. However, often\nwe would like to include more structural information than mere sparsity. In\nthis work, we focus on learning so-called ``scale-free'' models, a common\nfeature that appears in many real-work networks. We replace the L1\nregularization with a power law regularization and  optimize the objective\nfunction by a sequence of iteratively reweighted L1 regularization\nproblems, where the regularization coefficients of nodes with high degree are\nreduced, encouraging the appearance of hubs with high degree. Our method can be\neasily adapted to improve <i>any</i> existing L1-based methods, such as\ngraphical lasso, neighborhood selection, and JSRM when the underlying networks\nare believed to be scale free or have dominating hubs. We demonstrate in\nsimulation that our method significantly outperforms the a baseline L1\nmethod at learning scale-free networks and hub networks, and also illustrate\nits behavior on gene expression data. \n\n<br><p>[<a href=\"bib.html#aistats11a\"> BibTex </a>] | [<a href=\"aistats11a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=tsp11></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Fault Detection via Nonparametric Belief Propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Bickson, Baron, Ihler, Avissar, Dolev</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We consider the problem of identifying a pattern of faults from a set of noisy\nlinear measurements.  Unfortunately,  maximum a posteriori probability\nestimation of the fault pattern is computationally intractable. To solve the\nfault identification problem, we propose a non-parametric belief propagation\napproach.  We show empirically that our belief propagation solver is more\naccurate than recent state-of-the-art algorithms including interior point\nmethods and semidefinite programming. Our superior performance is explained by\nthe fact that we take into account both the binary nature of the individual\nfaults and the sparsity of the fault pattern arising from their rarity.\n<br><p>[<a href=\"bib.html#tsp11\"> BibTex </a>] | [<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5714757\"> Link </a>] </p><br><hr noshade>\n<a name=icml11></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Bounding the Partition Function using Holder's Inequality</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We describe an algorithm for approximate inference in graphical models based on\nH\\\"older's inequality that provides upper and lower bounds on common summation\nproblems such as computing the partition function or probability of evidence\nin a graphical model.\nOur algorithm unifies and extends several existing approaches,\nincluding variable elimination techniques such as mini-bucket elimination and\nvariational methods such tree reweighted belief propagation and\nconditional entropy decomposition.\nWe show that our method inherits benefits from each approach\nto provide significantly better bounds on sum-product tasks.\n\n<br><p>[<a href=\"bib.html#icml11\"> BibTex </a>] | [<a href=\"icml11.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai11></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Fast parallel and adaptive updates for dual-decomposition solvers</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sumer, Acar, Ihler, Mettu</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  Dual-decomposition (DD) methods are quickly becoming important tools\n  for estimating the minimum energy state of a graphical model.  DD\n  methods decompose a complex model into a collection of simpler\n  subproblems that can be solved exactly (such as trees), that in\n  combination provide upper and lower bounds on the exact solution.\n  Subproblem choice can play a major role:\n  larger subproblems tend to improve the bound more per iteration,\n  while smaller subproblems enable highly parallel solvers and can\n  benefit from re-using past solutions when there are few changes\n  between iterations.\n\n  We propose an algorithm that can balance many of these aspects to\n  speed up convergence.  Our method uses a cluster tree\n  data structure that has been proposed for adaptive exact inference\n  tasks, and we apply it in this paper to dual-decomposition\n  approximate inference.  This approach allows us to process large\n  subproblems to improve the bounds at each iteration, while allowing\n  a high degree of parallelizability and taking advantage of\n  subproblems with sparse updates.\n  For both synthetic inputs and a real-world stereo matching problem,\n  we demonstrate that our algorithm is able to achieve significant\n  improvement in convergence time.\n\n<br><p>[<a href=\"bib.html#aaai11\"> BibTex </a>] | [<a href=\"aaai11.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai11c></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Tightening MRF relaxations with planar subproblems</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Yarkony, Morshed, Ihler, Fowlkes</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We describe a new technique for computing lower-bounds on the minimum energy\nconfiguration of a planar Markov Random Field (MRF).  Our method successively\nadds large numbers of constraints and enforces consistency over binary\nprojections of the original problem state space.  These constraints are\nrepresented in terms of subproblems in a dual-decomposition framework that is\noptimized using subgradient techniques.  The complete set of constraints we\nconsider enforces cycle consistency over the original graph.  In practice we\nfind that the method converges quickly on most problems with the addition of a\nfew subproblems and outperforms existing methods for some interesting classes\nof hard potentials.\n\n<br><p>[<a href=\"bib.html#uai11c\"> BibTex </a>] | [<a href=\"uai11c.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai11b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Planar cycle covering graphs</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Yarkony, Ihler, Fowlkes</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We describe a new variational lower-bound on the minimum energy configuration\nof a planar binary Markov Random Field (MRF).  Our method is based on adding\nauxiliary nodes to every face of a planar embedding of the graph in order to\ncapture the effect of unary potentials.  A ground state of the resulting\napproximation can be computed efficiently by reduction to minimum-weight\nperfect matching. We show that optimization of variational parameters achieves\nthe same lower-bound as dual-decomposition into the set of all cycles of the\noriginal graph.  We demonstrate that our variational optimization converges\nquickly and provides high-quality solutions to hard combinatorial problems\n10-100x faster than competing algorithms that optimize the same bound.\n\n<br><p>[<a href=\"bib.html#uai11b\"> BibTex </a>] | [<a href=\"uai11b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai11a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Variational algorithms for marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Marginal MAP problems are notoriously difficult tasks for graphical models. We\nderive a general variational framework for solving marginal MAP problems,\nin which we apply analogues of the Bethe, tree-reweighted, and mean field approximations.\nWe then derive a ``mixed\" message passing algorithm and a convergent\nalternative using CCCP to solve the BP-type approximations.  Theoretically, we\ngive conditions under which the decoded solution is a global or local optimum,\nand obtain novel upper bounds on solutions.\nExperimentally we demonstrate that our algorithms outperform related approaches.\nWe also show that EM and variational EM comprise\na special case of our framework.\n\n<br><p>[<a href=\"bib.html#uai11a\"> BibTex </a>] | [<a href=\"uai11a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=jmlr11></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Adaptive exact inference in graphical models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sumer, Acar, Ihler, Mettu</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Many algorithms and applications involve repeatedly solving\nvariations of the same inference problem, for example to introduce\nnew evidence to the model or to change conditional dependencies. As\nthe model is updated, the goal of \\emph{adaptive inference} is to\ntake advantage of previously computed quantities to perform\ninference more rapidly than from scratch.  In this paper, we present\nalgorithms for adaptive exact inference on general graphs that can\nbe used to efficiently compute marginals and update MAP\nconfigurations under arbitrary changes to the input factor graph and\nits associated elimination tree. After a linear time preprocessing\nstep, our approach enables updates to the model and the computation\nof any marginal in time that is logarithmic in the size of the input\nmodel. Moreover, in contrast to max-product our approach can also be\nused to update MAP configurations in time that is roughly\nproportional to the number of updated entries, rather than the size\nof the input model.  \nTo evaluate the practical effectiveness of our\nalgorithms, we implement and test them using synthetic data as well as\nfor two real-world computational biology applications. Our experiments show that\nadaptive inference can achieve substantial speedups over performing\ncomplete inference as the model undergoes small changes over time.\n\n<br><p>[<a href=\"bib.html#jmlr11\"> BibTex </a>] | [<a href=\"jmlr11.pdf\"> PDF </a>] | [<a href=\"jmlr11.ps\"> PS </a>] </p><br><hr noshade>\n<a name=discml11></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Mini-bucket elimination with moment matching</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Flerova, Ihler, Dechter, Otten</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We investigate a hybrid of two styles of algorithms for deriving bounds for \noptimization tasks over graphical models: non-iterative message-passing schemes\nexploiting variable duplication to reduce cluster sizes (e.g. MBE) and iterative\nmethods that re-parameterize the problem's functions aiming to produce good\nbounds even if functions are processed independently (e.g. MPLP). In this work\nwe combine both ideas, augmenting MBE with re-parameterization, which we call\nMBE with Moment Matching (MBE-MM). The results of preliminary empirical\nevaluations show the clear promise of the hybrid scheme over its individual com-\nponents (e.g., pure MBE and pure MPLP). Most significantly, we demonstrate the\npotential of the new bounds in improving the power of mechanically generated\nheuristics for branch and bound search.\n\n<br><p>[<a href=\"bib.html#discml11\"> BibTex </a>] | [<a href=\"discml11.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=icml12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Distributed parameter estimation via pseudo-likelihood</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Estimating statistical models within sensor networks requires distributed\nalgorithms, in which both data and computation are distributed across the nodes\nof the network. We propose a general approach for distributed learning based on\ncombining local estimators defined by pseudo-likelihood components,\nencompassing a number of combination methods, and provide both theoretical and\nexperimental analysis.  We show that simple linear combination or max-voting\nmethods, when combined with second-order information, are statistically\ncompetitive with more advanced and costly joint optimization. Our algorithms\nhave many attractive properties including low communication and computational\ncost and \"any-time\" behavior.\n\n<br><p>[<a href=\"bib.html#icml12\"> BibTex </a>] | [<a href=\"icml12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Approximating the sum operation for marginal-MAP inference</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Cheng, Chen, Dong, Xu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We study the marginal-MAP problem on graphical models, and present a novel\napproximation method based on direct approximation of the sum operation.  A\nprimary difficulty of marginal-MAP problems lies in the non-commutativity of\nthe sum and max operations, so that even in highly structured models,\nmarginalization may produce a densely connected graph over the variables to be\nmaximized, resulting in an intractable potential function with exponential\nsize.  We propose a chain decomposition approach for summing over the\nmarginalized variables, in which we produce a structured approximation to the\nMAP component of the problem consisting of only pairwise potentials.  We show\nthat this approach is equivalent to the maximization of a specific variational\nfree energy, and it provides an upper bound of the optimal probability.\nFinally, experimental results demonstrate that our method performs favorably\ncompared to previous methods.\n\n<br><p>[<a href=\"bib.html#aaai12\"> BibTex </a>] | [<a href=\"aaai12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=pnas12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Brain and muscle Arnt-like protein-1 (BMAL1) controls circadian cell proliferation and susceptibility to UVB-induced DNA damage in the epidermis</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Geyfman et al.</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The role of the circadian clock in skin and the identity of genes participating\nin its chronobiology remain largely unknown, leading us to define the circadian\ntranscriptome of mouse skin at two different stages of the hair cycle, telogen\nand anagen. The circadian transcriptomes of telogen and anagen skin are largely\ndistinct, with the former dominated by genes involved in cell proliferation and\nmetabolism. The expression of many metabolic genes is antiphasic to cell\ncycle-related genes, the former peaking during the day and the latter at night.\nConsistently, accumulation of reactive oxygen species, a byproduct of oxidative\nphosphorylation, and S-phase are antiphasic to each other in telogen skin.\nFurthermore, the circadian variation in S-phase is controlled by BMAL1\nintrinsic to keratinocytes, because keratinocyte-specific deletion of Bmal1\nobliterates time-of-day\u2013dependent synchronicity of cell division in the\nepidermis leading to a constitutively elevated cell proliferation. In agreement\nwith higher cellular susceptibility to UV-induced DNA damage during S-phase, we\nfound that mice are most sensitive to UVB-induced DNA damage in the epidermis\nat night. Because in the human epidermis maximum numbers of keratinocytes go\nthrough S-phase in the late afternoon, we speculate that in humans the\ncircadian clock imposes regulation of epidermal cell proliferation so that skin\nis at a particularly vulnerable stage during times of maximum UV exposure, thus\ncontributing to the high incidence of human skin cancers. \n<br><p>[<a href=\"bib.html#pnas12\"> BibTex </a>] | [<a href=\"http://www.pnas.org/content/109/29/11758\"> Link </a>] </p><br><hr noshade>\n<a name=ssp12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>A graphical model representation of the track-oriented multiple hypothesis tracker</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Frank, Smyth, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The track-oriented multiple hypothesis tracker is currently the preferred\nmethod for tracking multiple targets in clutter with medium to high\ncomputational resources. This method maintains a structured representation of\nthe track posterior distribution, which it repeatedly extends and optimizes\nover.  This representation of the posterior admits probabilistic inference\ntasks beyond MAP estimation that have yet to be explored.  To this end we\nformulate the posterior as a graphical model and show that belief propagation\ncan be used to approximate the track marginals.  These approximate marginals\nenable an online parameter estimation scheme that improves tracker performance\nin the presence of parameter misspecification.\n\n<br><p>[<a href=\"bib.html#ssp12\"> BibTex </a>] | [<a href=\"ssp12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai12c></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Join-graph based cost-shifting schemes</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ihler, Flerova, Dechter, Otten</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We develop several algorithms taking advantage of two common approaches for\nbounding MPE queries in graphical models: mini-bucket elimination and\nmessage-passing updates for linear programming relaxations.  Both methods are\nquite similar, and offer useful perspectives for the other; our hybrid\napproaches attempt to balance the advantages of each.  We demonstrate the power\nof our hybrid algorithms through extensive empirical evaluation.  Most notably,\na Branch and Bound search guided by the heuristic function calculated by one of\nour new algorithms has recently won first place in the PASCAL2 inference\nchallenge.\n\n<br><p>[<a href=\"bib.html#uai12c\"> BibTex </a>] | [<a href=\"uai12c.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai12b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>A cluster-cumulant expansion at the fixed points of belief propagation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Welling, Gelfand, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We introduce a new cluster-cumulant expansion (CCE) based on the fixed points\nof iterative belief propagation (IBP). This expansion is similar in spirit to\nthe loop-series (LS) recently introduced in Chertkov and Chernyak (2006). However,\nin contrast to the latter, the CCE enjoys the following important qualities: 1)\nit is defined for arbitrary state spaces 2) it is easily extended to fixed\npoints of generalized belief propagation (GBP), 3)  disconnected groups of\nvariables will not contribute to the CCE and 4) the accuracy of the expansion\nempirically improves upon that of the LS. The CCE is based on the same\nMobius transform as the Kikuchi approximation, but unlike GBP does not\nrequire storing the beliefs of the GBP-clusters nor does it suffer from\nconvergence issues during belief updating.\n\n<br><p>[<a href=\"bib.html#uai12b\"> BibTex </a>] | [<a href=\"uai12b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai12a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Belief propagation for structured decision making</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Variational inference algorithms such as belief propagation have had tremendous\nimpact on our ability to learn and use graphical models, and give many insights\nfor developing or understanding exact and approximate inference.  However,\nvariational approaches have not been widely adoped for <i>decision making</i>\nin graphical models, often formulated through influence diagrams and including\nboth centralized and decentralized (or multi-agent) decisions.  In this work,\nwe present a general variational framework for solving structured cooperative decision-making\nproblems, use it to propose several belief propagation-like algorithms, and\nanalyze them both theoretically and empirically.\n\n<br><p>[<a href=\"bib.html#uai12a\"> BibTex </a>] | [<a href=\"uai12a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=eccv12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Fast planar correlation clustering for image segmentation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Yarkony, Ihler, Fowlkes</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We describe a new optimization scheme for finding high-quality correlation\nclusterings in planar graphs that uses weighted perfect matching as a\nsubroutine. Our method provides lower-bounds on the energy of the optimal\ncorrelation clustering that are typically fast to compute and tight in\npractice. We demonstrate our algorithm on the problem of image segmentation\nwhere this approach outperforms existing global optimization techniques in\nminimizing the objective and is competitive with the state of the art in\nproducing high-quality segmentations. \n\n<br><p>[<a href=\"bib.html#eccv12\"> BibTex </a>] | [<a href=\"eccv12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Variational inference for crowdsourcing</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Peng, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Crowdsourcing has become a popular paradigm for labeling large datasets.\nHowever, it has given rise to the computational task of aggregating the\ncrowdsourced labels provided by a collection of unreliable annotators. We\napproach this problem by transforming it into a standard inference problem in\ngraphical models, and applying approximate variational methods, including\nbelief propagation (BP) and mean field (MF).  We show that our BP algorithm\ngeneralizes both majority voting and a recent algorithm by Karger et al.\n(2011), while our MF method is closely related to a commonly used EM algorithm.\nIn both case, we find that the performance of the algorithms critically depends\non the choice of a prior distribution on the workers' reliability; by choosing\nthe prior properly, both BP and MF (and EM) perform surprisingly well on both\nsimulated and real-world datasets, competitive with state-of-the-art algorithms\nbased on more complicated modeling assumptions.\n\n<br><p>[<a href=\"bib.html#nips12\"> BibTex </a>] | [<a href=\"nips12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=discml12></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Winning the PASCAL 2011 MAP Challenge with Enhanced AND/OR Branch-and-Bound</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Otten, Ihler, Kask, Dechter</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>This paper describes our entry for the MAP/MPE track of the PASCAL 2011\nProbabilistic Inference Challenge, which placed first in all three time limit \ncategories, 20 seconds, 20 minutes, and 1 hour. Our baseline is a branch-and-bound\nalgorithm that explores the AND/OR context-minimal search graph of a graphical\nmodel guided by a mini-bucket heuristic. Augmented with recent advances that\nconvert the algorithm into an anytime scheme, that improve the heuristic power\nvia cost-shifting schemes, and using enhanced variable ordering schemes, it \nconstitutes one of the most powerful MAP/MPE inference methods to date.\n\n<br><p>[<a href=\"bib.html#discml12\"> BibTex </a>] | [<a href=\"discml12.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=iccp13></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Image enhancement in projectors via optical pixel shift and overlay</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Sajadi, Qoc-Lai, Ihler, Gopi, Majumder</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Earlier work has explored enhancing the perceived resolution of a display by\nshifting multiple different low-resolution images by fractions of a pixel and\noverlaying them in a temporally multiplexed fashion. This increases the\nmanufacturing cost and also sacrifices the temporal resolution that can\ncompromise other capabilities like 3D active stereo. In this paper we propose a\nmethod to achieve the same goal in projectors by performing the pixel shift and\nsuperposition optically by introducing a simple and inexpensive optical\nensemble of a set of lenses on the projector light path. This does not\nsacrifice the temporal resolution and is extremely easy to implement in\npractice.\n\nHowever, instead of overlaying different images, we overlay an image with one\nor more sub-pixel shifted copies of itself.  Therefore, we seek a single n \u00d7 n\nimage which when shifted and overlaid with itself creates a perceptually closer\nto a higher resolution 2n \u00d7 2n target image. This changes the optimization\nformulation significantly and requires solving a system of sparse linear\nequations. We take advantage of this sparsity and design a parallel\nimplementation of this optimization in GPUs for real-time computation of the\ninput image critical for its practical implementation. But, since this system\nis more constrained that using multiple overlaid images, the enhancement of\nresolution is compromised. However, since the optical design is very simple\nand inexpensive, it can be deployed on a variety of low-cost projectors and\nstill offer a significant image quality benefit.\n\n<br><p>[<a href=\"bib.html#iccp13\"> BibTex </a>] | [<a href=\"iccp13.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=jmlr13></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Variational Algorithms for Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The marginal maximum a posteriori probability (MAP) estimation problem, which\ncalculates the mode of the marginal posterior distribution of a subset of\nvariables with the remaining variables marginalized, is an important inference\nproblem in many models, such as those with hidden variables or uncertain\nparameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has\nattracted less attention in the literature compared to the joint MAP\n(maximization) and marginalization problems. We derive a general dual\nrepresentation for marginal MAP that naturally integrates the marginalization\nand maximization operations into a joint variational optimization problem,\nmaking it possible to easily extend most or all variational-based algorithms to\nmarginal MAP. In particular, we derive a set of \u201cmixed-product\" message passing\nalgorithms for marginal MAP, whose form is a hybrid of max-product, sum-product\nand a novel \u201cargmax-product\" message updates. We also derive a class of\nconvergent algorithms based on proximal point methods, including one that\ntransforms the marginal MAP problem into a sequence of standard marginalization\nproblems. Theoretically, we provide guarantees under which our algorithms give\nglobally or locally optimal solutions, and provide novel upper bounds on the\noptimal objectives. Empirically, we demonstrate that our algorithms\nsignificantly outperform the existing approaches, including a state-of-the-art\nalgorithm based on local search methods.\n\n<br><p>[<a href=\"bib.html#jmlr13\"> BibTex </a>] | [<a href=\"jmlr13.pdf\"> PDF </a>] | [<a href=\"http://jmlr.org/papers/volume14/liu13b/liu13b.pdf\"> Link </a>] </p><br><hr noshade>\n<a name=acml13></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Linear Approximation to ADMM for MAP inference</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Forouzan, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Maximum a posteriori (MAP) inference is one of the fundamental inference tasks\nin graphical models. MAP inference is in general NP-hard, making approximate\nmethods of interest for many problems. One successful class of approximate\ninference algorithms is based on linear programming (LP) relaxations. The\naugmented Lagrangian method can be used to overcome a lack of strict convexity\nin LP relaxations, and the Alternating Direction Method of Multipliers (ADMM)\nprovides an elegant algorithm for finding the saddle point of the augmented\nLagrangian. Here we present an ADMM-based algorithm to solve the primal form of\nthe MAP-LP whose closed form updates are based on a linear approximation\ntechnique. Our technique gives efficient, closed form updates that converge to\nthe global optimum of the LP relaxation. We compare our algorithm to two\nexisting ADMM-based MAP-LP methods, showing that our technique is faster on\ngeneral, non-binary or non-pairwise models.\n<br><p>[<a href=\"bib.html#acml13\"> BibTex </a>] | [<a href=\"acml13.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips13b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Variational planning for graph-based MDPs</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Cheng, Liu, Chen, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Markov Decision Processes (MDPs) are extremely useful for modeling and solving\nsequential decision making problems.  Graph-based MDPs provide a compact\nrepresentation for MDPs with large numbers of random variables.  However, the\ncomplexity of exactly solving a graph-based MDP usually grows exponentially in\nthe number of variables, which limits their application.  We present a new\nvariational framework to describe and solve the planning problem of MDPs, and\nderive both exact and approximate planning algorithms.  In particular, by\nexploiting the graph structure of graph-based MDPs, we propose a factored\nvariational value iteration algorithm in which the value function is first\napproximated by the multiplication of local-scope value functions, then solved\nby minimizing a Kullback-Leibler (KL) divergence.  The KL divergence is\noptimized using the belief propagation algorithm, with complexity exponential\nin only the cluster size of the graph.  Experimental comparison on different\nmodels shows that our algorithm outperforms existing approximation algorithms\nat finding good policies.\n\n<br><p>[<a href=\"bib.html#nips13b\"> BibTex </a>] | [<a href=\"nips13b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips13a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Scoring workers in crowdsourcing: How many control questions are enough?</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Steyvers, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We study the problem of estimating continuous quantities, such as prices,\nprobabilities, and point spreads, using a crowdsourcing approach.  A\nchallenging aspect of combining the crowd's answers is that workers'\nreliabilities and biases are usually unknown and highly diverse.  Control items\nwith known answers can be used to evaluate workers' performance, and hence improve\nthe combined results on the target items with unknown answers.  This raises the\nproblem of how many control items to use when the total number of items each workers can\nanswer is limited: more control items evaluates the workers better, but leaves\nfewer resources for the target items that are of direct interest, and vice versa.\nWe give theoretical results for this problem under different scenarios, and\nprovide a simple rule of thumb for crowdsourcing practitioners.  As a byproduct,\nwe also provide theoretical analysis of the accuracy of different\nconsensus methods.\n\n<br><p>[<a href=\"bib.html#nips13a\"> BibTex </a>] | [<a href=\"nips13a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=pos13></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Does better inference mean better learning?</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Gelfand, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Maximum Likelihood learning of graphical models is not possible in problems\nwhere inference is intractable. In such settings it is common to use\napproximate inference (e.g. Loopy BP) and maximize the so-called ``surrogate''\nlikelihood objective. We examine the effect of using different\napproximate inference methods and, therefore, different surrogate likelihoods,\non the accuracy of parameter estimation. In particular, we consider\nmethods that utilize a control parameter to trade\ncomputation for accuracy. We demonstrate empirically that cheaper, but worse\nquality approximate inference methods should be used in the small data setting\nas they exhibit smaller variance and are more robust to model mis-specification.\n\n<br><p>[<a href=\"bib.html#pos13\"> BibTex </a>] | [<a href=\"pos13.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=hbm14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Feed-forward hierarchical model of the ventral visual stream applied to functional brain image classification</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Keator, Fallon, Lakatos, Fowlkes, Potkin, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Functional brain imaging is a common tool in monitoring the progression of\nneurodegenerative and neurological disorders. Identifying functional brain\nimaging derived features that can accurately detect neurological disease is of\nprimary importance to the medical community. Research in computer vision\ntechniques to identify objects in photographs have reported high accuracies in\nthat domain, but their direct applicability to identifying disease in\nfunctional imaging is still under investigation in the medical community. In\nparticular, Serre et al. (2005) introduced a biophysically inspired filtering\nmethod emulating visual processing in striate cortex which they applied to\nperform object recognition in photographs. In this work, the model described by\nSerre et al. (2005) is extended to three-dimensional volumetric images to\nperform signal detection in functional brain imaging (PET, SPECT). The filter\noutputs are used to train both neural network and logistic regression\nclassifiers and tested on two distinct datasets: ADNI Alzheimer\u2019s disease\n2-deoxy-D-glucose (FDG) PET and National Football League players Tc99m HMPAO\nSPECT. The filtering pipeline is analyzed to identify which steps are most\nimportant for classification accuracy. Our results compare favorably with other\npublished classification results and outperform those of a blinded expert human\nrater, suggesting the utility of this approach.\n\n<br><p>[<a href=\"bib.html#hbm14\"> BibTex </a>] | [<a href=\"http://onlinelibrary.wiley.com/doi/10.1002/hbm.22149/full\"> Link </a>] </p><br><hr noshade>\n<a name=tsp14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Beyond MAP estimation with the track-oriented multiple-hypothesis tracker</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Frank, Smyth, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>The track-oriented multiple hypothesis tracker (TOMHT) is a popular algorithm\nfor tracking multiple targets in a cluttered environment. In tracking parlance\nit is known as a multi-scan, maximum a posteriori (MAP) estimator \u2013 multi-scan\nbecause it enumerates possible data associations jointly over several scans,\nand MAP because it seeks the most likely data association conditioned on the\nobservations. This paper extends the TOMHT, building on its internal\nrepresentation to support probabilistic queries other than MAP estimation.\nSpecifically, by summing over the TOMHT's pruned space of data association\nhypotheses one can compute marginal probabilities of individual tracks.  Since\nthis summation is generally intractable, any practical implementation must\nreplace it with an approximation.  We introduce a factor graph representation\nof the TOMHT\u2019s data association posterior and use variational message-passing\nto approximate track marginals. In an empirical evaluation, we show that\nmarginal estimates computed through message-passing compare favorably to those\ncomputed through explicit summation over the k-best hypotheses, especially as\nthe number of possible hypotheses increases. We also show that track marginals\nenable parameter estimation in the TOMHT via a natural extension of the\nexpectation maximization algorithm used in single-target tracking. In our\nexperiments, online EM updates using approximate marginals significantly\nincreased tracker robustness to poor initial parameter specification.\n\n<br><p>[<a href=\"bib.html#tsp14\"> BibTex </a>] | [<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=06766651\"> Link </a>] </p><br><hr noshade>\n<a name=icml14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Marginal structured SVM with hidden variables</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ping, Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In this work, we propose the marginal structured SVM (MSSVM) for structured\nprediction with hidden variables. MSSVM properly accounts for the uncertainty\nof hidden variables, and can significantly outperform the previously proposed\nlatent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art\nmethods,  especially when that uncertainty is large.  Our method also results\nin a smoother objective function, making gradient-based optimization of MSSVMs\nconverge significantly faster than for LSSVMs.  We also show that our method\nconsistently outperforms hidden conditional random fields (HCRFs; Quattoni et\nal. (2007)) on both simulated and real-world datasets.  Furthermore, we propose\na unified framework that includes both our and several other existing methods\nas special cases, and provides insights into the comparison of different models\nin practice.\n\n<br><p>[<a href=\"bib.html#icml14\"> BibTex </a>] | [<a href=\"icml14.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>AND/OR Search for Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Marinescu, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Marginal MAP problems are known to be very difficult tasks for graphical models\nand are so far solved exactly by systematic search guided by a join-tree upper\nbound. In this paper, we develop new AND/OR branch and bound algorithms for\nmarginal MAP that use heuristics extracted from weighted mini-buckets enhanced\nwith message-passing updates. We demonstrate the effectiveness of the resulting\nsearch algorithms against previous join-tree based approaches, which we also\nextend to apply to high induced width models, through extensive empirical\nevaluations. Our results show not only orders-of-magnitude improvements over\nthe state-of-the-art, but also the ability to solve problem instances well\nbeyond the reach of previous approaches.\n<br><p>[<a href=\"bib.html#uai14\"> BibTex </a>] | [<a href=\"uai14.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=socs14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Beyond Static Mini-Bucket: Towards Integrating with Iterative Cost-Shifting Based Dynamic Heuristics</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lam, Kask, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We explore the use of iterative cost-shifting as a dynamic heuristic\ngenerator for solving MPE in graphical models via Branch and Bound. When\nmini-bucket elimination is limited by its memory budget, it may not provide\ngood heuristics. This can happen often when the graphical model has a\nvery high induced width with large variable domain sizes. In addition, we\nexplore a hybrid setup where both MBE and the iterative cost-shifting\nbound are used in a combined heuristic. We compare these approaches with\nthe most advanced statically generated heuristics.\n\n<br><p>[<a href=\"bib.html#socs14\"> BibTex </a>] | [<a href=\"socs14.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips14></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Distributed Estimation, Information Loss and Exponential Families</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Distributed learning of probabilistic models from multiple data repositories\nwith minimum communication is increasingly important.  We study a simple\ncommunication-efficient learning framework that first calculates the local\nmaximum likelihood estimates (MLE) based on the data subsets, and then combines\nthe local MLEs to achieve the best possible approximation to the global MLE\ngiven the whole dataset.  We study this framework's statistical properties,\nshowing that the efficiency loss compared to the global setting relates to how\nmuch the underlying distribution families deviate from full exponential\nfamilies, drawing connection to the theory of information loss by Fisher, Rao\nand Efron.  We show that the ``full-exponential-family-ness\" represents the\nlower bound of the error rate of arbitrary combinations of local MLEs, and is\nachieved by a KL-divergence-based combination method but not by a more common\nlinear combination method.  We also study the empirical properties of both\nmethods, showing that the KL method significantly outperforms linear\ncombination in practical settings with issues such as model misspecification,\nnon-convexity, and heterogeneous data partitions.\n\n<br><p>[<a href=\"bib.html#nips14\"> BibTex </a>] | [<a href=\"nips14.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=ijcai15></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Pushing Forward Marginal MAP with Best-First Search</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Marinescu, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  Marginal MAP is known to be a difficult task for graphical models,\n  particularly because the evaluation of each MAP assignment involves\n  a conditional likelihood computation. In order to minimize the\n  number of likelihood evaluations, we focus in this paper on\n  best-first search strategies for exploring the space of partial MAP\n  assignments. We analyze the potential relative benefits of several\n  best-first search algorithms and demonstrate their effectiveness\n  against recent branch and bound schemes through extensive empirical\n  evaluations. Our results show that best-first search improves\n  significantly over existing depth-first approaches, in many cases by\n  several orders of magnitude, especially when guided by relatively\n  weak heuristics.\n\n<br><p>[<a href=\"bib.html#ijcai15\"> BibTex </a>] | [<a href=\"ijcai15.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=fusion15></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Boosting Crowdsourcing with Expert Labels: Local vs. Global Effects</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Ihler, Fisher</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Crowdsourcing provides a cheap but efficient approach for large-scale data and\ninformation collection. \nHowever, human judgments are inherently noisy, ambiguous and sometimes biased,\nand should be calibrated by additional (usually much more expensive) expert or\ntrue labels. % expert or In this work, we study the optimal allocation of the\ntrue labels to best calibrate the crowdsourced labels.  We frame the problem as\na submodular optimization, and propose a greedy allocation strategy that\nexhibits an interesting trade-off between a <it>local effect</it>, which\nencourages acquiring true labels for the most uncertain items, and a <it>global\neffect</it>, which favors the true labels of the most \"influential\" items,\nwhose information can propagate to help the prediction of other items.  We show\nthat exploiting and monitoring the global effect yields a significantly better\nselection strategy, and also provides potentially valuable information for\nother tasks such as designing stopping rules.\n\n\n<br><p>[<a href=\"bib.html#fusion15\"> BibTex </a>] | [<a href=\"fusion15.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai15b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Incremental Region Selection for Mini-bucket Elimination Bounds</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Forouzan, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Region choice is a key issue for many approximate inference bounds.\nMini-bucket elimination avoids the space and time complexity of exact inference\nby using a top-down partitioning approach that mimics the construction of a\njunction tree and aims to minimize the number of regions subject to a bound on\ntheir size; however, these methods rarely take into account functions' values.\nIn contrast, message passing algorithms often use \"cluster pursuit\" methods to\nselect regions, a bottom-up approach in which a pre-defined set of clusters\n(such as triplets) is scored and incrementally added.  In this work, we develop\na hybrid approach that balances the advantages of both perspectives, providing\nlarger regions chosen in an intelligent, energy-based way.  Our method is\napplicable to bounds on a variety of inference tasks, and we demonstrate its\npower empirically on a broad array of problem types.\n\n<br><p>[<a href=\"bib.html#uai15b\"> BibTex </a>] | [<a href=\"uai15b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai15a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Estimating the Partition Function by Discriminance Sampling</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Peng, Ihler, Fisher</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Importance sampling (IS) and its variant, annealed IS (AIS) have been widely\nused for estimating the partition function in graphical models, such as Markov\nrandom fields and deep generative models. However, IS tends to underestimate\nthe partition function and is subject to high variance when the proposal\ndistribution is more peaked than the target distribution.  On the other hand,\n\"reverse\" versions of IS and AIS tend to overestimate the partition function,\nand degenerate when the target distribution is more peaked than the proposal\ndistribution.  In this work, we present a simple, general method that gives\nmuch more reliable and robust estimates than either IS (AIS) or reverse IS\n(AIS).  Our method works by converting the estimation problem into a simple\nclassification problem that discriminates between the samples drawn from the\ntarget and the proposal.  We give extensive theoretical and empirical\njustification; in particular, we show that an annealed version of our method\nsignificantly outperforms both AIS and reverse AIS as proposed by Burda et al.\n(2015), which has been the state-of-the-art for likelihood evaluation in deep\ngenerative models. \n\n<br><p>[<a href=\"bib.html#uai15a\"> BibTex </a>] | [<a href=\"uai15a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips15b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Decomposition Bounds for Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ping, Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Marginal MAP inference involves making MAP predictions in systems defined with latent variables\nor missing information. It is significantly more difficult than pure marginalization and MAP tasks,\nfor which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist.\nIn this work, we generalize dual decomposition to a generic <it>power sum</it> inference task,\nwhich includes marginal MAP, along with pure marginalization and MAP, as special cases.\nOur method is based on a block coordinate descent algorithm on a new convex decomposition bound,\nthat is guaranteed to converge monotonically, and can be parallelized efficiently.\nWe demonstrate our approach on marginal MAP queries defined\non real-world problems from the UAI approximate\ninference challenge, showing that our framework is faster and more reliable than previous methods.\n\n<br><p>[<a href=\"bib.html#nips15b\"> BibTex </a>] | [<a href=\"nips15b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips15a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Probabilistic Variational Bounds for Graphical Models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Liu, Fisher, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds\non the partition function, but are often loose and difficult to use in an \"any-time\"\nfashion, expending more computation for tighter bounds.  On the other hand, Monte Carlo estimators\nsuch as importance sampling\nhave excellent any-time behavior, but depend critically on the proposal distribution.\nWe propose a simple Monte Carlo based inference method that augments convex variational bounds\nby adding importance sampling (IS).\nWe argue that convex variational methods naturally provide good IS proposals that \"cover\"\nthe target probability,\nand reinterpret the variational optimization as designing a proposal\nto minimize an upper bound on the variance of our IS estimator.\nThis both provides an accurate estimator and\nenables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art\nvariational bounds, and provide certificates of accuracy given enough samples relative to the error in the\ninitial bound.\n\n<br><p>[<a href=\"bib.html#nips15a\"> BibTex </a>] | [<a href=\"nips15a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai16></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>From Exact to Anytime Solutions for Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lee, Marinescu, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>This paper explores the anytime performance of search-based\nalgorithms for solving the Marginal MAP task over graphical\nmodels. The current state-of-the-art for solving this challenging\ntask is based on best-first search exploring the AND/OR graph with\nthe guidance of heuristics based on mini-bucket and variational\ncost-shifting principles.  Yet, those schemes are uncompromising in\nthat they solve the problem exactly, or not at all, and often suffer\nfrom memory problems. In this work, we explore the well known\nprinciple of weighted search for converting best-first search\nsolvers into anytime schemes.  The weighted best-first search\nschemes report a solution early in the process by using inadmissible\nheuristics, and subsequently improve the solution.  While it was\ndemonstrated recently that weighted schemes can yield effective\nanytime behavior for pure MAP tasks, Marginal MAP is far more\nchallenging (e.g., a conditional sum must be evaluated for every\nsolution). Yet, in an extensive empirical analysis we show that\nweighted schemes are indeed highly effective anytime solvers for Marginal MAP\nyielding the most competitive schemes to date for this task.\n\n<br><p>[<a href=\"bib.html#aaai16\"> BibTex </a>] | [<a href=\"aaai16.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=jhm16></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>A Deep Neural Network Modeling Framework to Reduce Bias in Satellite Precipitation Products</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Tao, Gao, Hsu, Sorooshian, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Despite the advantage of global coverage at high spatiotemporal resolutions, satellite remotely sensed precipitation estimates still suffer from insufficient accuracy that needs to be improved for weather, climate, and hydrologic applications. This paper presents a framework of a deep neural network (DNN) that improves the accuracy of satellite precipitation products, focusing on reducing the bias and false alarms. The state-of-the-art deep learning techniques developed in the area of machine learning specialize in extracting structural information from a massive amount of image data, which fits nicely into the task of retrieving precipitation data from satellite cloud images. Stacked denoising autoencoder (SDAE), a widely used DNN, is applied to perform bias correction of satellite precipitation products. A case study is conducted on the Precipitation Estimation from Remotely Sensed Information Using Artificial Neural Networks Cloud Classification System (PERSIANN-CCS) with spatial resolution of 0.08\u00b0 \u00d7 0.08\u00b0 over the central United States, where SDAE is used to process satellite cloud imagery to extract information over a window of 15 \u00d7 15 pixels. In the study, the summer of 2012 (June\u2013August) and the winter of 2012/13 (December\u2013February) serve as the training periods, while the same seasons of the following year (summer of 2013 and winter of 2013/14) are used for validation purposes. To demonstrate the effectiveness of the methodology outside the study area, three more regions are selected for additional validation. Significant improvements are achieved in both rain/no-rain (R/NR) detection and precipitation rate quantification: the results make 33% and 43% corrections on false alarm pixels and 98% and 78% bias reductions in precipitation rates over the validation periods of the summer and winter seasons, respectively.\n<br><p>[<a href=\"bib.html#jhm16\"> BibTex </a>] | [<a href=\"http://journals.ametsoc.org/doi/abs/10.1175/JHM-D-15-0075.1\"> Link </a>] </p><br><hr noshade>\n<a name=city16></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Cell-to-Cell Activity Prediction for Smart Cities</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Cici, Alimpertis, Ihler, Markopoulou</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In this paper, we analyze data from a large mobile\nphone provider in Europe, pertaining to time series of aggregate\ncommunication volume $A_{i,j}(t) > 0$ between cells $i$ and $j$,\nfor all pairs of cells in a city over a month. We develop a\nmethodology for predicting the future (in particular whether\ntwo cells will talk to each other $A_{i,j}(t) > 0$) based on past\nactivity. Our data set is sparse, with 80% of the values being\nzero, which makes prediction challenging. We formulate the\nproblem as binary classification and, using decision trees and\nrandom forests, we are able to achieve 85% accuracy. By giving\nhigher weight to false positives, which cost more to network\noperators, than false negatives, we improved recall from 40% to\n94%. We briefly outline potential applications of this prediction\ncapability to improve network planning, green small cells, and\nunderstanding urban ecology, all of which can inform policies\nand urban planning.\n<br><p>[<a href=\"bib.html#city16\"> BibTex </a>] | [<a href=\"city16.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=cec16></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Deep Neural Networks for Precipitation Estimation from Remotely Sensed Information</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Tao, Gao, Ihler, Hsu, Sorooshian</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>This paper investigates the application of deep neural networks to\nprecipitation estimation from remotely sensed information. Specifically, a\nstacked denoising auto-encoder is used to automatically extract features from\nthe infrared cloud images and estimate the amount of precipitation, referred as\nPERSIANN-SDAE. Due to the challenging imbalance in precipitation data, a\nKullback-Leibler divergence is incorporated in the objective function to\npreserve the distribution of it. PERSIANN-SDAE is compared with a shallow\nneural network with hand designed features and an operational satellite-based\nprecipitation estimation product. The experimental results demonstrate the\neffectiveness of PERSIANN-SDAE in estimating precipitation accurately while\npreserving its distribution. It outperforms both the shallow neural network and\nthe operational product.\n<br><p>[<a href=\"bib.html#cec16\"> BibTex </a>] | [<a href=\"https://ieeexplore.ieee.org/document/7743945/\"> Link </a>] </p><br><hr noshade>\n<a name=nips16></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Learning Infinite RBMs with Frank-Wolfe</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ping, Liu, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>In this work, we propose an infinite restricted Boltzmann machine (RBM), whose maximum likelihood estimation (MLE) corresponds to a constrained convex optimization. We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity. As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization.  The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.\n<br><p>[<a href=\"bib.html#nips16\"> BibTex </a>] | [<a href=\"nips16.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai17b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Anytime Best+Depth-First Search for Bounding Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Marinescu, Lee, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We introduce new anytime search algorithms that combine\nbest-first  with  depth-first  search  into  hybrid  schemes  for\nMarginal MAP inference in graphical models. The main goal\nis to facilitate the generation of upper bounds (via the best-first part) \nalongside the lower bounds of solutions (via the\ndepth-first part) in an anytime fashion. We compare against\ntwo  of  the  best  current  state-of-the-art  schemes  and  show\nthat our best+depth search scheme produces higher quality\nsolutions faster while also producing a bound on their accuracy, \nwhich can be used to measure solution quality during\nsearch.  An extensive empirical evaluation demonstrates the\neffectiveness of our new methods which enjoy the strength\nof best-first (optimality of search) and of depth-first (memory\nrobustness), leading to solutions for difficult instances where\nprevious solvers were unable to find even a single solution.\n<br><p>[<a href=\"bib.html#aaai17b\"> BibTex </a>] | [<a href=\"aaai17b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai17a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Anytime Anyspace AND/OR Search for Bounding the Partition Function</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lou, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Bounding the partition function is a key inference task in\nmany graphical models. In this paper, we develop an anytime\nanyspace search algorithm taking advantage of AND/OR tree\nstructure and optimized variational heuristics to tighten deterministic \nbounds on the partition function. We study how our\npriority-driven best-first search scheme can improve on state-of-the-art \nvariational bounds in an anytime way within limited\nmemory resources, as well as the effect of the AND/OR framework \nto exploit conditional independence structure within the\nsearch process within the context of summation. We compare\nour resulting bounds to a number of existing methods, and\nshow that our approach offers a number of advantages on real-world \nproblem instances taken from recent UAI competitions.\n<br><p>[<a href=\"bib.html#aaai17a\"> BibTex </a>] | [<a href=\"aaai17a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aistats17></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Belief Propagation in Conditional RBMs for Structured Prediction</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Ping, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Restricted Boltzmann machines (RBMs) and conditional RBMs (CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence (CD) and its variants. Belief propagation (BP) algorithms are believed to be slow for structured prediction on conditional RBMs (e.g., Mnih et al. [2011]), and not as good as CD when applied in learning (e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs.\n<br><p>[<a href=\"bib.html#aistats17\"> BibTex </a>] | [<a href=\"aistats17.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=abi17></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Abstraction Sampling in Graphical Models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Dechter, Broka, Kask, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We present a new sampling scheme for approximating hard to compute queries over\ngraphical models, such as computing the partition function. The scheme builds\nupon exact algorithms that traverse a weighted directed state-space graph\nrepresenting a global function over a graphical model. With the aid of an\nabstraction function and randomization, the state space can be compacted to\nfacilitate tractable computation, yielding a Monte Carlo Estimate that is\nunbiased. \n<br><p>[<a href=\"bib.html#abi17\"> BibTex </a>] | [<a href=\"abi17.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=nips17></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Dynamic importance sampling for anytime bounds of the partition function</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lou, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Computing the partition function is a key inference task in many graphical\nmodels.  In this paper, we propose a dynamic importance sampling scheme that\nprovides  anytime finite-sample bounds for the partition function.  Our\nalgorithm balances the advantages of the three major inference strategies,\nheuristic search, variational bounds, and Monte Carlo methods, blending\nsampling with search to refine a variationally defined proposal.  Our algorithm\ncombines and generalizes recent work on anytime search (Lou et al., 2017) and probabilistic bounds (Liu et al., 2015)\nof the partition function.  By using an intelligently chosen weighted average\nover the samples, we construct an unbiased estimator of the partition function\nwith strong finite-sample confidence intervals that inherit both the rapid\nearly improvement rate of sampling and the long-term benefits of an improved\nproposal from search.  This gives significantly improved anytime behavior, and\nmore flexible trade-offs between memory, time, and solution quality.  We\ndemonstrate the effectiveness of our approach empirically on real-world problem\ninstances taken from recent UAI competitions.\n\n<br><p>[<a href=\"bib.html#nips17\"> BibTex </a>] | [<a href=\"nips17.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=wpi18></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Generalized Dual Decomposition for Bounding Maximum Expected Utility of Influence Diagrams with Perfect Recall</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lee, Ihler, Dechter</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We introduce a generalized dual decomposition bound for computing the maximum\nexpected utility of influence diagrams based on the dual decomposition method\ngeneralized to Lp space. The main goal is to devise an approximation scheme\nfree from translations required by existing variational approaches while\nexploiting the local structure of sum of utility functions as well as the\nconditional independence of probability functions. In this work, the\ngeneralized dual decomposition method is applied to the algebraic framework\ncalled valuation algebra for influence diagrams which handles probability and\nexpected utility as a pair. The proposed approach allows a sequential decision\nproblem to be decomposed as a collection of sub-decision problems of bounded\ncomplexity and the upper bound of maximum expected utility to be computed by\ncombining the local expected utilities. Thus, it has a flexible control of\nspace and time complexity for computing the bound. In addition, the upper\nbounds can be further minimized by reparameterizing the utility functions.\nSince the global objective function for the minimization is nonconvex, we\npresent a gradient based local search algorithm in which the outer loop\ncontrols the randomization of the initial configurations and the inner loop\ntightens the upper-bound based on block coordinate descent with gradients\nperturbed by a random noise. The experimental evaluation demonstrates\nhighlights of the proposed approach on finite horizon MDP/POMDP instances. \n<br><p>[<a href=\"bib.html#wpi18\"> BibTex </a>] | [<a href=\"wpi18.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai18b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Anytime anyspace AND/OR best-first search for bounding marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Lou, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Marginal MAP is a key task in Bayesian inference and decision-making.  It is\nknown to be very difficult in general, particularly because the evaluation of\neach MAP assignment requires solving an internal summation problem.  In this\npaper, we propose a best-first search algorithm that provides anytime upper\nbounds for marginal MAP in graphical models.  It folds the computation of\nexternal maximization and internal summation into an AND/OR tree search\nframework, and solves them simultaneously using a unified best-first search\nalgorithm.  The algorithm avoids some unnecessary computation of summation\nsub-problems associated with MAP assignments, and thus yields significant time\nsavings.  Furthermore, our algorithm is able to operate within limited memory.\nEmpirical evaluation on three challenging benchmarks demonstrates that our\nunified best-first search algorithm using pre-compiled variational heuristics\noften provides tighter anytime upper bounds compared to those state-of-the-art\nbaselines.\n<br><p>[<a href=\"bib.html#aaai18b\"> BibTex </a>] | [<a href=\"aaai18b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=aaai18a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Lifted Generalized Dual Decomposition</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Gallo, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Many real-world problems, such as Markov Logic Networks (MLNs) with evidence,\ncan be represented as a highly symmetric graphical model perturbed by\nadditional potentials.  In these models, variational inference approaches that\nexploit exact model symmetries are often forced to ground the entire problem,\nwhile methods that exploit approximate symmetries (such as by constructing an\nover-symmetric approximate model) offer no guarantees on solution quality.  In\nthis paper, we present a method based on a lifted variant of the generalized\ndual decomposition (GenDD) for marginal MAP inference which provides a\nprincipled way to exploit symmetric sub-structures in a graphical model.  We\ndevelop a coarse-to-fine inference procedure that provides any-time upper\nbounds on the objective.  The upper bound property of GenDD provides a\nprincipled way to guide the refinement process, providing good any-time\nperformance and eventually arriving at the ground optimal solution.\n<br><p>[<a href=\"bib.html#aaai18a\"> BibTex </a>] | [<a href=\"aaai18a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=jhm18></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>A Two-Stage Deep Neural Network Framework for Precipitation Estimation from Bispectral Satellite Information</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Tao, Hsu, Ihler, Gao, Sorooshian</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Compared to ground precipitation measurements, satellite-based precipitation\nestimation products have the advantage of global coverage and high\nspatiotemporal resolutions. However, the accuracy of satellite-based\nprecipitation products is still insufficient to serve many weather, climate,\nand hydrologic applications at high resolutions. In this paper, the authors\ndevelop a state-of-the-art deep learning framework for precipitation estimation\nusing bispectral satellite information, infrared (IR), and water vapor (WV)\nchannels. Specifically, a two-stage framework for precipitation estimation from\nbispectral information is designed, consisting of an initial rain/no-rain\n(R/NR) binary classification, followed by a second stage estimating the nonzero\nprecipitation amount. In the first stage, the model aims to eliminate the large\nfraction of NR pixels and to delineate precipitation regions precisely. In the\nsecond stage, the model aims to estimate the pointwise precipitation amount\naccurately while preserving its heavily skewed distribution. Stacked denoising\nautoencoders (SDAEs), a commonly used deep learning method, are applied in both\nstages. Performance is evaluated along a number of common performance measures,\nincluding both R/NR and real-valued precipitation accuracy, and compared with\nan operational product, Precipitation Estimation from Remotely Sensed\nInformation Using Artificial Neural Networks\u2013Cloud Classification System\n(PERSIANN-CCS). For R/NR binary classification, the proposed two-stage model\noutperforms PERSIANN-CCS by 32.56\\% in the critical success index (CSI). For\nreal-valued precipitation estimation, the two-stage model is 23.40\\% lower in\naverage bias, is 44.52\\% lower in average mean squared error, and has a 27.21\\%\nhigher correlation coefficient. Hence, the two-stage deep learning framework\nhas the potential to serve as a more accurate and more reliable satellite-based\nprecipitation estimation product. The authors also provide some future\ndirections for development of satellite-based precipitation estimation products\nin both incorporating auxiliary information and improving retrieval algorithms. \n<br><p>[<a href=\"bib.html#jhm18\"> BibTex </a>] | [<a href=\"https://doi.org/10.1175/JHM-D-17-0077.1\"> Link </a>] </p><br><hr noshade>\n<a name=ijcai18></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Stochastic Anytime Search for Bounding Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Marinescu, Dechter, Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>  The Marginal MAP inference task is known to be extremely hard\n  particularly because the evaluation of each complete MAP assignment\n  involves an exact likelihood computation (a combinatorial sum). For\n  this reason, most recent state-of-the-art solvers that focus on\n  computing anytime upper and lower bounds on the optimal value are\n  limited to solving instances with tractable conditioned summation\n  subproblems. In this paper, we develop new search-based bounding\n  schemes for Marginal MAP that produce anytime upper and lower bounds\n  without performing exact likelihood computations. The empirical\n  evaluation demonstrates the effectiveness of our new methods against\n  the current best-performing search-based bounds.\n\n<br><p>[<a href=\"bib.html#ijcai18\"> BibTex </a>] | [<a href=\"ijcai18.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=icml18></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>ContextNet: Deep Learning for Star Galaxy Classification</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Noble Kennamer, David Kirkby, Alexander Ihler, Francisco Javier Sanchez-Lopez</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We present a framework to compose artificial neural networks in cases where the\ndata cannot be treated as independent events. Our particular motivation is star\ngalaxy classification for ground based optical surveys. Due to a turbulent\natmosphere and imperfect instruments, a single image of an astronomical object\nis not enough to definitively classify it as a star or galaxy. Instead the\ncontext of the surrounding objects imaged at the same time need to be\nconsidered in order to make an optimal classification. The model we present is\ndivided into three distinct ANNs: one designed to capture local features about\neach object, the second to compare these features across all objects in an\nimage, and the third to make a final prediction for each object based on the\nlocal and compared features. By exploiting the ability to replicate the weights\nof an ANN, the model can handle an arbitrary and variable number of individual\nobjects embedded in a larger exposure. We train and test our model on\nsimulations of a large up and coming ground based survey, the Large Synoptic\nSurvey Telescope (LSST). We compare to the state of the art approach, showing\nimproved overall performance as well as better performance for a specific class\nof objects that is important for the LSST.\n\n<br><p>[<a href=\"bib.html#icml18\"> BibTex </a>] | [<a href=\"icml18.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai18a></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Abstraction Sampling in Graphical Models</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Filjor Broka, Rina Dechter, Alexander Ihler, Kalev Kask</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We present a new sampling scheme for approximating hard to compute\nqueries over graphical models, such as computing the partition function.  The\nscheme builds upon exact algorithms  that traverse a weighted directed\nstate-space graph representing a global function over a graphical model (e.g.,\nprobability distribution).  With the aid of an abstraction function and\nrandomization, the state space can be compacted (or trimmed) to facilitate\ntractable computation, yielding a Monte Carlo Estimate that  is unbiased.  We\npresent the general scheme  and analyze its properties analytically and\nempirically, investigating two specific ideas for picking abstractions -\ntargeting reduction of variance or search space size.\n\n<br><p>[<a href=\"bib.html#uai18a\"> BibTex </a>] | [<a href=\"uai18a.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai18b></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Join Graph Decomposition Bounds for Influence Diagrams</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Junkyu Lee, Alexander Ihler, Rina Dechter</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We introduce a new decomposition method for bounding the maximum expected\nutility of influence diagrams. While most current schemes use reductions to the\nMarginal Map task over a Bayesian Network, our approach is direct, aiming to\navoid the large explosion in the model size that  often results by such\nreductions.  In this paper, we extend to influence diagrams the principles of\ndecomposition methods that were applied earlier to probabilistic inference,\nutilizing an algebraic framework called valuation algebra which effectively\ncaptures both multiplicative and additive local structures present in influence\ndiagrams.  Empirical evaluation on four benchmarks  demonstrates the\neffectiveness of our approach compared to reduction-based approaches and\nillustrates significant improvements in the upper bounds on maximum expected\nutility. \n\n<br><p>[<a href=\"bib.html#uai18b\"> BibTex </a>] | [<a href=\"uai18b.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=uai18c></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Finite-sample Bounds for Marginal MAP</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Qi Lou, Rina Dechter, Alexander Ihler</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>Marginal MAP is a key task in Bayesian inference and decision-making, and known\nto be very challenging in general.  In this paper, we present an algorithm that\nblends heuristic search and importance sampling to provide anytime\nfinite-sample bounds for marginal MAP along with predicted MAP solutions.  We\nconvert bounding marginal MAP to a surrogate task of bounding a series of\nsummation problems of an augmented graphical model, and then adapt dynamic\nimportance sampling (Lou et al., 2017) a recent advance in bounding the\npartition function, to provide finite-sample bounds for the surrogate task.\nThose bounds are guaranteed to be tight given enough time, and the values of\nthe predicted MAP solutions will converge to the optimum.  Our algorithm runs\nin an anytime/anyspace manner, which gives flexible trade-offs between memory,\ntime, and solution quality.  We demonstrate the effectiveness of our approach\nempirically on multiple challenging benchmarks in comparison with some\nstate-of-the-art search algorithms.\n\n<br><p>[<a href=\"bib.html#uai18c\"> BibTex </a>] | [<a href=\"uai18c.pdf\"> PDF </a>] </p><br><hr noshade>\n<a name=eccv18></a><table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">\n<tr> <td colspan=\"2\"> <p class=\"title\"><b>Accelerating Dynamic Programs via Nested Benders Decomposition with Application to Multi-Person Pose Estimation</b></p> </td> </tr> <tr> <td width=\"65%\"><p class=\"author\">Wang, Ihler, Kording, Yarkony</p></td><td width=\"35%\"><div align=\"right\"><p class=\"author\"></p></div></td> </tr> </table>\n<p>We present a novel approach to solve dynamic programs (DP) with exponential\nstate space, which are the foundations to solving many computer vision problems.\nContrary to typical DP approach which has to enumerate all combinations of\nstates of parent node and child node(s) in order to compute the optimal\nmessage(s) from child node(s), we propose an algorithm that applies Nested\nBenders Decomposition (NBD) to iteratively lower bound the message(s) from child\nnode(s). We apply our NBD based DP along with a novel Maximum Weight Set Packing\n(MWSP) formulation of multi-person pose estimation, demonstrating that our\nalgorithm is provably optimal at terimiation, while also operates\nin linear time for practical problems, gaining up to 500x speed up over\ntraditional DP algorithm which is of polynomial complexity.\n\n<br><p>[<a href=\"bib.html#eccv18\"> BibTex </a>] | [<a href=\"eccv18.pdf\"> PDF </a>] </p><br><hr noshade>\n \r\n", "encoding": "utf-8"}