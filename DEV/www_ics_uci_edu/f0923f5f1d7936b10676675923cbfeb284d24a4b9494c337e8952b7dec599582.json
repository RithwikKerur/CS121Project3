{"url": "https://www.ics.uci.edu/~eppstein/junkyard/jordan-splay.html", "content": "<HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:callahan@condor.cs.jhu.edu\">callahan@condor.cs.jhu.edu</A> (Paul Callahan)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Jordan Sorting with Splay Trees conjecture\n<B>Date:</B>           12 Aug 1995 13:40:08 -0400\n<B>Organization:</B>   The Johns Hopkins University CS Department\n</PRE><HR><PRE>\nJordan Sorting is the problem of sorting a list of numbers by\nx-coordinate, assuming that the numbers represent intersections of a\nnon-self-intersecting (i.e. Jordan) curve with the x-axis, and that\nthey are given in the order that they appear along this curve.  It has\nbeen known for about a decade (as far as I my research has determined)\nthat Jordan Sorting can be performed in linear time.\n\nWhat I'm curious about is the following conjecture given in a\npaper that presents a linear time algorithm (reference given below):\n\n    Our second remark is that there may be a much simpler way\n    to sort Jordan sequences in linear time: we merely insert\n    the items in the sequence one-at-a-time into a splay tree \n    ... and then access them in sorted order.  ... On the basis\n    of Sleator and Tarjan's dynamic optimality conjecture, we \n    conjecture than this sorts Jordan sequences in O(n) time.\n\nThis is from \"Sorting Jordan Sequences in Linear Time Using\nLevel-Linked Search Trees\" (Hoffman, Mehlhorn, Rosensteihl, and\nTarjan, _Information and Control_ 68 (1986) pp.170--184).\n\nSeveral later papers claim to present simpler algorithms, but the\nmost recent one I've seen (1990) is still not nearly as simple\nas the approach using splay trees.  So, I'm curious if this\nconjecture has been treated either theoretically or experimentally.\n\nFrom a theoretical perspective, one would think that if the conjecture\nis true, it would at least be easier to prove than the dynamic\noptimality conjecture, and would still be a pretty interesting result.\n\nIt strikes me as even more attractive as an experimental issue,\nthough, because there is existing code for splay trees, and random\nJordan sequences are not hard to generate (of course, this leaves open\nthe issue of generating \"hard\" cases of Jordan Sorting).  In a sense,\nthere's no excuse at all for not performing this experiment.  My\nliterature search was far from exhaustive, so if there are\nexperimental results, I'd very much appreciate a reference.\n\nIt seems to me that if the conjecture is actually true, then one could\nperhaps make a careful study of the splay tree algorithm for Jordan\nSorting and try to infer what it is doing to achieve such good\nresults.  This would provide a non-trivial linear time algorithm that\nin some sense \"exists in nature\" rather than being the consequence of\nclever design.  If true, that strikes me (and some other people, I\nhope) as quite remarkable despite the fact that it would do no better\nthan a known optimal algorithm.\n\n-- \n        --- Paul Callahan --- <A HREF=\"mailto:callahan@cs.jhu.edu\">callahan@cs.jhu.edu</A> ---\n\n\"The first principle in science is to invent something nice to look at \n and then decide what it can do.\" -- Rowland Emett\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:callahan@biffvm.cs.jhu.edu\">callahan@biffvm.cs.jhu.edu</A> (Paul Callahan)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        A family of Jordan Sequences for which Splay Sort is Omega(n log n)\n<B>Date:</B>           25 Sep 1995 14:13:09 -0400\n<B>Organization:</B>   The Johns Hopkins University CS Department\n</PRE><HR><PRE>\nAbout a month and a half ago, I posted an inquiry about a conjecture\nrelated to sorting Jordan sequences.  A Jordan sequence is a list of\nintersections of a non-self-intersecting plane curve with the x-axis.\nThe Jordan sequence sorting problem is to sort these intersections by\nx coordinate.  The non-crossing-curve restriction implies that the\nnumber of Jordan sequences of n values is O(c^n) for some c, rather\nthan n!, implying that a general-purpose comparison-based sorting\nalgorithm may not be optimal. In fact has been known for over a decade\nthat Jordan sequences can be sorted in O(n) time using only\ncomparisons (I emphasize the point about comparisons to avoid getting\nmired in the MSD-radix-sort flame war now in progress).\n\nAnyway, there is an appealing conjecture in two of the papers\npresenting linear time algorithms (Information and Control, 68 170-184\n(1986) and Information Processing Letters 35 85-92 (1990)) that if the\nvalues in a Jordan Sequence are simply inserted into a splay tree, and\nread off in sorted order, then this might also require O(n) time in\nthe worst case.  This general technique is usually called Splay Sort,\nand can for certain kinds of input be shown to require much less time\nthan a worst-case optimal comparision-based sort such as Mergesort.\nIf I understand the Jordan Sequence conjecture correctly, I can show that it is\nincorrect.  \n\nI don't know if the following result is new, but it's fairly\nspecialized and doesn't seem to lead anywhere, so I'm posting it here\nafter wondering for about a month what to do with it.  Because this posting\nnot a formal publication, I'm going to leave out some steps.  It\nshould not be too hard to fill in the details.  \n\nFirst, to prove a lower bound on sorting a sequence using Splay Sort,\nit will suffice to prove a lower bound for any algorithm that uses\ninsertions into a binary search tree using any rotation strategy.  In\nfact, the more general kind of lower bound is often easier to prove\nthan a lower bound for Splay Sort itself, owing to the work of Robert\nWilber in FOCS '86 (revised version in Siam J Comp vol. 18 no. 1),\nwhich provides two powerful methods for showing lower bounds on\naccessing sequences of values in Binary Search Trees.  We will not\nreally need these methods here, but only the corollary that accessing\nthe nodes of a binary tree containing the values 0,1,...,2^k-1 in\nbit-reversed order (i.e. taking the binary representations of these\nvalues in increasing order but reversing the \"significance\" of the\nbits) requires Omega(k 2^k) rotations\n\nIn the following, I'm going to use the claim that sorting a sequence by\ninserting it into a binary tree is as hard as accessing the same\nsequence in a tree containing these values.  I haven't proved this,\nbut it seems to me that each time a value is inserted (a new leaf\nadded at what was a \"null pointer\"), one could imagine that that value\nalready existed as a node in the tree but had not previously been\naccessed.  It should thus be possible to construct an initial tree\ncontaining the values to be sorted for which the rotation sequence is\nthe same whether one is inserting into an intially empty tree, or\naccessing from an initally full tree.  (Originally, I had a more\ncomplicated idea for a proof that would not require this claim\nbut I think it is true, and it results in a cleaner argument).\n\nNow, what remains is simply to construct a family of \"hard\" Jordan\nSequences such that for any value of n there is a sequence in this\nfamily containing at least n elements.  As it happens, the sequences\nin this family will bear a relationship to bit-reversed order that\nwill cause them to require Omega(n log n) rotations to sort using any\nbinary search tree, according to the result of Wilber.\n\nFirst I'll give an intuitive explanation of where these sequences come\nfrom.  If one were to take a looped, flattened out strip of paper and\nview it facing the edge, this would look like a very simple Jordan\nCurve.  I.e.:\n\n             +---------------------------------------+\n             |                                       |\n             +---------------------------------------+\n\n(unfortunately, ASCII graphics make it unclear, so let me repeat that\nthis is viewed on edge, not from above, as it might also appear).\n\nIf this is folded over on itself to double its thickness, we\nget another Jordan Curve, but which is somewhat more convoluted.\n\n       I.e:\n\n            +-------------------+\n            |                   |\n            +---------------+   |\n                            |   |\n            +---------------+   |\n            |                   |\n            +-------------------+\n\nWe can repeat this \"doubling over\" process as many times as we like,\neach time doubling the number of intersections of the curve with\na vertical line.  For larger examples, it becomes more convenient to\nrepresent the shape of the curve using two sets of matching\nparentheses (this is the representation normally used to prove a bound on the\nnumber of Jordan Sequences).  E.g., if we double over two more times,\nand rotate 90 degrees so the curve intersects the x-axis, we obtain:\n\n            (((((((())))))))\n            ()()(())(((())))\n\nMatching parentheses in the top sequence with arcs above and those in\nthe bottom sequence with arcs below gives us the actual curve.\n\nTo connect this idea with bit-reversed order, we will describe the\nfolding process as a transformation on a sequence of numbers that will\ndenote the actual Jordan Sequence in question.  The correspondence to\nfolding is not too hard to see, but will be omitted here, since it\nis not really needed once we obtain the final form of the construction.\n\nFirst, in the original loop, the sequence of intersections is the same\nboth on the curve and on the intersecting line, so we denote this\nsequence 0 1.\n\nNow, given such a sequence, we obtain the next, doubled over, sequence\nas follows:\n\n    (1) double each number in the sequence\n    (2) append the new sequence to its own reverse\n    (3) Add 1 to every odd-positioned number in the sequence\n        (the leftmost position is considered even).\n\nE.g.\n\n     0 1   --&gt;    0 2        by (1)\n           --&gt;    0 2 2 0    by (2)\n           --&gt;    0 3 2 1    by (3)\n\n     0 3 2 1  --&gt;  0 6 4 2   \n              --&gt;  0 6 4 2 2 4 6 0\n              --&gt;  0 7 4 3 2 5 6 1\n\n     0 7 4 3 2 5 6 1  --&gt;  0 14 8 6 4 10 12 2\n                      --&gt;  0 14 8 6 4 10 12 2 2 12 10 4 6 8 14 0\n                      --&gt;  0 15 8 7 4 11 12 3 2 13 10 5 6 9 14 1\n\n     etc.\n\nHere I'll resort to \"proof by example\" and simply point out that\nif you take the increasing sequence:\n\n       0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n\nand connect pairs of numbers adjacent in the sequence obtained in the\nprocess above with non-crossing arcs (also connecting the end values\n0 and 1) you will obtain the Jordan Curve determined by the set of \nmatching parentheses given earlier.\n\nRecall that for the lower bound, we want to connect this in some way\nto bit reversed order.  So, yet another way to obtain the above\nsequence is to define the following function f on k-bit binary numbers\ndenoted B = b    ... b\n             k-1      0\n\nIn the following, assume \"+\" denotes exclusive-or, and let\n\n            f(b   ... b )  = (b + b ) (b + b) ... (b   +b ) b0\n               k-1     0       1   0    2   0       k-1  0\n\nIn other words, reverse the order of bits 1 through k-1, keeping b\n                                                                  0\n\nin the same position.  Also, if b  = 1, then invert bits 1 through k-1.\n                              0\n\nE.g.,\n\n       B     f(B)\n      ---    ---\n      000    000\n      001    111   \n      010    100\n      011    011\n      100    010\n      101    101\n      110    110\n      111    001\n\nNow, to get the kth sequence determined by the process given\npreviously, we merely take the values i = 0,...,2^k-1 expressed in\nbinary, and apply the function f to each i in increasing order.  (We\noverload the meaning of f(i) to denote the result of converting i to a\nbinary string, applying f, and converting it back to an integer.)  It\ncan be seen that from the above table, the sequence for k=3 is \n0 7 4 3 2 5 6 1, which corresponds to that obtained previously.\n\nI imagine a lot of readers on this newsgroup don't find examples very\nconvincing, no matter how large they are and how nicely they work out.\nFortunately, once the function f has been defined as above, it\nsuffices to show only that the sequence f(0), f(1), ..., f(2^k-1) is a\nJordan Sequence, which is an easier task than showing it corresponds\nto the particular Jordan sequences illustrated earlier.\n\nTo do this, it is necessary to show that if the numbers 0,1,...,2^k-1\nare written in order left to right, then adjacent numbers in the sequence\nf(0),f(1), ..., f(2^k-1) can be connected by non-crossing arcs alternating\nabove and below the list of numbers.\n\nA key observation needed for such a proof is that two arcs, one from \na to b, and another from c to d cannot cross so long as a+b = c+d.  The\nother is that they cannot cross if a &lt; b &lt; c &lt; d.  The set of arcs\nfrom a to b can be put in k+1 equivalence classes according to a+b, so\nthat there is no crossing within each equivalence class.  The second\nobservation can then be used to exclude crossings between arcs in\ndistinct equivalence classes.\n\nSo, if I were writing a more rigorous article, it would now say something like\n\"Lemma: For all k<U>&gt;</U>0, the sequence f(0),f(1),...,f(2^k-1) is a\nJordan Sequence.\"  This would be followed by a proof, which is here\nleft as an exercise.\n\nWhat remains is to show that it requires Omega(k 2^k) comparisons to\nsort such a sequence using Splay Sort.  This can be seen by\nconsidering only those f(i) such that i is even.  Let R(i) denote the\nresult of reversing the order of the bits in the binary representation\nof i.  By definition of f, we know that for even values of i,\nf(i)=2*R(i/2), so the subsequence f(0), f(2), ..., f(2^k-2) must be in\nbit-reversed order.  Since the sequence f(0),f(1),...,f(2^k-1)\ncontains 2^(k-1) elements in bit-reversed order, the result of Wilber\nimplies that the time to access such a sequence in a binary search\ntree is Omega((k-1)2^(k-1)), and this also provides a bound on the time\nto sort the sequence using Splay Sort.\n\nThen, it follows that there exists a c such that for any value of n,\none can construct a Jordan Sequence of length greater than n that\nrequires c n log n comparisons to sort using Splay Sort, refuting the\nconjecture (as I understood it) that Splay Sort is a linear time\nalgorithm for sorting Jordan Sequences.\n\n-- \n        --- Paul Callahan --- <A HREF=\"mailto:callahan@cs.jhu.edu\">callahan@cs.jhu.edu</A> ---\n\n\"The first principle in science is to invent something nice to look at \n and then decide what it can do.\" -- Rowland Emett\n</PRE>\n<HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A> (Bob Wilber at PUMICE)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re: Jordan Sequences for which Splay Sort is Omega(n log n)\n<B>Date:</B>           27 Sep 1995 15:57:48 -0500\n<B>Organization:</B>   UTexas Mail-to-News Gateway\n</PRE><HR><PRE>\nPaul Callahan showed that a bit reversal permutation can be imbedded in a\nJordan sequence.\n\n&gt;In the following, I'm going to use the claim that sorting a sequence by\n&gt;inserting it into a binary tree is as hard as accessing the same\n&gt;sequence in a tree containing these values.  I haven't proved this,\n&gt;but it seems to me that each time a value is inserted (a new leaf\n&gt;added at what was a \"null pointer\"), one could imagine that that value\n&gt;already existed as a node in the tree but had not previously been\n&gt;accessed.  It should thus be possible to construct an initial tree\n&gt;containing the values to be sorted for which the rotation sequence is\n&gt;the same whether one is inserting into an intially empty tree, or\n&gt;accessing from an initally full tree.\n\nThis is the crux of the matter -- can the lower bound in [1] be used to\nget an an Omega(n log n) lower bound on Jordan sorting via insertion into\na symmetrically ordered binary tree?  That lower bound assumed that all\nelements are in the tree at the start, that they are being accessed in some\nspecified order (allowing rotations in the tree), and that no insertions or\ndeletions are done.\n\n&gt;(Originally, I had a more\n&gt;complicated idea for a proof that would not require this claim\n&gt;but I think it is true, and it results in a cleaner argument).\n\nI believe your claim is false.  But I think the lower bound can be patched\nup to work for this case.\n\nConsider how Jordan sorting with a binary tree proceeds:\n\nFirst, we traverse the simple closed curve, each time we encounter an \nintersection with the x axis, we insert that intersection point into a binary\ntree that is symmetrically ordered by the x coordinate.\n\nSecond, we access the items in the tree, in order by x coordinate.\n\nAt the end of the first pass the items are sorted by x coordinate, so the\nsecond pass accesses the nodes in sequential order.  This pass takes linear\ntime, including when the splay algorithm is used to do the accessing, as was\nproved by Tarjan [2].\n\nSo any lower bound must be applied to the first pass, when the items are\nbeing inserted.  I find it convenient to \"reverse the film\" -- items are\ninserted into a tree in sequential order by x coordinate, in linear time,\nand then they are deleted in the order they appear on the curve (that is,\ndeleted according to a Jordan sequence).  We want to show that the deletion\nphase takes n log n time.\n\nWe need to extend the model to allow for deletions.  I think the following\ndefinition covers all the cases:\n\nDefinition:  A *standard deletion* of a node v can occur when v has at most\none child.  In that case, if v has no children (it is a leaf) it is simply\nremoved.  Otherwise v is removed and v's only child is made a child of v's\nparent (if v was not the root) or is made the root (if v was the root).\n\nIn general, to delete a node v you do some sequence of rotations so that\nv has at most one child, and then you do a standard deletion of v.\n\nFor example, as I recall deletion of a node v in a splay tree proceeds as\nfollows:\n\n1.  v is splayed to the root and removed, leaving its left and right subtrees,\n    L and R.\n\n2.  The largest node in L, r, is splayed to the root of L.\n\n3.  R is made the right subtree of r.\n\nThis can be cast into the \"standard model\" as follows:\n\n1.  v is splayed to the root.  Its left subtree is L.\n\n2.  The largest node in L, r, is splayed to the root of L, making r the left\n    child of v.\n\n3.  r is rotated over v.\n\n4.  v has no left child; a standard deletion of v is done.\n\nClearly this has the same complexity as the usual deletion routine.\n\nIn the model when we delete v we must also count the cost of accessing v from\nthe root.  In order to fold all the costs into rotations, we require that the\nalgorithm be \"normalized\" so that before deleting any node it first rotates\nthat node to the root.  This can always be done without increasing the\ntime by more than a constant factor, because if the original algorithm does\na standard deletion of some node v at depth d, it could first rotatate v to\nthe root, then rotate v back to where it was (using 2d - 2 rotations), and\nthen delete v.  The cost of the extra rotations is just a constant times the\ncost of following the path from the root to v.  Likewise a look up of v is\nalways done by rotating v to the root.\n\nTwo lower bound methods were given in [1], I will only patch up the first one\n(both methods give an Omega(n log n) bound for accessing the bit reversal\npermutation).\n\nIt is important to understand that the intuition behind both lower bounds is\nthat items you access \"get in the way\" of items that you want to access later,\nand must be rotated over.  But if you can delete nodes, you can sometimes get a\nnode v out of the way by deleting it, which might be cheaper than forcing\nseveral nodes \"underneath\" to rotate over v.  So the ability to shrink the\ntree as you go might invalidate the lower bounds given in [1].\n\nHere's a very quick review of the first lower bound method.  We have some\nbinary search tree T, whose n nodes may be considered to be consecutive\nintegers.  We allow rotations and standard deletions on T.  A lower bound tree\nY with 2n - 1 nodes is constructed whose leaves are the initial nodes of T and \nwhose internal nodes lie between nodes in T, that is, they may be taken to be \nhalf integers.  T changes through rotations and deletions but Y is fixed.  For\ngetting the lower bound on the bit reversal permutation Y is taken to be a\nbalanced binary tree.  The sequence of nodes in T to be accessed is s[1], s[2],\n..., s[m].  (A node is \"accessed\" if we either look it up or delete it; in both\ncases such a node must be rotated to the root in a normalized algorithm.)\nGiven numbers x <U>&lt;</U> y, s{x, y} is the subsequence consisting of those s[i]s that\nare in the interval [x, y].  For each internal node u of Y, a score l(u) is\ncomputed as follows: Let x and y be the minimum and maximum leaves of the\nsubtree rooted at u, and let s' = s{x, y} and let m' be the length of s'.  Then\n\nl(u) = |{ i in [1, m'] | (s'(i) &lt; u and s'(i+1) &gt; u) or\n                         (s'(i) &gt; u and s'(i+1) &lt; u) }|.\n\nThat is, l(u) counts how many times the subsequence s' hops from the left\nsubtree of u to the right subtree, or vice versa.\n\nThe sum of the l(u)'s is a lower bound on the number of rotations that must\nbe done to access sequence s, if no deletions are done.\n\nA key part of the argument is that if s(i) &lt; u and s(i+1) &gt; u, then at some\npoint between the access of s(i) and the access of s(i+1) there had to be a\nrotation of some node c &gt; u over some node d &lt; u.  Furthermore, such a rotation\nhas no affect on the generalized subtrees T{x, u} and T{u, y} corresponding to\nthe leaves of u's left and right subtrees, so it is not already counted in\nthe scores of u's children.  (See [1] for the definition of a generalized\nsubtree.)\n\nBut suppose now that we allow standard deletions.  If, for example, s(i) is at\nthe root, has s(i+1) as its right child, and has no left child, then after\naccessing s(i) we can access s(i+1) simply by deleting s(i).  No rotations had\nto be done at all.  One might try to fix this by counting deletions of nodes in\n[x, y] in the score l(u), but this doesn't work because such such deletions\nwould be doubled counted in some of u's children.  In other words, either\nT{x, u} or T{u, y} is affected by any deletion of a node in [x, y].\n\nTo fix the lower bound note that if there is some j &gt; i such that s(j) <U>&lt;</U> s(i),\nthen even if s(i) is deleted it has to be the case that between the access of\ns(i) &lt; u and the access of s(i+1) &gt; u there's going to be a rotation of a node\nc &gt; u over a node d &lt; u.  (Use the first rotation after the access of s(i) that\ncauses s(j) to have an ancestor that is &gt; u; no standard deletion can cause\nthis to happen.)\n\nGiven sequence s of length m, say that i in [1, m] is an *end access* for s\nif either s(j) &gt; s(i) for all j in [i+1, m] or else s(j) &lt; s(i) for all\nj in [i+1, m].  That is, i is not an end access if for some j1, j2 &gt; i,\ns(j1) <U>&lt;</U> s(i) and s(j2) <U>&gt;</U> s(i).  Intuitively, end accesses are the nodes we\ncan get out of the way by deletion.\n\nModify the score for u as follows:\n\nl'(u) =  |{ i in [1, m] | ((s'(i) &lt; u and s'(i+1) &gt; u) or\n                           (s'(i) &gt; u and s'(i+1) &lt; u)) and\n                          i is not an end access in subsequence s'}|.\n\nNow the lower bound goes through, using l'(u) rather than l(u), because we\nonly count a rotation for u when s(i) &lt; u and s(i+1) &gt; u, and there is some\nj &gt; i with s(j) &lt; u.  (Or else s(i) &gt; u and s(i+1) &lt; u, and there is some\nj &gt; i with s(j) &gt; u.)\n\nFor the bit reversal permutation it is convenient to index the sequence from\n0, so that s(i) = bit_reversal(i).  For a bit reversal sequence on k bits,\nit is easy to show that there are only k+1 end accesses.  These are at those\n                        j\nelements s(i) equal to 2  - 1, for some j in [0, k].  So the score assigned\nto a node u at the j'th level of the lower bound tree, whose subsequence is a\nbit reversal permutation on j bits, is\n         j\nl'(u) = 2  - 1 - (j + 1).\n\n                         k-j\nThe j'th level of Y has 2    nodes, so summing all the scores we get\n      __\n      \\   k-j     j                    k\nL  =  /_ 2    * (2  - j - 2)  = (k-4)*2  + k + 4.\n  1 <U>&lt;</U> j <U>&lt;</U> k\n\n          k\nWith n = 2 ,  this gives an Omega(n log n) lower bound.\n\nSo any algorithm that sorts a Jordan sequence by inserting the elements into\na binary tree, maintained via rotations, requires n log n time.\n\nNote that at any single internal node u of the the lower bound tree the\nsubtracting out of the end accesses can cause a radical reduction in u's score.\nFor example, if the subsequence for u = 10.5 is\n    1 20 2 19 3 18 4 17 5 16 6 15 7 14 8 13 9 12 10 11\nthen l(u) = 19 but l'(u) = 0.\n\nIndeed, if the nodes are in the following \"zig-zag\" binary tree\n\n     1\n      \\\n       20\n      /\n     2\n      \\\n       19\n      /\n     3\n      \\\n       ...\n\nThey can be accessed in the stated order solely through standard deletions,\nwith no rotations.\n\nQuestion:  Are there sequences of n nodes where the lower bound for accessing\nwith deletions is d(n) and the lower bound for accessing without deletions is\nnot O(d(n) + n)?  Such a sequence will need many end accesses in many of its\nsubsequences.\n\n[1]  R. Wilber, \"Lower Bounds for Accessing Binary Search Trees With Rotations\"\n     SIAM J. Computing, 18(1) (1989) pp. 56-67.\n\n[2]  R. Tarjan, \"Sequential Access in Splay Trees Takes Linear Time\",\n     Combinatorica 5 (1985)  pp. 367-378.\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A> (Bob Wilber at PUMICE)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re[2]: Jordan Sequences for which Splay Sort is n log n\n<B>Date:</B>           1 Oct 1995 13:28:32 -0500\n<B>Organization:</B>   UTexas Mail-to-News Gateway\n</PRE><HR><PRE>\nThere is a statement I made in my last post that needs some correction and\nclarification.\n\nPaul Callahan said:\n&gt;In the following, I'm going to use the claim that sorting a sequence by\n&gt;inserting it into a binary tree is as hard as accessing the same\n&gt;sequence in a tree containing these values.  I haven't proved this,\n&gt;but it seems to me that each time a value is inserted (a new leaf\n&gt;added at what was a \"null pointer\"), one could imagine that that value\n&gt;already existed as a node in the tree but had not previously been\n&gt;accessed.  It should thus be possible to construct an initial tree\n&gt;containing the values to be sorted for which the rotation sequence is\n&gt;the same whether one is inserting into an intially empty tree, or\n&gt;accessing from an initally full tree.\n&gt;(Originally, I had a more\n&gt;complicated idea for a proof that would not require this claim\n&gt;but I think it is true, and it results in a cleaner argument).\n\nand I said\n&gt;I believe your claim is false...\n\nIt is clear that the model of insertion that Callahan had in mind is that nodes\nare always inserted as leaves in the appropriate part of the tree.  It that\ncase his claim is easily proven to be true.  Starting from an empty tree, carry\nout a sequence of rotations and insertions of new leaves until all nodes have\nbeen inserted, creating some tree T.  Now carry out the sequence backward,\nstarting from T, except that when you would delete a leaf (the backwards\nversion of inserting it), instead leave the node in place and mark it as\ndeleted.  Subsequent rotations in the backwards sequence do not involve any\nmarked leaves.  Furthermore, leaves marked as deleted stay at the fringe of the\ntree, that is, it is never the case that a marked leaf has an unmarked leaf as\na child.  So the backward sequence of rotations can be carried out.  In the \nfinal tree all nodes are marked.  This is the tree that satisfies the claim.\n\nWhat I had in mind was a more general model of insertion, namely, the reverse\nof \"standard deletions\", and it is with this more general type of insertion\nthat the claim appears to be false.  A standard insertion of a node v into a\ntree T is defined as follows:\n\n1.  Let x be the largest node &lt; v in T, and let y be the smallest node &gt; v in\n    T.  (For now assume that v is neither smaller than every node in T, nor\n    bigger than every node in T.)\n\n2.  Either x is an ancestor of y or y is an ancestor of x.  Assume the former.\n    Let u[0] = x, u[1], u[2], ..., u[k] = y be the path from x to y.  u[1] is\n    the right child of x and for i &gt; 1, u[i] is the left child of u[i-1].\n    Node v can be made a child of any one of the u[i]s.  If it is made a child\n    of x, it is a right child, otherwise it is a left child.  v has no left\n    child and is given u[i+1] as a right child (unless i = k, in which case\n    v is a leaf).\n\n3.  If v is less than every node in T, we can either make v the root (with T as\n    its right subtree) or can insert v as a left child of any of the nodes\n    along the leftmost path of T.  Similarly when v is greater than every node\n    in T.\n\n4.  The cost of a standard insertion is the cost of following the path from\n    the root to v after v is inserted.\n\nNote that the argument used above to prove Callahan's claim for insertions of\nleaves doesn't work for the more general type of insertion, because marked\nnodes might be left in the middle of the tree (with unmarked nodes as children)\nand this can make subsequent rotations in the reverse sequence invalid.\n\nIt is trivial to emulate a splay tree insertion by means of rotations and a\nstandard insertion, with no change in the cost of the operation.  I don't\nsee any way to emulate a splay tree insertion with rotations and an insertion\nof a leaf, while retaining the original cost.\n\nSo the \"patched up\" lower bound of my previous post seems to be necessary to be\nable to claim that a splay sort of a bit reversal permutation, or of a Jordan\nsequence, requires n log n time.\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:callahan@biffvm.cs.jhu.edu\">callahan@biffvm.cs.jhu.edu</A> (Paul Callahan)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re: Re[2]: Jordan Sequences for which Splay Sort is n log n\n<B>Date:</B>           2 Oct 1995 11:49:20 -0400\n<B>Organization:</B>   The Johns Hopkins University CS Department\n</PRE><HR><PRE>\nIn article &lt;06edc080@lightstone.com&gt;,\nBob Wilber at PUMICE &lt;<A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A>&gt; wrote:\n&gt;There is a statement I made in my last post that needs some correction and\n&gt;clarification.\n&gt;\n...\n&gt;It is clear that the model of insertion that Callahan had in mind is that nodes\n&gt;are always inserted as leaves in the appropriate part of the tree.  It that\n&gt;case his claim is easily proven to be true.  \n...\n&gt;What I had in mind was a more general model of insertion, namely, the reverse\n&gt;of \"standard deletions\", and it is with this more general type of insertion\n&gt;that the claim appears to be false.  \n\nOK, this is a point I hadn't considered, so I guess things are a bit\nmore complicated than I had hoped.  As I mentioned, I had a backup\nargument.  This relied on a different claim that seemed clear at the\ntime, namely that to insert an element into a binary tree, one must\naccess either its successor or predecessor in symmetric order.\n\nNow that you bring up the issue of general insertions, even the latter\nclaim seems less clear to me, because it is only true in the\ncase of leaf insertions that one must link the node directly to\neither its successor or predecessor.  However, it should still hold\nbecause one cannot certify that a node is being inserted into the\ncorrect place without having seen its successor and predecessor (at least\nassuming no extra information is maintained in the tree).\n\nAnyway, instead of bounding the rotations from the very beginning, we\ncan simply ignore the cost of inserting the first half of the\nbit-reversed sequence (even-valued elements).  Then, to bound the cost\nof inserting the remaining (odd-valued) elements, we would find the\ncost of accessing any sequence of successors or predecessors of these\nelements (which I think should be Omega(n log n)).  Since the above\nposter seems to have patched up my argument to his own satisfaction,\nthere is probably no need for this (especially if the correspondence\nbetween his name and the author of one of the cited papers is not\ncoincidental.)\n\nActually, this brings up another unstated assumption I would have to\nuse, which is that the cost of accessing *any* subsequence of n/2\nelements from a list of n elements in bit-reversed order is \nOmega(n log n).  I would be surprised if this were not true, but I don't know\nthe best way of proving it.  Clearly, there are some sequences that\nare \"hard\" to access but which contain an \"easy\" subsequence of length\nn/2, but intuitively it looks like any subsequence of a bit-reversed\npermutation should have the same property of breaking down the kind of\nlocality of reference needed to obtain o(n log n) rotations.  I'd\nappreciate any insight into this.  I suspect that the machinery\nneeded for the lower bound on bit-reversed sequences would be sufficient,\nbut maybe there is a simpler argument.\n\n-- \n        --- Paul Callahan --- <A HREF=\"mailto:callahan@cs.jhu.edu\">callahan@cs.jhu.edu</A> ---\n\n\"The first principle in science is to invent something nice to look at \n and then decide what it can do.\" -- Rowland Emett\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A> (Bob Wilber at PUMICE)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re: Jordan Sequences for which Splay Sort is n log n\n<B>Date:</B>           3 Oct 1995 14:34:15 -0500\n<B>Organization:</B>   UTexas Mail-to-News Gateway\n</PRE><HR><PRE>\nI wrote:\n&gt;What I had in mind was a more general model of insertion, namely, the reverse\n&gt;of \"standard deletions\", and it is with this more general type of insertion\n&gt;that the claim appears to be false.  \n\nPaul Callahan wrote:\n&gt;OK, this is a point I hadn't considered, so I guess things are a bit\n&gt;more complicated than I had hoped.  As I mentioned, I had a backup\n&gt;argument.  This relied on a different claim that seemed clear at the\n&gt;time, namely that to insert an element into a binary tree, one must\n&gt;access either its successor or predecessor in symmetric order.\n&gt;\n&gt;Now that you bring up the issue of general insertions, even the latter\n&gt;claim seems less clear to me, because it is only true in the\n&gt;case of leaf insertions that one must link the node directly to\n&gt;either its successor or predecessor.  However, it should still hold\n&gt;because one cannot certify that a node is being inserted into the\n&gt;correct place without having seen its successor and predecessor (at least\n&gt;assuming no extra information is maintained in the tree).\n\nWell, this brings up an issue that arises with any sort of lower bound --\nwhat's the right model?  When inserting node v in a tree that contains\nimmediate predecessor x and immediate successor y, it is arguable that one\nshould charge for the cost of finding both x and y, in order to certify that v\nis being inserted in a legal spot (let's call these two nodes bounds(v)).\nInstead, for standard insertions I charge the cost of following the path from\nthe root to the newly inserted node.  My reasons for choosing the cost model I\ndid are as follows:\n\n1.  Symmetry.  The cost of a standard insertion is the same as the cost of a\nstandard deletion of the same node in the time reversed sequence.  It is clear\nthat when deleting node v it is not necessary to charge for finding bounds(v).\n\n2.  Generality.  This is a more generous cost model than the one that charges\nfor finding bounds(v), so any lower bound obtained with this cost model will\napply to a broader class of algorithms.  In principle, an algorithm doesn't\nneed to look for bounds(v).  For example, store the minimum and maximum\nvalues in the entire tree, and in addition, store with each non-root node\nv the minimum value of the subtree it is a root of, if v is a right child of\nits parent, or the maximum value of the subtree it is a root of, if v is a\nleft child of its parent.  With these values we can verify the validity of an\ninsertion while only having to look as far as the child of the node being\ninserted.  These values can be maintained in constant time under rotations.\nFor example, if x is a left child of z and y is a right child of x, then when\nx is rotated over z, x gets z's old extremum value (either a minimum or a \nmaximum), z gets y's old minimum, and y gets x's old maximum.  When a standard\ndeletion or insertion is done, the cost of making the required updates of\nextrema can be charged against the cost of following the path from the root to\nthe inserted or deleted node.\n\nMost balanced search tree algorithms need *some* sort of extra information in\nthe tree.  For example, red-black trees need a bit per node that tells whether\nthey're red or black (splay trees are unusual in that they don't have any such\nextra information).  So, while storing one extra value in each node is more\noverhead than some other search algorithms have, it seems arbitrary to reject\nout of hand algorithms that do that.  And there might be a less costly way to\navoid looking at bounds(v).\n\n3.  Simplicity.  If I charge for finding bounds(v), I have to figure out what\nbounds(v) is.  For a highly regular sequence like the bit reversal permutation\nthis is easy, but in general it might be harder than counting end accesses.\n\n&gt;Anyway, instead of bounding the rotations from the very beginning, we\n&gt;can simply ignore the cost of inserting the first half of the\n&gt;bit-reversed sequence (even-valued elements).  Then, to bound the cost\n&gt;of inserting the remaining (odd-valued) elements, we would find the\n&gt;cost of accessing any sequence of successors or predecessors of these\n&gt;elements (which I think should be Omega(n log n)).\n\nWell, yes, if for inserting each node v you charge for accessing bounds(v)\nI believe this works.  For the second half of the bit reversal permutation,\nbr(n/2), br(n/2 + 1), ..., br(n-1), the nodes accessed are\nbounds(br(n/2)), bounds(br(n/2 + 1)), ..., bounds(br(n-1)), from which we can\nextract the subsequence br(0), br(1), ..., br(n/2 - 1).  And this takes\nOmega(n log n) time to access.  A complicating factor is that as you proceed,\nnodes are being inserted into the tree (you're not just doing accesses) but\nI don't think this affects the validity of the lower bound proofs.  This is\nsufficient to establish a bound on splay sorting, since a splay insertion of v\ndoes in fact access bounds(v).\n\n&gt;Since the above poster seems to have patched up my argument to his own\n&gt;satisfaction, there is probably no need for this (especially if the\n&gt;correspondence between his name and the author of one of the cited papers is\n&gt;not coincidental.)\n\nThe probability of two people with my name looking at this problem is low.\n\n&gt;Actually, this brings up another unstated assumption I would have to\n&gt;use, which is that the cost of accessing *any* subsequence of n/2\n&gt;elements from a list of n elements in bit-reversed order is \n&gt;Omega(n log n).  I would be surprised if this were not true, but I don't know\n&gt;the best way of proving it.\n\nI thought this was self evident.  Suppose I can access some sequence s[1],\ns[2], ..., s[n] in time f(n).  Then the same algorithm also accesses any\nsubsequence, s[i_1], s[i_2], ..., s[i_k] in time f(n).  (Just ignore the\naccesses you don't care about.)  So any lower bound on accessing the\nsubsequence is necessarily a lower bound on accessing the full sequence.\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:callahan@condor.cs.jhu.edu\">callahan@condor.cs.jhu.edu</A> (Paul Callahan)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re: Jordan Sequences for which Splay Sort is n log n\n<B>Date:</B>           3 Oct 1995 17:23:32 -0400\n<B>Organization:</B>   The Johns Hopkins University CS Department\n</PRE><HR><PRE>\nIn article &lt;0718e6e0@lightstone.com&gt;,\nBob Wilber at PUMICE &lt;<A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A>&gt; wrote:\n\n&gt;I thought this was self evident.  Suppose I can access some sequence s[1],\n&gt;s[2], ..., s[n] in time f(n).  Then the same algorithm also accesses any\n&gt;subsequence, s[i_1], s[i_2], ..., s[i_k] in time f(n).  (Just ignore the\n&gt;accesses you don't care about.)  So any lower bound on accessing the\n&gt;subsequence is necessarily a lower bound on accessing the full sequence.\n\nActually, I meant that the bound should also hold in the other\ndirection for bit-reversed sequences.  What I want to show is that the\nlower bound for the full bit-reversed sequence is (to within a\nconstant) a lower bound for any subsequence containing half the\nelements.\n\nThere are sequences for which this doesn't hold.  For example, interleave\na bit-reversed sequence with a sequence in increasing order.  Then\nthe full sequence requires Omega(n log n) rotations for access, but\nit contains a subsequence (the one in increasing order) that requires\nO(n) rotations.  But what I'm claiming is that the bit-reversed\nsequence does not contain any such \"easy\" sequence as a subsequence.\n\nIs this true?\n-- \n        --- Paul Callahan --- <A HREF=\"mailto:callahan@cs.jhu.edu\">callahan@cs.jhu.edu</A> ---\n\n\"The first principle in science is to invent something nice to look at \n and then decide what it can do.\" -- Rowland Emett\n</PRE><HR><PRE>\n<B>From:</B>           <A HREF=\"mailto:rew@lightstone.com\">rew@lightstone.com</A> (Bob Wilber at PUMICE)\n<B>Newsgroups:</B>     comp.theory\n<B>Subject:</B>        Re: Jordan Sequences for which Splay Sort is n log n\n<B>Date:</B>           4 Oct 1995 11:59:49 -0500\n<B>Organization:</B>   UTexas Mail-to-News Gateway\n</PRE><HR><PRE>\n&gt;Actually, I meant that the bound should also hold in the other\n&gt;direction for bit-reversed sequences.  What I want to show is that the\n&gt;lower bound for the full bit-reversed sequence is (to within a\n&gt;constant) a lower bound for any subsequence containing half the\n&gt;elements.\n&gt;\n&gt;There are sequences for which this doesn't hold.  For example, interleave\n&gt;a bit-reversed sequence with a sequence in increasing order.  Then\n&gt;the full sequence requires Omega(n log n) rotations for access, but\n&gt;it contains a subsequence (the one in increasing order) that requires\n&gt;O(n) rotations.  But what I'm claiming is that the bit-reversed\n&gt;sequence does not contain any such \"easy\" sequence as a subsequence.\n&gt;\n&gt;I this true?\n\nI suspect that it is, but I don't have a proof.\n\nHowever, you don't need such a strong claim for your proof to work.  Let's look\nat it again.  We have a proof that accessing a bit reversal permutation of\nlength n via a binary search tree algorithm requires c * n log n time,\nfor some c &gt; 0.\n\nWe insert into an initially empty tree a bit reversal permutation on k bits,\n          k\nwith n = 2  elements.  We want to show that inserting the last n/2 elements\nrequires accessing the n/2 nodes already in tree in bit reversal order.  Fix k\nat 4.  When br(n/2) = 0001 is inserted, we must access the lower of\nbounds(0001), namely, the lower of 0000 and 0010.  The higher of these\ntwo nodes is an ancestor of the lower, so we actually visit both and are free\nto charge for accessing either one.  So charge for acessing 0000.  When we\ninsert br(n/2 + 1) = 1001, we must access 1000 and 1010.  Charge for 1000.\nWhen we insert br(n/2 + 1) = 0101, we must access 0100 and 0110.  Charge for\n0100.  The sequence of accesses we charge for, 0000, 1000, 0100, 1100, ....,\n1110, is a full bit reversal permutation on k-1 bits (the k'th bit being fixed\nat 0), with no missing elements.  So this takes c*(n/2) log (n/2) =\nOmega(n log n) time.\n\nThe only lower bound being applied here is to a complete bit reversal\npermutation, not some arbitrary subsequence of a bit reversal permutation.\n</PRE>\n", "encoding": "ascii"}