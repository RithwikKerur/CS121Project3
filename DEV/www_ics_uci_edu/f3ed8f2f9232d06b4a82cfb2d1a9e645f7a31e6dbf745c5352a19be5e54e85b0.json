{"url": "https://www.ics.uci.edu/~eppstein/161/960227.html", "content": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Knuth-Morris-Pratt Algorithm</title>\n<meta name=\"Owner\" value=\"eppstein\">\n<meta name=\"Reply-To\" value=\"eppstein@ics.uci.edu\">\n</head>\n<body>\n<h1>ICS 161: Design and Analysis of Algorithms<br>\nLecture notes for February 27, 1996</h1>\n\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n<hr>\n<p></p>\n\n<h1>Knuth-Morris-Pratt string matching</h1>\n\nThe problem: given a (short) pattern and a (long) text, both\nstrings, determine whether the pattern appears somewhere in the\ntext. <a href=\"960222.html\">Last time</a> we saw how to do this\nwith finite automata. This time we'll go through the <a href= \n\"people.html#knuth\">Knuth</a>-<a href=\n\"people.html#morris\">Morris</a>-<a href=\n\"people.html#pratt\">Pratt</a> (KMP) algorithm, which can be thought\nof as an efficient way to build these automata. I also have some <a\nhref=\"http://www.ics.uci.edu/~eppstein/161/kmp/\">working C++ source\ncode</a> which might help you understand the algorithm better. \n\n<p>First let's look at a naive solution.<br>\nsuppose the text is in an array: char T[n]<br>\nand the pattern is in another array: char P[m].</p>\n\n<p>One simple method is just to try each possible position the\npattern could appear in the text.</p>\n\n<p><b>Naive string matching</b>:</p>\n\n<pre>\n    for (i=0; T[i] != '\\0'; i++)\n    {\n    for (j=0; T[i+j] != '\\0' &amp;&amp; P[j] != '\\0' &amp;&amp; T[i+j]==P[j]; j++) ;\n    if (P[j] == '\\0') found a match\n    }\n</pre>\n\nThere are two nested loops; the inner one takes O(m) iterations and\nthe outer one takes O(n) iterations so the total time is the\nproduct, O(mn). This is slow; we'd like to speed it up. \n\n<p>In practice this works pretty well -- not usually as bad as this\nO(mn) worst case analysis. This is because the inner loop usually\nfinds a mismatch quickly and move on to the next position without\ngoing through all m steps. But this method still can take O(mn) for\nsome inputs. In one bad example, all characters in T[] are \"a\"s,\nand P[] is all \"a\"'s except for one \"b\" at the end. Then it takes m\ncomparisons each time to discover that you don't have a match, so\nmn overall.</p>\n\n<p>Here's a more typical example. Each row represents an iteration\nof the outer loop, with each character in the row representing the\nresult of a comparison (X if the comparison was unequal). Suppose\nwe're looking for pattern \"nano\" in text \"banananobano\".</p>\n\n<pre>\n     0  1  2  3  4  5  6  7  8  9 10 11\n      T: b  a  n  a  n  a  n  o  b  a  n  o\n\n    i=0: X\n    i=1:    X\n    i=2:       n  a  n  X\n    i=3:          X\n    i=4:             n  a  n  o\n    i=5:                X\n    i=6:                   n  X\n    i=7:                         X\n    i=8:                            X\n    i=9:                               n  X\n    i=10:                                 X\n</pre>\n\nSome of these comparisons are wasted work! For instance, after\niteration i=2, we know from the comparisons we've done that\nT[3]=\"a\", so there is no point comparing it to \"n\" in iteration\ni=3. And we also know that T[4]=\"n\", so there is no point making\nthe same comparison in iteration i=4. \n\n<h2>Skipping outer iterations</h2>\n\nThe Knuth-Morris-Pratt idea is, in this sort of situation, after\nyou've invested a lot of work making comparisons in the inner loop\nof the code, you know a lot about what's in the text. Specifically,\nif you've found a partial match of j characters starting at\nposition i, you know what's in positions T[i]...T[i+j-1]. \n\n<p>You can use this knowledge to save work in two ways. First, you\ncan skip some iterations for which no match is possible. Try\noverlapping the partial match you've found with the new match you\nwant to find:</p>\n\n<pre>\n    i=2: n  a  n\n    i=3:    n  a  n  o\n</pre>\n\nHere the two placements of the pattern conflict with each other --\nwe know from the i=2 iteration that T[3] and T[4] are \"a\" and \"n\",\nso they can't be the \"n\" and \"a\" that the i=3 iteration is looking\nfor. We can keep skipping positions until we find one that doesn't\nconflict: \n\n<pre>\n    i=2: n  a  n\n    i=4:       n  a  n  o\n</pre>\n\nHere the two \"n\"'s coincide. Define the <i>overlap</i> of two\nstrings x and y to be the longest word that's a suffix of x and a\nprefix of y. Here the overlap of \"nan\" and \"nano\" is just \"n\". (We\ndon't allow the overlap to be all of x or y, so it's not \"nan\"). In\ngeneral the value of i we want to skip to is the one corresponding\nto the largest overlap with the current partial match: \n\n<p><b>String matching with skipped iterations</b>:</p>\n\n<pre>\n    i=0;\n    while (i&lt;n)\n    {\n    for (j=0; T[i+j] != '\\0' &amp;&amp; P[j] != '\\0' &amp;&amp; T[i+j]==P[j]; j++) ;\n    if (P[j] == '\\0') found a match;\n    i = i + max(1, j-overlap(P[0..j-1],P[0..m]));\n    }\n</pre>\n\n<h2>Skipping inner iterations</h2>\n\nThe other optimization that can be done is to skip some iterations\nin the inner loop. Let's look at the same example, in which we\nskipped from i=2 to i=4: \n\n<pre>\n    i=2: n  a  n\n    i=4:       n  a  n  o\n</pre>\n\nIn this example, the \"n\" that overlaps has already been tested by\nthe i=2 iteration. There's no need to test it again in the i=4\niteration. In general, if we have a nontrivial overlap with the\nlast partial match, we can avoid testing a number of characters\nequal to the length of the overlap. \n\n<p>This change produces (a version of) the KMP algorithm:</p>\n\n<p><b><a name=\"nest\">KMP, version 1</a></b>:</p>\n\n<pre>\n    i=0;\n    o=0;\n    while (i&lt;n)\n    {\n    for (j=o; T[i+j] != '\\0' &amp;&amp; P[j] != '\\0' &amp;&amp; T[i+j]==P[j]; j++) ;\n    if (P[j] == '\\0') found a match;\n    o = overlap(P[0..j-1],P[0..m]);\n    i = i + max(1, j-o);\n    }\n</pre>\n\nThe only remaining detail is how to compute the overlap function.\nThis is a function only of j, and not of the characters in T[], so\nwe can compute it once in a <i>preprocessing</i> stage before we\nget to this part of the algorithm. First let's see how fast this\nalgorithm is. \n\n<h2>KMP time analysis</h2>\n\nWe still have an outer loop and an inner loop, so it looks like the\ntime might still be O(mn). But we can count it a different way to\nsee that it's actually always less than that. The idea is that\nevery time through the inner loop, we do one comparison\nT[i+j]==P[j]. We can count the total time of the algorithm by\ncounting how many comparisons we perform. \n\n<p>We split the comparisons into two groups: those that return\ntrue, and those that return false. If a comparison returns true,\nwe've determined the value of T[i+j]. Then in future iterations, as\nlong as there is a nontrivial overlap involving T[i+j], we'll skip\npast that overlap and not make a comparison with that position\nagain. So each position of T[] is only involved in one true\ncomparison, and there can be n such comparisons total. On the other\nhand, there is at most one false comparison per iteration of the\nouter loop, so there can also only be n of those. As a result we\nsee that this part of the KMP algorithm makes at most 2n\ncomparisons and takes time O(n).</p>\n\n<h2>KMP and finite automata</h2>\n\nIf we look just at what happens to j during the algorithm above,\nit's sort of like a finite automaton. At each step j is set either\nto j+1 (in the inner loop, after a match) or to the overlap o\n(after a mismatch). At each step the value of o is just a function\nof j and doesn't depend on other information like the characters in\nT[]. So we can draw something like an automaton, with arrows\nconnecting values of j and labeled with matches and mismatches. \n\n<p><img src=\"kmp.gif\" width=\"580\" height=\"197\"></p>\n\n<p> The difference between this and the automata we are used to is\nthat it has only two arrows out of each circle, instead of one per\ncharacter. But we can still simulate it just like any other\nautomaton, by placing a marker on the start state (j=0) and moving\nit around the arrows. Whenever we get a matching character in T[]\nwe move on to the next character of the text. But whenever we get a\nmismatch we look at the same character in the next step, except for\nthe case of a mismatch in the state j=0.</p>\n\n<p>So in this example (the same as the one above) the automaton\ngoes through the sequence of states:</p>\n\n<pre>\n    j=0\n            mismatch T[0] != \"n\"\n    j=0\n            mismatch T[1] != \"n\"\n    j=0\n            match T[2] == \"n\"\n    j=1\n            match T[3] == \"a\"\n    j=2\n            match T[4] == \"n\"\n    j=3\n            mismatch T[5] != \"o\"\n    j=1\n            match T[5] == \"a\"\n    j=2\n            match T[6] == \"n\"\n    j=3\n            match T[7] == \"o\"\n    j=4\n            found match\n    j=0\n            mismatch T[8] != \"n\"\n    j=0\n            mismatch T[9] != \"n\"\n    j=0\n            match T[10] == \"n\"\n    j=1\n            mismatch T[11] != \"a\"\n    j=0\n            mismatch T[11] != \"n\"\n</pre>\n\nThis is essentially the same sequence of comparisons done by the <a\nhref=\"#nest\">KMP pseudocode</a> above. So this automaton provides\nan equivalent definition of the KMP algorithm. \n\n<p> As one student pointed out in lecture, the one transition in\nthis automaton that may not be clear is the one from j=4 to j=0. In\ngeneral, there should be a transition from j=m to some smaller\nvalue of j, which should happen on any character (there are no more\nmatches to test before making this transition). If we want to find\nall occurrences of the pattern, we should be able to find an\noccurrence even if it overlaps another one. So for instance if the\npattern were \"nana\", we should find both occurrences of it in the\ntext \"nanana\". So the transition from j=m should go to the next\nlongest position that can match, which is simply\nj=overlap(pattern,pattern). In this case overlap(\"nano\",\"nano\") is\nempty (all suffixes of \"nano\" use the letter \"o\", and no prefix\ndoes) so we go to j=0.</p>\n\n<h2>Alternate version of KMP</h2>\n\nThe automaton above can be translated back into pseudo-code,\nlooking a little different from the pseudo-code we saw before but\nperforming the same comparisons. \n\n<p><b>KMP, version 2</b>:</p>\n\n<pre>\n    j = 0;\n    for (i = 0; i &lt; n; i++)\n    for (;;) {      // loop until break\n        if (T[i] == P[j]) { // matches?\n        j++;        // yes, move on to next state\n        if (j == m) {   // maybe that was the last state\n            found a match;\n            j = overlap[j];\n        }\n        break;\n        } else if (j == 0) break;   // no match in state j=0, give up\n        else j = overlap[j];    // try shorter partial match\n    }\n</pre>\n\nThe code inside each iteration of the outer loop is essentially the\nsame as the function <tt>match</tt> from the <a href= \n\"http://www.ics.uci.edu/~eppstein/161/kmp/\">C++ implementation</a>\nI've made available. One advantage of this version of the code is\nthat it tests characters one by one, rather than performing random\naccess in the T[] array, so (as in the implementation) it can be\nmade to work for stream-based input rather than having to read the\nwhole text into memory first. \n\n<p>The overlap[j] array stores the values of\noverlap(pattern[0..j-1],pattern), which we still need to show how\nto compute.</p>\n\n<p>Since this algorithm performs the same comparisons as the other\nversion of KMP, it takes the same amount of time, O(n). One way of\nproving this bound directly is to note, first, that there is one\ntrue comparison (in which T[i]==P[j]) per iteration of the outer\nloop, since we break out of the inner loop when this happens. So\nthere are n of these total. Each of these comparisons results in\nincreasing j by one. Each iteration of the inner loop in which we\ndon't break out of the loop results in executing the statement\nj=overlap[j], which decreases j. Since j can only decrease as many\ntimes as it's increased, the total number of times this happens is\nalso O(n).</p>\n\n<h2>Computing the overlap function</h2>\n\nRecall that we defined the <i>overlap</i> of two strings x and y to\nbe the longest word that's a suffix of x and a prefix of y. The\nmissing component of the KMP algorithm is a computation of this\noverlap function: we need to know overlap(P[0..j-1],P) for each\nvalue of j&gt;0. Once we've computed these values we can store them\nin an array and look them up when we need them. \n\n<p>To compute these overlap functions, we need to know for strings\nx and y not just the longest word that's a suffix of x and a prefix\nof y, but all such words. The key fact to notice here is that if w\nis a suffix of x and a prefix of y, and it's not the longest such\nword, then it's also a suffix of overlap(x,y). (This follows simply\nfrom the fact that it's a suffix of x that is shorter than\noverlap(x,y) itself.) So we can list all words that are suffixes of\nx and prefixes of y by the following loop:</p>\n\n<pre>\n    while (x != empty) {\n    x = overlap(x,y);\n    output x;\n    }\n</pre>\n\nNow let's make another definition: say that shorten(x) is the\nprefix of x with one fewer character. The next simple observation\nto make is that shorten(overlap(x,y)) is still a prefix of y, but\nis also a suffix of shorten(x). \n\n<p>So we can find overlap(x,y) by adding one more character to some\nword that's a suffix of shorten(x) and a prefix of y. We can just\nfind all such words using the loop above, and return the first one\nfor which adding one more character produces a valid overlap:</p>\n\n<p><b>Overlap computation</b>:</p>\n\n<pre>\n    z = overlap(shorten(x),y)\n    while (last char of x != y[length(z)])\n    {\n    if (z = empty) return overlap(x,y) = empty\n    else z = overlap(z,y)\n    }\n    return overlap(x,y) = z\n</pre>\n\nSo this gives us a recursive algorithm for computing the overlap\nfunction in general. If we apply this algorithm for x=some prefix\nof the pattern, and y=the pattern itself, we see that all recursive\ncalls have similar arguments. So if we store each value as we\ncompute it, we can look it up instead of computing it again. (This\nsimple idea of storing results instead of recomputing them is known\nas <i>dynamic programming</i>; we discussed it somewhat in <a href= \n\"960109.html#dynprog\">the first lecture</a> and will see it in more\ndetail <a href=\"960229.html\">next time</a>.) \n\n<p>So replacing x by P[0..j-1] and y by P[0..m-1] in the pseudocode\nabove and replacing recursive calls by lookups of previously\ncomputed values gives us a routine for the problem we're trying to\nsolve, of computing these particular overlap values. The following\npseudocode is taken (with some names changed) from the\ninitialization code of the <a href= \n\"http://www.ics.uci.edu/~eppstein/161/kmp/\">C++ implementation</a>\nI've made available. The value in overlap[0] is just a flag to make\nthe rest of the loop simpler. The code inside the for loop is the\npart that computes each overlap value.</p>\n\n<p><b>KMP overlap computation</b>:</p>\n\n<pre>\n    overlap[0] = -1;\n    for (int i = 0; pattern[i] != '\\0'; i++) {\n    overlap[i + 1] = overlap[i] + 1;\n    while (overlap[i + 1] &gt; 0 &amp;&amp;\n           pattern[i] != pattern[overlap[i + 1] - 1])\n        overlap[i + 1] = overlap[overlap[i + 1] - 1] + 1;\n    }\n    return overlap;\n</pre>\n\nLet's finish by analyzing the time taken by this part of the KMP\nalgorithm. The outer loop executes m times. Each iteration of the\ninner loop decreases the value of the formula overlap[i+1], and\nthis formula's value only increases by one when we move from one\niteration of the outer loop to the next. Since the number of\ndecreases is at most the number of increases, the inner loop also\nhas at most m iterations, and the total time for the algorithm is\nO(m). \n\n<p>The entire KMP algorithm consists of this overlap computation\nfollowed by the main part of the algorithm in which we scan the\ntext (using the overlap values to speed up the scan). The first\npart takes O(m) and the second part takes O(n) time, so the total\ntime is O(m+n).</p>\n\n<hr>\n<p><a href=\"/~eppstein/161/\">ICS 161</a> -- <a href=\"/\">Dept.\nInformation &amp; Computer Science</a> -- <a href= \n\"http://www.uci.edu/\">UC Irvine</a><br>\n<small>Last update: \n<!--#flastmod file=\"960227.html\" --></small></p>\n</body>\n</html>\n\n", "encoding": "ascii"}