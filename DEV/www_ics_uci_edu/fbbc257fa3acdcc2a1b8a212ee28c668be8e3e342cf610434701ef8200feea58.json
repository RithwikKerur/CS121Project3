{"url": "https://www.ics.uci.edu/~rickl/courses/cs-171/0-ihler-2016-fq/Projects/ConnectK/notes.txt", "content": "\n\nSome misc notes from Fall 2016:\n\n1) Make sure the students know *precisely* which modules should be loaded on openlab before compiling & running.\nI used:\n  module load java/1.8.0_20 python/3.5.1 gcc/5.4.0\nOtherwise there will be many re-compilation or dynamic library inclusion problems.\n\nCompilation is awful, as some students do not use makefiles, while others use various different types of\nmake environments.  I agreed to support simple command-line compile (all c++ files in the directory) and\ndirectories with \"Makefile\" using basic \"make\".\n\n2) I wrote scripts to try to automate checking of various submission aspects.  Downloading from Canvas\nended up with files in a directory structure:\n  Submissions/AssignmentName/groupname_Num1_Num2_User_Num3_Fname-K/...\nwhere Num1, Num2, Num3 are canvas-provided user ID numbers, User is the Canvas userid, \ngroupname is the Canvas group name, Fname is the students zipfile name, and K is their \nupload number (nothing, -1, -2, etc.)\nI tried to just use the canvas groupname, although unfortunately it did not force these to be unique.\n\nMy scripts would take a short \"AssignmentName\" (e.g., M03 for milestone 3 in directory M03_Name)\nand process it, extracting the group names and results in the output.\n\n3) For testing, we tested various aspects:\n (a) Could we re-compile the code (C++ or Java)?\n (b) Could we interact with the code in a basic way (test IO) without a crash or exception\n (c) Could the code (consistently) beat a trivial, random player?\n (d) Could the code (consistently) beat a very simple heuristically guided player?\n \nThe TA's code for doing these things is in python, but poorly documented and monolithic, so much of \nthe same effort is replicated in bash command-line scripts (also not particularly well documented,\nbut broken into steps).  Often we would have to look at the groups whose work did not run and\ntry to figure out the source of the problem (missing or renamed files, old versions of the code, etc.)\n\n4) I ran two versions of the tournament with different rules & board sizes.  The original tournament\ncode had a lot of problematic bugs, but I think the current version is functional (but may only work\nunder linux).  \n\nOne issue is that the tournament ranking is quite fragile (not stable to changes in order, etc.).\nIt would be an interesting student project to replace it with a \"TrueSkill\" style ranking procedure,\nwhere random or selected matches are held to hone in on the true play quality of each AI.\n\n\n\nUnfixed bugs: \n* I think I did not fix a bug that \"Connect-1\" does not work \n\n\n\n", "encoding": "ascii"}